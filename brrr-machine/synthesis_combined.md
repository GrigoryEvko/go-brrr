# Preamble: Theoretical Foundations and Architectural Principles

## Introduction and Motivation

The brrr-machine represents a novel approach to multi-language program analysis, founded on the principle that programming languages constitute *parameterized instantiations* of a universal computational model rather than fundamentally distinct formal systems.

<div class="principle">

**Principle 1.1** (Language Parameterization). Programming languages differ in their configuration parameters rather than their essential semantics. Each language is characterized by specific choices across orthogonal semantic dimensions: memory management, type discipline, null handling, effect tracking, and concurrency model.

</div>

This theoretical perspective—wherein languages differ in their configuration parameters rather than their essential semantics—yields several significant capabilities that distinguish the brrr-machine from conventional static analyzers.

### Principal Contributions

<div class="contributionbox">

A unified analysis framework applicable across heterogeneous language ecosystems (<span class="sans-serif">Python</span>, <span class="sans-serif">Rust</span>, <span class="sans-serif">Go</span>, <span class="sans-serif">C</span>/<span class="sans-serif">C++</span>, <span class="sans-serif">JavaScript</span>, <span class="sans-serif">TypeScript</span>, <span class="sans-serif">Java</span>). Analyses are specified once at the IR level and instantiated for each source language through systematic lowering transformations.

</div>

<div class="contributionbox">

Precise identification of safety invariant violations at language boundaries (FFI, IPC, RPC, serialization interfaces). The framework formally characterizes which properties are preserved, weakened, or invalidated when control or data crosses linguistic boundaries.

</div>

<div class="contributionbox">

Analysis results compose across function, module, and language boundaries via the <span class="sans-serif">IFDS</span>/<span class="sans-serif">IDE</span> algorithmic framework and algebraic effect composition. Summaries computed for callees are reusable across call sites without re-analysis, enabling scalability to large codebases.

</div>

<div class="contributionbox">

All analyses derive soundness from established theoretical foundations: abstract interpretation (Cousot & Cousot), separation logic (Reynolds), algebraic effects (Plotkin & Power), and substructural type theory (Girard). Proofs are mechanized in F\* where feasible.

</div>

<div class="contributionbox">

Despite theoretical rigor, the implementation leverages asymptotically efficient algorithms: $`\mathcal{O}(ED^3)`$ interprocedural dataflow via <span class="sans-serif">IFDS</span>, $`\mathcal{O}(n \cdot \alpha(n))`$ pointer analysis via Steensgaard unification, demand-driven evaluation for interactive workloads, and incremental re-analysis via dirty-marking with DRedL lattice updates.

</div>

## The Core Thesis: Languages as Parameterized Type Theories

The central theoretical claim of this work is that programming languages constitute *parameterized instantiations* of a universal computational substrate rather than fundamentally distinct formal systems.

<div class="definition">

**Definition 2.1** (Universal Computational Substrate). The brrr-machine IR constitutes a *universal computational substrate* from which each programming language derives as a specific *parameter configuration*. Formally, a language $`L`$ is defined by a configuration tuple:
``` math
L = \langle \mathcal{M}, \mathcal{T}, \mathcal{N}, \mathcal{E}, \mathcal{C} \rangle
```
where $`\mathcal{M}`$ is the memory model, $`\mathcal{T}`$ is the type discipline, $`\mathcal{N}`$ is the null handling strategy, $`\mathcal{E}`$ is the effect tracking mode, and $`\mathcal{C}`$ is the concurrency model.

</div>

### Language Parameter Configuration Matrix

Each language is characterized by a specific configuration across several orthogonal semantic dimensions:

<div class="adjustbox">

max width=

<div id="tab:lang-params">

| **Language** | **Memory** | **Types** | **Null** | **Effects** | **Concurrency** |
|:---|:---|:---|:---|:---|:---|
| <span class="sans-serif">C</span> | Manual | Static | Nullable | Untracked | POSIX Threads |
| <span class="sans-serif">C++</span> | Manual$`^*`$ | Static | Nullable | Untracked | POSIX Threads |
| <span class="sans-serif">Rust</span> | Affine | Static | Option | Untracked | Threads+Async |
| <span class="sans-serif">Go</span> | GC | Static | Nullable | Untracked | CSP Channels |
| <span class="sans-serif">Java</span> | GC | Static | Nullable | Checked | JMM Threads |
| <span class="sans-serif">Python</span> | GC | Dynamic | Nullable | Unchecked | GIL-Protected |
| <span class="sans-serif">JavaScript</span> | GC | Dynamic | Nullable | Untracked | Event Loop |
| <span class="sans-serif">TypeScript</span> | GC | Gradual | Nullable$`^\dagger`$ | Untracked | Event Loop |
| <span class="sans-serif">Swift</span> | ARC | Static | Optional | Checked | Actors+Async |
| <span class="sans-serif">Kotlin</span> | GC | Static | Optional | Checked | Coroutines |

Language parameter configuration matrix. $`^*`$<span class="sans-serif">C++</span> supports RAII. $`^\dagger`$<span class="sans-serif">TypeScript</span> nullable depends on `strictNullChecks` flag.

</div>

</div>

This formulation enables uniform treatment of cross-language interactions through explicit parameter reconciliation at linguistic boundaries.

<div class="definition">

**Definition 2.2** (Boundary Reconciliation). When control or data crosses from language $`L_1`$ to language $`L_2`$, the boundary reconciliation function $`\mathcal{R}`$ computes:
``` math
\mathcal{R}(L_1, L_2) = \{ p \in \mathsf{Properties} \mid
    \text{$p$ preserved across } L_1 \to L_2 \}
```
Properties not in this set require explicit guards or are flagged as potential risks.

</div>

## Foundational Literature and Theoretical Pillars

This framework synthesizes contributions from 29 foundational papers in programming language theory, organized into seven coherent theoretical pillars.

### Pillar 1: Abstract Interpretation — Semantic Foundation

<div class="pillarbox">

- **\[Cousot77\]** Cousot & Cousot. “Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints.” POPL 1977.

- **\[Cousot92\]** Cousot & Cousot. “Comparing the Galois Connection and Widening/Narrowing Approaches to Abstract Interpretation.” PLILP 1992.

**Criticality:** Essential — Provides mathematical foundation for all analyses.

</div>

#### Theoretical Contribution

Static analysis is formalized as computing sound approximations of program semantics. The abstraction-concretization relationship forms a Galois connection $`\langle \alpha, \gamma \rangle`$ between concrete and abstract semantic domains. Soundness derives from monotonicity of abstract transfer functions with respect to the lattice ordering.

<div class="definition">

**Definition 3.1** (Galois Connection). A *Galois connection* between posets $`(\mathcal{C}, \leq_\mathcal{C})`$ and $`(\mathcal{A}, \leq_\mathcal{A})`$ is a pair of monotone functions $`\langle \alpha, \gamma \rangle`$ such that:
``` math
\forall c \in \mathcal{C}, a \in \mathcal{A}: \quad
  \alpha(c) \leq_\mathcal{A}a \iff c \leq_\mathcal{C}\gamma(a)
```

</div>

<div class="theorem">

**Theorem 3.2** (Soundness via Galois Connection). *If $`\langle \alpha, \gamma \rangle`$ forms a Galois connection and the abstract transfer function $`f^\sharp`$ satisfies $`\alpha\circ f \sqsubseteq f^\sharp \circ \alpha`$, then the abstract analysis is sound: any property verified abstractly holds concretely.*

</div>

<div class="artifactbox">

**Implementation Artifacts:**

- **Required:**

  - Complete lattice typeclass with verified algebraic laws (F\*)

  - Galois connection interface with mechanized soundness proof

  - Widening/narrowing operators for non-Noetherian domains

  - Chaotic iteration with Bourdoncle weak topological ordering

- **Deferred:**

  - Relational numeric domains (Octagon, Polyhedra) — Phase 2

  - Automatic widening point inference — use loop header heuristic

</div>

### Pillar 2: Program Representation — Unified Graph Structure

<div class="pillarbox">

- **\[Yamaguchi14\]** Yamaguchi et al. “Modeling and Discovering Vulnerabilities with Code Property Graphs.” IEEE S&P 2014.

- **\[Ferrante87\]** Ferrante, Ottenstein & Warren. “The Program Dependence Graph and Its Use in Optimization.” TOPLAS 1987.

- **\[Horwitz90\]** Horwitz, Reps & Binkley. “Interprocedural Slicing Using Dependence Graphs.” TOPLAS 1990.

- **\[Weiser84\]** Weiser. “Program Slicing.” IEEE TSE 1984.

**Criticality:** Essential (Yamaguchi), High (others).

</div>

#### Theoretical Contribution

The Code Property Graph (<span class="sans-serif">CPG</span>) unifies Abstract Syntax Tree (<span class="sans-serif">AST</span>), Control Flow Graph (<span class="sans-serif">CFG</span>), and Program Dependence Graph (<span class="sans-serif">PDG</span>) into a single queryable structure. This representation reduces all program analyses to graph reachability and traversal problems. The <span class="sans-serif">PDG</span> captures both data dependencies (def-use chains) and control dependencies, enabling precise backward and forward slicing.

<div class="definition">

**Definition 3.3** (Code Property Graph). A *Code Property Graph* $`\textsf{CPG}= (V, E, \lambda, \mu)`$ consists of:

- $`V`$: set of nodes representing program elements (statements, expressions, declarations)

- $`E \subseteq V \times V \times \mathsf{EdgeType}`$: labeled edges where $`\mathsf{EdgeType} = \{\textsf{AST}, \textsf{CFG}, \textsf{PDG}_{\mathsf{data}}, \textsf{PDG}_{\mathsf{ctrl}}, \mathsf{Call}, \mathsf{Effect}\}`$

- $`\lambda : V \to \mathsf{NodeLabel}`$: node labeling function

- $`\mu : E \to \mathsf{EdgeLabel}`$: edge labeling function

</div>

<div class="theorem">

**Theorem 3.4** (CPG Completeness). *The <span class="sans-serif">CPG</span> representation is complete for interprocedural analysis: any path-sensitive dataflow fact computable on the original program is computable via graph reachability queries on the <span class="sans-serif">CPG</span>.*

</div>

<div class="artifactbox">

**Implementation Artifacts:**

- **Required:**

  - Unified <span class="sans-serif">CPG</span> type with <span class="sans-serif">AST</span>, <span class="sans-serif">CFG</span>, <span class="sans-serif">PDG</span>, and effect edge categories

  - Efficient traversal primitives (successors, predecessors, transitive closure, filtered reachability)

  - System Dependence Graph (<span class="sans-serif">SDG</span>) extension for interprocedural analysis

  - Effect edges (novel extension to Yamaguchi formulation)

- **Deferred:**

  - External graph database backend — in-memory representation suffices

  - Domain-specific query language — typed Rust/F\* traversals preferred

</div>

### Pillar 3: Interprocedural Dataflow — Algorithmic Framework

<div class="pillarbox">

- **\[Reps95\]** Reps, Horwitz & Sagiv. “Precise Interprocedural Dataflow Analysis via Graph Reachability.” POPL 1995.

- **\[Reps97\]** Reps. “Program Analysis via Graph Reachability.” Information and Software Technology, 1997.

- **\[Sridharan05\]** Sridharan & Bodı́k. “Demand-Driven Points-to Analysis for Java.” OOPSLA 2005.

- **\[Smaragdakis11\]** Bravenboer & Smaragdakis. “Strictly Declarative Specification of Sophisticated Points-to Analyses.” OOPSLA 2009.

- **\[Jordan16\]** Jordan et al. “Soufflé: On Synthesis of Program Analyzers.” CAV 2016.

- **\[Madsen16\]** Madsen et al. “From Datalog to Flix: A Declarative Language for Fixed Points on Lattices.” PLDI 2016.

**Criticality:** High — Core algorithmic infrastructure.

</div>

#### Theoretical Contribution

Interprocedural dataflow analysis reduces to graph reachability over an exploded supergraph. The <span class="sans-serif">IFDS</span> algorithm achieves $`\mathcal{O}(ED^3)`$ complexity for distributive dataflow problems. Context-free language (<span class="sans-serif">CFL</span>) reachability provides context sensitivity through matched call-return parentheses. Demand-driven formulations compute only query-relevant facts.

<div class="definition">

**Definition 3.5** (<span class="sans-serif">IFDS</span> Problem). An <span class="sans-serif">IFDS</span> problem is a tuple $`(G^*, D, M)`$ where:

- $`G^* = (N^*, E^*)`$ is the supergraph (interprocedural <span class="sans-serif">CFG</span>)

- $`D`$ is a finite set of dataflow facts

- $`M : E^* \to 2^{D \times D}`$ assigns a distributive transfer function to each edge, represented as a relation on $`D`$

</div>

<div class="theorem">

**Theorem 3.6** (<span class="sans-serif">IFDS</span> Complexity). *The <span class="sans-serif">IFDS</span> tabulation algorithm solves interprocedural distributive dataflow problems in time $`\mathcal{O}(E \cdot D^3)`$ where $`E = |E^*|`$ and $`D = |D|`$.*

</div>

<div class="designnote">

*Design Note 3.1*. The <span class="sans-serif">IDE</span> (Interprocedural Distributive Environment) extension is expressible as lattice-extended Datalog (Flix formulation) where the lattice element represents the micro-function space. <span class="sans-serif">IDE</span> is appropriate for analyses requiring environment transformers (constant propagation, linear constant analysis). <span class="sans-serif">IFDS</span> suffices for binary fact problems (taint tracking, nullability analysis).

</div>

<div class="artifactbox">

**Implementation Artifacts:**

- **Required:**

  - <span class="sans-serif">IFDS</span> tabulation algorithm with interprocedural summary edge caching

  - <span class="sans-serif">CFL</span>-reachability solver for context-sensitive analysis

  - Demand-driven query interface for interactive workloads

  - Optional Datalog compilation backend (Soufflé target)

- **Deferred:**

  - Custom Datalog interpreter — compilation to native code preferred

</div>

### Pillar 4: Pointer and Alias Analysis — Precision Infrastructure

<div class="pillarbox">

- **\[Andersen94\]** Andersen. “Program Analysis and Specialization for the C Programming Language.” PhD Thesis, DIKU, 1994.

- **\[Steensgaard96\]** Steensgaard. “Points-to Analysis in Almost Linear Time.” POPL 1996.

- **\[Calcagno09\]** Calcagno et al. “Compositional Shape Analysis by Means of Bi-Abduction.” POPL 2009.

**Criticality:** High — Precision foundation for all client analyses.

</div>

#### Theoretical Contribution

Pointer analysis computes may-alias and points-to relations. Andersen’s inclusion-based formulation achieves cubic complexity $`\mathcal{O}(n^3)`$ with higher precision. Steensgaard’s unification-based approach achieves near-linear complexity $`\mathcal{O}(n \cdot \alpha(n))`$ with reduced precision. Bi-abduction enables compositional heap analysis by simultaneously inferring preconditions (anti-frame) and postcondition frames.

<div class="definition">

**Definition 3.7** (Points-to Analysis). A *points-to analysis* computes a function $`\mathsf{pts} : \mathsf{Var} \to 2^{\mathsf{Loc}}`$ such that for all program executions, if variable $`v`$ holds location $`\ell`$, then $`\ell \in \mathsf{pts}(v)`$.

</div>

<div class="theorem">

**Theorem 3.8** (Pointer Analysis Complexity Trade-off).

- ***Andersen (inclusion-based):** $`\mathcal{O}(n^3)`$ complexity, higher precision*

- ***Steensgaard (unification-based):** $`\mathcal{O}(n \cdot \alpha(n))`$ complexity, lower precision*

*where $`\alpha`$ is the inverse Ackermann function.*

</div>

<div class="artifactbox">

**Implementation Artifacts:**

- **Required:**

  - Steensgaard unification for initial fast points-to computation

  - Andersen inclusion solver for precision-critical analysis paths

  - Bi-abduction engine for compositional memory reasoning

- **Deferred:**

  - Full shape analysis — prioritize points-to infrastructure first

  - BDD-based set representations — complexity not yet justified

</div>

### Pillar 5: Effect Systems — Computational Behavior Tracking

<div class="pillarbox">

- **\[Moggi91\]** Moggi. “Notions of Computation and Monads.” Information and Computation, 1991.

- **\[Plotkin03\]** Plotkin & Power. “Algebraic Operations and Generic Effects.” Applied Categorical Structures, 2003.

- **\[Plotkin09\]** Plotkin & Pretnar. “Handlers of Algebraic Effects.” ESOP 2009.

- **\[Leijen17\]** Leijen. “Type Directed Compilation of Row-Typed Algebraic Effects.” POPL 2017.

**Criticality:** High — Semantic framework for behavior characterization.

</div>

#### Theoretical Contribution

Effect systems distinguish pure values from effectful computations. Moggi’s monadic semantics provides compositional treatment of effects. Plotkin’s algebraic effects formulation enables modular effect handlers. Leijen’s row-polymorphic effect types support effect inference and effect polymorphism without explicit effect annotations.

<div class="definition">

**Definition 3.9** (Effect Row). An *effect row* is either:

- $`\langle \rangle`$: the empty row (pure computation)

- $`\langle e \mid \rho \rangle`$: row extension with effect $`e`$ and tail $`\rho`$

- $`\rho`$: a row variable (for polymorphism)

Effect rows form a lattice under the subsumption ordering $`\sqsubseteq`$.

</div>

<div class="theorem">

**Theorem 3.10** (Effect Composition). *Effects compose via row join: if $`f : A \xrightarrow{\varepsilon_1} B`$ and $`g : B \xrightarrow{\varepsilon_2} C`$, then $`g \circ f : A \xrightarrow{\varepsilon_1 \sqcup\varepsilon_2} C`$.*

</div>

<div class="artifactbox">

**Implementation Artifacts:**

- **Required:**

  - Effect taxonomy aligned with brrr-machine semantic model

  - Row-polymorphic effect type representation for function signatures

  - Effect composition lattice with subsumption ordering

  - Effect violation detection (e.g., use-after-free as resource effect violation, null dereference as totality effect violation)

- **Deferred:**

  - Full algebraic effect handlers — analysis focus, not execution

  - Complete effect inference for arbitrary source languages

</div>

### Pillar 6: Substructural Types and Ownership — Resource Invariants

<div class="pillarbox">

- **\[Girard87\]** Girard. “Linear Logic.” Theoretical Computer Science, 1987.

- **\[Reynolds02\]** Reynolds. “Separation Logic: A Logic for Shared Mutable Data Structures.” LICS 2002.

- **\[Jung18\]** Jung et al. “RustBelt: Securing the Foundations of the Rust Programming Language.” POPL 2018.

- **\[Aiken99\]** Aiken. “Introduction to Set Constraint-Based Program Analysis.” Science of Computer Programming, 1999.

**Criticality:** High — Resource safety and ownership verification.

</div>

#### Theoretical Contribution

Substructural type systems provide resource-sensitive reasoning:

- **LINEAR:** resources used exactly once (no implicit weakening)

- **AFFINE:** resources used at most once (Rust’s ownership model)

Separation logic enables local reasoning about mutable heap state through the frame rule. Iris provides step-indexed partial commutative monoids (cameras) for ownership verification. Set constraints unify type inference with dataflow analysis.

<div class="remark">

**Remark 3.11** (Clarification). Rust implements *affine* typing (implicit drop permitted), not strictly linear typing.

</div>

<div class="definition">

**Definition 3.12** (Substructural Type System). A type system is *substructural* if it restricts the structural rules:

- **Linear:** No weakening (must use) and no contraction (use exactly once)

- **Affine:** No contraction only (use at most once, implicit drop permitted)

- **Relevant:** No weakening only (must use, can duplicate)

</div>

<div class="theorem">

**Theorem 3.13** (Frame Rule). *In separation logic, if $`\{P\}\, c\, \{Q\}`$ is valid and $`c`$ does not modify variables free in $`R`$, then $`\{P * R\}\, c\, \{Q * R\}`$ is valid.*

</div>

<div class="artifactbox">

**Implementation Artifacts:**

- **Required:**

  - Resource algebra (camera) infrastructure for ownership tracking

  - Separation logic assertion language for heap specifications

  - Per-location ownership state machine (Acquired $`\to`$ InUse $`\to`$ Released)

  - Set constraint solver for unified type/flow analysis

- **Deferred:**

  - Complete Iris mechanization in F\* — adapt essential constructs only

  - Step-indexed semantics — not required for static analysis application

</div>

### Pillar 7: Security Analysis — Information Flow and Vulnerability Detection

<div class="pillarbox">

- **\[Denning77\]** Denning & Denning. “Certification of Programs for Secure Information Flow.” Communications of the ACM, 1977.

- **\[Livshits05\]** Livshits & Lam. “Finding Security Vulnerabilities in Java Applications with Static Analysis.” USENIX Security 2005.

- **\[Tripp09\]** Tripp et al. “TAJ: Effective Taint Analysis of Web Applications.” PLDI 2009.

**Criticality:** High — Security property verification.

</div>

#### Theoretical Contribution

Information flow security is characterized by lattice-theoretic ordering of security classes; secure programs permit only upward information flow. Taint analysis tracks propagation of untrusted data from sources (user input, network) to security-sensitive sinks (SQL queries, system calls). TAJ demonstrates industrial-scale taint analysis via hybrid thin slicing that focuses on security-relevant data dependencies.

<div class="definition">

**Definition 3.14** (Information Flow Security). A program satisfies *information flow security* with respect to a security lattice $`(\mathcal{S}, \leq)`$ if for all variables $`x, y`$:
``` math
\text{information flows from $x$ to $y$} \implies \mathsf{level}(x) \leq \mathsf{level}(y)
```

</div>

<div class="definition">

**Definition 3.15** (Taint Analysis). *Taint analysis* tracks propagation of untrusted data from *sources* (user input, network) to security-sensitive *sinks* (SQL queries, system calls). A taint violation occurs when tainted data reaches a sink without passing through a *sanitizer*.

</div>

<div class="artifactbox">

**Implementation Artifacts:**

- **Required:**

  - Taint lattice with source/sink/sanitizer semantic model

  - Context-sensitive taint propagation via <span class="sans-serif">IFDS</span> tabulation

  - Thin slicing for security-relevant dependency extraction

  - SARIF output format for IDE and CI/CD integration

- **Deferred:**

  - Implicit flow tracking — explicit dataflow only in initial version

  - Full declassification policy language — simple sanitizer model first

</div>

### Cross-Cutting Concerns: Multi-Language Interoperability and Incrementality

<div class="pillarbox">

- **\[Matthews07\]** Matthews & Findler. “Operational Semantics for Multi-Language Programs.” TOPLAS 2009.

- **\[Goguen92\]** Goguen & Burstall. “Institutions: Abstract Model Theory for Specification and Programming.” JACM, 1992.

- **\[Hammer14\]** Hammer et al. “Adapton: Composable, Demand-Driven Incremental Computation.” PLDI 2014.

- **\[Wagner98\]** Wagner & Graham. “Efficient and Flexible Incremental Parsing.” TOPLAS, 1998.

- **\[Szabo18\]** Szabó et al. “Incrementalizing Lattice-Based Program Analyses in Datalog.” OOPSLA 2018.

- **\[Distefano19\]** Distefano et al. “Scaling Static Analyses at Facebook.” Communications of the ACM, 2019.

**Criticality:** Essential (Szabó, Distefano), High (Matthews, Hammer).

</div>

#### Theoretical Contribution

Matthews-Findler provides operational semantics for multi-language programs with explicit boundary terms mediating cross-language calls. Goguen’s institutions provide categorical abstraction over logical systems, enabling formal treatment of language heterogeneity. Adapton introduces demand-driven incremental computation with memoization. Szabó’s DRedL provides lattice-based incremental Datalog evaluation achieving 65x–243x speedup. Distefano demonstrates industrial deployment at 100M+ LOC scale with 70% developer fix rates via diff-time analysis.

<div class="artifactbox">

**Implementation Artifacts:**

- **Required:**

  - Cross-language boundary analysis (Matthews-Findler formulation)

  - Property preservation verification at linguistic boundaries

  - Incremental analysis via dirty-marking with DRedL lattice propagation

  - Tree-sitter integration for incremental syntax tree maintenance

  - Diff-time deployment pipeline (Infer-style CI/CD integration)

  - Interprocedural summary caching with dependency-based invalidation

- **Deferred:**

  - Full Adapton framework — simpler dirty-marking with DRedL suffices

  - Custom incremental parser — tree-sitter provides adequate solution

</div>

## Document Organization and Conventions

Each subsequent part of this specification adheres to a consistent organizational structure to facilitate both implementation and verification.

### Section Template

<div class="sectiontemplatebox">

1.  **Objective Statement**  
    Precise characterization of the capability being specified

2.  **Foundational References**  
    Authoritative literature providing theoretical grounding

3.  **Theoretical Framework**  
    Mathematical formalization and semantic definitions

4.  **Design Rationale**  
    Engineering decisions with explicit justification

5.  **Formal Specification**  
    F\* mechanization with verified properties where applicable

6.  **Integration Interfaces**  
    Dependencies and contracts with adjacent components

</div>

### Notational Conventions

<div class="notation">

*Notation 4.1*. Throughout this document:

- Mathematical notation follows standard PL theory conventions

- F\* code blocks contain mechanically verifiable specifications

- **\[Author##\]** citations reference entries in Appendix A

- Complexity bounds use standard asymptotic notation ($`\mathcal{O}`$, $`\Omega`$, $`\Theta`$)

- $`\sqsubseteq`$ denotes lattice ordering; $`\sqcup`$ and $`\sqcap`$ denote join and meet

- $`\alpha`$ and $`\gamma`$ denote abstraction and concretization functions

- $`\llbracket \cdot \rrbracket`$ denotes semantic brackets (denotation)

- Effect rows use angle bracket notation: $`\langle E \mid \rho \rangle`$

</div>

# Theoretical Foundations

<div class="tcolorbox">

See Appendix D.10.5 for full analysis.

**This Part** uses two-valued lattices with explicit “Maybe” variants:
``` math
\mathsf{TaintLevel} = \mathsf{Tainted} \mid \mathsf{Untainted} \mid \mathsf{Unknown}
```

**TVLA** (Sagiv 2002) uses three-valued logic with Kleene semantics:
``` math
\mathsf{three\_value} = \mathsf{TV0}~(\text{false}) \mid \mathsf{TV1}~(\text{true}) \mid \mathsf{TV\frac{1}{2}}~(\text{unknown})
```
Information ordering: $`\frac{1}{2} \sqsubseteq 0`$ and $`\frac{1}{2} \sqsubseteq 1`$

**Advantages of Three-Valued:**

- Principled — proper information ordering

- Kleene semantics for $`\land`$, $`\lor`$, $`\lnot`$ are well-defined

- Embedding theorem (12.23) links concrete to abstract soundly

**Resolution:**

- Section 5.4: Shape analysis uses three-valued foundation

- Section 12.23: Formal embedding theorem

- Section 12.24: Instrumentation predicates in three-valued logic

- Existing domains: Can be viewed as three-valued with $`\mathsf{Unknown} = \frac{1}{2}`$

</div>

<div class="tcolorbox">

See Section 2.1.5b for full analysis.

**Critical:** VCs sound under EAGER evaluation may be UNSOUND under LAZY!

Under lazy evaluation, binders may be thunks that never evaluate. If a thunk has type $`\{v:\mathsf{Int} \mid \mathsf{false}\}`$, the VC includes “false” as assumption, making ANY conclusion trivially valid — but execution never realizes this!

**Resolution (LiquidHaskell):**

- Add `eval_mode` to `language_config` (Section 9.1)

- `EvalStrict`: Classical VC translation is sound

- `EvalLazy`: Use stratified types (Div/Wnf/Fin) - Section 2.1.5b

- `EvalHybrid`: Default strict, stratify lazy constructs (generators, etc.)

**Affected Analyses:**

- Refinement type checking: Must track divergence labels

- Widening (Section 2.1.5): Divergence labels propagate across iterations

- Termination analysis: Proven termination upgrades $`\mathsf{Div} \to \mathsf{Fin}`$

</div>

## Abstract Interpretation: The Central Dogma

**Papers:** Cousot & Cousot 1977, Cousot & Cousot 1992, Zilberstein 2023 (Outcome Logic)

Every analysis in the brrr-machine is an abstract interpretation. This is not merely a design choice—it is the only mathematically sound approach to static analysis that provides:

1.  **Dual soundness** — Both verification (over-approx) AND bug detection (under-approx)

2.  **Termination guarantees** — Analysis always finishes (with widening)

3.  **Compositionality** — Analyses can be combined systematically

See Section 2.1.8 for the critical distinction between over-approximation (proving safety) and under-approximation (proving bugs exist).

<div class="remark">

**Remark 1.1** (F\* Code Style Throughout This Document). The F\* code in this document serves as *formal specification* of the brrr-machine’s theoretical foundations. While the code captures the essential mathematical structures (lattices, Galois connections, transfer functions), some syntactic constructs (such as `instance` declarations and inline `forall` constraints in type definitions) are F\*-inspired notation rather than directly compilable F\* code. The key types and operations can be translated to valid F\* with appropriate module structure.

Each F\* code block includes:

- **Type signatures** expressing the mathematical structure

- **Operations** implementing lattice and domain operations

- **Comments** linking to source papers and theoretical foundations

Proof obligations are expressed as separate predicates that can be proven as F\* lemmas.

</div>

### The Concrete and Abstract Worlds

<div class="tcolorbox">

**Concrete Semantics** $`\llbracket P \rrbracket : \mathsf{Program} \to \wp(\mathsf{State})`$

The concrete semantics maps a program to all states it can reach. This set is typically infinite or astronomically large. We cannot compute it directly.

**Abstract Semantics** $`\llbracket P \rrbracket^\sharp : \mathsf{Program} \to \mathsf{AbstractDomain}`$

The abstract semantics computes a finite representation that SOUNDLY APPROXIMATES the concrete semantics. We can compute this efficiently.

**The Key Relationship:**

For all concrete executions $`c`$:
``` math
c \in \llbracket P \rrbracket \implies \alpha(c) \sqsubseteq \llbracket P \rrbracket^\sharp
```
Where $`\alpha`$ is the abstraction function and $`\sqsubseteq`$ is the abstract ordering.

</div>

### Galois Connections

The relationship between concrete and abstract is formalized as a Galois connection—the cornerstone of abstract interpretation:

<div class="definition">

**Definition 1.2** (Galois Connection). A Galois connection between posets $`(C, \leq_C)`$ and $`(A, \leq_A)`$ is a pair of monotone functions:
``` math
\begin{aligned}
  \alpha &: C \to A \quad \text{(abstraction)} \\
  \gamma &: A \to C \quad \text{(concretization)}
\end{aligned}
```
Such that for all $`c \in C`$ and $`a \in A`$:
``` math
\alpha(c) \leq_A a \iff c \leq_C \gamma(a)
```

</div>

<div class="remark">

**Remark 1.3** (Equivalent Characterization). When both posets are complete lattices, the Galois connection property is equivalent to:
``` math
\begin{aligned}
  \gamma \circ \alpha &\sqsupseteq \mathrm{id}_C \quad \text{(soundness: concretizing abstraction gives superset)} \\
  \alpha \circ \gamma &\sqsubseteq \mathrm{id}_A \quad \text{(optimality: abstracting concretization is below)}
\end{aligned}
```

</div>

<div class="center">

</div>

The $`\alpha`$-$`\gamma`$ pair forms an “adjunction” — $`\alpha`$ is left adjoint to $`\gamma`$.

<div class="remark">

**Remark 1.4** (Why Galois Connections Matter for Brrr-Machine). Every abstract domain we use must form a Galois connection with the concrete semantics. This guarantees:

- **Soundness**: If abstract analysis says “property holds,” it truly does

- **Best Abstraction**: $`\alpha`$ gives the most precise abstraction of any concrete element

- **Composition**: Galois connections compose — we can layer abstractions

</div>

### Complete Lattices

Both concrete and abstract domains must be complete lattices:

<div class="definition">

**Definition 1.5** (Complete Lattice). A complete lattice $`(L, \sqsubseteq, \bot, \top, \sqcup, \sqcap)`$ consists of:

- Carrier set $`L`$

- Partial order $`\sqsubseteq`$ (reflexive, antisymmetric, transitive)

- Bottom element $`\bot`$ (least element)

- Top element $`\top`$ (greatest element)

- Join $`\sqcup`$ (least upper bound for any subset)

- Meet $`\sqcap`$ (greatest lower bound for any subset)

</div>

<div class="definition">

**Definition 1.6** (Required Properties).
``` math
\begin{aligned}
  \bot \sqsubseteq x &\quad \text{for all } x \quad \text{(bottom is least)} \\
  x \sqsubseteq \top &\quad \text{for all } x \quad \text{(top is greatest)} \\
  x \sqsubseteq x \sqcup y \text{ and } y \sqsubseteq x \sqcup y &\quad \text{(join is upper bound)} \\
  x \sqcap y \sqsubseteq x \text{ and } x \sqcap y \sqsubseteq y &\quad \text{(meet is lower bound)} \\
  x \sqsubseteq z \land y \sqsubseteq z \Rightarrow x \sqcup y \sqsubseteq z &\quad \text{(join is least upper bound)}
\end{aligned}
```

</div>

<div class="example">

**Example 1.7** (Common Lattices in Program Analysis).

1.  **Powerset Lattice** $`\wp(S)`$:

    - Elements: subsets of $`S`$

    - Order: $`\subseteq`$ (subset inclusion)

    - Bottom: $`\emptyset`$ (empty set)

    - Top: $`S`$ (full set)

    - Join: $`\cup`$ (union), Meet: $`\cap`$ (intersection)

2.  **Flat Lattice** $`\mathsf{Flat}(S)`$:

    - Elements: $`\{\bot\} \cup S \cup \{\top\}`$

    - $`\bot \sqsubseteq x \sqsubseteq \top`$ for all $`x \in S`$

    - Elements of $`S`$ are incomparable

    - Used for: constants, definitely-assigned analysis

3.  **Interval Lattice**:

    - Elements: $`[lo, hi]`$ where $`lo \leq hi`$, plus $`\bot`$

    - Order: $`[a,b] \sqsubseteq [c,d]`$ iff $`c \leq a`$ and $`b \leq d`$

    - Join: $`[a,b] \sqcup [c,d] = [\min(a,c), \max(b,d)]`$

    - Used for: numeric bounds, array index analysis

</div>

### Fixpoint Computation

Programs have loops. Abstract interpretation handles this via fixpoint computation:

<div class="theorem">

**Theorem 1.8** (Tarski’s Fixpoint Theorem). *If $`f : L \to L`$ is a monotone function on a complete lattice $`L`$, then:*

1.  *$`f`$ has a **least fixpoint**: $`\mathrm{lfp}(f) = \bigsqcap\{ x \in L \mid f(x) \sqsubseteq x \}`$*

2.  *$`f`$ has a **greatest fixpoint**: $`\mathrm{gfp}(f) = \bigsqcup\{ x \in L \mid x \sqsubseteq f(x) \}`$*

</div>

<div class="definition">

**Definition 1.9** (Kleene Iteration). Constructive computation of the least fixpoint:
``` math
\mathrm{lfp}(f) = \bigsqcup\{ f^n(\bot) \mid n \in \mathbb{N} \}
```
That is: $`\bot, f(\bot), f(f(\bot)), f(f(f(\bot))), \ldots`$

This sequence is ascending (by monotonicity of $`f`$). If the lattice has no infinite ascending chains, it stabilizes.

</div>

<div class="example">

**Example 1.10** (Reaching Definitions).
``` math
\begin{aligned}
  \mathsf{State} &= \wp(\mathsf{Definition}) \\
  f(S) &= \mathsf{gen} \cup (S \setminus \mathsf{kill}) \quad \text{(transfer function)}
\end{aligned}
```
Starting from $`\emptyset`$, we iterate:
``` math
\begin{aligned}
  f^0(\emptyset) &= \emptyset \\
  f^1(\emptyset) &= \mathsf{gen} \\
  f^2(\emptyset) &= \mathsf{gen} \cup (\mathsf{gen} \setminus \mathsf{kill}) = \mathsf{gen} \quad \text{STABLE!}
\end{aligned}
```
The fixpoint is $`\mathsf{gen}`$ — the definitions that reach this point.

</div>

### Widening and Narrowing

**The Problem:** For infinite-height lattices (like integers), Kleene iteration may not terminate.

<div class="example">

**Example 1.11** (Non-Terminating Iteration). Consider the loop: `while (x < 100) { x = x + 1; }`

Abstract state at loop head:
``` math
\begin{aligned}
  \text{Iteration 0:} & \quad x \in [0, 0] \\
  \text{Iteration 1:} & \quad x \in [0, 1] \\
  \text{Iteration 2:} & \quad x \in [0, 2] \\
  & \vdots \\
  \text{Iteration 100:} & \quad x \in [0, 100]
\end{aligned}
```
100 iterations just for this simple loop! In general: unbounded.

</div>

**The Solution (Cousot 1992):** Widening and narrowing operators.

<div class="definition">

**Definition 1.12** (Widening Operator). A widening operator $`\nabla : L \times L \to L`$ satisfies:

1.  **Upper bound:** $`x \sqsubseteq (x \nabla y)`$ and $`y \sqsubseteq (x \nabla y)`$

2.  **Termination:** All ascending chains $`x_0 \nabla x_1 \nabla x_2 \nabla \cdots`$ stabilize

The widening ACCELERATES convergence by over-approximating.

</div>

<div class="definition">

**Definition 1.13** (Narrowing Operator). A narrowing operator $`\triangle : L \times L \to L`$ satisfies:

1.  **Bounded:** $`y \sqsubseteq x \Rightarrow y \sqsubseteq (x \triangle y) \sqsubseteq x`$

2.  **Termination:** All descending chains $`x_0 \triangle x_1 \triangle x_2 \triangle \cdots`$ stabilize

The narrowing RECOVERS PRECISION after widening.

</div>

<div class="definition">

**Definition 1.14** (Standard Interval Widening).
``` math
[a,b] \nabla [c,d] = \begin{bmatrix}
    \text{if } c < a \text{ then } -\infty \text{ else } a, \\
    \text{if } d > b \text{ then } +\infty \text{ else } b
  \end{bmatrix}
```
That is: if bound changes, extrapolate to infinity.

</div>

<div class="example">

**Example 1.15** (With Widening).
``` math
\begin{aligned}
  \text{Iteration 0:} & \quad x \in [0, 0] \\
  \text{Iteration 1:} & \quad x \in [0, 0] \nabla [0, 1] = [0, +\infty] \quad \text{(upper bound changed!)}
\end{aligned}
```
STABLE in 2 iterations!

Then narrowing with loop condition $`x < 100`$:
``` math
x \in [0, +\infty] \triangle [0, 99] = [0, 99]
```
Final result: $`x \in [0, 99]`$ — precise and efficient.

</div>

<div class="remark">

**Remark 1.16** (Analysis Strategy).

1.  Apply widening at loop heads (and recursive call entries)

2.  Compute post-fixpoint using widened iteration

3.  Apply narrowing iterations to recover precision

4.  Finite iterations guaranteed by widening/narrowing properties

</div>

### Lazy Evaluation and Verification Condition Soundness

**Paper:** Vazou et al. 2014 (LiquidHaskell)

<div class="tcolorbox">

Standard refinement type systems ASSUME all free variables in an environment are bound to VALUES. This assumption holds trivially under eager (call-by-value) evaluation but FAILS under lazy evaluation.

Under lazy evaluation, a binding like:

<div class="center">

`let n = diverge 1 in ...`

</div>

means the VC can include assumptions about `n` that are NEVER realized because `n` may never be evaluated.

The “false” refinement in `diverge`’s output type contaminates the VC:
``` math
\mathsf{false} \land y = 0 \Rightarrow v = 0 \Rightarrow v > 0
```
This is VALID (contradiction in antecedent) but UNSOUND under laziness!

**Consequence:** For `EvalLazy` languages (Haskell), we MUST use stratified types. See `eval_mode` in `language_config` (Section 9.1).

</div>

<div class="definition">

**Definition 1.17** (Stratified Types for Lazy Languages). The solution is to stratify types based on termination guarantees:

**Divergence Stratification (from LiquidHaskell):**

- **Div types** (unlabeled) — May diverge: CANNOT assume refinements hold

- **Wnf types** ($`\downarrow`$ labeled) — Reduces to WHNF: CAN assume head-form properties

- **Fin types** ($`\Downarrow`$ labeled) — Reduces to finite value: FULL refinement power

**Type Ordering:**
``` math
\llbracket \{v:B^{\Downarrow} \mid r\} \rrbracket \subseteq \llbracket \{v:B^{\downarrow} \mid r\} \rrbracket \subseteq \llbracket \{v:B \mid r\} \rrbracket
```

**VC Translation Rule:**
``` math
\begin{aligned}
  \text{Standard (strict):} & \quad (|x:\{v:B \mid r\}|) = r[x/v] \\
  \text{Stratified (lazy):} & \quad (|x:\{v:B \mid r\}|) = \text{if } \mathsf{is\_value}(x) \text{ then } r[x/v] \text{ else } \mathsf{true}
\end{aligned}
```

</div>

<div class="fstarcode">

(\* Divergence labels for basic types \*) type divergence_label = \| DivMay (\* May diverge - CANNOT assume refinement holds \*) \| DivWhnf (\* Reduces to weak head normal form - partial guarantee \*) \| DivFin (\* Reduces to finite value - FULL refinement power \*)

(\* Stratified type wraps a base type with divergence information \*) type stratified_type (b : ir_type) = base : b; label : divergence_label; refinement : predicate;

(\* VC TRANSLATION DEPENDS ON DIVERGENCE LABEL \*) val translate_binding : lang:language_config -\> x:var_id -\> st:stratified_type -\> predicate let translate_binding lang x st = match lang.eval_mode with \| EvalStrict -\> (\* Strict: always safe to assume refinement \*) substitute st.refinement x \| EvalLazy \| EvalHybrid -\> (\* Lazy: only assume refinement if value is guaranteed \*) match st.label with \| DivMay -\> (\* Cannot assume refinement - binder may diverge \*) PTrue \| DivWhnf \| DivFin -\> (\* Safe to assume refinement \*) substitute st.refinement x

(\* CASE EXPRESSIONS UPGRADE DIVERGENCE LABELS \*) val case_scrutinee_upgrade : stratified_type -\> stratified_type let case_scrutinee_upgrade st = st with label = DivWhnf (\* Forced to WHNF by case evaluation \*)

(\* TERMINATION ANALYSIS UPGRADES Div TO Fin \*) val upgrade_if_terminating : stratified_type -\> terminates:bool -\> stratified_type let upgrade_if_terminating st terminates = if terminates then st with label = DivFin else st

(\* SUBTYPING FOR STRATIFIED TYPES: Fin \<: Wnf \<: Div \*) val label_subtype : divergence_label -\> divergence_label -\> bool let label_subtype l1 l2 = match l1, l2 with \| \_, DivMay -\> true (\* Anything is subtype of Div \*) \| DivFin, DivFin -\> true \| DivFin, DivWhnf -\> true (\* Fin \<: Wnf \*) \| DivWhnf, DivWhnf -\> true \| \_, \_ -\> false

</div>

<div class="remark">

**Remark 1.18** (Integration with Abstract Interpretation). **Interaction with Widening (Section 2.1.5):**

For `EvalLazy` languages, widening must ALSO track divergence labels. A variable in a loop may:

1.  Always terminate (`DivFin`) — safe for full refinement

2.  Sometimes diverge (`DivMay`) — must weaken refinements

When widening across loop iterations, if ANY iteration path can diverge, the result label must be `DivMay`. This is SOUND but may lose precision.

**Hybrid Languages** (`EvalHybrid` - Python, JavaScript):

- Default expressions are strict (`EvalStrict` rules apply)

- Generator expressions / async functions are lazy (need stratification)

- Detect via syntactic markers: `yield`, `async`, `lazy_static!`, etc.

</div>

### Neural Invariant Synthesis (Optional Enhancement)

**Paper:** Si et al. 2018 (Code2Inv - NeurIPS)

<div class="tcolorbox">

**Problem:** Traditional widening ALWAYS terminates but loses precision. Complex invariants like $`(x < 0 \lor y > 0)`$ cannot be discovered by widening.

**Insight:** Use reinforcement learning to SYNTHESIZE invariants directly. Neural networks learn to generate invariant candidates; Z3 verifies them.

**Architecture (Code2Inv):**

1.  GNN encodes program graph (AST + CFG + data flow)

2.  TreeLSTM decoder generates invariant predicates autoregressively

3.  Attention mechanism focuses on relevant program variables

4.  Z3 provides reward signal (valid/invalid + counterexamples)

**Hybrid Strategy:**

1.  Try neural synthesis with timeout (e.g., 1000 Z3 queries)

2.  If found: verify and use (PRECISE invariant)

3.  If timeout: fall back to widening (SOUND over-approximation)

**Empirical:** Code2Inv solves 106/133 benchmarks with 10–100x fewer queries.

</div>

**Key Innovation: CEGAR with Neural Guidance**

Code2Inv integrates counterexample-guided abstraction refinement (CEGAR) with neural learning. Counterexamples from Z3 serve dual purposes:

1.  **Coarse feedback:** Which Hoare condition failed (pre/inv/post)

2.  **Fine-grained reward:** Ratio of examples satisfied guides learning

<div class="fstarcode">

module BrrrMachine.NeuralInvariant

(\* Invariant predicate AST - interpretable output \*) type inv_pred = \| InvCompare : op:cmp_op -\> l:arith_expr -\> r:arith_expr -\> inv_pred \| InvAnd : inv_pred -\> inv_pred -\> inv_pred \| InvOr : inv_pred -\> inv_pred -\> inv_pred \| InvNot : inv_pred -\> inv_pred

type cmp_op = \| CLeq \| CGeq \| CLt \| CGt \| CEq

(\* Verification outcome from Z3 \*) type verification_result = \| Verified : verification_result \| PreViolation : cex:valuation -\> verification_result \| InvViolation : cex:valuation -\> verification_result (\* Inductiveness failed \*) \| PostViolation : cex:valuation -\> verification_result

(\* Hybrid synthesis: neural first, widening fallback \*) type invariant_source = \| NeuralSynthesized : inv:inv_pred -\> queries:nat -\> invariant_source \| WideningDerived : domain_elem:interval_env -\> invariant_source

val synthesize_hybrid : prog:cpg -\> pre:formula -\> post:formula -\> neural_budget:nat -\> invariant_source

(\* SOUNDNESS: Either neural produces verified invariant, or widening provides sound over-approximation \*) val hybrid_soundness : result:invariant_source -\> Lemma (match result with \| NeuralSynthesized inv \_ -\> is_valid_invariant inv \| WideningDerived widened -\> is_sound_postfixpoint widened)

</div>

<div class="remark">

**Remark 1.19** (When to Use Neural Synthesis).

- Complex invariants outside interval/octagon domains

- Disjunctive invariants ($`x < 0`$ OR $`y > 0`$)

- When widening loses too much precision

- Transfer learning from similar codebases

</div>

**Cross-References:**

- Section 3.1.8: GNN program graph embeddings (required for neural synthesis)

- Section 2.1.5: Traditional widening (fallback mechanism)

- Section 4.4.2: Z3 integration for verification queries

### F\* Specification: Abstract Domains

The following F\* code formalizes the mathematical structures underlying abstract interpretation. The `partial_order` record captures the ordering relation on abstract values. The `complete_lattice` extends this with join/meet operations and bounds. The `galois_connection` type captures the fundamental relationship between concrete and abstract domains. Note that F\* expresses proof obligations as refinement types and separate lemmas rather than inline `forall` constraints.

<div class="fstarcode">

(\* This module defines the core structures for abstract domains, ensuring that every domain we implement satisfies the mathematical requirements for sound abstract interpretation.

KEY TYPES: - partial_order: ordering relation with reflexivity, antisymmetry, transitivity - complete_lattice: partial order with join, meet, top, and bottom - galois_connection: the alpha/gamma pair linking concrete to abstract - abstract_domain: lattice with optional widening/narrowing operators \*)

module BrrrMachine.AbstractDomain

(\* PARTIAL ORDER - Core ordering operations \*) noeq type partial_order (a : Type) = leq : a -\> a -\> bool;

(\* Partial order axioms expressed as separate predicates \*) let po_reflexive (#a:Type) (po:partial_order a) : prop = forall (x:a). po.leq x x == true

let po_antisymmetric (#a:Type) (po:partial_order a) : prop = forall (x y:a). (po.leq x y == true / po.leq y x == true) ==\> x == y

let po_transitive (#a:Type) (po:partial_order a) : prop = forall (x y z:a). (po.leq x y == true / po.leq y z == true) ==\> po.leq x z == true

(\* A valid partial order satisfies all three axioms \*) let is_partial_order (#a:Type) (po:partial_order a) : prop = po_reflexive po / po_antisymmetric po / po_transitive po

(\* COMPLETE LATTICE - Partial order with bounds and join/meet \*) noeq type complete_lattice (a : Type) = lat_leq : a -\> a -\> bool; bot : a; top : a; join : a -\> a -\> a; meet : a -\> a -\> a; (\* Join of arbitrary sets - needed for complete lattice \*) join_set : (a -\> bool) -\> a; meet_set : (a -\> bool) -\> a;

(\* Lattice axioms as predicates \*) let bot_is_least (#a:Type) (lat:complete_lattice a) : prop = forall (x:a). lat.lat_leq lat.bot x == true

let top_is_greatest (#a:Type) (lat:complete_lattice a) : prop = forall (x:a). lat.lat_leq x lat.top == true

let join_is_lub (#a:Type) (lat:complete_lattice a) : prop = (forall (x y:a). lat.lat_leq x (lat.join x y) == true) / (forall (x y:a). lat.lat_leq y (lat.join x y) == true) / (forall (x y z:a). (lat.lat_leq x z == true / lat.lat_leq y z == true) ==\> lat.lat_leq (lat.join x y) z == true)

let meet_is_glb (#a:Type) (lat:complete_lattice a) : prop = (forall (x y:a). lat.lat_leq (lat.meet x y) x == true) / (forall (x y:a). lat.lat_leq (lat.meet x y) y == true) / (forall (x y z:a). (lat.lat_leq z x == true / lat.lat_leq z y == true) ==\> lat.lat_leq z (lat.meet x y) == true)

(\* GALOIS CONNECTION - The fundamental abstraction relationship \*) noeq type galois_connection (c : Type) (a : Type) = gc_concrete_lat : complete_lattice c; gc_abstract_lat : complete_lattice a; (\* Abstraction function: concrete -\> abstract \*) alpha : c -\> a; (\* Concretization function: abstract -\> concrete \*) gamma : a -\> c;

(\* The Galois connection law: alpha(c) \<= a iff c \<= gamma(a) \*) let galois_law (#c \#a:Type) (gc:galois_connection c a) : prop = forall (x:c) (y:a). (gc.gc_abstract_lat.lat_leq (gc.alpha x) y == true) \<==\> (gc.gc_concrete_lat.lat_leq x (gc.gamma y) == true)

(\* Derived property: gamma . alpha is extensive (soundness) \*) let gamma_alpha_extensive (#c \#a:Type) (gc:galois_connection c a) : prop = forall (x:c). gc.gc_concrete_lat.lat_leq x (gc.gamma (gc.alpha x)) == true

(\* Derived property: alpha . gamma is reductive (optimality) \*) let alpha_gamma_reductive (#c \#a:Type) (gc:galois_connection c a) : prop = forall (y:a). gc.gc_abstract_lat.lat_leq (gc.alpha (gc.gamma y)) y == true

(\* ABSTRACT DOMAIN WITH WIDENING \*) noeq type abstract_domain (a : Type) = ad_lattice : complete_lattice a; (\* Widening operator - optional, required for infinite-height lattices \*) widen : option (a -\> a -\> a); (\* Narrowing operator - optional, for precision recovery \*) narrow : option (a -\> a -\> a);

(\* Widening must be an upper bound \*) let widen_is_upper_bound (#a:Type) (ad:abstract_domain a) : prop = match ad.widen with \| Some w -\> forall (x y:a). ad.ad_lattice.lat_leq x (w x y) == true / ad.ad_lattice.lat_leq y (w x y) == true \| None -\> True

(\* Narrowing must be bounded between y and x when y \<= x \*) let narrow_is_bounded (#a:Type) (ad:abstract_domain a) : prop = match ad.narrow with \| Some n -\> forall (x y:a). ad.ad_lattice.lat_leq y x == true ==\> (ad.ad_lattice.lat_leq y (n x y) == true / ad.ad_lattice.lat_leq (n x y) x == true) \| None -\> True

</div>

The key insight is that the Galois connection law $`\alpha(c) \leq a \Leftrightarrow c \leq \gamma(a)`$ ensures soundness: any property proven in the abstract domain holds in the concrete domain. The derived properties ($`\gamma \circ \alpha`$ extensive, $`\alpha \circ \gamma`$ reductive) follow from this fundamental law.

Transfer functions are the core of abstract interpretation—they describe how abstract values flow through program statements. The key requirement is **monotonicity**: if input abstractions are more precise (lower in the lattice), output abstractions must also be more precise. This ensures fixpoint iteration converges to a meaningful result.

<div class="remark">

**Remark 1.20** (F\* Code Style). The following F\* code uses refinement types and type-level constraints to express monotonicity and other properties. The notation `{| abstract_domain a |}` represents a typeclass constraint requiring `a` to be an abstract domain. While not all constructs match standard F\* syntax exactly, the specification intent is clear and can be translated to valid F\* with appropriate module structure.

</div>

<div class="fstarcode">

(\* A transfer function must be monotone for fixpoint to exist. Monotonicity: if x \<= y in the lattice, then f(x) \<= f(y). This ensures Kleene iteration converges to the least fixpoint. \*)

(\* Monotone transfer function type - refinement encodes monotonicity \*) type transfer_function (a:Type) (lat:complete_lattice a) = f:(a -\> a) forall (x y:a). lat.lat_leq x y == true ==\> lat.lat_leq (f x) (f y) == true

(\* FLIX-STYLE MULTI-ARGUMENT TRANSFER FUNCTIONS (Madsen 2016) For lattice-extended Datalog rules like: LocalVar(r, sum(x, y)) :- AddExp(r, v1, v2), LocalVar(v1, x), LocalVar(v2, y) The transfer function ’sum’ must be monotone in ALL arguments: x1 \<= x2 / y1 \<= y2 ==\> sum(x1, y1) \<= sum(x2, y2) This ensures the unique minimal model theorem (Madsen 2016, Theorem 1). See Section 4.1.7 for full Flix integration. \*)

(\* Binary monotone transfer function (e.g., abstract addition) \*) type transfer_function2 (a:Type) (lat:complete_lattice a) = f:(a -\> a -\> a) forall (x1 x2 y1 y2:a). (lat.lat_leq x1 x2 == true / lat.lat_leq y1 y2 == true) ==\> lat.lat_leq (f x1 y1) (f x2 y2) == true

(\* Filter function for Flix-style guards - no monotonicity required \*) type filter_function (a:Type) = a -\> bool

(\* Fixpoint computation with widening \*) let rec compute_fixpoint_aux (#a:Type) (lat:complete_lattice a) (ad:abstract_domain a) (f:transfer_function a lat) (current:a) (iteration:nat) (max_iter:nat) : a = if iteration \>= max_iter then current else let next = f current in if lat.lat_leq next current && lat.lat_leq current next then current (\* Fixpoint reached: next = current \*) else let widened = match ad.widen with \| Some w -\> w current next \| None -\> next in compute_fixpoint_aux lat ad f widened (iteration + 1) max_iter

let compute_fixpoint (#a:Type) (lat:complete_lattice a) (ad:abstract_domain a) (f:transfer_function a lat) : a = compute_fixpoint_aux lat ad f lat.bot 0 1000

(\* SOUNDNESS THEOREM - See Section 12.2 for full statement \*) (\* abstract_interpretation_sound: If abstract transfer over-approximates concrete transfer, then abstract fixpoint over-approximates concrete fixpoint. Full proof in Part XII: Key Soundness Theorems. \*)

</div>

The fixpoint computation algorithm implements Kleene iteration with widening. When the lattice has infinite ascending chains (e.g., intervals), widening forces convergence by extrapolating to infinity. The optional narrowing phase then recovers precision.

### Widening and Narrowing Theoretical Foundation (Cousot 1992)

<div class="tcolorbox">

Cousot 1992 proves: For infinite domains (intervals, polyhedra), there exists NO finite Galois connection that computes equivalent results.

**Consequence:** We MUST use widening for loops with numeric variables. Cannot replace with “just add more precision” — mathematically impossible.

**Key Insight:** Discovered invariants may NOT appear in program text! McCarthy F91 analysis discovers $`[91, \mathsf{maxint}-10]`$ — neither constant in code.

</div>

<div class="fstarcode">

(\* TWO-PHASE ALGORITHM: Phase 1 (Upward): Iterate with widening until post-fixpoint Phase 2 (Downward): Iterate with narrowing to refine precision \*)

(\* Widening properties (Cousot 1992, equations 6-8): 1. x x y 2. y x y 3. For increasing chains, widened chain stabilizes in finite steps \*) type widening_op (#a:Type) (#lat:lattice a) = f:(a -\> a -\> a) (\* Upper bound \*) (forall x y. lat.order x (f x y)) / (forall x y. lat.order y (f x y)) (\* Termination: not expressible in types, proven separately \*)

(\* Narrowing properties (Cousot 1992, equations 10-11): If y x then y (x y) x \*) type narrowing_op (#a:Type) (#lat:lattice a) = f:(a -\> a -\> a) forall x y. lat.order y x ==\> lat.order y (f x y) / lat.order (f x y) x

(\* TWO-PHASE FIXPOINT with widening and narrowing \*) val two_phase_fixpoint : \#a:Type -\> \| lat:lattice a \| -\> transfer:(a -\> a) -\> widen:widening_op \#a \#lat -\> narrow:narrowing_op \#a \#lat -\> a

let two_phase_fixpoint \#a \#lat transfer widen narrow = (\* PHASE 1: Upward iteration with widening \*) let rec upward x = let fx = transfer x in if lat.order fx x then x (\* Post-fixpoint! \*) else upward (widen x fx) (\* Widen and continue \*) in let post = upward lat.bottom in (\* PHASE 2: Downward iteration with narrowing \*) let rec downward x fuel = if fuel = 0 then x else let fx = transfer x in let x’ = narrow x fx in if x’ = x then x (\* Stable \*) else downward x’ (fuel - 1) (\* Narrow and continue \*) in downward post 3 (\* 3 iterations typically sufficient \*)

(\* INTERVAL WIDENING (Cousot 1992, Equation 18) \*) let interval_widen : widening_op \#interval \#interval_lattice = fun x y -\> match x, y with \| IBot, y -\> y \| x, IBot -\> x \| IRange l0 u0, IRange l1 u1 -\> (\* Lower: if y goes below, jump to -infinity or 0 \*) let l’ = if bound_geq l1 (Finite 0) && bound_lt l1 l0 then Finite 0 else if bound_lt l1 l0 then NegInf else l0 in (\* Upper: if y goes above, jump to +infinity or 0 \*) let u’ = if bound_lt u0 u1 && bound_leq u1 (Finite 0) then Finite 0 else if bound_lt u0 u1 then PosInf else u0 in IRange l’ u’

(\* INTERVAL NARROWING (Cousot 1992, Equation 19) \*) let interval_narrow : narrowing_op \#interval \#interval_lattice = fun x y -\> match x, y with \| IBot, \_ -\> IBot \| \_, IBot -\> IBot \| IRange l0 u0, IRange l1 u1 -\> let l’ = if l0 = NegInf then l1 else l0 in let u’ = if u0 = PosInf then u1 else u0 in IRange l’ u’

</div>

<div class="tcolorbox">

<div class="center">

|           | **Widening (This section)** | **Bounded Unrolling (Section 4.3)** |
|:----------|:----------------------------|:------------------------------------|
| Type      | OVER-approximation          | UNDER-approximation                 |
| Sound for | ABSENCE                     | PRESENCE                            |
| Proves    | “no bugs exist”             | “bugs exist”                        |
| May have  | False positives             | False negatives                     |
| Always    | Terminates                  | Terminates                          |

</div>

**When to Use:**

- Widening: Proving safety properties (verification)

- Unrolling: Finding bugs (bug detection)

- Both: Hybrid analysis (Section 4.3.3)

**Mathematical Justification:**

- Widening: Ensures chain condition (termination)

- Unrolling: Fuel-bounded exploration (termination)

- Both are SOUND for their respective purposes

</div>

### Probabilistic Abstract Domains (Cousot 2012)

**Paper:** Cousot & Monerau 2012 (Probabilistic Abstract Interpretation)

<div class="tcolorbox">

A probabilistic program is a measurable function $`S_p\llbracket P \rrbracket : \Omega \to D`$ where:

- $`\Omega`$ = probability space (scenarios)

- $`D`$ = standard semantics domain (traces, states, etc.)

- $`\mu`$ = probability measure on $`\Omega`$

This separation enables LIFTING existing analyses to probabilistic setting.

</div>

<div class="definition">

**Definition 1.21** (The Three Abstraction Axes). **(I) Abstract the SEMANTICS** ($`D \to A`$):

- Apply any classical abstraction (intervals, octagons, etc.)

- Lifts pointwise: $`\alpha(S_p\llbracket P \rrbracket) = \lambda\omega.\, \alpha(S_p\llbracket P \rrbracket(\omega))`$

- Result: probabilistic abstract semantics

**(II) Abstract the SCENARIO SPACE** ($`\Omega \to \Omega'`$):

- Merge scenarios: surjective $`q : \Omega \to \Omega'`$

- Non-determinism as abstraction: forget probability info

- SAFE ABSTRACTION: $`\Omega' = \{\bullet\}`$ (singleton) recovers classical AI!

- Key theorem: Classical abstract interpretation = $`\Omega`$-abstraction to singleton

**(III) Abstract by DISTRIBUTIONS** ($`S_p\llbracket P \rrbracket \to \mathsf{Law}`$):

- Keep only the probability distribution, not the function

- Order on laws: $`\nu \sqsubseteq \nu'`$ iff $`\forall Q \in A.\; \nu(\downarrow Q) \geq \nu'(\downarrow Q)`$

- More precise law = more probability on precise properties

</div>

<div class="remark">

**Remark 1.22** (Connection to Standard AI). When $`\Omega = \{\bullet\}`$ (singleton), probabilistic AI reduces EXACTLY to classical AI. This is because:

- Only probabilities are 0 and 1

- The $`\sqsubseteq`$ order on laws becomes $`\sqsubseteq`$ on abstract states

- All machinery collapses to standard fixpoint iteration

</div>

<div class="fstarcode">

(\* Definition 1: Probabilistic Semantics A probabilistic semantics S_p\[\[P\]\] in D_p = Omega -\> D is a measurable function from a probability space (Omega, E, mu) into a semantics domain D. Meaning: when scenario omega is picked (randomly according to mu), the execution of program P yields the (non)-deterministic semantics S_p\[\[P\]\](omega) in D. \*)

(\* Probability space components \*) type prob_space (omega : Type) = events : omega -\> bool; (\* sigma-algebra of observable events \*) measure : (omega -\> bool) -\> real; (\* Probability measure mu \*) measure_empty : measure (fun \_ -\> false) == 0.0; measure_full : measure (fun \_ -\> true) == 1.0; measure_countable_additive : (\* mu(union E_i) = sum mu(E_i) for disjoint E_i \*) forall (es : nat -\> (omega -\> bool)). pairwise_disjoint es ==\> measure (countable_union es) == series (fun i -\> measure (es i));

(\* Probabilistic semantics: measurable function from Omega to D \*) type prob_semantics (omega : Type) (d : Type) = omega -\> d

(\* Definition 2: Probability of a program property The probability that program P has property Phi in O is: Pr(S_p\[\[P\]\] in Phi) = mu(omega in Omega \| S_p\[\[P\]\](omega) in Phi) \*) val property_probability : \#omega:Type -\> \#d:Type -\> prob_space omega -\> prob_semantics omega d -\> (d -\> bool) -\> (\* Property Phi \*) real (\* Probability Pr(S_p\[\[P\]\] in Phi) \*) let property_probability \#omega \#d ps sem prop = ps.measure (fun w -\> prop (sem w))

(\* AXIS I: Semantics Abstraction \*) val lift_abstraction : \#c:Type -\> \#a:Type -\> \| lc:lattice c \| -\> \| la:lattice a \| -\> galois_connection c a -\> galois_connection (prob_semantics omega c) (prob_semantics omega a) let lift_abstraction \#c \#a \#lc \#la gc = alpha = fun sem -\> (fun w -\> gc.alpha (sem w)); (\* Pointwise lift \*) gamma = fun sem -\> (fun w -\> gc.gamma (sem w)); (\* Galois law preserved pointwise \*)

(\* AXIS II: Scenario Space Abstraction Key insight: Classical AI = abstraction to singleton Omega’ = \* \*) val scenario_abstraction : \#omega:Type -\> \#omega’:Type -\> \#a:Type -\> \| la:lattice a \| -\> (omega -\> omega’) -\> (\* Surjective quotient q \*) prob_semantics omega a -\> prob_semantics omega’ a let scenario_abstraction \#omega \#omega’ \#a \#la q sem = fun w’ -\> la.join_set (fun w -\> q w == w’) (fun w -\> sem w) (\* Join all semantics for scenarios mapping to w’ \*)

(\* SAFE ABSTRACTION: Forget all probabilistic information Result is classical abstract interpretation! \*) type singleton = \| Point val safe_abstraction : \#omega:Type -\> \#a:Type -\> \| la:lattice a \| -\> prob_semantics omega a -\> a (\* Single abstract value = classical AI \*) let safe_abstraction \#omega \#a \#la sem = la.join_set (fun \_ -\> true) sem (\* Join ALL scenarios \*)

(\* AXIS III: Distribution Abstraction (Laws) \*) (\* A law is a probability distribution on abstract domain A \*) type law (a : Type) = (a -\> bool) -\> real (\* Phi \|-\> Pr(in Phi) \*)

(\* Order on laws: nu nu’ iff more probability on precise values \*) val law_order : \#a:Type -\> \| la:lattice a \| -\> law a -\> law a -\> bool let law_order \#a \#la nu nu’ = forall (q : a). nu (downset q) \>= nu’ (downset q) where downset q = fun x -\> la.leq x q

(\* Abstraction from semantics to laws \*) val law_abstraction : \#omega:Type -\> \#a:Type -\> prob_space omega -\> prob_semantics omega a -\> law a let law_abstraction \#omega \#a ps sem = fun prop -\> ps.measure (fun w -\> prop (sem w))

</div>

<div class="remark">

**Remark 1.23** (Probabilistic Transfer Functions and Loops). **Conditionals with Unknown Branch Probability:**

When $`\Pr(b~\text{true}) = p_b`$ is uncertain, $`p_b \in P_b \subseteq [0,1]`$:
``` math
\llbracket \texttt{if } b \texttt{ then } C_1 \texttt{ else } C_2 \rrbracket(l_s)(\Phi) =
    p_b \times \Pr(\llbracket C_1 \rrbracket(s) \in \Phi \mid b) + (1-p_b) \times \Pr(\llbracket C_2 \rrbracket(s) \in \Phi \mid \lnot b)
```

**Loops with Probabilistic Iteration Count:**

When $`p_{\mathsf{loop}}(i) = \Pr(\text{exactly } i \text{ iterations})`$ is known:
``` math
\llbracket \texttt{while } b \texttt{ do } C \rrbracket(l_s)(\Phi) = \sum_{i \geq 0} p_{\mathsf{loop}}(i) \times \Pr(\llbracket S \rrbracket(s) \in \Phi \mid i \text{ iterations})
```
Ad-hoc unrolling: unroll $`N`$ times, bound remainder by $`p_{\mathsf{loop}}(N)`$.

</div>

<div class="example">

**Example 1.24** (Application: Branch Prediction for JIT Compilers). **Probabilistic CFG Abstraction (Example 10, Cousot 2012):**

From trace semantics to control flow graph with branch probabilities:
``` math
\Pr\langle c,c'\rangle|c = \Pr(\mathsf{succ}(c,c') \mid \mathsf{reach}(c))
```
This is the conditional probability of taking edge $`c \to c'`$ given control at $`c`$.

Applications:

1.  Register allocation guided by hot paths

2.  Cache/scratchpad allocation

3.  Branch prediction without profiling

</div>

<div class="remark">

**Remark 1.25** (Connection to Markov Chain Analysis (Section 8.1)). Every probabilistic program induces a Markov chain on states. The transition matrix $`[\mathsf{succ}(s,s')]_{s,s' \in \Sigma}`$ captures same steady-state behavior. This abstraction:

- Forgets execution history (Markov property)

- Enables model checking techniques (PRISM, etc.)

- Connects to probabilistic temporal logic (PCTL, LTL)

</div>

See Section 12.38 for formal soundness theorems.

**Cross-References:**

- For unified over/under approximation applicable to probabilistic programs, see Section 2.1.8b (Local Completeness Logic).

- Probabilistic programs can use $`\mathsf{LCL}_A`$ with probabilistic abstract domains for combined correctness/incorrectness analysis.

- The SAFE abstraction ($`\Omega`$ to singleton) connects probabilistic AI to standard dual soundness (Section 2.1.8).

### Concrete Abstract Domains

The following sections define specific abstract domains used in the brrr-machine. Each domain is a complete lattice with appropriate transfer functions for the analysis operations it supports.

<div class="definition">

**Definition 1.26** (Interval Domain). The interval domain tracks integer bounds $`[l, u]`$ where $`l \leq u`$. The ordering is subset inclusion: $`[l_1, u_1] \sqsubseteq [l_2, u_2]`$ iff $`l_2 \leq l_1`$ and $`u_1 \leq u_2`$. The domain forms a complete lattice with:

- $`\bot`$: empty interval (unreachable code)

- $`\top`$: $`[-\infty, +\infty]`$ (any integer)

- $`\sqcup`$: hull of intervals

- $`\sqcap`$: intersection of intervals

Because the interval lattice has infinite ascending chains (e.g., $`[0,1] \sqsubset [0,2] \sqsubset \ldots`$), **widening is essential** for termination.

</div>

The F\* code below implements the interval domain. Note the widening operator extrapolates to infinity when bounds grow, ensuring termination. The narrowing operator can then recover precision using loop guards.

<div class="fstarcode">

module BrrrMachine.Domains open BrrrMachine.AbstractDomain

(\* INTERVAL DOMAIN - For numeric bounds analysis Source: Cousot & Cousot 1977 Used for: - Array bounds checking - Integer overflow detection - Loop bound analysis \*)

type bound = \| NegInf : bound \| Finite : n:int -\> bound \| PosInf : bound

let bound_leq (b1 b2 : bound) : bool = match b1, b2 with \| NegInf, \_ -\> true \| \_, PosInf -\> true \| Finite n1, Finite n2 -\> n1 \<= n2 \| \_, \_ -\> false

type interval = \| IBot : interval (\* Empty - unreachable \*) \| IRange : lo:bound -\> hi:bound -\> interval (\* \[lo, hi\] \*)

let interval_leq (i1 i2 : interval) : bool = match i1, i2 with \| IBot, \_ -\> true \| \_, IBot -\> false \| IRange lo1 hi1, IRange lo2 hi2 -\> bound_leq lo2 lo1 && bound_leq hi1 hi2 (\* \[lo1,hi1\] \*)

let interval_join (i1 i2 : interval) : interval = match i1, i2 with \| IBot, i -\> i \| i, IBot -\> i \| IRange lo1 hi1, IRange lo2 hi2 -\> IRange (bound_min lo1 lo2) (bound_max hi1 hi2)

let interval_meet (i1 i2 : interval) : interval = match i1, i2 with \| IBot, \_ -\> IBot \| \_, IBot -\> IBot \| IRange lo1 hi1, IRange lo2 hi2 -\> let lo = bound_max lo1 lo2 in let hi = bound_min hi1 hi2 in if bound_leq lo hi then IRange lo hi else IBot

(\* Widening: extrapolate to infinity when bounds grow \*) let interval_widen (i1 i2 : interval) : interval = match i1, i2 with \| IBot, i -\> i \| i, IBot -\> i \| IRange lo1 hi1, IRange lo2 hi2 -\> let lo’ = if bound_leq lo2 lo1 then lo1 else NegInf in let hi’ = if bound_leq hi1 hi2 then hi1 else PosInf in IRange lo’ hi’

(\* Narrowing: use condition to refine \*) let interval_narrow (i1 i2 : interval) : interval = match i1, i2 with \| IBot, \_ -\> IBot \| \_, IBot -\> IBot \| IRange lo1 hi1, IRange lo2 hi2 -\> let lo’ = if lo1 = NegInf then lo2 else lo1 in let hi’ = if hi1 = PosInf then hi2 else hi1 in IRange lo’ hi’

instance interval_domain : abstract_domain interval = lat = po = leq = interval_leq; (\* proofs omitted for brevity \*) ; bot = IBot; top = IRange NegInf PosInf; join = interval_join; meet = interval_meet; join_set = (\* ... \*); meet_set = (\* ... \*); (\* proofs omitted \*) ; widen = Some interval_widen; narrow = Some interval_narrow; (\* proofs omitted \*)

(\* Arithmetic transfer functions on intervals \*) let interval_add (i1 i2 : interval) : interval = match i1, i2 with \| IBot, \_ \| \_, IBot -\> IBot \| IRange lo1 hi1, IRange lo2 hi2 -\> IRange (bound_add lo1 lo2) (bound_add hi1 hi2)

let interval_mul (i1 i2 : interval) : interval = match i1, i2 with \| IBot, \_ \| \_, IBot -\> IBot \| IRange lo1 hi1, IRange lo2 hi2 -\> (\* Take min/max of all corner products \*) let corners = \[ bound_mul lo1 lo2; bound_mul lo1 hi2; bound_mul hi1 lo2; bound_mul hi1 hi2 \] in IRange (list_min corners) (list_max corners)

</div>

<div class="definition">

**Definition 1.27** (Taint Domain). The taint domain tracks whether values originate from untrusted sources (user input, network, etc.). It forms a diamond lattice with four elements:

<div class="center">

</div>

**Critical insight:** The `TMaybe` element breaks the Galois insertion property. This means taint analysis can prove the *presence* of bugs (true positives) but *cannot* prove their absence. See the code comments for detailed explanation.

</div>

<div class="fstarcode">

(\* TAINT DOMAIN Source: Denning 1977 (information flow), Livshits 2005 (taint tracking) Used for: SQL injection, XSS, command injection, path traversal

KEY INSIGHT: Taint analysis is a security analysis that tracks "trust level" of data through the program. Data from untrusted sources (user input) is TTainted; data from trusted sources is TUntainted. A bug exists when TTainted data reaches a sensitive sink. \*)

type taint_level = \| TBot (\* Unreachable \*) \| TUntainted (\* Known safe - from trusted sources \*) \| TMaybe (\* Unknown - could be either \*) \| TTainted (\* Definitely tainted - from untrusted sources \*)

(\* Taint forms a diamond lattice: TTainted /  TUntainted TMaybe  / TBot \*)

let taint_leq (t1 t2 : taint_level) : bool = match t1, t2 with \| TBot, \_ -\> true \| \_, TTainted -\> true \| TUntainted, TUntainted -\> true \| TUntainted, TMaybe -\> true \| TMaybe, TMaybe -\> true \| \_, \_ -\> false

let taint_join (t1 t2 : taint_level) : taint_level = match t1, t2 with \| TBot, t \| t, TBot -\> t \| TTainted, \_ \| \_, TTainted -\> TTainted \| TUntainted, TUntainted -\> TUntainted \| \_, \_ -\> TMaybe

let taint_meet (t1 t2 : taint_level) : taint_level = match t1, t2 with \| TTainted, t \| t, TTainted -\> t \| TBot, \_ \| \_, TBot -\> TBot \| TUntainted, TUntainted -\> TUntainted \| \_, \_ -\> TUntainted (\* Conservative: meet with unknown is safe \*)

instance taint_domain : abstract_domain taint_level = lat = po = leq = taint_leq; (\* ... \*) ; bot = TBot; top = TTainted; join = taint_join; meet = taint_meet; (\* ... \*) ; widen = None; (\* Finite domain, no widening needed \*) narrow = None;

(\* IMPORTANT: TMaybe BREAKS GALOIS INSERTION The taint lattice with TMaybe does NOT form a Galois insertion because: - gamma(TMaybe) = "all concrete states" (we don’t know) - alpha(gamma(TMaybe)) = TTainted (abstracting "all" gives tainted) - But TMaybe != TTainted, so alpha . gamma != id

CONSEQUENCE: Taint analysis can only prove INCORRECTNESS (bugs exist), not correctness (no bugs). This aligns with the dual soundness principle (Section 2.1.8): use over-approximation for safety, under-approximation for bug-finding. Taint findings are REAL bugs, not false positives, but absence of findings does NOT guarantee safety. \*)

(\* Taint propagation: any tainted input taints output \*) let taint_propagate (inputs : list taint_level) : taint_level = List.fold_left taint_join TBot inputs

</div>

<div class="fstarcode">

(\* NULLABILITY DOMAIN Used for: Null dereference, optional value tracking, uninitialized vars \*)

type nullability = \| NBot (\* Unreachable \*) \| NNull (\* Definitely null \*) \| NNonNull (\* Definitely not null \*) \| NMaybe (\* Could be either - TOP \*)

let nullability_leq (n1 n2 : nullability) : bool = match n1, n2 with \| NBot, \_ -\> true \| \_, NMaybe -\> true \| NNull, NNull -\> true \| NNonNull, NNonNull -\> true \| \_, \_ -\> false

let nullability_join (n1 n2 : nullability) : nullability = match n1, n2 with \| NBot, n \| n, NBot -\> n \| NNull, NNull -\> NNull \| NNonNull, NNonNull -\> NNonNull \| \_, \_ -\> NMaybe

instance nullability_domain : abstract_domain nullability = lat = bot = NBot; top = NMaybe; join = nullability_join; meet = nullability_meet; (\* ... \*) ; widen = None; narrow = None;

(\* After null check: refine nullability \*) let after_null_check (n : nullability) (is_null_branch : bool) : nullability = match n, is_null_branch with \| NMaybe, true -\> NNull (\* In "if x == null" branch, x is null \*) \| NMaybe, false -\> NNonNull (\* In "if x != null" branch, x is non-null \*) \| \_, \_ -\> n

</div>

<div class="fstarcode">

(\* RESOURCE STATE DOMAIN Source: theory.md Chapter 14 Used for: Use-after-free, double-free, resource leaks, file handles \*)

type resource_state = \| RSBot (\* Unreachable \*) \| RSUnacquired (\* Not yet acquired \*) \| RSAcquired (\* Acquired, ready for use \*) \| RSInUse (\* Currently being used \*) \| RSReleased (\* Released, cannot be used \*) \| RSTop (\* Unknown state \*)

(\* State machine transitions: Unacquired –acquire–\> Acquired –use–\> InUse \| \| \| \| v v Released \<–release–+

Violations: - RSReleased -\> use = UseAfterFree - RSReleased -\> release = DoubleFree - RSAcquired at scope exit = Leak \*)

let resource_leq (r1 r2 : resource_state) : bool = match r1, r2 with \| RSBot, \_ -\> true \| \_, RSTop -\> true \| RSUnacquired, RSUnacquired -\> true \| RSAcquired, RSAcquired -\> true \| RSInUse, RSInUse -\> true \| RSReleased, RSReleased -\> true \| \_, \_ -\> false

let resource_join (r1 r2 : resource_state) : resource_state = match r1, r2 with \| RSBot, r \| r, RSBot -\> r \| RSTop, \_ \| \_, RSTop -\> RSTop \| r1, r2 when r1 = r2 -\> r1 \| \_, \_ -\> RSTop (\* Different states merge to unknown \*)

(\* Transfer functions for resource operations \*) let resource_acquire (r : resource_state) : resource_state \* option resource_error = match r with \| RSBot -\> (RSBot, None) \| RSUnacquired -\> (RSAcquired, None) \| RSAcquired -\> (RSTop, Some DoubleAcquire) \| RSInUse -\> (RSTop, Some DoubleAcquire) \| RSReleased -\> (RSAcquired, None) (\* Re-acquire after release is OK \*) \| RSTop -\> (RSTop, None)

let resource_use (r : resource_state) : resource_state \* option resource_error = match r with \| RSBot -\> (RSBot, None) \| RSUnacquired -\> (RSTop, Some UseBeforeAcquire) \| RSAcquired -\> (RSInUse, None) \| RSInUse -\> (RSInUse, None) \| RSReleased -\> (RSTop, Some UseAfterRelease) \| RSTop -\> (RSTop, None)

let resource_release (r : resource_state) : resource_state \* option resource_error = match r with \| RSBot -\> (RSBot, None) \| RSUnacquired -\> (RSTop, Some ReleaseBeforeAcquire) \| RSAcquired -\> (RSReleased, None) \| RSInUse -\> (RSReleased, None) \| RSReleased -\> (RSTop, Some DoubleRelease) \| RSTop -\> (RSTop, None)

type resource_error = \| UseBeforeAcquire \| UseAfterRelease \| DoubleAcquire \| DoubleRelease \| ReleaseBeforeAcquire \| LeakOnExit

instance resource_domain : abstract_domain resource_state = lat = bot = RSBot; top = RSTop; join = resource_join; (\* ... \*) ; widen = None; narrow = None;

</div>

<div class="fstarcode">

(\* OWNERSHIP DOMAIN Source: Girard 1987 (substructural logic), Jung 2018 (Iris/RustBelt) Used for: Ownership tracking (Rust-style AFFINE), borrow checking, move detection

NOTE: Rust uses AFFINE typing (use at most once, can drop without use). Strictly LINEAR requires exactly one use. We model the affine fragment. \*)

type ownership = \| OBot (\* Unreachable \*) \| OUnowned (\* Not owned by anyone \*) \| OOwned (\* Exclusively owned \*) \| OBorrowed (\* Temporarily borrowed (immutable) \*) \| OMutBorrowed (\* Temporarily borrowed (mutable) \*) \| OShared (\* Reference-counted / shared \*) \| OMoved (\* Ownership transferred away \*) \| OTop (\* Unknown \*)

(\* Ownership rules (from RustBelt): - Owned can be moved or borrowed - Borrowed can be cloned (shared borrow) - MutBorrowed is exclusive - Moved cannot be used \*)

let ownership_leq (o1 o2 : ownership) : bool = match o1, o2 with \| OBot, \_ -\> true \| \_, OTop -\> true \| o1, o2 when o1 = o2 -\> true \| \_, \_ -\> false

let ownership_join (o1 o2 : ownership) : ownership = match o1, o2 with \| OBot, o \| o, OBot -\> o \| OTop, \_ \| \_, OTop -\> OTop \| o1, o2 when o1 = o2 -\> o1 \| OOwned, OBorrowed \| OBorrowed, OOwned -\> OOwned (\* Borrow ends \*) \| \_, \_ -\> OTop

(\* Transfer functions \*) let ownership_move (o : ownership) : ownership \* option ownership_error = match o with \| OOwned -\> (OMoved, None) \| OMoved -\> (OTop, Some UseAfterMove) \| OBorrowed -\> (OTop, Some MoveWhileBorrowed) \| OMutBorrowed -\> (OTop, Some MoveWhileBorrowed) \| \_ -\> (OTop, None)

let ownership_borrow (o : ownership) : ownership \* option ownership_error = match o with \| OOwned -\> (OBorrowed, None) \| OBorrowed -\> (OBorrowed, None) (\* Can borrow from borrow \*) \| OMutBorrowed -\> (OTop, Some BorrowWhileMutBorrowed) \| OMoved -\> (OTop, Some UseAfterMove) \| \_ -\> (OTop, None)

let ownership_mut_borrow (o : ownership) : ownership \* option ownership_error = match o with \| OOwned -\> (OMutBorrowed, None) \| OBorrowed -\> (OTop, Some MutBorrowWhileBorrowed) \| OMutBorrowed -\> (OTop, Some MutBorrowWhileMutBorrowed) \| OMoved -\> (OTop, Some UseAfterMove) \| \_ -\> (OTop, None)

type ownership_error = \| UseAfterMove \| MoveWhileBorrowed \| BorrowWhileMutBorrowed \| MutBorrowWhileBorrowed \| MutBorrowWhileMutBorrowed

</div>

### Occurrence Type Domain

**Source:** Tobin-Hochstadt & Felleisen 2008 (Typed Scheme)

<div class="tcolorbox">

**Problem:** Union types lose precision after type tests.

``` python
def process(x: int | str) -> int:
  if isinstance(x, str):
    return len(x)    # Type of x should be str here, not int|str!
  return x           # Type of x should be int here
```

**Solution:** Track TYPE PROPOSITIONS through control flow.

- Type tests generate predicates about variables

- Branch conditions refine types in each branch

- Join points combine refinements conservatively

**Complements Gradual Typing (Section 9.1.2):**

- Gradual typing: `?` type at module BOUNDARIES

- Occurrence typing: Refinement WITHIN module from type tests

- Together: Sound dynamic + static typing in same codebase

</div>

<div class="definition">

**Definition 1.28** (Key Concepts from Tobin-Hochstadt 2008). **Visible vs Latent Predicates:**

- **Visible:** Type proposition known to hold at current point.  
  Example: After `if isinstance(x, str):` we have visible predicate $`x:\mathsf{str}`$

- **Latent:** Type proposition attached to a function result.  
  Example: `string?(v)` returns true implies $`v`$ has type $`\mathsf{str}`$. The function “remembers” what type test it performed.

**Propositional Type Environments (Section 3.2):**
``` math
\begin{aligned}
  \Gamma &= \{x : \tau, \ldots\} \quad \text{(Standard type assignment)} \\
  \Gamma|\psi &= \text{Environment refined by proposition } \psi \\
  \psi &::= \tau(x) \mid \lnot\tau(x) \mid \psi_1 \land \psi_2 \mid \psi_1 \lor \psi_2 \mid \mathsf{true} \mid \mathsf{false}
\end{aligned}
```

**Environment Refinement Operations:**
``` math
\begin{aligned}
  \Gamma + \psi &= \text{Environment assuming } \psi \text{ is TRUE} \\
  \Gamma + \tau(x) &= \Gamma[x := \Gamma(x) \cap \tau] \quad \text{(Restrict } x \text{ to } \tau\text{)} \\
  \Gamma + \lnot\tau(x) &= \Gamma[x := \Gamma(x) - \tau] \quad \text{(Remove } \tau \text{ from } x\text{)} \\
  \Gamma - \psi &= \Gamma + \lnot\psi \quad \text{(Environment assuming } \psi \text{ is FALSE)}
\end{aligned}
```

**Type Operations:**
``` math
\begin{aligned}
  \mathsf{restrict}(\sigma, \tau) &= \sigma \cap \tau \quad \text{(Narrow } \sigma \text{ to } \tau\text{)} \\
  \mathsf{remove}(\sigma, \tau) &= \sigma - \tau \quad \text{(Remove } \tau \text{ from } \sigma\text{)}
\end{aligned}
```

</div>

<div class="fstarcode">

module BrrrMachine.Domains.Occurrence

(\* TYPE PROPOSITIONS Propositions express facts about the types of variables. \*) type type_prop = \| PropHasType : var:string -\> ty:ir_type -\> type_prop (\* var has type ty \*) \| PropNotType : var:string -\> ty:ir_type -\> type_prop (\* var does NOT have type ty \*) \| PropAnd : type_prop -\> type_prop -\> type_prop \| PropOr : type_prop -\> type_prop -\> type_prop \| PropTrue : type_prop \| PropFalse : type_prop (\* Contradiction - unreachable \*)

(\* Negate a proposition \*) let rec negate_prop (p : type_prop) : type_prop = match p with \| PropHasType v t -\> PropNotType v t \| PropNotType v t -\> PropHasType v t \| PropAnd p1 p2 -\> PropOr (negate_prop p1) (negate_prop p2) \| PropOr p1 p2 -\> PropAnd (negate_prop p1) (negate_prop p2) \| PropTrue -\> PropFalse \| PropFalse -\> PropTrue

(\* VISIBLE AND LATENT PREDICATES \*) type visible_predicate = prop : type_prop; source : node_id; (\* Where predicate was established \*)

type latent_predicate = positive : type_prop; (\* Proposition if function returns true \*) negative : type_prop; (\* Proposition if function returns false \*) target_var : option string; (\* Variable the predicate applies to \*)

(\* Type predicates built into languages \*) let builtin_predicates : map string latent_predicate = Map.of_list \[ (\* Python \*) ("isinstance", positive = PropTrue; negative = PropTrue; target_var = None ); (\* TypeScript \*) ("typeof", positive = PropTrue; negative = PropTrue; target_var = None ); (\* Go \*) ("type_assertion", positive = PropTrue; negative = PropTrue; target_var = None ); \]

(\* TYPE RESTRICTION AND REMOVAL \*) (\* restrict(sigma, tau) = sigma intersect tau \*) let rec restrict (sigma : ir_type) (tau : ir_type) : ir_type = match sigma with \| TUnion types -\> let restricted = List.filter_map (fun t -\> let r = restrict t tau in if is_bottom r then None else Some r ) types in (match restricted with \| \[\] -\> TBottom \| \[t\] -\> t \| ts -\> TUnion ts) \| \_ -\> if subtype sigma tau then sigma else if subtype tau sigma then tau else TBottom

(\* remove(sigma, tau) = sigma minus tau \*) let rec remove (sigma : ir_type) (tau : ir_type) : ir_type = match sigma with \| TUnion types -\> let remaining = List.filter_map (fun t -\> let r = remove t tau in if is_bottom r then None else Some r ) types in (match remaining with \| \[\] -\> TBottom \| \[t\] -\> t \| ts -\> TUnion ts) \| \_ -\> if subtype sigma tau then TBottom else sigma

</div>

<div class="fstarcode">

(\* Maps variables to types, refined by propositions. \*) type prop_type_env = bindings : map string ir_type; active_props : list visible_predicate;

(\* Refine environment assuming proposition is TRUE: Gamma + psi \*) let rec refine_positive (env : prop_type_env) (prop : type_prop) : prop_type_env = match prop with \| PropHasType var ty -\> (match Map.find var env.bindings with \| Some current_ty -\> let refined = restrict current_ty ty in env with bindings = Map.add var refined env.bindings \| None -\> env) \| PropNotType var ty -\> (match Map.find var env.bindings with \| Some current_ty -\> let refined = remove current_ty ty in env with bindings = Map.add var refined env.bindings \| None -\> env) \| PropAnd p1 p2 -\> refine_positive (refine_positive env p1) p2 \| PropOr p1 p2 -\> (\* Conservative: only refine if BOTH branches agree \*) let env1 = refine_positive env p1 in let env2 = refine_positive env p2 in join_environments env1 env2 \| PropTrue -\> env \| PropFalse -\> (\* Unreachable - mark all types as bottom \*) env with bindings = Map.map (fun \_ -\> TBottom) env.bindings

(\* Refine environment assuming proposition is FALSE: Gamma - psi \*) let refine_negative (env : prop_type_env) (prop : type_prop) : prop_type_env = refine_positive env (negate_prop prop)

(\* Join two environments at control flow merge \*) let join_environments (env1 env2 : prop_type_env) : prop_type_env = let bindings = Map.merge (fun \_ t1 t2 -\> match t1, t2 with \| Some t1, Some t2 -\> Some (type_join t1 t2) \| Some t, None \| None, Some t -\> Some t \| None, None -\> None ) env1.bindings env2.bindings in bindings; active_props = \[\] (\* Props lost at join \*)

</div>

<div class="fstarcode">

(\* Detect type-testing patterns in code. \*) type type_test_result = tested_var : string; test_type : ir_type; is_positive : bool; (\* true = "is type", false = "is not type" \*)

(\* Detect isinstance(x, T) pattern in Python \*) let detect_isinstance (call : ir_expr) : option type_test_result = match call with \| ECall (EVar "isinstance") \[EVar var; EType ty\] -\> Some tested_var = var; test_type = ty; is_positive = true \| \_ -\> None

(\* Detect typeof x === "T" pattern in TypeScript/JavaScript \*) let detect_typeof (expr : ir_expr) : option type_test_result = match expr with \| EBinOp OpEq (ECall (EVar "typeof") \[EVar var\]) (EString ty_name) -\> let ty = string_to_type ty_name in Some tested_var = var; test_type = ty; is_positive = true \| EBinOp OpNeq (ECall (EVar "typeof") \[EVar var\]) (EString ty_name) -\> let ty = string_to_type ty_name in Some tested_var = var; test_type = ty; is_positive = false \| \_ -\> None

(\* Detect x !== null / x !== undefined pattern \*) let detect_null_check (expr : ir_expr) : option type_test_result = match expr with \| EBinOp OpNeq (EVar var) ENull -\> Some tested_var = var; test_type = TNull; is_positive = false \| EBinOp OpNeq (EVar var) EUndefined -\> Some tested_var = var; test_type = TUndefined; is_positive = false \| EBinOp OpEq (EVar var) ENull -\> Some tested_var = var; test_type = TNull; is_positive = true \| \_ -\> None

(\* Detect "key" in obj pattern for TypeScript discriminated unions \*) let detect_in_check (expr : ir_expr) : option type_test_result = match expr with \| EBinOp OpIn (EString key) (EVar var) -\> (\* Having key means var has type with that property \*) Some tested_var = var; test_type = TWithProperty key; is_positive = true \| \_ -\> None

</div>

<div class="fstarcode">

(\* OCCURRENCE TYPING TRANSFER FUNCTION \*) val occurrence_transfer : prop_type_env -\> ir_stmt -\> prop_type_env

let occurrence_transfer env stmt = match stmt with \| SIf cond then_branch else_branch -\> (\* Detect type test in condition \*) let test = detect_type_test cond in (match test with \| Some tested_var; test_type; is_positive -\> let true_prop = if is_positive then PropHasType tested_var test_type else PropNotType tested_var test_type in (\* Refine environments for each branch \*) let then_env = refine_positive env true_prop in let else_env = refine_negative env true_prop in (\* Join at merge point \*) join_environments then_env else_env \| None -\> env) \| SAssign var expr -\> (\* Assignment overwrites type - no longer refined \*) env with bindings = Map.add var (infer_type expr) env.bindings; active_props = List.filter (fun p -\> not (prop_mentions_var p.prop var) ) env.active_props \| \_ -\> env

(\* INTEGRATION WITH ABSTRACT INTERPRETATION \*) type occurrence_domain = env : prop_type_env;

let occurrence_lattice : lattice occurrence_domain = bot = env = bindings = Map.empty; active_props = \[\] ; top = env = bindings = Map.singleton "\_" TTop; active_props = \[\] ; leq = fun d1 d2 -\> Map.for_all (fun v t1 -\> match Map.find v d2.env.bindings with \| Some t2 -\> subtype t1 t2 \| None -\> true ) d1.env.bindings; join = fun d1 d2 -\> env = join_environments d1.env d2.env ; meet = fun d1 d2 -\> env = bindings = Map.merge (fun \_ t1 t2 -\> match t1, t2 with \| Some t1, Some t2 -\> Some (type_meet t1 t2) \| \_ -\> None ) d1.env.bindings d2.env.bindings; active_props = d1.env.active_props @ d2.env.active_props ;

</div>

**Cross-References:**

- Section 9.1.2: Occurrence typing complements gradual typing at boundaries

- Section 9.3: Guards can use occurrence typing propositions

- Section 9.5: Full occurrence typing analysis algorithm

- Section 12.3.5: F\* soundness theorem for occurrence typing

### DeepPoly: Abstract Domain for Neural Networks

**Paper:** Singh, Gehr, Puschel, Vechev 2019 (POPL)

<div class="tcolorbox">

**Problem:** Neural networks in safety-critical systems (autonomous driving, medical diagnosis) require robustness certification. Prior methods (SMT, MILP) cannot scale beyond $`\sim`$<!-- -->2K neurons.

**DeepPoly Insight:** Restricted polyhedral domain with backsubstitution.

- One lower bound + one upper bound per neuron (prevents blowup)

- Affine expressions relate neurons to predecessors

- Backsubstitution computes tight bounds through layers

**Abstract Element** for neuron $`x_i`$:
``` math
a_i = (a_i^{\leq}, a_i^{\geq}, l_i, u_i)
```
where:
``` math
\begin{aligned}
  a_i^{\leq}(x) \leq x_i \leq a_i^{\geq}(x) &\quad \text{(symbolic polyhedral bounds)} \\
  l_i \leq x_i \leq u_i &\quad \text{(concrete interval bounds)}
\end{aligned}
```

**Key Transformers:**

- Affine layer: EXACT (no precision loss)

- ReLU layer: Area-minimizing linear approximation (sound)

- Sigmoid/Tanh: Tangent-based linear bounds (sound)

**Performance:** 100–1000x faster than MILP, scales to 62K neurons.

</div>

<div class="remark">

**Remark 1.29** (ReLU Approximation Strategy). For $`\mathsf{ReLU}(x) = \max(0, x)`$ when interval crosses zero ($`l < 0 < u`$):

Two candidate approximations based on AREA MINIMIZATION:

- Option (b): Lower = 0, Upper = $`\lambda x + \mu`$ (area = $`0.5 \cdot u \cdot (u-l)`$)

- Option (c): Lower = $`x`$, Upper = $`\lambda x + \mu`$ (area = $`0.5 \cdot (-l) \cdot (u-l)`$)

Choose the option with SMALLER area in $`(x, \text{output})`$ plane. This geometric insight is KEY to DeepPoly’s precision.

</div>

<div class="remark">

**Remark 1.30** (Backsubstitution for Precision). To compute tight bounds for neuron $`x_i`$:

1.  Start with symbolic constraint $`a_i(x_0, \ldots, x_{i-1})`$

2.  Recursively substitute constraints for intermediate neurons

3.  Continue until only INPUT variables remain

4.  Apply concrete input bounds to get tight interval

This enables RELATIONAL reasoning across many layers without storing exponential constraints.

</div>

The following F\* code implements the DeepPoly abstract domain. The key type is `neuron_constraint`, which stores both symbolic polyhedral bounds (affine expressions relating to predecessors) and concrete interval bounds. The `relu_transform` function implements the area-minimizing approximation for ReLU neurons.

<div class="fstarcode">

(\* This module implements the DeepPoly abstract domain for neural network verification. The key insight is to maintain RELATIONAL constraints between neurons (via affine expressions) while using backsubstitution to compute tight bounds. This achieves much better precision than interval analysis while remaining tractable for large networks. \*)

module BrrrMachine.Domains.DeepPoly

(\* Affine expression: c_0 + sum(c_i \* x_i) \*) type affine_expr = const : real; coeffs : list (nat \* real); (\* (neuron_index, coefficient) \*)

(\* DeepPoly constraint for single neuron \*) type neuron_constraint = nc_lower : affine_expr; (\* Lower polyhedral bound \*) nc_upper : affine_expr; (\* Upper polyhedral bound \*) nc_lo : real; (\* Interval lower bound \*) nc_hi : real; (\* Interval upper bound \*)

(\* Network abstract state \*) type deeppoly_state = constraints : list neuron_constraint; input_bounds : list (real \* real); num_inputs : nat;

(\* Concretization: set of all concrete values satisfying constraints \*) val gamma : deeppoly_state -\> set (list real)

(\* RELU TRANSFORMER - Area-minimizing approximation \*) val relu_transform : neuron_constraint -\> neuron_constraint let relu_transform nc = if nc.nc_hi \<= 0.0 then (\* Always inactive: output = 0 \*) nc_lower = const = 0.0; coeffs = \[\] ; nc_upper = const = 0.0; coeffs = \[\] ; nc_lo = 0.0; nc_hi = 0.0 else if nc.nc_lo \>= 0.0 then (\* Always active: identity \*) nc else (\* Crossing zero: area-minimizing approximation \*) let lambda = nc.nc_hi /. (nc.nc_hi -. nc.nc_lo) in let mu = -. nc.nc_lo \*. nc.nc_hi /. (nc.nc_hi -. nc.nc_lo) in (\* Choose smaller area: compare \|u\| vs \|l\| \*) if nc.nc_hi \<= -. nc.nc_lo then (\* Option (b): lower = 0 has smaller area \*) nc_lower = const = 0.0; coeffs = \[\] ; nc_upper = scale_add_affine lambda nc.nc_upper mu; nc_lo = 0.0; nc_hi = nc.nc_hi else (\* Option (c): lower = x has smaller area \*) nc_lower = nc.nc_lower; nc_upper = scale_add_affine lambda nc.nc_upper mu; nc_lo = nc.nc_lo; nc_hi = nc.nc_hi

(\* BACKSUBSTITUTION - Compute tight bounds via recursive substitution \*) val backsubstitute : constraints:list neuron_constraint -\> expr:affine_expr -\> input_bounds:list (real \* real) -\> (real \* real) (\* Tight lower and upper bounds \*)

(\* SOUNDNESS THEOREMS \*) val affine_layer_exact : state:deeppoly_state -\> weights:matrix -\> bias:list real -\> Lemma (gamma (transform_affine state weights bias) = image (apply_affine weights bias) (gamma state))

val relu_layer_sound : state:deeppoly_state -\> Lemma (image apply_relu (gamma state) ‘subset‘ gamma (transform_relu state))

val network_certification_sound : layers:list layer -\> input_region:deeppoly_state -\> Lemma (forall x. x ‘in‘ gamma input_region ==\> network_output layers x ‘in‘ gamma (analyze layers input_region))

</div>

<div class="remark">

**Remark 1.31** (Domain Taxonomy Extension). DeepPoly introduces a new category of abstract domains:

**Domain Precision Hierarchy (extended):**
``` math
\text{Intervals} < \text{Zonotopes} < \text{DeepPoly} < \text{Full Polyhedra} < \text{MILP}
```
(fast) (slow)

DeepPoly achieves a sweet spot:

- More precise than intervals/zonotopes (relational constraints)

- Much faster than polyhedra/MILP (restricted form)

- Scales to production networks (62K+ neurons)

</div>

**Cross-References:**

- Section 2.1.6: Abstract domain formalization (DeepPoly is an instance)

- Section 2.1.7: Concrete domains (DeepPoly extends this taxonomy)

- Section 2.1.5c: Neural invariant synthesis (complementary technique)

### Dual Soundness: Over and Under Approximation

**Papers:** O’Hearn 2020 (Incorrectness Logic - historical, superseded by OL), Zilberstein 2023 (Outcome Logic)

<div class="tcolorbox">

The synthesis previously conflated these — they serve DIFFERENT purposes.

</div>

<div class="definition">

**Definition 1.32** (Verification Soundness (Over-Approximation)).
``` math
\alpha(\text{concrete}) \subseteq \text{abstract}
```
“If analysis says SAFE, it truly is safe.”

- May have FALSE POSITIVES (false alarms)

- Cannot have false negatives (missed bugs)

- Used for: proving absence of bugs, verification

This is what Cousot abstract interpretation provides.

</div>

<div class="definition">

**Definition 1.33** (Detection Soundness (Under-Approximation)).
``` math
\text{abstract} \subseteq \text{concrete}
```
“If analysis says BUG, it truly is a bug.”

- May have FALSE NEGATIVES (missed bugs)

- Cannot have false positives (false alarms)

- Used for: proving presence of bugs, bug finding

This is what O’Hearn’s Incorrectness Logic provided historically, now superseded by Outcome Logic (OL).

</div>

<div class="tcolorbox">

<div class="center">

|                 |               |                       |
|:---------------:|:-------------:|:---------------------:|
| **Proven Safe** | **Uncertain** |   **Proven Buggy**    |
|  (over-approx)  |               |    (under-approx)     |
|  No bugs exist  | May have bugs | Bug definitely exists |
| 100% confident  | Needs review  |    100% confident     |

</div>

</div>

<div class="remark">

**Remark 1.34** (Why This Matters).

1.  **Verification mode:** We want to PROVE code is safe. Use over-approximation. False positives acceptable; false negatives catastrophic.

2.  **Bug-finding mode:** We want to FIND real bugs. Use under-approximation. False negatives acceptable; false positives waste developer time.

3.  **Combined mode:** Use BOTH to classify findings into proven-safe, proven-buggy, and uncertain categories.

</div>

<div class="definition">

**Definition 1.35** (Consequence Rule Reversal). **Hoare (over-approx):**

<div class="mathpar">

{p’} C {q’}

</div>

STRONGER pre, WEAKER post

**Incorrectness (under-approx):**

<div class="mathpar">

\[p’\] C \[q’\]

</div>

WEAKER pre, STRONGER post

</div>

<div class="definition">

**Definition 1.36** (Disjunction Behavior). **Over-approx:** Must track ALL paths

<div class="mathpar">

{p_1 p_2}C{q_1 q_2}

</div>

Intersection of posts (must hold in both)

**Under-approx:** Can DROP paths

<div class="mathpar">

\[p_1 p_2\]C\[q_1 q_2\]

</div>

Union of posts (may drop one path!)

This justifies PARTIAL COVERAGE in bug-finding — sound to forget paths.

</div>

<div class="remark">

**Remark 1.37** (Integration with Outcome Logic (OL)). **Critical:** Incorrectness Logic (IL) is incompatible with abstract interpretation (Ascari et al. 2022). OL fixes this while preserving IL’s true-positive property.

**Why OL Over IL:**

1.  IL breaks abstraction — reverse consequence rule + abstract AI don’t compose

2.  IL’s “One-Sided If” generates imprecise preconditions

3.  IL can’t handle probabilistic programs

**OL provides:**

- Unified framework for correctness AND incorrectness

- Compatible with abstract interpretation

- Manifest/latent bug classification (see Section 12.5)

- Falsification completeness (Theorem 5.1)

Use OL as the foundation for dual-mode analysis.

</div>

See Section 12.4 for formal theorems and Section 12.5 for manifest error detection.

### Local Completeness Logic (Bruni et al. 2023)

**Paper:** Bruni, Giacobazzi, Gori, Ranzato 2023 (A Correctness and Incorrectness Program Logic)

<div class="tcolorbox">

Global completeness ($`\alpha \circ f = f^\sharp \circ \alpha`$) is RARE — only trivial domains achieve it.

LOCAL completeness (at specific inputs) is ACHIEVABLE and USEFUL.

$`\mathsf{LCL}_A`$ combines BOTH correctness AND incorrectness in ONE framework, parameterized by abstract domain $`A`$.

</div>

<div class="theorem">

**Theorem 1.38** (Global Completeness Is Rare (Giacobazzi et al. 2015, Theorem 4.5)). *For Turing-complete languages, only TRIVIAL domains are globally complete:*

1.  *Identity abstraction (concrete = abstract) — useless*

2.  *Top abstraction (all programs equivalent) — useless*

***Consequence:** For any useful abstract domain, there exist inputs where the analysis produces false alarms (over-approximation is imprecise).*

***The Solution:** LOCAL COMPLETENESS — Instead of requiring completeness for ALL inputs, require it only for SPECIFIC inputs of interest.*

</div>

<div class="definition">

**Definition 1.39** (Local Completeness (Definition 4.1)). Abstract domain $`A`$ is *locally complete* for $`f:C \to C`$ at concrete value $`c \in C`$, written $`\mathcal{C}^A_c(f)`$, if:
``` math
\mathcal{C}^A_c(f) \iff \alpha \circ f(c) = \alpha \circ f \circ \gamma \circ \alpha(c)
```
Meaning: For input $`c`$, the abstraction of $`f(c)`$ equals the abstraction of applying $`f`$ to the “best” approximation of $`c`$.

**Global vs Local:**
``` math
\begin{aligned}
  \text{Global:} & \quad \forall c \in C.\; \mathcal{C}^A_c(f) \quad \text{(for all inputs --- rare)} \\
  \text{Local:} & \quad \mathcal{C}^A_c(f) \quad \text{(for specific input } c \text{ --- achievable)}
\end{aligned}
```

</div>

<div class="definition">

**Definition 1.40** (The $`\mathsf{LCL}_A`$ Proof System). **$`\mathsf{LCL}_A`$ Triple:** $`\vdash_A [p]\, r\, [q]`$

Meaning:

1.  $`q \subseteq \llbracket r \rrbracket p`$ ($`q`$ under-approximates strongest postcondition)

2.  $`\mathcal{C}^A_p(r)`$ holds ($`r`$ is locally complete for input $`p`$ in $`A`$)

3.  $`\alpha(q) = \alpha(\llbracket r \rrbracket p)`$ (abstractions coincide)

**Key Insight:** If $`\vdash_A [p]\, r\, [q]`$ is provable, then:

- $`q`$ contains only TRUE alarms (under-approximation property)

- If $`q`$ has no alarms, then $`\llbracket r \rrbracket p`$ has no alarms (completeness at $`p`$)

- The analysis is BOTH sound for bugs AND complete for safety at $`p`$

</div>

<div class="definition">

**Definition 1.41** (Key Rules of $`\mathsf{LCL}_A`$). **Transfer Rule:** Base case for primitive commands

<div class="mathpar">

\_A \[p\]  e  \[e p\]

</div>

**Relax Rule:** Weaken precondition, strengthen postcondition (within abstraction)

<div class="mathpar">

\_A \[p\]  r  \[q\]

</div>

**Sequence Rule:** Composition

<div class="mathpar">

\_A \[p\]  r_1;r_2  \[q\]

</div>

**Join Rule:** Non-deterministic choice

<div class="mathpar">

\_A \[p\]  r_1+r_2  \[q_1 q_2\]

</div>

**Iterate Rule:** Loop with local completeness check

<div class="mathpar">

\_A \[p\]  r^\*  \[p q\]

</div>

</div>

<div class="theorem">

**Theorem 1.42** (Connection to Incorrectness Logic (Section 6)). *$`\mathsf{IL} = \mathsf{LCL}_{A_{\mathsf{tr}}}`$*

*When $`A = A_{\mathsf{tr}}`$ (the trivial “top” abstraction that makes all properties equivalent):*

- *Every transfer function is globally complete in $`A_{\mathsf{tr}}`$*

- *$`\mathsf{LCL}_{A_{\mathsf{tr}}}`$ coincides EXACTLY with O’Hearn’s Incorrectness Logic*

- *This is because $`A_{\mathsf{tr}}`$ provides NO precision — can’t distinguish any states*

***Implication:***

- *IL is a SPECIAL CASE of $`\mathsf{LCL}_A`$*

- *For any non-trivial $`A`$, $`\mathsf{LCL}_A`$ is MORE POWERFUL than IL*

- *$`\mathsf{LCL}_A`$ can prove CORRECTNESS (when $`q`$ has no alarms)*

- *IL can only prove INCORRECTNESS (existence of bugs)*

</div>

<div class="theorem">

**Theorem 1.43** (Soundness of $`\mathsf{LCL}_A`$ (Theorem 5.5)). *If $`\vdash_A [p]\, r\, [q]`$ then:*

1.  *$`q \subseteq \llbracket r \rrbracket p`$ (under-approximation)*

2.  *$`\llbracket r \rrbracket^\sharp_A \alpha(p) = \alpha(q) = \alpha(\llbracket r \rrbracket p)`$ (local completeness)*

</div>

<div class="corollary">

**Corollary 1.44** (Precision (Corollary 5.6)). *If $`\vdash_A [p]\, r\, [q]`$ then for all $`a \in A`$:
``` math
\llbracket r \rrbracket p \subseteq \gamma(a) \iff q \subseteq \gamma(a)
```
Meaning: $`q`$ and $`\llbracket r \rrbracket p`$ have the SAME abstract over-approximations.*

</div>

<div class="tcolorbox">

Given abstract domain $`A`$ and input $`p`$, analyze program $`C`$:

Attempt to prove $`\vdash_A [p]\, C\, [q]`$

Case 1:  
$`q`$ contains alarm states  
$`\Rightarrow`$ **Proven Buggy** (under-approximation guarantees true bugs)

Case 2:  
$`q`$ contains no alarm states AND proof succeeds  
$`\Rightarrow`$ **Proven Safe** (local completeness at $`p`$ means no false negatives)

Case 3:  
Proof fails (local completeness obligation not met)  
$`\Rightarrow`$ **Uncertain** (need more precise domain or different input abstraction)

</div>

<div class="remark">

**Remark 1.45** (Domain Refinement for Local Completeness (Section 8)). When local completeness fails, REFINE the domain.

If $`\mathcal{C}^A_p(f)`$ fails, we can:

1.  Compute the BEST CORRECT APPROXIMATION (bca) for $`f`$ at $`p`$

2.  Extend $`A`$ to include this value

3.  Re-attempt the proof with refined domain

This gives a DIRECTED refinement strategy — refine only where needed, not globally. Much more practical than complete abstract refinement.

</div>

<div class="remark">

**Remark 1.46** (Integration with Outcome Logic). **$`\mathsf{LCL}_A`$ + OL Integration:**

Both frameworks address over/under approximation unification:

- OL (Zilberstein 2023): Unified semantics for correctness and incorrectness

- $`\mathsf{LCL}_A`$ (Bruni 2023): Unified proof system parameterized by abstract domain

**Recommended Approach:**

1.  Use OL for SEMANTICS (what correctness/incorrectness mean)

2.  Use $`\mathsf{LCL}_A`$ for ANALYSIS (how to compute verdicts)

3.  Abstract domain $`A`$ determines precision/cost tradeoff

4.  Local completeness obligations guide where to refine $`A`$

</div>

See Section 12.4 for formal theorems and Section 12.5 for manifest error detection.

**Cross-References:**

- For probabilistic programs, $`\mathsf{LCL}_A`$ can be instantiated with probabilistic abstract domains (Section 2.1.6c).

- The three abstraction axes (semantics, scenarios, distributions) provide orthogonal precision control.

- Local completeness extends to probabilistic settings: completeness at input distribution rather than input set.

### Unified Constraint Domain Framework

**Paper:** Xi & Pfenning 1999 (Dependent Types in Practical Programming)

<div class="tcolorbox">

The domains in 2.1.7 (interval, taint, nullability, resource, ownership) appear ad-hoc but share a COMMON STRUCTURE.

Each is an instantiation of Dependent ML (Xi 1999) with a different constraint system $`C`$. This unifies our implementation and theory.

</div>

<div class="definition">

**Definition 1.47** (The DML(C) Pattern). Types indexed by CONSTRAINT DOMAIN values:
``` math
\begin{aligned}
  \mathsf{DML}(C) = \{&\\
    &\mathsf{index\_sort} : \mathsf{Type}; \quad \text{(What indices look like)} \\
    &\mathsf{index\_term} : \mathsf{Type}; \quad \text{(Terms over indices)} \\
    &\mathsf{constraint} : \mathsf{Type}; \quad \text{(Constraints over indices)} \\
    &\mathsf{satisfiable} : \mathsf{constraint} \to \mathsf{bool}; \\
    &\mathsf{implies} : \mathsf{constraint} \to \mathsf{constraint} \to \mathsf{bool}; \\
    &\mathsf{widen} : \mathsf{constraint} \to \mathsf{constraint} \to \mathsf{constraint}; \\
  \}&
\end{aligned}
```

</div>

<div class="definition">

**Definition 1.48** (Our Domains as DML(C)).

<div class="center">

| **Domain** | **Constraint System $`C`$** |
|:---|:---|
| Interval | Linear integer arithmetic ($`a \cdot x + b \cdot y \leq c`$) |
| Nullability | Boolean constraints ($`\mathsf{isNull} \lor \lnot\mathsf{isNull}`$) |
| Taint | Security lattice ($`\mathsf{level}_1 \sqsubseteq \mathsf{level}_2`$) |
| Resource | Finite state constraints ($`\mathsf{state} \in \{S_1, S_2, \ldots\}`$) |
| Ownership | Capability constraints ($`\mathsf{unique} \oplus \mathsf{dup}`$) |

</div>

</div>

<div class="remark">

**Remark 1.49** (Benefits of Unified View).

1.  **Implementation:** One parametric domain module, many instantiations

2.  **Composition:** Cross-domain reasoning via constraint conjunction

3.  **Decidability:** Constraint satisfiability gives analysis termination

4.  **Theory:** Single soundness proof covers all domains

</div>

<div class="fstarcode">

(\* UNIFIED CONSTRAINT DOMAIN SIGNATURE \*) class constraint_domain (c : Type) = index_sort : Type; index_term : Type; constraint\_ : Type; (\* Core decision procedures \*) satisfiable : constraint\_ -\> bool; implies : constraint\_ -\> constraint\_ -\> bool; (\* Abstract interpretation operations \*) meet : constraint\_ -\> constraint\_ -\> constraint\_; join : constraint\_ -\> constraint\_ -\> constraint\_; widen : constraint\_ -\> constraint\_ -\> constraint\_;

(\* ALL OUR DOMAINS AS INSTANCES \*)

(\* Interval: C = linear integer constraints \*) instance interval_cd : constraint_domain interval = index_sort = Int; index_term = LinExpr; (\* a_1 x_1 + a_2 x_2 + ... + c \*) constraint\_ = LinConstraint; satisfiable = fourier_motzkin_sat; implies = fun c1 c2 -\> not (satisfiable (c1 / not c2)); meet = fun c1 c2 -\> c1 / c2; join = convex_hull; widen = interval_widen;

(\* Taint: C = security lattice constraints \*) instance taint_cd : constraint_domain taint = index_sort = SecurityLevel; index_term = LevelExpr; constraint\_ = FlowConstraint; (\* l_1 l_2 \*) satisfiable = lattice_sat; implies = lattice_implies; meet = fun c1 c2 -\> c1 / c2; join = lub_constraint; widen = id; (\* Finite \*)

</div>

<div class="example">

**Example 1.50** (Application: Array Bounds Verification). With this framework, we can statically eliminate bounds checks:

<div class="fstarcode">

(\* Refined array type with length index \*) type array_refined (a : Type) (n : index_term) = data : array a; length_proof : squash (data.length == eval n);

(\* Bounds check elimination \*) val access_safe : ctx:constraint\_ -\> (\* What we know at this point \*) arr:array_refined a n -\> (\* Array with length n \*) idx:index_term -\> (\* Index expression \*) implies ctx (0 \<= idx / idx \< n) == true -\> (\* Provable in-bounds \*) a (\* No runtime check needed! \*)

</div>

This is how Liquid Haskell (Vazou 2014) and Dependent ML work.

</div>

See Section 12.11 for complete F\* formalization.

# Program Representation

## The Code Property Graph

**Foundational Papers**: Yamaguchi et al. 2014, Ferrante et al. 1987, Horwitz et al. 1990, Weiser 1984

The Code Property Graph (CPG) is the central data structure of the brrr-machine. It unifies multiple program representations into a single queryable graph, enabling all analyses to be expressed as graph traversals.

### Why CPG?

<div class="tcolorbox">

Traditional analyzers maintain separate data structures:

- AST for syntactic queries

- CFG for control flow

- Call graph for interprocedural analysis

- Def-use chains for data flow

This leads to:

- Redundant storage

- Complex synchronization

- Difficult cross-cutting queries

</div>

<div class="tcolorbox">

Merge all representations into **one** graph:

- Nodes are shared (a statement is both AST node and CFG node)

- Edges are labeled by type (`AST_CHILD`, `CFG_NEXT`, `DATA_DEP`, etc.)

- Queries mix edge types freely

**Benefits**:

- Single data structure to maintain

- Natural expression of complex queries

- Efficient traversals via graph algorithms

</div>

### CPG Components

<div class="definition">

**Definition 1.51** (Code Property Graph). The Code Property Graph is the union of five component graphs:
``` math
\mathsf{CPG} = \mathsf{AST} \cup \mathsf{CFG} \cup \mathsf{PDG} \cup \mathsf{CallGraph} \cup \mathsf{EffectGraph}
```
Each component contributes specific node interpretations and edge types.

</div>

#### AST (Abstract Syntax Tree)

- **Nodes**: Every syntactic element (functions, statements, expressions, etc.)

- **Edges**: $`\mathsf{Child}(i)`$ from parent to $`i`$-th child

- **Purpose**: Syntactic structure, source location mapping

#### CFG (Control Flow Graph)

- **Nodes**: Shared with AST statement/expression nodes

- **Edges**: `NEXT`, `TRUE_BRANCH`, `FALSE_BRANCH`, `EXCEPTION`

- **Purpose**: Possible execution orders

#### PDG (Program Dependence Graph)

**Source**: Ferrante et al. 1987

- **Nodes**: Shared with AST

- **Edges**: $`\mathsf{DataDep}(\mathit{var})`$, $`\mathsf{CtrlDep}(\mathit{branch})`$

- **Purpose**: Semantic dependencies for slicing

#### Call Graph

- **Nodes**: Shared with AST function/call nodes

- **Edges**: $`\mathsf{Calls}(\mathit{site})`$, $`\mathsf{ParamIn}(i)`$, $`\mathsf{ParamOut}`$

- **Purpose**: Interprocedural analysis

#### Effect Graph (brrr-machine Extension)

- **Nodes**: Effect nodes (read, write, alloc, free, io, etc.)

- **Edges**: $`\mathsf{Effect}(\mathit{kind})`$, `EFFECT_ORDER`, `EFFECT_CONFLICT`

- **Purpose**: Effect tracking for bug detection

### Edge Types in Detail

The following F\* formalization defines the complete edge type vocabulary for the Code Property Graph. Each edge type is modeled as a constructor of an algebraic data type, with some edges carrying payload data (e.g., variable names for data dependencies, branch directions for control dependencies).

**Key design principles**:

- **AST edges** preserve syntactic parent-child relationships with explicit position indices

- **CFG edges** distinguish conditional branches (true/false) from unconditional flow

- **PDG edges** carry the variable name involved in the dependency, enabling precise slicing

- **Call graph edges** follow Horwitz 1990’s System Dependence Graph structure with summary edges

- **Effect edges** are a brrr-machine extension for tracking side effects and their ordering

- **Channel edges** integrate Honda 1998/2008 session types for concurrent program analysis

<div class="fstarcode">

(\* ================================================== CPG EDGE TYPES ================================================== \*)

type cpg_edge_label = (\* ————————————————– AST EDGES — Syntactic structure ————————————————– \*) \| AstChild : index:nat -\> cpg_edge_label (\* Parent contains child at position index. Enables: tree traversals, subtree extraction, pattern matching \*)

\| AstNextSibling : cpg_edge_label (\* Sibling relationship for sequential AST nodes. Enables: statement sequence traversal \*)

(\* ————————————————– CFG EDGES — Control flow Source: standard compiler construction ————————————————– \*) \| CfgNext : cpg_edge_label (\* Unconditional successor. From: any statement To: next statement in sequence \*)

\| CfgTrue : cpg_edge_label (\* Successor when condition is true. From: if/while/for condition To: then-branch / loop body \*)

\| CfgFalse : cpg_edge_label (\* Successor when condition is false. From: if/while/for condition To: else-branch / after loop \*)

\| CfgException : exn_type:string -\> cpg_edge_label (\* Successor on exception of given type. From: potentially-throwing statement To: matching catch block or function exit \*)

\| CfgEntry : cpg_edge_label (\* Function entry edge. From: function node To: first statement \*)

\| CfgExit : cpg_edge_label (\* Function exit edge. From: return statement or last statement To: function exit node \*)

(\* ————————————————– PDG EDGES — Data and control dependencies Source: Ferrante 1987 ————————————————– \*) \| DataDep : var:string -\> cpg_edge_label (\* Data dependence: target uses value defined at source. From: definition of var To: use of var Enables: reaching definitions, taint propagation \*)

\| CtrlDep : branch:bool -\> cpg_edge_label (\* Control dependence: target’s execution depends on source’s branch. From: branch condition To: statement that executes only if branch goes this way The bool indicates which branch (true/false) Enables: slicing, dead code detection \*)

\| OutputDep : var:string -\> cpg_edge_label (\* Output dependence: both source and target write to var. From: earlier write To: later write Enables: race detection, write-write conflict \*)

\| AntiDep : var:string -\> cpg_edge_label (\* Anti-dependence: source reads var, target writes var. From: read To: write Enables: race detection, read-write conflict \*)

(\* ————————————————– CALL GRAPH EDGES — Interprocedural structure Source: Horwitz 1990 (SDG) ————————————————– \*) \| Calls : site_id:nat -\> cpg_edge_label (\* Call edge from call site to callee. From: call expression To: callee function entry \*)

\| ParamIn : index:nat -\> cpg_edge_label (\* Actual-to-formal parameter binding. From: actual argument expression To: formal parameter \*)

\| ParamOut : cpg_edge_label (\* Return value flow. From: return statement value To: call site result \*)

\| Summary : cpg_edge_label (\* Summary edge: transitive dependence through procedure. From: input parameter To: output (return or modified parameter) Enables: efficient interprocedural analysis without re-analyzing callee \*)

(\* ————————————————– EFFECT EDGES — Side effect tracking Source: brrr-machine theory (our contribution) ————————————————– \*) \| Effect : kind:effect_kind -\> cpg_edge_label (\* Statement has this effect. From: statement To: effect node \*)

\| EffectOrder : cpg_edge_label (\* Effect ordering: source must happen before target. Derived from CFG + data dependencies \*)

\| EffectConflict : cpg_edge_label (\* Effects may conflict (race condition candidate). Between: effects on same location from different threads \*)

\| EffectCause : cpg_edge_label (\* Causal chain: source effect enables/causes target effect. Enables: root cause analysis \*)

(\* ————————————————– CHANNEL EDGES — Honda 1998/2008 session type structure These edges integrate channel operations into the CPG for analysis of communication patterns, protocol conformance, and deadlock detection. ————————————————– \*) \| ChannelFlow : chan_id:nat -\> cpg_edge_label (\* Data flow through channel: send -\> recv. From: NChannelSend node To: NChannelRecv node on same channel Enables: taint propagation through channels, data flow analysis \*)

\| SessionOf : cpg_edge_label (\* Associates prefix node with its session. From: NPrefixNode To: NSessionNode \*)

\| ParticipantOf : cpg_edge_label (\* Associates participant node with its session. From: NParticipantNode To: NSessionNode \*)

\| LocalTypeOf : cpg_edge_label (\* Associates process with its local session type. From: NFunction or NBlock (implementing participant) To: NParticipantNode (with local type) \*)

\| SessionCausality : kind:causality_kind -\> cpg_edge_label (\* Causality dependency between prefixes (Honda 2008 Section 3.2). From: earlier prefix node To: later prefix node kind: II (input-input), IO (input-output), OO (output-output) Enables: deadlock detection, protocol conformance, linearity checking \*)

\| SessionNext : cpg_edge_label (\* Sequential composition within session type. From: prefix node To: next prefix node in sequence \*)

\| ChannelCreate : cpg_edge_label (\* Channel creation to channel use. From: NChannelCreate node To: NChannelSend, NChannelRecv, or NChannelClose node \*)

\| ChannelAlias : cpg_edge_label (\* Channel aliasing: source and target refer to same channel. From: channel variable assignment To: channel variable use Enables: alias analysis for channels \*)

\| DelegationTransfer : cpg_edge_label (\* Session delegation: capability transfer. From: NChannelDelegate (sender) To: NChannelRecv (receiver of capability) Enables: tracking session ownership transfer \*)

and causality_kind = \| CausalityII (\* Input-Input: two inputs at same participant must be ordered \*) \| CausalityIO (\* Input-Output: output depends on data from input \*) \| CausalityOO (\* Output-Output: two outputs from same sender must be ordered \*)

</div>

### Node Types

The CPG node vocabulary captures all syntactic and semantic elements of a program. Unlike traditional ASTs that only represent syntax, CPG nodes unify multiple views:

- **AST nodes** represent syntactic program structure (functions, statements, expressions)

- **CFG nodes** mark control flow entry/exit points and join locations

- **Channel nodes** (Honda 1998/2008) represent concurrent communication operations

- **Effect nodes** track side effects for bug detection and resource analysis

**Type signature conventions**:

- Node kinds with `name:string` carry the identifier name for lookup and display

- Node kinds with `index:nat` carry positional information (parameters, children)

- Channel nodes carry `chan_id:nat` to track channel identity across operations

- Session nodes reference `session_id` for protocol conformance checking

The following F\* formalization defines the complete node type vocabulary:

<div class="fstarcode">

(\* ================================================== CPG NODE TYPES ================================================== \*)

type cpg_node_kind = (\* ————————————————– AST NODES — Program structure ————————————————– \*) \| NFile : path:string -\> cpg_node_kind \| NModule : name:string -\> cpg_node_kind \| NClass : name:string -\> cpg_node_kind \| NFunction : name:string -\> params:list string -\> cpg_node_kind \| NMethod : name:string -\> receiver:string -\> params:list string -\> cpg_node_kind \| NParameter : name:string -\> index:nat -\> cpg_node_kind \| NLocalVar : name:string -\> cpg_node_kind \| NGlobalVar : name:string -\> cpg_node_kind

(\* Statements \*) \| NBlock : cpg_node_kind \| NExprStmt : cpg_node_kind \| NVarDecl : name:string -\> cpg_node_kind \| NAssign : target:string -\> cpg_node_kind \| NIf : cpg_node_kind \| NWhile : cpg_node_kind \| NFor : cpg_node_kind \| NForEach : cpg_node_kind \| NReturn : cpg_node_kind \| NThrow : cpg_node_kind \| NTry : cpg_node_kind \| NCatch : exn_type:string -\> cpg_node_kind \| NBreak : cpg_node_kind \| NContinue : cpg_node_kind

(\* Expressions \*) \| NLiteral : lit_kind:literal_kind -\> cpg_node_kind \| NIdentifier : name:string -\> cpg_node_kind \| NBinaryOp : op:string -\> cpg_node_kind \| NUnaryOp : op:string -\> cpg_node_kind \| NCall : cpg_node_kind \| NMethodCall : method_name:string -\> cpg_node_kind \| NFieldAccess : field:string -\> cpg_node_kind \| NIndexAccess : cpg_node_kind \| NLambda : cpg_node_kind \| NNew : type_name:string -\> cpg_node_kind \| NConditional : cpg_node_kind (\* ternary \*) \| NCast : target_type:string -\> cpg_node_kind

(\* ————————————————– CFG NODES — Control flow structure ————————————————– \*) \| NEntry : func:string -\> cpg_node_kind \| NExit : func:string -\> cpg_node_kind \| NJoin : cpg_node_kind (\* Merge point for branches \*) \| NLoopHead : cpg_node_kind (\* Target for widening \*)

(\* ————————————————– CHANNEL NODES — Honda 1998/2008 session type operations Integrates with AST (channel operations in syntax), CFG (control flow through channel ops), PDG (data dependencies through channels), and call graph (channels passed to functions). ————————————————– \*) \| NChannelCreate : chan_id:nat -\> elem_type:string -\> buffer_size:nat -\> cpg_node_kind (\* Channel creation: ch := make(chan T) or mpsc::channel() \*)

\| NChannelSend : chan_id:nat -\> cpg_node_kind (\* Send operation: ch \<- v or tx.send(v) \*)

\| NChannelRecv : chan_id:nat -\> binding:option string -\> cpg_node_kind (\* Receive operation: v := \<-ch or rx.recv() \*)

\| NChannelClose : chan_id:nat -\> cpg_node_kind (\* Close operation: close(ch) or drop(tx) \*)

\| NChannelSelect : cases:nat -\> has_default:bool -\> cpg_node_kind (\* Select statement: select case ... or tokio::select! \*)

\| NChannelSelectCase : chan_id:nat -\> direction:chan_direction -\> cpg_node_kind (\* Individual case in select \*)

\| NChannelBranch : chan_id:nat -\> labels:list string -\> cpg_node_kind (\* Session type branching: offer branches on channel \*)

\| NChannelChoice : chan_id:nat -\> label:string -\> cpg_node_kind (\* Session type selection: select branch on channel \*)

\| NChannelDelegate : chan_id:nat -\> delegated_chan:nat -\> cpg_node_kind (\* Session delegation: transfer capability through channel \*)

(\* Session type nodes — for protocol conformance checking \*) \| NSessionNode : session_id:nat -\> global_type:string -\> cpg_node_kind (\* Session (conversation) node \*)

\| NParticipantNode : session_id:nat -\> participant:nat -\> local_type:string -\> cpg_node_kind (\* Participant in a session with its local type \*)

\| NPrefixNode : session_id:nat -\> kind:prefix_kind -\> channel:nat -\> payload:string -\> cpg_node_kind (\* Communication prefix in session type \*)

(\* ————————————————– EFFECT NODES — Side effects ————————————————– \*) \| NEffect : kind:effect_kind -\> cpg_node_kind

and chan_direction = ChanSend \| ChanRecv

and prefix_kind = PrefixSend \| PrefixRecv \| PrefixSelect \| PrefixBranch

and literal_kind = \| LitInt : value:int -\> literal_kind \| LitFloat : value:float -\> literal_kind \| LitString : value:string -\> literal_kind \| LitBool : value:bool -\> literal_kind \| LitNull : literal_kind \| LitUndefined : literal_kind

and effect_kind = \| EffRead : loc:abstract_loc -\> effect_kind \| EffWrite : loc:abstract_loc -\> effect_kind \| EffAlloc : size:nat -\> effect_kind \| EffFree : loc:abstract_loc -\> effect_kind \| EffCall : target:string -\> effect_kind \| EffThrow : exn_type:string -\> effect_kind \| EffCatch : exn_type:string -\> effect_kind \| EffIO : io_kind -\> effect_kind \| EffSpawn : task_id:nat -\> effect_kind \| EffJoin : task_id:nat -\> effect_kind \| EffLock : lock_id:nat -\> effect_kind \| EffUnlock : lock_id:nat -\> effect_kind \| EffSend : chan:nat -\> effect_kind \| EffRecv : chan:nat -\> effect_kind (\* Channel effects — Honda 1998/2008 session types \*) \| EffChanCreate : chan:nat -\> elem_type:string -\> buffer_size:nat -\> effect_kind \| EffChanClose : chan:nat -\> effect_kind \| EffSelect : chan:nat -\> label:string -\> effect_kind \| EffBranch : chan:nat -\> labels:list string -\> effect_kind \| EffDelegate : chan:nat -\> delegated_chan:nat -\> effect_kind

and io_kind = \| IOFileRead \| IOFileWrite \| IONetworkRead \| IONetworkWrite \| IOStdin \| IOStdout \| IOStderr \| IOEnvRead \| IOTimeRead \| IORandom

</div>

### CPG Data Structure

The CPG data structure organizes nodes and edges into an efficiently queryable form. The design prioritizes:

1.  **Fast traversal**: Pre-computed indices for outgoing/incoming edges enable $`O(1)`$ edge lookup

2.  **Label-based filtering**: Edges indexed by label type support efficient type-specific queries

3.  **Source location mapping**: Bidirectional mapping between nodes and source locations enables IDE integration

4.  **Language parameterization**: Each CPG carries its language configuration for language-aware analysis

**Key type signatures**:

- `node_id = nat`: Unique identifier for each node, enabling $`O(1)`$ lookup in hash maps

- `source_location`: Captures file path and line/column ranges for error reporting

- `cpg_node`: Combines node kind with metadata (source location, type info, abstract state)

- `cpg_edge`: Triple of source node, target node, and edge label

- `cpg`: The complete graph with indices for efficient traversal

<div class="fstarcode">

(\* ================================================== THE CODE PROPERTY GRAPH ================================================== \*)

module BrrrMachine.CPG

type node_id = nat

type source_location = file : string; start_line : nat; start_col : nat; end_line : nat; end_col : nat;

type cpg_node = id : node_id; kind : cpg_node_kind; source_loc : option source_location; (\* Type information (when available) \*) type_info : option type_info; (\* Abstract state attached during analysis \*) abstract_state : option abstract_state; (\* Original source text (for error messages) \*) source_text : option string;

type cpg_edge = source : node_id; target : node_id; label : cpg_edge_label;

type cpg =

(\* Core graph structure \*) nodes : map node_id cpg_node; edges : list cpg_edge;

(\* Indices for efficient traversal \*) outgoing : map node_id (list cpg_edge); (\* node -\> outgoing edges \*) incoming : map node_id (list cpg_edge); (\* node -\> incoming edges \*) by_label : map cpg_edge_label (list cpg_edge); (\* label -\> edges \*)

(\* Entry points \*) functions : map string node_id; (\* function name -\> entry node \*) entry_points : list node_id; (\* main functions, tests, handlers \*)

(\* Reverse mappings \*) node_by_loc : map source_location node_id; (\* for IDE integration \*)

(\* Language configuration \*) language : language_config;

</div>

### CPG Traversal Primitives

The power of the CPG is in its traversals. We define a small set of primitives that compose to express any analysis. This section formalizes the traversal algebra from Yamaguchi 2014 in F\*, providing type-safe implementations of graph queries.

**Core insight**: By treating traversals as first-class functions on node sets, we can compose complex vulnerability queries from simple primitives using function composition.

<div class="definition">

**Definition 1.52** (Traversal Algebra (Yamaguchi 2014, Section 7)). Traversals are *functions on node sets*:
``` math
T : \mathcal{P}(V) \to \mathcal{P}(V)
```
This enables compositional query construction via function composition. Complex vulnerability patterns are built by composing primitive traversals.

**Key insight**: Graph queries become first-class values that can be stored, composed, and reused. A traversal $`T`$ applied to node set $`X`$ yields a new node set $`T(X)`$.

</div>

<div class="fstarcode">

(\* ================================================== CPG TRAVERSAL PRIMITIVES Source: Yamaguchi 2014 (adapted) ================================================== \*)

module BrrrMachine.CPG.Traversal

(\* A traversal is a function from a set of nodes to a set of nodes \*) type traversal = set node_id -\> set node_id

(\* Traversal composition: apply traversals in sequence (left-to-right) \*) val compose : list traversal -\> traversal let compose traversals nodes = List.fold_left (fun ns t -\> t ns) nodes traversals

(\* Alternative: compose two traversals \*) val compose2 : traversal -\> traversal -\> traversal let compose2 t1 t2 nodes = t2 (t1 nodes)

(\* Identity traversal \*) let id_traversal : traversal = fun nodes -\> nodes

</div>

The traversal type `set node_id -> set node_id` captures the essence of graph queries: given a starting set of nodes, produce the set of reachable nodes. The `compose` function enables building complex queries by chaining simpler ones.

#### TNODES: Tree Nodes (AST Subtree Collection)

<div class="remark">

**Remark 1.53** (Yamaguchi 2014). $`\mathsf{TNODES}(V)`$ returns all nodes in subtrees rooted at $`V`$. This is essential for syntax-based queries that need to examine entire expressions or statement subtrees.

</div>

<div class="fstarcode">

(\* ————————————————– TNODES: Tree Nodes (AST Subtree Collection) Yamaguchi 2014: "TNODES(V) returns all nodes in subtrees rooted at V" ————————————————– \*)

val tnodes : cpg -\> traversal let tnodes g roots = let rec collect_subtree node acc = (\* Get all AST children of this node \*) let children = out g (Set.singleton node) (LabelPrefix "AstChild") in (\* Recursively collect all descendants \*) Set.fold (fun child acc’ -\> collect_subtree child acc’) children (Set.add node acc) in Set.fold (fun root acc -\> collect_subtree root acc) roots Set.empty

</div>

#### MATCH: Find Nodes Matching Predicate in Subtrees

<div class="remark">

**Remark 1.54** (Yamaguchi 2014). $`\mathsf{MATCH}_p(V) = \mathsf{FILTER}_p(\mathsf{TNODES}(V))`$. Combines subtree collection with filtering to find specific patterns within AST subtrees. Essential for syntax-aware vulnerability queries.

</div>

<div class="fstarcode">

(\* ————————————————– MATCH_p: Find Nodes Matching Predicate in Subtrees Yamaguchi 2014: "MATCH_p(V) = FILTER_p(TNODES(V))" ————————————————– \*)

val match_pred : cpg -\> (cpg_node -\> bool) -\> traversal let match_pred g pred roots = filter g (tnodes g roots) pred

(\* Convenience: match nodes by kind \*) val match_kind : cpg -\> cpg_node_kind -\> traversal let match_kind g kind roots = match_pred g (fun n -\> n.kind = kind) roots

(\* Convenience: match nodes by name pattern (regex) \*) val match_name : cpg -\> string -\> traversal let match_name g pattern roots = match_pred g (fun n -\> matches_regex pattern (node_name n)) roots

</div>

The `match_pred` function demonstrates the composability principle: it combines `tnodes` (subtree collection) with `filter` (predicate testing) to find specific patterns within AST subtrees. This is the foundation for syntax-aware vulnerability queries.

#### Traversal Combinators

Traversal combinators provide set-theoretic operations on traversal results, enabling complex query patterns:

<div class="fstarcode">

(\* ————————————————– TRAVERSAL COMBINATORS Higher-order functions for building complex traversals ————————————————– \*)

(\* Union: nodes reachable by either traversal \*) val union_t : traversal -\> traversal -\> traversal let union_t t1 t2 nodes = Set.union (t1 nodes) (t2 nodes)

(\* Intersection: nodes reachable by both traversals \*) val inter_t : traversal -\> traversal -\> traversal let inter_t t1 t2 nodes = Set.inter (t1 nodes) (t2 nodes)

(\* Difference: nodes in first but not second \*) val diff_t : traversal -\> traversal -\> traversal let diff_t t1 t2 nodes = Set.diff (t1 nodes) (t2 nodes)

</div>

#### Buffer Overflow Query Example

<div class="example">

**Example 1.55** (Buffer Overflow Detection (Yamaguchi 2014, Section 8.4)). This example demonstrates compositional query construction for detecting buffer overflows in write handlers where a `count` parameter reaches `memcpy` without bounds checking.

**Pattern**: Function parameter matching “count” flows to memcpy’s size argument without sanitization (bounds check, allocation check, `min()`).

</div>

<div class="fstarcode">

(\* ————————————————– EXAMPLE: Buffer Overflow Query (Yamaguchi 2014, Section 8.4)

Demonstrates compositional query construction for detecting buffer overflows in write handlers where count parameter reaches memcpy without bounds checking.

Pattern: Function parameter matching "count" flows to memcpy’s size argument without sanitization (bounds check, allocation check, min()). ————————————————– \*)

val buffer_overflow_query : cpg -\> set node_id let buffer_overflow_query g = (\* Step 1: Find parameters with names suggesting count/size \*) let count_params = filter g (all_nodes g) (fun n -\> is_parameter n && matches_regex ".\*c(ou)?nt.\*" (node_name n)) in

(\* Step 2: Find nodes that reach via data dependence \*) let reaches_from_count = Set.fold (fun param acc -\> Set.union acc (reaches g param (LabelPrefix "DataDep")) ) count_params Set.empty in

(\* Step 3: Filter to memcpy size argument (position 2) \*) let memcpy_size_args = filter g reaches_from_count (fun n -\> is_call_argument g n.id "memcpy" 2 \|\| is_call_argument g n.id "copy_from_user" 2) in

(\* Step 4: Exclude if sanitized (bounds check, alloc, min) \*) let sanitizers = filter g (all_nodes g) (fun n -\> contains_bounds_check n \|\| is_allocation_with_param n \|\| node_contains_text n "min") in

(\* Step 5: Return unsanitized paths \*) let sanitized = Set.fold (fun s acc -\> Set.union acc (reached_by g s (LabelPrefix "DataDep")) ) sanitizers Set.empty in

Set.diff memcpy_size_args sanitized

</div>

This query demonstrates the full power of compositional traversals:

1.  **Source identification**: Filter for parameters matching a name pattern

2.  **Reachability**: Follow data dependencies transitively

3.  **Sink filtering**: Identify dangerous function arguments

4.  **Sanitizer exclusion**: Remove paths that pass through bounds checks

The result is a precise set of potentially vulnerable code locations that require manual review.

#### Generic Vulnerability Query Structure

<div class="fstarcode">

(\* Generic vulnerability query structure \*) type vuln_query = source : cpg -\> traversal; (\* Where tainted data originates \*) sink : cpg -\> traversal; (\* Where it must not reach unsanitized \*) sanitizers : cpg -\> traversal; (\* What makes data safe \*)

val execute_vuln_query : cpg -\> vuln_query -\> set (node_id \* node_id) let execute_vuln_query g query = let sources = query.source g (all_nodes g) in let sinks = query.sink g (all_nodes g) in let safe = query.sanitizers g (all_nodes g) in (\* Find source-sink pairs where path exists without sanitizer \*) reaches_set g sources sinks (LabelPrefix "DataDep") \|\> Set.filter (fun (src, snk) -\> let path_nodes = path_between g src snk (LabelPrefix "DataDep") in Set.is_empty (Set.inter path_nodes safe))

</div>

#### Basic Traversals

<div class="fstarcode">

(\* ————————————————– BASIC TRAVERSALS ————————————————– \*)

(\* OUT: follow outgoing edges matching label pattern \*) val out : cpg -\> set node_id -\> label_pattern -\> set node_id let out g nodes pattern = e.target \| e \<- concat_map (fun n -\> Map.find_default n \[\] g.outgoing) (Set.to_list nodes), matches pattern e.label

(\* IN: follow incoming edges matching label pattern \*) val in\_ : cpg -\> set node_id -\> label_pattern -\> set node_id let in\_ g nodes pattern = e.source \| e \<- concat_map (fun n -\> Map.find_default n \[\] g.incoming) (Set.to_list nodes), matches pattern e.label

(\* FILTER: keep only nodes satisfying predicate \*) val filter : cpg -\> set node_id -\> (cpg_node -\> bool) -\> set node_id let filter g nodes pred = Set.filter (fun n -\> match Map.find n g.nodes with \| Some node -\> pred node \| None -\> false) nodes

(\* MAP: transform node set \*) val map_nodes : cpg -\> set node_id -\> (cpg_node -\> ’a) -\> set ’a let map_nodes g nodes f = Set.map (fun n -\> match Map.find n g.nodes with \| Some node -\> f node \| None -\> failwith "invalid node") nodes

</div>

#### Transitive Closures

<div class="fstarcode">

(\* ————————————————– TRANSITIVE CLOSURES ————————————————– \*)

(\* REACHES: transitive closure following edge pattern \*) val reaches : cpg -\> node_id -\> label_pattern -\> set node_id let reaches g start pattern = let rec go visited frontier = if Set.is_empty frontier then visited else let next = out g frontier pattern in let new_nodes = Set.diff next visited in go (Set.union visited new_nodes) new_nodes in go (Set.singleton start) (Set.singleton start)

(\* REACHED_BY: reverse transitive closure \*) val reached_by : cpg -\> node_id -\> label_pattern -\> set node_id let reached_by g target pattern = let rec go visited frontier = if Set.is_empty frontier then visited else let prev = in\_ g frontier pattern in let new_nodes = Set.diff prev visited in go (Set.union visited new_nodes) new_nodes in go (Set.singleton target) (Set.singleton target)

(\* REACHES_SET: from any source to any target \*) val reaches_set : cpg -\> set node_id -\> set node_id -\> label_pattern -\> set (node_id \* node_id) let reaches_set g sources targets pattern = let reachable_from_sources = Set.fold (fun src acc -\> Map.add src (reaches g src pattern) acc) sources Map.empty in (src, tgt) \| src \<- Set.to_list sources, tgt \<- Set.to_list targets, Set.mem tgt (Map.find_default src Set.empty reachable_from_sources)

</div>

#### Program Slicing

<div class="definition">

**Definition 1.56** (Program Slicing (Weiser 1984, Horwitz 1990)). The following concepts map to CPG terminology:

Slicing Criterion  
A tuple $`C = (i, V)`$ where $`i`$ is a statement and $`V`$ is a set of variables. The slice preserves behavior at $`i`$ for variables $`V`$.

REF(n) / DEF(n)  
Variables referenced/defined at statement $`n`$:

- $`\mathsf{REF}(n)`$ $`\to`$ Read effects, `get_used_vars` in CPG

- $`\mathsf{DEF}(n)`$ $`\to`$ Write effects, `get_defined_var` in CPG

- `DataDep` edges encode the REF/DEF relationship directly

INFL(b)  
The influence range of branch $`b`$—statements whose execution depends on $`b`$’s outcome. Captured by `CtrlDep` edges in CPG.

</div>

<div class="theorem">

**Theorem 1.57** (Undecidability (Weiser 1984, Theorem 1)). *Finding statement-minimal slices is undecidable—equivalent to the halting problem.*

***Consequence**: We compute *conservative approximations* via dataflow analysis. Our slices may include extra statements but will **never** exclude statements that affect the slicing criterion.*

</div>

<div class="remark">

**Remark 1.58** (Algorithm Correspondence).

- Weiser’s $`R^0_C(n)`$ computation $`\to`$ `backward_slice` with `DataDep` edges

- Weiser’s $`B^i_C`$ (branch influence) $`\to`$ `CtrlDep` edge following

- Weiser’s fixpoint $`S_C = \bigcup S^i_C`$ $`\to`$ `reached_by` transitive closure

**Cross-reference**: Section 4.1 (IFDS) for context-sensitive slicing; Section 8.1 (Taint Analysis) uses forward slicing.

</div>

The following F\* code implements the slicing algorithms. The key functions are:

- `backward_slice`: Find all nodes that could affect a given criterion (for debugging, understanding)

- `forward_slice`: Find all nodes that could be affected by a criterion (for impact analysis)

- `thin_slice`: A more precise backward slice following only relevant dependencies (from TAJ, Tripp 2009)

<div class="fstarcode">

(\* ————————————————– SLICING (from Weiser 1984, Horwitz 1990) ————————————————– \*)

(\* BACKWARD_SLICE: all nodes that could affect the slicing criterion \*) val backward_slice : cpg -\> set node_id -\> set node_id let backward_slice g criteria = (\* Follow DATA_DEP and CTRL_DEP edges backwards \*) let dep_pattern = LabelOr \[LabelPrefix "DataDep"; LabelPrefix "CtrlDep"\] in reached_by_set g criteria dep_pattern

(\* FORWARD_SLICE: all nodes that could be affected by the slicing criterion \*) val forward_slice : cpg -\> set node_id -\> set node_id let forward_slice g criteria = let dep_pattern = LabelOr \[LabelPrefix "DataDep"; LabelPrefix "CtrlDep"\] in reaches_set g criteria dep_pattern

(\* THIN_SLICE: backward slice following only relevant dependencies Source: Tripp 2009 (TAJ) \*) val thin_slice : cpg -\> node_id -\> string -\> set node_id let thin_slice g sink sink_arg = (\* Only follow dependencies that affect the sink argument \*) let rec go visited frontier relevant_vars = if Set.is_empty frontier then visited else let preds = in\_ g frontier (LabelPrefix "DataDep") in let relevant_preds = Set.filter (fun n -\> (\* Node defines a relevant variable \*) match get_defined_var g n with \| Some v -\> Set.mem v relevant_vars \| None -\> false) preds in let new_vars = Set.fold (fun n acc -\> Set.union acc (get_used_vars g n)) relevant_preds Set.empty in let new_nodes = Set.diff relevant_preds visited in go (Set.union visited new_nodes) new_nodes (Set.union relevant_vars new_vars) in go (Set.singleton sink) (Set.singleton sink) (Set.singleton sink_arg)

</div>

#### Two-Phase Interprocedural Slicing

<div class="tcolorbox">

**CRITICAL**: Weiser’s transitive closure slicing is **WRONG** for interprocedural analysis!

**The problem**: Transitive closure can produce *spurious* results by “descending into a called procedure and then ascending to an incorrect calling context.”

**Example**:

- Main calls A, which calls Add

- Main also calls A, which calls Increment, which calls Add

If slicing from Increment, Weiser incorrectly includes the first Main$`\to`$A$`\to`$Add path because it doesn’t track calling contexts.

**THE FIX**: Two-phase slicing with restricted edge following.

</div>

<div class="fstarcode">

(\* ————————————————– TWO-PHASE INTERPROCEDURAL SLICING (from Horwitz 1990)

CRITICAL: Weiser’s transitive closure slicing is WRONG for interprocedural!

THE FIX: Two-phase slicing with restricted edge following. ————————————————– \*)

(\* PHASE 1: Ascend to callers — DON’T follow param-out edges \*) let slice_phase1 (g : cpg) (criteria : set node_id) : set node_id = let exclude = \[ParamOut; DefOrder\] in reach_backwards g criteria exclude

(\* PHASE 2: Descend into callees — DON’T follow call/param-in edges \*) let slice_phase2 (g : cpg) (phase1_nodes : set node_id) : set node_id = let exclude = \[Calls; ParamIn; DefOrder\] in reach_backwards g phase1_nodes exclude

(\* Combined two-phase interprocedural slice \*) val interprocedural_slice : cpg -\> set node_id -\> set node_id let interprocedural_slice g criteria = let phase1 = slice_phase1 g criteria in let phase2 = slice_phase2 g phase1 in Set.union phase1 phase2

(\* WHY THIS WORKS: Phase 1 can ASCEND from a called procedure to its callers via call/param-in edges, but cannot DESCEND via param-out.

Phase 2 can DESCEND into called procedures via param-out, but cannot ASCEND via call/param-in.

This ensures we never: - Descend into procedure P - Then ascend to caller Q (that didn’t actually call P in our context)

The result is a CONTEXT-SENSITIVE slice: only nodes on REALIZABLE paths (matched call/return parentheses) are included. \*)

(\* FORWARD SLICE (dual): for impact analysis \*) val interprocedural_forward_slice : cpg -\> set node_id -\> set node_id let interprocedural_forward_slice g criteria = (\* Phase 1: Forward in callers, don’t descend via param-in/call \*) let phase1 = reach_forward g criteria \[ParamIn; Calls; DefOrder\] in (\* Phase 2: Forward into callees, don’t ascend via param-out \*) let phase2 = reach_forward g phase1 \[ParamOut; DefOrder\] in Set.union phase1 phase2

(\* See Section 12.9 for soundness proofs \*)

</div>

The two-phase algorithm is essential for **context-sensitive** interprocedural analysis. It ensures that only *realizable paths*—those with matched call/return parentheses—are included in the slice. This prevents the spurious results that plague naive transitive closure approaches.

**Connection to IFDS**: The two-phase restriction corresponds to respecting the Dyck language of matched parentheses in CFL-reachability (see Section 4.1).

#### Taint Analysis Traversals

<div class="fstarcode">

(\* ————————————————– TAINT ANALYSIS TRAVERSALS Source: Livshits 2005, Tripp 2009 ————————————————– \*)

(\* UNSANITIZED_PATHS: find source-to-sink paths avoiding sanitizers \*) val unsanitized_paths : cpg -\> sources:set node_id -\> sinks:set node_id -\> sanitizers:set node_id -\> list (list node_id) (\* Paths from source to sink \*)

let unsanitized_paths g sources sinks sanitizers = (\* BFS from sources, stopping at sanitizers \*) let rec bfs_paths frontier visited paths = if Set.is_empty frontier then paths else let next_edges = concat_map (fun n -\> Map.find_default n \[\] g.outgoing) frontier in let data_edges = List.filter (fun e -\> match e.label with DataDep \_ -\> true \| \_ -\> false) next_edges in let next_nodes = Set.of_list (List.map (fun e -\> e.target) data_edges) in let unsanitized = Set.diff next_nodes sanitizers in let new_nodes = Set.diff unsanitized visited in (\* Record complete paths \*) let complete = Set.inter new_nodes sinks in let new_paths = (\* ... reconstruct paths ... \*) in bfs_paths new_nodes (Set.union visited new_nodes) (paths @ new_paths) in bfs_paths sources sources \[\]

</div>

#### Label Patterns

<div class="fstarcode">

(\* ————————————————– LABEL PATTERNS — for flexible edge matching ————————————————– \*)

type label_pattern = \| LabelExact : cpg_edge_label -\> label_pattern \| LabelPrefix : string -\> label_pattern (\* e.g., "DataDep" matches DataDep(\_) \*) \| LabelAny : label_pattern \| LabelOr : list label_pattern -\> label_pattern \| LabelAnd : list label_pattern -\> label_pattern \| LabelNot : label_pattern -\> label_pattern

let rec matches (pattern : label_pattern) (label : cpg_edge_label) : bool = match pattern with \| LabelExact l -\> label = l \| LabelPrefix prefix -\> String.is_prefix prefix (label_to_string label) \| LabelAny -\> true \| LabelOr patterns -\> List.exists (fun p -\> matches p label) patterns \| LabelAnd patterns -\> List.for_all (fun p -\> matches p label) patterns \| LabelNot p -\> not (matches p label)

</div>

### CPG Construction

This section describes the algorithm for constructing a Code Property Graph from source code. The construction proceeds in phases, building each component graph layer by layer.

**Construction phases**:

1.  **Parsing**: Convert source to AST, create AST nodes and edges

2.  **CFG construction**: Add control flow edges between statements

3.  **Dominator computation**: Required for control dependence

4.  **PDG construction**: Add data and control dependence edges

5.  **Call graph**: Add interprocedural edges

6.  **Effect graph**: Add side effect tracking edges

7.  **Indexing**: Build efficient lookup structures

<div class="tcolorbox">

The phased approach below is **ONLY** valid for languages without virtual dispatch (C, basic procedural code). For OOP languages (Java, Python, JS, C++), call graph construction **MUST** be interleaved with pointer analysis.

See Section 5.3 (On-the-Fly Call Graph Construction) for the correct approach for dynamic dispatch languages.

**Source**: Qilin (He, Lu, Xue 2022)—“pointer analysis and call graph construction are mutually dependent and must be solved together”

</div>

<div class="fstarcode">

(\* ================================================== CPG CONSTRUCTION ALGORITHM (PHASED - for non-OOP languages only) WARNING: For OOP/dynamic dispatch, use interleaved approach in Section 5.3 ================================================== \*)

module BrrrMachine.CPG.Builder

(\* Main construction pipeline - PHASED VERSION \*) (\* Use build_cpg_interleaved for Java/Python/JS/C++ \*) val build_cpg : language_config -\> source_file -\> cpg let build_cpg lang source = (\* Phase 1: Parse to AST and create AST nodes/edges \*) let ast = parse lang source in let cpg = create_ast_nodes ast in

(\* Phase 2: Build CFG edges \*) let cpg = build_cfg_edges cpg in

(\* Phase 3: Compute dominators for control dependence \*) let dom = compute_dominators cpg in let pdom = compute_post_dominators cpg in

(\* Phase 4: Build PDG edges \*) let cpg = build_control_dep_edges cpg pdom in let cpg = build_data_dep_edges cpg in

(\* Phase 5: Build call graph edges - STATIC ONLY \*) (\* WARNING: This misses virtual/dynamic calls! \*) let cpg = build_static_call_edges cpg in

(\* Phase 6: Build effect edges \*) let cpg = build_effect_edges cpg lang in

(\* Phase 7: Create indices \*) let cpg = build_indices cpg in

cpg

</div>

#### Control Dependence Construction

Control dependence captures which statements’ execution depends on which branch decisions. This is essential for program slicing: if a statement is control-dependent on a branch, changing the branch outcome could affect whether that statement executes.

<div class="definition">

**Definition 1.59** (Control Dependence (Ferrante 1987)). Node $`Y`$ is *control-dependent* on edge $`(A, B)`$ if:

1.  $`Y`$ post-dominates $`B`$

2.  $`Y`$ does not strictly post-dominate $`A`$

**Intuition**: $`Y`$ executes only if the $`A \to B`$ branch is taken.

</div>

<div class="fstarcode">

(\* ————————————————– CONTROL DEPENDENCE (Ferrante 1987)

Definition: Node Y is control-dependent on edge (A,B) if: 1. Y post-dominates B 2. Y does not strictly post-dominate A

Intuition: Y executes only if the A-\>B branch is taken. ————————————————– \*)

val build_control_dep_edges : cpg -\> post_dominators -\> cpg let build_control_dep_edges cpg pdom = (\* For each CFG edge (A,B) \*) fold_edges cpg (fun cpg edge -\> match edge.label with \| CfgTrue \| CfgFalse \| CfgNext -\> let a = edge.source in let b = edge.target in let branch = (edge.label = CfgTrue) in (\* Find least common ancestor in post-dominator tree \*) let lca = pdom_lca pdom a b in (\* All nodes on path from B to LCA (exclusive) are control-dependent on A \*) let path = pdom_path pdom b lca in let ctrl_dep_nodes = List.filter (fun n -\> n \<\> lca) path in (\* Add control dependence edges \*) List.fold_left (fun cpg n -\> add_edge cpg source = a; target = n; label = CtrlDep branch ) cpg ctrl_dep_nodes \| \_ -\> cpg ) cpg

</div>

#### Data Dependence Construction

Data dependence captures which statements’ values flow to which other statements. This is the foundation for taint analysis: if statement $`X`$ defines a value that statement $`Y`$ uses, then taint at $`X`$ propagates to $`Y`$.

<div class="definition">

**Definition 1.60** (Data Dependence). Node $`Y`$ is *data-dependent* on node $`X`$ through variable $`v`$ if:

1.  $`X`$ defines $`v`$

2.  $`Y`$ uses $`v`$

3.  There is a CFG path from $`X`$ to $`Y`$ with no intervening definition of $`v`$

</div>

<div class="fstarcode">

(\* ————————————————– DATA DEPENDENCE (via reaching definitions)

Definition: Node Y is data-dependent on node X through variable v if: 1. X defines v 2. Y uses v 3. There is a CFG path from X to Y with no intervening definition of v ————————————————– \*)

val build_data_dep_edges : cpg -\> cpg let build_data_dep_edges cpg = (\* Compute reaching definitions for each node \*) let reach_defs = compute_reaching_definitions cpg in (\* For each node that uses a variable \*) fold_nodes cpg (fun cpg node -\> let uses = get_used_vars cpg node.id in Set.fold (fun var cpg -\> (\* Find reaching definitions of this variable \*) let defs = get_reaching_defs reach_defs node.id var in (\* Add data dependence edge from each definition \*) Set.fold (fun def_node cpg -\> add_edge cpg source = def_node; target = node.id; label = DataDep var ) defs cpg ) uses cpg ) cpg

</div>

#### Reaching Definitions

Reaching definitions is the classic dataflow problem that computes, for each program point, which definitions of each variable could reach that point without being killed by an intervening redefinition.

**Transfer function**: $`\mathsf{out} = \mathsf{gen} \cup (\mathsf{in} \setminus \mathsf{kill})`$

The `gen` set contains the definition at the current node (if any), while `kill` removes any prior definitions of the same variable. The algorithm iterates to a fixpoint, propagating definitions along CFG edges.

<div class="fstarcode">

(\* ————————————————– REACHING DEFINITIONS (classic dataflow) ————————————————– \*)

type def = node : node_id; var : string type reaching_defs = map node_id (set def)

val compute_reaching_definitions : cpg -\> reaching_defs let compute_reaching_definitions cpg = (\* Transfer function: out = gen $`\cup`$ (in  kill) \*) let transfer node in_set = let gen = match get_defined_var cpg node with \| Some v -\> Set.singleton node = node; var = v \| None -\> Set.empty in let kill = match get_defined_var cpg node with \| Some v -\> Set.filter (fun d -\> d.var \<\> v) in_set \| None -\> Set.empty in Set.union gen (Set.diff in_set kill) in (\* Fixpoint iteration \*) let rec iterate current = let next = Map.mapi (fun node \_ -\> let preds = get_cfg_predecessors cpg node in let in_set = Set.unions (List.map (fun p -\> Map.find_default p Set.empty current) preds) in transfer node in_set ) current in if Map.equal Set.equal current next then current else iterate next in let init = Map.map (fun \_ -\> Set.empty) cpg.nodes in iterate init

</div>

### Neural Program Graph Embeddings (Optional Enhancement)

This section describes optional neural network enhancements to the CPG that enable learning-based analysis. While the core CPG provides a sound foundation for static analysis, neural embeddings can capture patterns that are difficult to express with hand-crafted rules.

**Paper**: Si et al. 2018 (Code2Inv—NeurIPS)

<div class="tcolorbox">

**INSIGHT**: The CPG already encodes AST, CFG, and data flow edges. A Graph Neural Network (GNN) can learn *dense vector embeddings* that capture semantic patterns beyond what static traversals can express.

**ARCHITECTURE (Code2Inv)**:

1.  Convert program to SSA form (explicit data dependencies)

2.  Build program graph with 6 edge types:

    - AST parent/child (bidirectional)

    - CFG forward/backward (bidirectional)

    - Data flow (SSA connections, bidirectional)

3.  Message passing with edge-type-specific weight matrices

4.  GRU aggregation for each node embedding update

**GNN Update Rule** (each round $`t`$):
``` math
h_v^{(t+1)} = \mathsf{GRU}\left(h_v^{(t)}, \sum_{(u,v,k)} W_k \cdot h_u^{(t)}\right)
```
where $`W_k`$ is the weight matrix for edge type $`k`$.

**USE CASES**:

- Loop invariant synthesis (Section 2.1.5c)

- Bug pattern detection via learned embeddings

- Semantic code search and similarity

</div>

<div class="fstarcode">

(\* ================================================== NEURAL PROGRAM GRAPH EMBEDDING Source: Si et al. 2018 (Code2Inv) ================================================== \*)

module BrrrMachine.CPG.NeuralEmbedding

(\* Embedding dimension - typically 64 or 128 \*) let embedding_dim : nat = 64

type node_embedding = vec float embedding_dim

(\* Edge types for GNN message passing - extends cpg_edge_label \*) type gnn_edge_type = \| GNNAstChild : gnn_edge_type (\* AST parent to child \*) \| GNNAstParent : gnn_edge_type (\* AST child to parent (reverse) \*) \| GNNCfgNext : gnn_edge_type (\* Control flow forward \*) \| GNNCfgPrev : gnn_edge_type (\* Control flow backward \*) \| GNNDataFlow : gnn_edge_type (\* SSA data dependency \*) \| GNNDataFlowRev : gnn_edge_type (\* Reverse data dependency \*)

(\* GNN parameters - one weight matrix per edge type \*) type gnn_params = edge_weights : map gnn_edge_type (matrix embedding_dim embedding_dim); gru_weights : gru_params embedding_dim; token_embeds : map token_type node_embedding; (\* Initial node embeddings \*)

(\* Program embedding: all nodes + global graph summary \*) type program_embedding = node_embeds : map node_id node_embedding; global_embed : node_embedding; (\* Aggregated graph representation \*)

(\* Convert CPG edge labels to GNN edge types \*) val cpg_to_gnn_edge : cpg_edge_label -\> bool -\> gnn_edge_type let cpg_to_gnn_edge label is_reverse = match label, is_reverse with \| AstChild \_, false -\> GNNAstChild \| AstChild \_, true -\> GNNAstParent \| CfgNext, false \| CfgTrue, false \| CfgFalse, false -\> GNNCfgNext \| CfgNext, true \| CfgTrue, true \| CfgFalse, true -\> GNNCfgPrev \| DataDep \_, false -\> GNNDataFlow \| DataDep \_, true -\> GNNDataFlowRev \| \_ -\> GNNCfgNext (\* Default for other edge types \*)

(\* Single round of message passing \*) val message_pass_round : cpg:cpg -\> params:gnn_params -\> current:map node_id node_embedding -\> map node_id node_embedding

let message_pass_round cpg params current = Map.mapi (fun nid \_ -\> (\* Aggregate messages from all incoming edges \*) let incoming = get_all_incoming_edges cpg nid in let message = List.fold_left (fun acc (src, label) -\> let etype = cpg_to_gnn_edge label true in let w = Map.find etype params.edge_weights in let h_src = Map.find src current in vec_add acc (mat_vec_mul w h_src) ) (zero_vec embedding_dim) incoming in (\* GRU update \*) gru_cell params.gru_weights (Map.find nid current) message ) current

(\* Full encoding with T message passing rounds \*) val encode_program : cpg:cpg -\> params:gnn_params -\> rounds:natrounds \> 0 -\> program_embedding

let encode_program cpg params rounds = (\* Initialize embeddings from token types \*) let initial = Map.mapi (fun nid node -\> let token = node_to_token node in Map.find_default token (zero_vec embedding_dim) params.token_embeds ) cpg.nodes in (\* T rounds of message passing \*) let final = iterate_n rounds (message_pass_round cpg params) initial in (\* Global embedding via max pooling \*) let global = Map.fold (fun \_ emb acc -\> vec_max acc emb) final (zero_vec embedding_dim) in node_embeds = final; global_embed = global

(\* Attention over program nodes - for invariant synthesis \*) val compute_attention : query:node_embedding -\> program:program_embedding -\> map node_id float (\* Attention weights sum to 1 \*)

let compute_attention query program = let scores = Map.map (fun emb -\> vec_dot query emb) program.node_embeds in softmax scores

</div>

The F\* code above defines a complete GNN-based program embedding system:

- `gnn_edge_type`: Maps CPG edge labels to GNN-compatible edge types (with bidirectional variants)

- `message_pass_round`: Single iteration of message passing with edge-type-specific weights

- `encode_program`: Full encoding with $`T`$ rounds of message passing

- `compute_attention`: Attention mechanism for focusing on relevant program nodes

**Key insight**: The GNN learns to aggregate information from a node’s graph neighborhood, capturing semantic relationships that depend on both local syntax and global program structure.

<div class="remark">

**Remark 1.61** (Integration with CPG).

- GNN embeddings are an **optional enhancement** to the static CPG

- The CPG provides the graph structure; GNN learns dense representations

- Pre-trained embeddings can be loaded for transfer learning

- Use embeddings for neural invariant synthesis (Section 2.1.5c)

</div>

<div class="remark">

**Remark 1.62** (When to Use GNN Embeddings).

- Learning-based analysis (invariant synthesis, bug detection)

- Semantic code similarity/search

- When static patterns are insufficient

</div>

#### Semantic Identifier Embeddings (DeepBugs)

While GNN embeddings capture program structure, identifier names carry crucial semantic information that traditional static analysis ignores. This section describes how to leverage name semantics for bug detection.

**Source**: Pradel & Sen 2018—“DeepBugs: A Learning Approach to Name-Based Bug Detection” (OOPSLA)

<div class="tcolorbox">

**INSIGHT**: Traditional static analysis **ignores** identifier names. But names encode *semantic intent* that can reveal bugs:

- `setTimeout(delay, callback)` vs `setTimeout(callback, delay)`

- `height + width` (bug?) vs `height + height`

- `j < tokens.length` (should be `j < token.length`?)

**DEEPBUGS APPROACH**:

1.  Create *semantic embeddings* for identifiers via word2vec

2.  Split identifiers into subtokens: `getUserName` $`\to`$ \[`get`, `User`, `Name`\]

3.  Train binary classifiers: correct code vs synthetic bugs

4.  Detect swapped arguments, wrong operators, wrong operands

**RESULTS**: 89–95% accuracy, 68% true positive rate on real bugs  
**SPEED**: $`<`$<!-- -->20ms per file (pre-computed embeddings + matrix ops)

</div>

<div class="fstarcode">

(\* ================================================== SEMANTIC IDENTIFIER EMBEDDINGS Source: Pradel & Sen 2018 (DeepBugs)

Capture semantic meaning of identifiers via word2vec-style embeddings. "src" and "source" have similar embeddings; "src" and "dst" are opposites. ================================================== \*)

module BrrrMachine.CPG.NameEmbeddings

(\* ————————————————– SUBTOKEN SPLITTING

Split identifiers into semantically meaningful subtokens: - camelCase: getUserName -\> \[get, user, name\] - snake_case: user_name -\> \[user, name\] - Combined: getUser_ID -\> \[get, user, id\] ————————————————– \*)

val split_camel_case : string -\> list string let split_camel_case name = (\* Split on case transitions: "getUserName" -\> \["get", "User", "Name"\] \*) regex_split "\[A-Z\]" name \|\> List.map String.lowercase

val split_snake_case : string -\> list string let split_snake_case name = String.split "\_" name \|\> List.filter (fun s -\> String.length s \> 0)

val split_identifier : string -\> list string let split_identifier name = let camel = split_camel_case name in let snake = split_snake_case name in List.deduplicate (camel @ snake) \|\> List.map String.lowercase

(\* ————————————————– EMBEDDING MODEL

Pre-trained word2vec model maps subtokens to dense vectors. Identifier embedding = average of subtoken embeddings. ————————————————– \*)

type embedding_dim = n:natn \> 0 type embedding (dim:embedding_dim) = vector float dim

noeq type name_embedding_model (dim:embedding_dim) = subtoken_embeddings : map string (embedding dim); oov_embedding : embedding dim; (\* Out-of-vocabulary fallback \*)

val embed_identifier : \#dim:embedding_dim -\> model:name_embedding_model dim -\> name:string -\> embedding dim

let embed_identifier \#dim model name = let subtokens = split_identifier name in let embeddings = List.filter_map (fun st -\> Map.find_opt st model.subtoken_embeddings ) subtokens in if List.is_empty embeddings then model.oov_embedding else vector_average embeddings

</div>

The `split_identifier` function handles common naming conventions (camelCase, snake_case) to extract meaningful subtokens. The `embed_identifier` function averages subtoken embeddings, falling back to an out-of-vocabulary embedding for unknown tokens.

<div class="fstarcode">

(\* Cosine similarity between identifier embeddings \*) val name_similarity : \#dim:embedding_dim -\> model:name_embedding_model dim -\> name1:string -\> name2:string -\> float

let name_similarity \#dim model name1 name2 = let e1 = embed_identifier model name1 in let e2 = embed_identifier model name2 in cosine_similarity e1 e2

</div>

##### Name-Based Bug Detectors

DeepBugs defines three detector types, each trained on synthetically generated bugs. The detectors extract identifier embeddings from code locations and classify whether the code is likely buggy.

**Detector types**:

1.  **Swapped Arguments**: Detects when function arguments may be in the wrong order based on name semantics

2.  **Wrong Binary Operator**: Detects when an operator may be incorrect (e.g., `==` vs `!=`)

3.  **Wrong Binary Operand**: Detects when a variable may be the wrong one based on name similarity

<div class="fstarcode">

(\* ————————————————– NAME-BASED BUG DETECTORS

DeepBugs defines three detector types, each trained on synthetic bugs: 1. Swapped Arguments: setTimeout(delay, fn) vs setTimeout(fn, delay) 2. Wrong Binary Operator: x == y vs x != y 3. Wrong Binary Operand: height + width vs height + height ————————————————– \*)

type ml_prediction = is_bug : bool; confidence : float; (\* 0.0 to 1.0 \*) bug_type : string; explanation : option string;

(\* Feature extraction for call sites \*) val extract_call_features : \#dim:embedding_dim -\> model:name_embedding_model dim -\> cpg:cpg -\> call_node:node_id -\> option (list (embedding dim))

let extract_call_features \#dim model cpg call_node = match get_node cpg call_node with \| Some (NCall callee args) -\> let callee_emb = embed_identifier model callee in let arg_embs = List.map (fun arg -\> match get_identifier_name cpg arg with \| Some name -\> embed_identifier model name \| None -\> model.oov_embedding ) args in Some (callee_emb :: arg_embs) \| \_ -\> None

(\* Feature extraction for binary operations \*) val extract_binop_features : \#dim:embedding_dim -\> model:name_embedding_model dim -\> cpg:cpg -\> expr_node:node_id -\> option (list (embedding dim))

let extract_binop_features \#dim model cpg expr_node = match get_node cpg expr_node with \| Some (NBinOp left op right) -\> let left_emb = match get_identifier_name cpg left with \| Some name -\> embed_identifier model name \| None -\> model.oov_embedding in let right_emb = match get_identifier_name cpg right with \| Some name -\> embed_identifier model name \| None -\> model.oov_embedding in let op_emb = embed_operator model op in Some \[left_emb; op_emb; right_emb\] \| \_ -\> None

</div>

##### Synthetic Bug Generation for Training

A key insight from DeepBugs is that training data can be generated by simple mutations of correct code. Correct code serves as positive examples; mutated code serves as negative examples. This avoids the need for expensive manual bug labeling.

<div class="fstarcode">

(\* ————————————————– SYNTHETIC BUG GENERATION FOR TRAINING

Key insight: Generate training data by simple mutations. Correct code = positive examples; mutated code = negative examples. ————————————————– \*)

type mutation_type = \| SwapArguments : idx1:nat -\> idx2:nat -\> mutation_type \| ReplaceOperator : new_op:binop -\> mutation_type \| ReplaceOperand : side:bool -\> new_var:string -\> mutation_type

type training_example = features : list (embedding embedding_dim); label : bool; (\* true = correct, false = buggy \*) mutation : option mutation_type;

val generate_training_examples : cpg:cpg -\> model:name_embedding_model embedding_dim -\> detector_type:string -\> list training_example

let generate_training_examples cpg model detector_type = match detector_type with \| "swapped_args" -\> (\* Find all call sites, generate original + swapped versions \*) let calls = find_all_calls cpg in List.concat_map (fun call -\> match extract_call_features model cpg call with \| Some features when List.length features \>= 3 -\> let original = features; label = true; mutation = None in let swapped = features = swap_in_list features 1 2; label = false; mutation = Some (SwapArguments 0 1) in \[original; swapped\] \| \_ -\> \[\] ) calls \| "wrong_operator" -\> let binops = find_all_binops cpg in List.concat_map (fun binop -\> match extract_binop_features model cpg binop with \| Some features -\> let original = features; label = true; mutation = None in (\* Generate mutants with different operators \*) let mutants = List.map (fun new_op -\> features = replace_op_embedding features new_op; label = false; mutation = Some (ReplaceOperator new_op) ) (alternative_operators (get_operator binop)) in original :: mutants \| None -\> \[\] ) binops \| \_ -\> \[\]

</div>

##### Name-Based Security Role Classification

Function names often indicate their security relevance. Names like “sanitize”, “escape”, and “encode” strongly suggest sanitizer functions, while “input”, “request”, and “param” suggest sources. This section shows how to leverage name embeddings for automatic security role classification.

**Application**: Augment taint analysis configuration by discovering likely sanitizers that were not explicitly configured.

<div class="fstarcode">

(\* ————————————————– NAME-BASED SECURITY ROLE CLASSIFICATION

Names often indicate security relevance: - "sanitize", "escape", "encode" -\> likely sanitizer - "input", "request", "param" -\> likely source - "query", "exec", "eval" -\> likely sink ————————————————– \*)

type security_role = \| LikelySource \| LikelySink \| LikelySanitizer \| LikelyValidator \| SecurityNeutral

(\* Canonical security term embeddings \*) noeq type security_classifier (dim:embedding_dim) = source_centroid : embedding dim; (\* avg of "input", "request", "param" \*) sink_centroid : embedding dim; (\* avg of "query", "exec", "eval" \*) sanitizer_centroid : embedding dim; (\* avg of "escape", "sanitize", "encode" \*) validator_centroid : embedding dim; (\* avg of "validate", "check", "verify" \*) threshold : float;

val classify_security_role : \#dim:embedding_dim -\> classifier:security_classifier dim -\> model:name_embedding_model dim -\> func_name:string -\> option (security_role \* float)

let classify_security_role \#dim classifier model func_name = let func_emb = embed_identifier model func_name in let similarities = \[ (LikelySource, cosine_similarity func_emb classifier.source_centroid); (LikelySink, cosine_similarity func_emb classifier.sink_centroid); (LikelySanitizer, cosine_similarity func_emb classifier.sanitizer_centroid); (LikelyValidator, cosine_similarity func_emb classifier.validator_centroid); \] in let (best_role, best_sim) = List.max_by snd similarities in if best_sim \>= classifier.threshold then Some (best_role, best_sim) else None

(\* Augment taint configuration with name-based hints \*) val augment_taint_config : \#dim:embedding_dim -\> classifier:security_classifier dim -\> model:name_embedding_model dim -\> cpg:cpg -\> base_config:taint_config -\> taint_config

let augment_taint_config \#dim classifier model cpg base_config = let functions = get_all_functions cpg in let classified = List.filter_map (fun f -\> classify_security_role classifier model f.name ) functions in let new_sanitizers = List.filter_map (fun (f, role, \_) -\> if role = LikelySanitizer then Some f.id else None ) (List.zip functions classified) in base_config with sanitizers = Set.union base_config.sanitizers (Set.of_list new_sanitizers)

</div>

<div class="remark">

**Remark 1.63** (Integration with Taint Analysis (Section 8.1.2)).

- Use name-based classification to discover likely sanitizers automatically

- Augment configured sanitizer list with name-inferred candidates

- Report confidence level based on embedding similarity score

</div>

<div class="remark">

**Remark 1.64** (Integration with Static Analysis (Section 13.4 Layer 5.5)).

- ML predictions feed into confidence calculation at Layer 6

- Combine static analysis violations with ML bug predictions

- Higher confidence when static + ML agree

</div>

<div class="remark">

**Remark 1.65** (Cross-References).

- Section 2.1.5c: Neural invariant synthesis (primary consumer)

- Section 3.1.2: CPG components (provides graph structure)

- Section 3.1.3: Edge types (mapped to GNN edge types)

- Section 8.1.2: Taint analysis (name-based sanitizer detection)

- Section 4.4.7: Optimistic concolic (ML can prioritize paths)

</div>

# Analysis Algorithms

<div class="pillarbox">

**IFDS (Section <a href="#sec:ifds" data-reference-type="ref" data-reference="sec:ifds">1</a>):**

- Requires DISTRIBUTIVE transfer functions: $`f(a \sqcup b) = f(a) \sqcup f(b)`$

- Tracks DATAFLOW FACTS

- $`\mathcal{O}{ED^3}`$ guaranteed complexity

- Path-INSENSITIVE

**Bi-Abduction/Eval (Section <a href="#sec:under-approx" data-reference-type="ref" data-reference="sec:under-approx">3</a>):**

- NOT distributive

- Multiple valid $`(M, F)`$ pairs

- Tracks SEPARATION LOGIC assertions

- May be exponential

- Path-SENSITIVE

**These are different algorithms for different problems:**

- Taint analysis $`\rightarrow`$ IFDS (fast, whole-program)

- Shape/memory analysis $`\rightarrow`$ Eval (precise, local)

- Hybrid: IFDS finds candidates, Eval verifies (Section <a href="#sec:ifds-eval-hybrid" data-reference-type="ref" data-reference="sec:ifds-eval-hybrid">3.4</a>)

**DO NOT** try to force bi-abduction into IFDS framework! The mathematical properties are incompatible.

</div>

<div class="pillarbox">

**Source**: Aiken 1999. See Appendix D.10.1 for full analysis.

**Insight**: IFDS is a RESTRICTED FRAGMENT of set constraints.

- IFDS requires distributive transfer functions

- Set constraints (Section 12.18) handle non-distributive cases

- Pointer analysis is NOT distributive—cannot use IFDS directly

**Resolution**:

- Use IFDS for taint, reaching definitions, live variables (distributive)

- Use set constraints for type inference, pointer analysis (general)

- Section 12.18 provides unified constraint framework

</div>

<div class="pillarbox">

**Sources**: Jordan 2016 (Souffle), Madsen 2016 (Flix)

**Old View**: “Don’t build custom Datalog engine, use existing or skip”

**New View**: COMPILE Datalog rules to specialized code, don’t interpret

Jordan 2016 demonstrates: Souffle achieves 50x+ speedup over bddbddb by eliminating interpretation overhead via staged specialization:

1.  Stage 1: Datalog $`\rightarrow`$ RAM (Relational Algebra Machine)

2.  Stage 2: RAM $`\rightarrow`$ Optimized RAM (Dilworth indices, join ordering)

3.  Stage 3: RAM $`\rightarrow`$ C++/Rust (specialized, parallel)

**Resolution for brrr-machine**:

- Development: Use interpreted Datalog (Crepe, Souffle -interpreter)

- Production: Compile analysis rules to specialized Rust

- See Section <a href="#sec:datalog-compilation" data-reference-type="ref" data-reference="sec:datalog-compilation">1.6</a> for compilation strategy details

</div>

<div class="pillarbox">

**Source**: Madsen 2016

**Standard Datalog**: Relations (finite sets of tuples)

**Flix Extension**: Lattice predicates (map from keys to lattice elements)

This unifies:

- IFDS = Flix where lattice is powerset of dataflow facts

- IDE = Flix where lattice is micro-function space

- Constant propagation = Flix where lattice is constant domain

All require MONOTONE transfer functions for soundness. See Section <a href="#sec:flix-lattice" data-reference-type="ref" data-reference="sec:flix-lattice">1.7</a> for lattice-extended Datalog details.

</div>

## IFDS: Interprocedural Finite Distributive Subset

**Paper**: Reps, Horwitz, Sagiv 1995

<span class="sans-serif">IFDS</span> is the workhorse algorithm for precise interprocedural dataflow analysis **when transfer functions are DISTRIBUTIVE** ($`f(a \sqcup b) = f(a) \sqcup f(b)`$). Its genius is reducing dataflow problems to graph reachability.

**Important**: <span class="sans-serif">IFDS</span> does NOT apply to pointer analysis (non-distributive). See Section 12.18 for set constraints handling non-distributive cases.

<div class="pillarbox">

<span class="sans-serif">IFDS</span> is lattice-extended Datalog where:

- Lattice = powerset of dataflow facts (finite)

- Transfer = gen/kill functions (distributive)

- Distributivity = transfer distributes over join

<span class="sans-serif">IDE</span> extends <span class="sans-serif">IFDS</span> where:

- Lattice = micro-function space (environment transformers)

- Transfer = function composition

- Enables: constant propagation, linear constant propagation

**Souffle Perspective (Jordan 2016)**: <span class="sans-serif">IFDS</span> expressed as Datalog can be COMPILED, not just interpreted:

1.  Stage 1: <span class="sans-serif">IFDS</span> $`\rightarrow`$ Datalog rules (declarative specification)

2.  Stage 2: Datalog $`\rightarrow`$ RAM (semi-naive evaluation, index selection)

3.  Stage 3: RAM $`\rightarrow`$ Rust/C++ (specialized code via templates)

This achieves 50x+ speedup over interpreted Datalog (bddbddb, muZ). See Section <a href="#sec:datalog-compilation" data-reference-type="ref" data-reference="sec:datalog-compilation">1.6</a> for compilation strategy details.

</div>

### The Key Insight

**The Problem**: Interprocedural analysis is hard because:

- Must track calling context (don’t mix up callers)

- Must handle recursion

- Naive approach: exponential in call depth

**Example (imprecise without context)**:

    def f(x):
      return x + 1
    a = f(1)   # Should be 2
    b = f(10)  # Should be 11

Without context sensitivity: `f` receives $`\{1, 10\}`$, returns $`\{2, 11\}`$, so both `a` and `b` get $`\{2, 11\}`$ — IMPRECISE!

With context sensitivity: Call 1 has `f` receive 1, return 2; Call 2 has `f` receive 10, return 11. Thus $`a = 2`$, $`b = 11`$ — PRECISE!

**The Reps Insight**: Frame the problem as GRAPH REACHABILITY with context encoded as MATCHED PARENTHESES:

- $`(_{1} \ldots )_{1}`$ means: call site 1, return to site 1

- $`(_{1} (_{2} )_{2} )_{1}`$ means: nested call, properly matched

INVALID paths like $`(_{1} )_{2}`$ are automatically rejected. This is <span class="sans-serif">CFL</span>-reachability with the Dyck language.

### The Exploded Supergraph

**Construction**: Given:

- Program with procedures $`p_1, \ldots, p_n`$

- <span class="sans-serif">CFG</span> for each procedure

- Finite set $`D`$ of dataflow facts ($`|D| = d`$)

Build the **Exploded Supergraph** $`G^\#`$:
``` math
\begin{aligned}
\text{NODES} &: \{ \langle n, d \rangle \mid n \text{ is a program point}, d \in D \cup \{0\} \} \\
\text{EDGES} &: \text{For each CFG edge } n_1 \rightarrow n_2 \text{ and its transfer function } f: \\
&\quad \text{Add edge } \langle n_1, d_1 \rangle \rightarrow \langle n_2, d_2 \rangle \text{ iff } d_2 \in f(\{d_1\})
\end{aligned}
```

The $`0`$ fact represents “nothing” — it’s the identity for the dataflow function.

<div class="theorem">

**Theorem 1.66** (Reps 1995). *Fact $`d`$ holds at program point $`n`$ if and only if there exists a **realizable path** from $`\langle s_{\text{main}}, 0 \rangle`$ to $`\langle n, d \rangle`$.*

*Realizable = matched call/return parentheses.*

</div>

### The Tabulation Algorithm

The tabulation algorithm is the heart of IFDS. It computes *path edges* that track same-level realizable paths through the exploded supergraph, and *summary edges* that capture the input-output behavior of procedures. The algorithm uses a worklist to propagate facts, handling four cases: intraprocedural edges, call edges (entering callees), exit edges (leaving procedures), and call-to-return edges (bypassing calls). The key insight is that summary edges allow reusing procedure analysis across multiple call sites.

The following F\* code defines the core data structures: `path_edge` represents a realizable path from procedure entry with fact `d1` to node `n` with fact `d2`; `summary_edge` captures a procedure’s effect from call site to return site. The `solve` function implements the worklist algorithm, achieving $`O(ED^3)`$ complexity where $`E`$ is the number of edges and $`D`$ is the domain size.

<div class="fstarcode">

module BrrrMachine.IFDS

(\* ————————————————– PROBLEM DEFINITION ————————————————– \*)

(\* An IFDS problem instance \*) type ifds_problem (d : Type) = (\* The supergraph \*) supergraph : cpg; (\* The dataflow domain — MUST BE FINITE \*) domain : finite_set d; (\* The zero fact \*) zero : d; (\* Flow functions for each edge type \*) flow_function : cpg_edge -\> d -\> set d; (\* Call flow: how facts map at call site \*) call_flow : node_id -\> d -\> set d; (\* Return flow: how facts map at return \*) return_flow : node_id -\> node_id -\> d -\> d -\> set d; (\* Call-to-return flow: for facts not passed to callee \*) call_to_return_flow : node_id -\> d -\> set d;

(\* ————————————————– DATA STRUCTURES ————————————————– \*)

(\* Path edge: same-level realizable path \*) type path_edge (d : Type) = (\* Procedure containing this path \*) proc_entry : node_id; (\* Starting fact at procedure entry \*) d1 : d; (\* Current node reached \*) n : node_id; (\* Current fact \*) d2 : d;

(\* Summary edge: captures procedure’s effect \*) type summary_edge (d : Type) = (\* Call site \*) call_site : node_id; (\* Fact at call \*) d1 : d; (\* Return site \*) return_site : node_id; (\* Fact at return \*) d2 : d;

(\* Solver state \*) type ifds_state (d : Type) = path_edges : set (path_edge d); summary_edges : set (summary_edge d); worklist : list (path_edge d);

(\* ————————————————– THE ALGORITHM ————————————————– \*)

val solve : \#d:Type -\> ifds_problem d -\> set (node_id \* d) let solve \#d prob = (\* Initialize with entry points \*) let entries = get_entry_points prob.supergraph in let init_edges = Set.of_list (List.map (fun entry -\> proc_entry = entry; d1 = prob.zero; n = entry; d2 = prob.zero ) entries) in let init_state = path_edges = init_edges; summary_edges = Set.empty; worklist = Set.to_list init_edges; in

(\* Main loop \*) let rec process state = match state.worklist with \| \[\] -\> state \| edge :: rest -\> let state’ = state with worklist = rest in let state” = process_edge prob edge state’ in process state” in let final = process init_state in (\* Extract results \*) Set.map (fun pe -\> (pe.n, pe.d2)) final.path_edges

val process_edge : \#d:Type -\> ifds_problem d -\> path_edge d -\> ifds_state d -\> ifds_state d let process_edge \#d prob edge state = let n = edge.n in let d2 = edge.d2 in match get_node_type prob.supergraph n with

(\* ——————————————————————— CASE 1: Call node ——————————————————————— \*) \| NCall -\> let callee = get_callee prob.supergraph n in let callee_entry = get_entry_node prob.supergraph callee in let return_site = get_return_site prob.supergraph n in

(\* Propagate to callee entry \*) let callee_facts = prob.call_flow n d2 in let callee_edges = Set.map (fun d3 -\> proc_entry = callee_entry; d1 = d3; n = callee_entry; d2 = d3 ) callee_facts in

(\* Apply existing summaries \*) let matching_summaries = Set.filter (fun se -\> se.call_site = n && Set.mem se.d1 callee_facts ) state.summary_edges in let summary_edges = Set.map (fun se -\> proc_entry = edge.proc_entry; d1 = edge.d1; n = return_site; d2 = se.d2 ) matching_summaries in

(\* Call-to-return flow (for facts not passed to callee) \*) let ctr_facts = prob.call_to_return_flow n d2 in let ctr_edges = Set.map (fun d3 -\> proc_entry = edge.proc_entry; d1 = edge.d1; n = return_site; d2 = d3 ) ctr_facts in propagate_all (Set.unions \[callee_edges; summary_edges; ctr_edges\]) state

(\* ——————————————————————— CASE 2: Exit node ——————————————————————— \*) \| NExit proc -\> let callers = get_call_sites prob.supergraph proc in (\* For each caller with matching entry fact \*) let new_edges = Set.concat_map (fun call_site -\> let return_site = get_return_site prob.supergraph call_site in (\* Find path edges reaching this call with matching callee entry fact \*) let matching_paths = Set.filter (fun pe -\> pe.n = call_site && Set.mem edge.d1 (prob.call_flow call_site pe.d2) ) state.path_edges in Set.concat_map (fun caller_edge -\> let d4 = caller_edge.d2 in let d5 = prob.return_flow call_site return_site d4 d2 in (\* Create summary edge \*) let new_summary = call_site = call_site; d1 = edge.d1; return_site = return_site; d2 = d2 in (\* Propagate to return site \*) let return_edges = Set.map (fun d5’ -\> proc_entry = caller_edge.proc_entry; d1 = caller_edge.d1; n = return_site; d2 = d5’ ) d5 in (new_summary, return_edges) ) matching_paths ) callers in let (new_summaries, new_path_edges) = Set.partition_pair new_edges in let state’ = state with summary_edges = Set.union state.summary_edges new_summaries in propagate_all (Set.flatten new_path_edges) state’

(\* ——————————————————————— CASE 3: Ordinary node ——————————————————————— \*) \| \_ -\> let successors = get_cfg_successors prob.supergraph n in let new_edges = Set.concat_map (fun succ -\> let d3_set = prob.flow_function source = n; target = succ; label = CfgNext d2 in Set.map (fun d3 -\> proc_entry = edge.proc_entry; d1 = edge.d1; n = succ; d2 = d3 ) d3_set ) successors in propagate_all new_edges state

val propagate_all : \#d:Type -\> set (path_edge d) -\> ifds_state d -\> ifds_state d let propagate_all \#d edges state = let new_edges = Set.diff edges state.path_edges in state with path_edges = Set.union state.path_edges new_edges; worklist = Set.to_list new_edges @ state.worklist

</div>

**Complexity Analysis**: Let $`N`$ = number of nodes in supergraph, $`E`$ = number of edges, $`D`$ = size of dataflow domain.

**Space**: Path edges: $`\mathcal{O}{N \times D^2}`$ — bounded by (proc_entry, $`d_1`$, $`n`$, $`d_2`$). Summary edges: $`\mathcal{O}{\text{Call\_sites} \times D^2}`$. Total: $`\mathcal{O}{N \times D^2}`$.

**Time**: Each path edge processed once: $`\mathcal{O}{N \times D^2}`$ iterations. Each iteration: $`\mathcal{O}{D}`$ work for flow functions. Total: $`\mathcal{O}{N \times D^3} = \mathcal{O}{E \times D^3}`$ since $`E \geq N`$.

**Key Insight**: Polynomial in $`D`$, not exponential! This is because we track (entry_fact, current_fact) pairs, not full paths through the exploded graph.

<div class="pillarbox">

**Source**: Arzt 2014 (FlowDroid). Cross-reference: Section 8.1.6.

For HEAP-SENSITIVE taint analysis, standard forward-only <span class="sans-serif">IFDS</span> is insufficient. FlowDroid extends <span class="sans-serif">IFDS</span> with bidirectional analysis:

- **Forward**: Propagate taint from sources, spawn backward on heap writes

- **Backward**: Find heap aliases, spawn forward with INACTIVE taint

The two directions SPAWN each other:

- Forward $`\rightarrow`$ finds `p.f = tainted` $`\rightarrow`$ spawns Backward to find aliases of `p.f`

- Backward $`\rightarrow`$ finds `q` aliased to `p` $`\rightarrow`$ spawns Forward with (`q.f`, INACTIVE)

**Important**: This is NOT pure <span class="sans-serif">IFDS</span>!

- Forward taint propagation: <span class="sans-serif">IFDS</span> (distributive)

- Backward alias finding: NOT <span class="sans-serif">IFDS</span> (may-alias is non-distributive)

- Integration: Ad-hoc spawning mechanism

A cleaner formalization would use <span class="sans-serif">IDE</span> (Section <a href="#sec:flix-lattice" data-reference-type="ref" data-reference="sec:flix-lattice">1.7</a>) for the backward phase. See Section 8.1.6.1 for full activation statement semantics.

</div>

### Common IFDS Problems

IFDS is a general framework that can express many dataflow analyses. Each instantiation requires defining: (1) a finite domain $`D`$ of dataflow facts, (2) flow functions for different edge types, and (3) the distributive merge operation (set union). The following code demonstrates two classic instantiations:

**Reaching Definitions**: The domain consists of (variable, definition-site) pairs. Flow functions implement gen/kill: assignments *generate* new definitions and *kill* previous definitions of the same variable. This analysis answers: “Which definitions of variable $`v`$ can reach program point $`p`$?”

**Taint Analysis**: The domain tracks tainted variables and fields. Sources introduce taint, sanitizers remove it, and assignments propagate it. This analysis answers: “Can user input reach a security-sensitive sink?” The taint analysis problem is foundational for detecting injection vulnerabilities.

<div class="fstarcode">

(\* ————————————————– REACHING DEFINITIONS ————————————————– \*)

type reaching_def = var : string; def_site : node_id

let reaching_definitions_problem (cpg : cpg) : ifds_problem reaching_def =

supergraph = cpg; domain = all_definitions cpg; zero = var = ""; def_site = 0 ; (\* Dummy zero \*)

flow_function = fun edge d -\> let n = edge.source in match get_defined_var cpg n with \| Some v when d.var = v -\> (\* Kill: this definition kills previous defs of same var \*) Set.empty \| Some v -\> (\* Gen: add this definition, keep others \*) Set.add var = v; def_site = n (Set.singleton d) \| None -\> Set.singleton d; (\* Pass through \*)

call_flow = fun call_site d -\> (\* Pass definitions through call \*) Set.singleton d;

return_flow = fun call_site return_site d_call d_exit -\> (\* Definitions from callee flow back \*) Set.singleton d_exit;

call_to_return_flow = fun call_site d -\> (\* Local definitions not affected by call \*) if is_local_def cpg d.var then Set.singleton d else Set.empty;

(\* ————————————————– TAINT ANALYSIS (via IFDS) Source: Livshits 2005 ————————————————– \*)

type taint_fact = \| TaintedVar : var:string -\> taint_fact \| TaintedField : base:string -\> field:string -\> taint_fact \| TaintedReturn : taint_fact (\* Return value is tainted \*)

let taint_analysis_problem (cpg : cpg) (sources : set node_id) (sanitizers : set node_id) : ifds_problem taint_fact =

supergraph = cpg; domain = all_taint_facts cpg; zero = TaintedVar ""; (\* Dummy \*)

flow_function = fun edge d -\> let n = edge.source in (\* Source: introduce taint \*) if Set.mem n sources then match get_assigned_var cpg n with \| Some v -\> Set.add (TaintedVar v) (Set.singleton d) \| None -\> Set.singleton d (\* Sanitizer: remove taint \*) else if Set.mem n sanitizers then match get_assigned_var cpg n with \| Some v when d = TaintedVar v -\> Set.empty \| \_ -\> Set.singleton d (\* Assignment: propagate taint \*) else match get_assignment cpg n with \| Some (lhs, rhs_vars) -\> let rhs_tainted = List.exists (fun v -\> d = TaintedVar v) rhs_vars in if rhs_tainted then Set.add (TaintedVar lhs) (Set.singleton d) else if d = TaintedVar lhs then Set.empty (\* Overwritten with clean value \*) else Set.singleton d \| None -\> Set.singleton d;

call_flow = fun call_site d -\> (\* Map tainted actuals to tainted formals \*) match d with \| TaintedVar v -\> let param_index = get_arg_index cpg call_site v in begin match param_index with \| Some i -\> Set.singleton (TaintedVar (get_formal cpg call_site i)) \| None -\> Set.empty end \| \_ -\> Set.empty;

return_flow = fun call_site return_site d_call d_exit -\> (\* Map tainted return to tainted result variable \*) match d_exit with \| TaintedReturn -\> begin match get_result_var cpg return_site with \| Some v -\> Set.singleton (TaintedVar v) \| None -\> Set.empty end \| \_ -\> Set.empty;

call_to_return_flow = fun call_site d -\> (\* Taint of locals not affected by call \*) match d with \| TaintedVar v when not (is_arg cpg call_site v) -\> Set.singleton d \| \_ -\> Set.empty;

(\* LIMITATION: This basic IFDS taint is FLOW-INSENSITIVE for heap. For PRECISE heap taint with flow-sensitivity, see Section 8.1.6.1 (FlowDroid activation statements) which extends IFDS with: - Access paths (x.f.g) instead of simple variables - Bidirectional analysis (forward taint + backward alias) - Activation statements for flow-sensitive heap precision

For CROSS-LANGUAGE taint, see Section 9.1.4 (PolyCruise) which uses LISR for language-agnostic def/use analysis. \*)

(\* ————————————————– UNINITIALIZED VARIABLE ANALYSIS ————————————————– \*)

type uninit_fact = \| Uninitialized : var:string -\> uninit_fact

let uninitialized_analysis_problem (cpg : cpg) : ifds_problem uninit_fact =

supergraph = cpg; domain = Set.map (fun v -\> Uninitialized v) (all_variables cpg); zero = Uninitialized "";

flow_function = fun edge d -\> let n = edge.source in match d with \| Uninitialized v -\> (\* Declaration without init: add uninit fact \*) if is_decl_without_init cpg n v then Set.singleton (Uninitialized v) (\* Assignment: remove uninit fact \*) else if assigns_to cpg n v then Set.empty (\* Use of uninit var: REPORT BUG \*) else if uses_var cpg n v then let \_ = report_uninit_use cpg n v in Set.singleton d (\* Keep tracking \*) else Set.singleton d; (\* ... call/return flows ... \*)

</div>

### IFDS Implementation Optimizations

**Sources**: Reps, Horwitz, Sagiv 1995; Naeem et al. 2010

<div class="pillarbox">

- <span class="sans-serif">IFDS</span> is DISTRIBUTIVE fragment only — see Section 12.18 (Set Constraints) and Appendix D.10.1 for non-distributive alternatives

- Taint analysis is classic <span class="sans-serif">IFDS</span> application — see Section 8.1

- <span class="sans-serif">IFDS</span> is OVER-approximate; for under-approximate bug finding use Eval algorithm — see Section <a href="#sec:under-approx" data-reference-type="ref" data-reference="sec:under-approx">3</a>

- For SOURCE-SINK problems (leak detection), consider SVF (Section 5.6) as alternative — SVF uses sparse value-flow graphs instead of exploded supergraph, which can be more efficient when memory regions $`R \ll`$ domain $`D`$

</div>

#### Representation Relations

<div class="fstarcode">

(\* ================================================== Key insight: Distributive functions can be compactly represented. ================================================== \*)

(\* ————————————————– THEOREM: A function f : 2^D -\> 2^D is distributive iff: f(X Y) = f(X) f(Y)

Such functions can be encoded as a RELATION R (D ) D where (d_1, d_2) R means "if d_1 holds, then d_2 is generated"

SPACE COMPLEXITY: O(D^2) instead of O(2^D) for arbitrary functions COMPOSITION: R_1 ; R_2 = (a,c) \| b. (a,b) R_1 (b,c) R_2 Relation composition = Function composition! ————————————————– \*)

type repr_relation (d : Type) = set (option d \* d) (\* None represents the "zero" fact - unconditional generation \*)

(\* Convert a transfer function to its representation relation \*) val repr_of_transfer : \#d:Type -\> all_facts:set d -\> (set d -\> set d) -\> repr_relation d let repr_of_transfer \#d all_facts f = (\* For each input fact (including 0), compute generated facts \*) let zero_gen = f Set.empty in (\* Unconditional generation: f() \*) let zero_edges = Set.map (fun d2 -\> (None, d2)) zero_gen in (\* For each fact d1, compute what d1 generates \*) let fact_edges = Set.concat_map (fun d1 -\> let out = f (Set.singleton d1) in Set.map (fun d2 -\> (Some d1, d2)) out ) all_facts in Set.union zero_edges fact_edges

(\* Compose two representation relations \*) val compose_repr : \#d:Type -\> repr_relation d -\> repr_relation d -\> repr_relation d let compose_repr \#d r1 r2 = (\* R_1 ; R_2 = relational composition \*) Set.concat_map (fun (a, b) -\> let matching = Set.filter (fun (b’, c) -\> match (b, b’) with \| (Some x, Some y) -\> x = y \| (None, None) -\> true \| \_ -\> false ) r2 in Set.map (fun (\_, c) -\> (a, c)) matching ) r1

(\* Apply a representation relation to get output facts \*) val apply_repr : \#d:Type -\> repr_relation d -\> set d -\> set d let apply_repr \#d r input = (\* Output = d2 \| (0, d2) R \*) let from_zero = Set.filter_map (fun (src, tgt) -\> match src with None -\> Some tgt \| Some \_ -\> None ) r in let from_facts = Set.concat_map (fun d1 -\> Set.filter_map (fun (src, tgt) -\> match src with Some d when d = d1 -\> Some tgt \| \_ -\> None ) r ) input in Set.union from_zero from_facts

(\* ————————————————– THEOREM (Correctness of Representation) For any distributive f and its representation R: apply_repr R input = f input ————————————————– \*)

val repr_correct : \#d:Type -\> all_facts:set d -\> f:(set d -\> set d) -\> Lemma (requires is_distributive f) (ensures forall input. apply_repr (repr_of_transfer all_facts f) input = f input)

</div>

#### H-Sparse Optimization

<div class="fstarcode">

(\* ================================================== Observation: Most transfer functions affect only h \<\< D facts. "Sparse" = few facts generated or killed per edge. ================================================== \*)

(\* COMPLEXITY IMPROVEMENT: Standard IFDS: O(E D^3) H-sparse IFDS: O(Call D^3 + h E D^2) where h = max facts affected per edge (the "sparsity parameter")

PRACTICAL EXAMPLES: For taint analysis: h -3 (one source, one propagation per stmt) For null analysis: h -2 (one variable nullified per stmt) For live variables: h (one def, one use per stmt typically)

INTUITION: Most statements touch few variables, so most transfer functions are nearly identity. Exploit this sparsity! \*)

type sparse_repr (d : Type) = gen : set d; (\* Facts generated unconditionally \*) kill : set d; (\* Facts killed (removed from input) \*) propagate : set (d \* d); (\* Conditional propagation: d1 d2 \*) (\* INVARIANT: \|gen\| + \|kill\| + \|propagate\| h \*)

(\* Check if a representation relation is h-sparse \*) val is_h_sparse : \#d:Type -\> repr_relation d -\> h:nat -\> bool let is_h_sparse \#d r h = Set.size r \<= h \* h (\* At most h^2 edges in representation \*)

(\* Convert full representation to sparse form (if possible) \*) val to_sparse_repr : \#d:Type -\> all_facts:set d -\> repr_relation d -\> option (sparse_repr d) let to_sparse_repr \#d all_facts r = (\* gen = facts generated from zero (unconditional) \*) let gen = Set.filter_map (fun (src, tgt) -\> match src with None -\> Some tgt \| \_ -\> None ) r in (\* identity = facts that map to themselves \*) let identity_facts = Set.filter (fun d -\> Set.mem (Some d, d) r ) all_facts in (\* kill = facts in domain but NOT in identity \*) let kill = Set.diff all_facts identity_facts in (\* propagate = non-identity, non-zero mappings \*) let propagate = Set.filter (fun (src, tgt) -\> match src with \| Some d when d \<\> tgt -\> true \| \_ -\> false ) r in Some gen; kill; propagate = Set.map (fun (Some s, t) -\> (s, t)) propagate

(\* Sparse propagation - only process affected facts \*) val propagate_sparse : \#d:Type -\> sparse_repr d -\> set d -\> set d let propagate_sparse \#d sr input = (\* 1. Remove killed facts \*) let after_kill = Set.diff input sr.kill in (\* 2. Apply conditional propagations \*) let propagated = Set.concat_map (fun d -\> let targets = Set.filter_map (fun (d’, d”) -\> if d’ = d then Some d” else None ) sr.propagate in if Set.is_empty targets then Set.singleton d else targets ) after_kill in (\* 3. Add generated facts \*) Set.union sr.gen propagated

(\* ————————————————– THEOREM (Sparse Complexity) For h-sparse problems: Time: O(Call D^3 + h E D^2) Space: O(N D^2 + Call D^2) When h \<\< D, this is significantly better than O(E D^3) ————————————————– \*)

</div>

#### Locally Separable Problems (Gen/Kill)

<div class="fstarcode">

(\* ================================================== A problem is "locally separable" if ALL transfer functions are gen/kill: f(X) = gen (X  kill) For such problems: O(E D) instead of O(E D^3) !!! ================================================== \*)

(\* EXAMPLES OF LOCALLY SEPARABLE PROBLEMS: 1. REACHING DEFINITIONS: gen = def – this definition reaches kill = prev defs of same var – previous defs killed

2\. LIVE VARIABLES: gen = used vars – used variables are live kill = defined var – defined var no longer live before

3\. AVAILABLE EXPRESSIONS: gen = computed expr – expression now available kill = exprs using modified var – invalidated expressions

4\. VERY BUSY EXPRESSIONS: gen = expr used on all paths – expression busy kill = exprs with modified vars – no longer busy

WHAT IS NOT LOCALLY SEPARABLE: - Pointer analysis (aliasing creates dependencies) - Constant propagation (values depend on other values) - Shape analysis (heap structure depends on operations) \*)

type gen_kill_function (d : Type) = gen : set d; kill : set d;

(\* Check if a transfer function is gen/kill form \*) val is_gen_kill : \#d:Type -\> (set d -\> set d) -\> option (gen_kill_function d) let is_gen_kill \#d f = (\* A function is gen/kill iff: 1. f() gives us the gen set 2. For all d, either d f(d) (preserved) or d f(d) (killed) \*) let gen = f Set.empty in (\* To find kill: facts d where d f(d) \*) (\* This requires knowing the domain, simplified here \*) Some gen; kill = Set.empty (\* computed from domain \*)

(\* Check if an IFDS problem is locally separable \*) val is_locally_separable : \#d:Type -\> ifds_problem d -\> bool let is_locally_separable \#d problem = (\* All flow functions must be gen/kill \*) forall_edges problem.supergraph (fun edge -\> match is_gen_kill (problem.flow_function edge) with \| Some \_ -\> true \| None -\> false )

(\* ————————————————– FAST SOLVER FOR LOCALLY SEPARABLE PROBLEMS Complexity: O(E D) instead of O(E D^3) KEY INSIGHT: For gen/kill, we don’t need the full IFDS machinery. Each fact can be tracked independently! ————————————————– \*)

val solve_gen_kill : \#d:Type -\> problem:ifds_problem d -\> Lemma (requires is_locally_separable problem) -\> (node_id -\> set d) let solve_gen_kill \#d problem \_ = (\* For gen/kill, facts are independent - track each separately \*) (\* This is essentially running \|D\| separate bit-vector dataflows \*) let solve_for_fact (fact : d) : set node_id = (\* Standard worklist for single-fact reachability \*) let rec worklist visited frontier = match frontier with \| \[\] -\> visited \| n :: rest -\> if Set.mem n visited then worklist visited rest else let visited’ = Set.add n visited in let succs = get_successors problem.supergraph n in let live_succs = List.filter (fun succ -\> let gk = get_gen_kill problem n succ in not (Set.mem fact gk.kill) (\* Not killed on this edge \*) ) succs in worklist visited’ (live_succs @ rest) in (\* Also include nodes where fact is generated \*) let gen_nodes = get_gen_nodes problem fact in worklist Set.empty (Set.to_list gen_nodes) in (\* Combine results for all facts \*) fun node -\> Set.filter (fun fact -\> Set.mem node (solve_for_fact fact) ) problem.domain

(\* COMPLEXITY ANALYSIS: - For each fact: O(E) traversal - Total: O(D E) = O(E D) - Compare to IFDS: O(E D^3)

PRACTICAL SPEEDUP: For D = 1000 facts: 1000^3 = 10^9 vs 10^3 = factor of 10^6 improvement! \*)

</div>

#### Demand-Driven IFDS

<div class="fstarcode">

(\* ================================================== Standard IFDS: Compute ALL reachable facts (exhaustive, whole-program) Demand-driven: Only compute facts needed to answer a SPECIFIC QUERY ================================================== \*)

(\* USE CASE: "Is variable x tainted at line 42?"

STANDARD IFDS: 1. Run full taint analysis on entire program 2. Look up result for (line 42, tainted(x)) 3. Cost: O(E D^3) even if we only care about ONE fact at ONE location

DEMAND-DRIVEN IFDS: 1. Start from query point (line 42, tainted(x)) 2. Search BACKWARD: "how could this fact become true?" 3. Stop when we reach a source or prove unreachable 4. Cost: O(relevant subgraph) \<\< O(full program)

RELATION TO CFL-REACHABILITY (Section 4.2): Demand-driven IFDS is backward CFL-reachability in the exploded supergraph. The Dyck language ensures matched call/return context.

DIFFERENCES FROM PURE CFL: - IFDS has transfer functions that transform facts - CFL just tracks reachability with grammar rules - Demand-driven IFDS = CFL + inverse transfer functions \*)

(\* Query type: does a fact hold at a specific program point? \*) type ifds_query (d : Type) = query_node : node_id; query_fact : d;

(\* Demand-driven solver \*) val demand_driven_ifds : \#d:Type -\> problem : ifds_problem d -\> query : ifds_query d -\> bool (\* Does fact hold at node? \*)

let demand_driven_ifds \#d problem query = (\* ALGORITHM: Backward search in exploded supergraph from query point \*) let target = (query.query_node, query.query_fact) in let source = (start_main problem.supergraph, Zero) in

(\* Compute inverse transfer functions for backward search \*) let inverse_flow edge d_out = (\* Find d_in such that d_out flow(edge)(d_in) \*) Set.filter (fun d_in -\> Set.mem d_out (problem.flow_function edge (Set.singleton d_in)) ) (Set.add Zero problem.domain) in

(\* Backward reachability with context sensitivity \*) let rec backward_search visited frontier = match frontier with \| \[\] -\> false (\* Query fact is not reachable from any source \*) \| (node, fact, call_stack) :: rest -\> (\* Check if we reached a source (entry of main with Zero fact) \*) if node = fst source && fact = Zero then true (\* Found a valid path! \*) else if Set.mem (node, fact, call_stack) visited then backward_search visited rest else let visited’ = Set.add (node, fact, call_stack) visited in (\* Get predecessors and inverse-flow facts \*) let preds = get_predecessors problem.supergraph node in let new_frontier = List.concat_map (fun pred -\> let edge = source = pred; target = node in let d_ins = inverse_flow edge fact in (\* Handle call/return context \*) match node_type problem.supergraph pred with \| NCall call_site -\> (\* Push call site onto stack \*) List.map (fun d -\> (pred, d, call_site :: call_stack)) (Set.to_list d_ins) \| NReturn ret_site -\> (\* Must match top of stack \*) (match call_stack with \| call :: rest_stack when matches_return call ret_site -\> List.map (fun d -\> (pred, d, rest_stack)) (Set.to_list d_ins) \| \_ -\> \[\]) (\* Invalid context - prune this path \*) \| \_ -\> List.map (fun d -\> (pred, d, call_stack)) (Set.to_list d_ins) ) preds in backward_search visited’ (new_frontier @ rest) in backward_search Set.empty \[(query.query_node, query.query_fact, \[\])\]

(\* ————————————————– WHEN TO USE DEMAND-DRIVEN VS EXHAUSTIVE

USE EXHAUSTIVE (standard IFDS) WHEN: - Need results for ALL program points - Building a whole-program summary - Analysis is cheap relative to program size

USE DEMAND-DRIVEN WHEN: - Only need specific query answers - Interactive tools (IDE integration) - Incremental analysis after edits - Query is localized (few relevant paths)

HYBRID APPROACH: 1. Run fast over-approximate analysis (exhaustive) 2. For reported issues, use demand-driven to verify 3. See Section 4.3.3 for IFDS + Eval combination ————————————————– \*)

</div>

#### Summary: IFDS Complexity Variants

<div class="center">

| **Variant** | **Time Complexity** | **When to Use** |
|:---|:---|:---|
| Standard IFDS | $`\mathcal{O}{E \cdot D^3}`$ | General distributive problems |
| H-sparse IFDS | $`\mathcal{O}{\text{Call} \cdot D^3 + h \cdot E \cdot D^2}`$ | Sparse facts (taint, null) |
| Locally separable | $`\mathcal{O}{E \cdot D}`$ | Gen/kill problems |
| Demand-driven | $`\mathcal{O}{\text{relevant paths}}`$ | Specific queries, IDE tools |

</div>

Where: $`E`$ = edges in supergraph, $`D`$ = size of dataflow domain, $`h`$ = sparsity parameter (max facts affected per edge), Call = number of call sites.

**Reminder**: <span class="sans-serif">IFDS</span> requires DISTRIBUTIVE transfer functions! For non-distributive (pointer analysis): Use set constraints (Section 12.18). For under-approximate bug finding: Use Eval algorithm (Section <a href="#sec:under-approx" data-reference-type="ref" data-reference="sec:under-approx">3</a>).

### Datalog Compilation Strategy (Souffle)

**Paper**: Jordan, Scholz, Subotic 2016 (CAV)

For high-performance deployment, <span class="sans-serif">IFDS</span> problems expressed as Datalog can be COMPILED to specialized code rather than interpreted.

<div class="pillarbox">

Jordan 2016 shows: Datalog-based analysis is elegant but SLOW when interpreted. Compilation achieves 50x+ speedup over bddbddb/muZ.

**Benchmark** (OpenJDK7: 1.4M variables, 350K objects, 160K methods):

- Souffle (compiled): 35 seconds (CI points-to)

- bddbddb: 30 minutes

- SQLite: 6 hours 20 minutes

- muZ: Did not finish

</div>

#### Staged Compilation via Futamura Projections

**Souffle Compilation Pipeline**:

**Stage 1**: Datalog $`\rightarrow`$ RAM (Relational Algebra Machine)
``` math
P_{\text{RAM}} = \text{Mix}(\text{Int}_{\text{dl}}, I_{\text{db}})
```
Where: $`\text{Int}_{\text{dl}}`$ = Semi-naive evaluation interpreter, $`I_{\text{db}}`$ = Intensional database (analysis rules). Result: Relational Algebra Machine program. Optimizations: Rule reordering, semi-naive transformation.

**Stage 2**: RAM $`\rightarrow`$ Optimized RAM. Index selection via DILWORTH’S THEOREM (see Section <a href="#sec:dilworth" data-reference-type="ref" data-reference="sec:dilworth">1.6.2</a>). Join reordering for cache efficiency. Stratum computation for stratified negation.

**Stage 3**: RAM $`\rightarrow`$ C++/Rust
``` math
P_{\text{C++}} = \text{Mix}(\text{Int}_{\text{RAM}}, P_{\text{RAM}})
```
Via template metaprogramming: inlined comparison functions, specialized B-tree iterators, OpenMP parallel annotations.

<div class="theorem">

**Theorem 1.67** (Semantic Preservation). *
``` math
\text{result} = \llbracket \text{Int} \rrbracket(\text{Source}, \text{Input}) = \llbracket \text{Source} \rrbracket_{\text{Int}}(\text{Input}) = \llbracket \text{Mix}(\text{Int}, \text{Source}) \rrbracket(\text{Input}) = \llbracket \text{Compiled} \rrbracket(\text{Input})
```
*

</div>

#### Optimal Index Selection (Dilworth’s Theorem)

**Problem**: Given $`N`$ index requirements from queries, find MINIMAL set of indices such that each requirement is subsumed by at least one selected index.

**Insight**: Index subsumption forms a partial order. A lexicographic index $`[a, b, c]`$ subsumes $`[a, b]`$ and $`[a]`$. That is: if I can look up by $`(a, b, c)`$, I can also look up by $`(a)`$ or $`(a, b)`$.

<div class="theorem">

**Theorem 1.68** (Dilworth’s Theorem). *The width of a partial order equals the minimum number of chains needed to cover all elements.*

</div>

**Algorithm** (polynomial time):

1.  Build lattice of required indices under subsumption

2.  Compute minimum chain partition (via bipartite matching)

3.  Select maximum element from each chain

**Result**: For analyses with 100s of index requirements, reduces to minimal set. Each chain’s maximum subsumes all requirements in that chain.

**Example**:

- Required indices: $`[a], [a,b], [a,c], [a,b,c], [b], [b,c]`$

- Chain 1: $`[a] < [a,b] < [a,b,c]`$ $`\rightarrow`$ select $`[a,b,c]`$

- Chain 2: $`[a,c]`$ $`\rightarrow`$ select $`[a,c]`$

- Chain 3: $`[b] < [b,c]`$ $`\rightarrow`$ select $`[b,c]`$

- Minimal cover: $`\{[a,b,c], [a,c], [b,c]\}`$ instead of all 6 indices

<div class="fstarcode">

module BrrrMachine.DatalogCompile

(\* Relational Algebra Machine operations \*) type ram_expr = \| RAMScan : relation:string -\> ram_expr \| RAMFilter : expr:ram_expr -\> column:nat -\> value:string -\> ram_expr \| RAMProject : expr:ram_expr -\> columns:list nat -\> ram_expr \| RAMJoin : left:ram_expr -\> right:ram_expr -\> left_col:nat -\> right_col:nat -\> ram_expr \| RAMUnion : left:ram_expr -\> right:ram_expr -\> ram_expr \| RAMDiff : left:ram_expr -\> right:ram_expr -\> ram_expr

type ram_stmt = \| RAMInsert : target:string -\> source:ram_expr -\> ram_stmt \| RAMLoop : delta:string -\> body:list ram_stmt -\> ram_stmt (\* Semi-naive \*) \| RAMSeq : stmts:list ram_stmt -\> ram_stmt

(\* Index specification \*) type index_spec = relation : string; key_columns : list nat; (\* Lexicographic order \*)

(\* Index subsumption: \[a,b,c\] subsumes \[a,b\] and \[a\] \*) let subsumes (super sub : index_spec) : bool = super.relation = sub.relation && List.length sub.key_columns \<= List.length super.key_columns && List.for_all2 (=) sub.key_columns (List.take (List.length sub.key_columns) super.key_columns)

(\* Dilworth-based minimal index selection \*) val compute_minimal_indices : required:list index_spec -\> minimal:list index_spec (\* Every required index is subsumed by some minimal index \*) forall r. List.mem r required ==\> exists m. List.mem m minimal / subsumes m r

</div>

#### Implementation Strategy for brrr-machine

<div class="pillarbox">

**Development Mode**: Use interpreted Datalog for rapid iteration

- Crepe (Rust embedded Datalog) for prototyping

- Souffle interpreter mode for debugging

- Modify rules without recompilation

**Production Mode**: Compile analysis rules to specialized Rust

- Express <span class="sans-serif">IFDS</span>/<span class="sans-serif">IDE</span> as Datalog rules

- Compile via Souffle $`\rightarrow`$ C++ or custom Datalog$`\rightarrow`$Rust pipeline

- Link compiled analysis into brrr-machine

**Why not just use Souffle directly?**

- Souffle generates C++, we want Rust integration

- We need lattice predicates (Section <a href="#sec:flix-lattice" data-reference-type="ref" data-reference="sec:flix-lattice">1.7</a>) which Souffle lacks

- But Souffle’s TECHNIQUES (RAM, Dilworth indices) are essential

</div>

### Lattice-Extended Datalog and IDE (Flix)

**Paper**: Madsen, Yee, Lhotak 2016 (PLDI)

Standard Datalog operates on relations (finite sets). Many analyses require lattices with potentially infinite domains. Flix extends Datalog with lattice predicates, enabling elegant expression of <span class="sans-serif">IFDS</span>, <span class="sans-serif">IDE</span>, and value analyses.

<div class="pillarbox">

**Standard Datalog (rel)**:

    rel Edge(x, y)
    Multiple tuples = set
    {Edge(1,2), Edge(1,3)} -> keeps both

**Flix Extension (lat)**:

    lat LocalVar(x: Var, v: Constant)
    Same key = JOIN lattice values
    {LocalVar(x, Cst(1)), LocalVar(x, Cst(2))} -> {LocalVar(x, Top)}

**What this enables**:

- Constant propagation (lattice = constants with Top)

- Interval analysis (lattice = intervals)

- <span class="sans-serif">IDE</span> algorithm (lattice = micro-function space)

- Strong update analysis (hybrid flow-sensitive/insensitive)

</div>

#### Lattice Predicates and Transfer Functions

    // Standard Datalog relation - set semantics
    rel AddExp(r: Var, x: Var, y: Var)    // r = x + y in source code

    // Flix lattice predicate - join semantics
    lat LocalVar(x: Var, v: Constant)     // x has abstract value v
    //           ^^^^    ^^^^^^^^^^
    //           key     lattice element (joined on same key)

    // User-defined lattice
    enum Constant {
      case Top,       // Unknown (join of all)
      case Cst(Int),  // Known constant
      case Bot        // Unreachable (bottom)
    }

    // Transfer function: MUST BE MONOTONE
    def sum(e1: Constant, e2: Constant): Constant =
      match (e1, e2) with {
        case (Bot, _) | (_, Bot) => Bot      // Unreachable stays unreachable
        case (Cst(n1), Cst(n2))  => Cst(n1 + n2)
        case _                   => Top       // Unknown
      }

    // Filter function: guards rule application
    def isMaybeZero(e: Constant): Bool =
      match e with {
        case Bot    => false    // Unreachable - no error
        case Cst(n) => n == 0
        case Top    => true     // Could be zero
      }

    // Flix rule with transfer function
    LocalVar(r, sum(x, y)) :- AddExp(r, v1, v2),
                              LocalVar(v1, x),
                              LocalVar(v2, y).

    // Flix rule with filter function
    ArithmeticError(r) :- isMaybeZero(y),
                          DivExp(r, n, d),
                          LocalVar(d, y).

#### IDE as Flix with Micro-Function Lattice

<div class="pillarbox">

<span class="sans-serif">IFDS</span> path edge: `PathEdge(d1, n, d2)` — fact `d2` holds at `n`

<span class="sans-serif">IDE</span> path edge: `PathEdge(d1, n, d2, f)` — with environment transformer $`f`$

The micro-function space forms a lattice under pointwise ordering:
``` math
\begin{aligned}
\bot &= \lambda x.\, \bot \quad \text{(constant bottom function)} \\
\top &= \lambda x.\, \top \quad \text{(constant top function)} \\
f \sqcup g &= \lambda x.\, (f\, x) \sqcup (g\, x) \quad \text{(pointwise join)}
\end{aligned}
```

<span class="sans-serif">IDE</span> key insight: transformers COMPOSE along paths:

    PathEdge(d1, m, d3, compose(f, g)) :-
      PathEdge(d1, n, d2, f),
      CFG(n, m),
      EdgeFunction(n, d2, m, d3, g).

</div>

<div class="fstarcode">

module BrrrMachine.IDE

(\* Micro-function: environment transformer \*) type micro_fn (v : Type) = v -\> v

(\* Micro-functions form a lattice under pointwise ordering \*) instance micro_fn_lattice (v : Type) \| complete_lattice v \| : complete_lattice (micro_fn v) = bot = (fun \_ -\> bot); top = (fun \_ -\> top); join = (fun f g -\> fun x -\> join (f x) (g x)); meet = (fun f g -\> fun x -\> meet (f x) (g x)); leq = (fun f g -\> forall x. leq (f x) (g x));

(\* IDE path edge: track micro-function along path \*) type ide_path_edge (d : Type) (v : Type) = entry_fact : d; current_node : node_id; current_fact : d; transformer : micro_fn v;

(\* Composition is the IDE transfer function \*) let ide_compose (#v:Type) (f g : micro_fn v) : micro_fn v = fun x -\> f (g x)

(\* Composition is monotone in both arguments \*) val ide_compose_monotone : \#v:Type -\> \| complete_lattice v \| -\> f1:micro_fn v -\> f2:micro_fn v -\> g1:micro_fn v -\> g2:micro_fn v -\> Lemma (requires micro_fn_leq f1 f2 / micro_fn_leq g1 g2) (ensures micro_fn_leq (ide_compose f1 g1) (ide_compose f2 g2))

(\* ————————————————– WHEN TO USE IDE vs IFDS

USE IFDS (Section 4.1): - Taint analysis (binary: tainted/untainted) - Uninitialized variables (set of "maybe uninitialized") - Typestate (finite state machine) - Any analysis where facts are BINARY or small finite set

USE IDE: - Constant propagation (need to track actual constant values) - Linear constant propagation (x = a\*y + b transformers) - Copy constant analysis (track "which variable copied from") - Any analysis where you need VALUE TRANSFORMATIONS along paths ————————————————– \*)

</div>

#### Semi-Naive Evaluation for Lattices

**Standard Semi-Naive (for relations)**:
``` math
\delta_R = \text{new TUPLES added to } R
```

**Flix Semi-Naive (for lattices)**:
``` math
\delta_L = (\text{key}, \text{old\_value}, \text{new\_value}) \text{ where } \text{new\_value} \sqsupset \text{old\_value}
```

The iteration terminates when no lattice element strictly increases.

<div class="theorem">

**Theorem 1.69** (Madsen 2016, Theorem 1). *Every Flix program has a unique minimal model, computable via iteration, PROVIDED all transfer functions are monotone.*

</div>

**Monotonicity Requirement**: For soundness, all transfer functions $`f`$ must satisfy:
``` math
x \sqsubseteq y \implies f(x) \sqsubseteq f(y)
```
This is the Flix equivalent of <span class="sans-serif">IFDS</span> distributivity requirement.

<div class="fstarcode">

(\* Transfer function monotonicity - critical for soundness \*) type monotone_transfer (#l:Type) \| complete_lattice l \| = f:(l -\> l) forall x y. leq x y ==\> leq (f x) (f y)

(\* Binary monotone transfer (e.g., addition in constant lattice) \*) type monotone_transfer2 (#l:Type) \| complete_lattice l \| = f:(l -\> l -\> l) forall x1 x2 y1 y2. leq x1 x2 / leq y1 y2 ==\> leq (f x1 y1) (f x2 y2)

(\* Semi-naive iteration for lattice predicates \*) type lat_delta (k l : Type) = changed_keys : set k; old_values : k -\> l; new_values : k -\> l; (\* Invariant: forall k in changed_keys. old_values k new_values k \*)

val flix_semi_naive : \#k:Type -\> \#l:Type -\> \| complete_lattice l \| -\> rules:list flix_rule -\> initial:lat_predicate k l -\> lat_predicate k l (\* Returns minimal model \*)

</div>

## CFL-Reachability

**Paper**: Reps 1997

<span class="sans-serif">CFL</span>-reachability generalizes <span class="sans-serif">IFDS</span> by allowing arbitrary context-free grammars to describe valid paths, not just matched parentheses.

### The Framework

<div class="definition">

**Definition 2.1** (CFL-Reachability). Given:

- Graph $`G = (V, E)`$ with edge labels from alphabet $`\Sigma`$

- Context-free grammar $`\mathcal{G}`$ with terminals $`\Sigma`$

- Start symbol $`S`$

**Question**: For which pairs $`(u, v)`$ is there a path from $`u`$ to $`v`$ whose edge labels form a string in $`L(\mathcal{G})`$?

</div>

**The Dyck Language (matched parentheses)**:
``` math
S \rightarrow S\, S \mid (_i\, S\, )_i \mid \varepsilon \quad \text{for each call site } i
```
This is exactly what <span class="sans-serif">IFDS</span> uses for context sensitivity.

**Other Useful Grammars**:

*Field-sensitive alias analysis*:
``` math
\begin{aligned}
\text{FlowsTo} &\rightarrow \texttt{new} \\
&\mid \text{FlowsTo}\ \texttt{assign} \\
&\mid \text{FlowsTo}\ \texttt{load}\ \text{FlowsTo}\ \texttt{store}
\end{aligned}
```
“x flows to y if there’s a path: new, then assigns, then matched load/store on same field”

*Typestate analysis*:
``` math
\begin{aligned}
\text{Valid} &\rightarrow \texttt{open}\ \text{Valid}\ (\texttt{read} \mid \texttt{write})^*\ \texttt{close} \\
&\mid \text{Valid}\ \text{Valid}
\end{aligned}
```
“Valid use is: open, then reads/writes, then close”

### Demand-Driven Analysis via CFL

**Paper**: Sridharan 2005

Demand-driven analysis answers specific queries without computing the full solution. For points-to analysis, instead of computing $`\mathit{pts}(v)`$ for all variables (expensive), we compute on-demand for a single variable at a specific program point. This is implemented as *backward* CFL-reachability: starting from the query $`(p, x)`$, we search backward through assignments and field accesses until we reach allocation sites.

The CFL grammar for demand-driven points-to is: $`\text{FlowsTo} \to \texttt{new} \mid \texttt{assign}\ \text{FlowsTo} \mid \texttt{load}_f\ \text{PointsTo}\ \texttt{store}_f\ \text{FlowsTo}`$. The matched parentheses ensure field-sensitivity (only matching load/store on the same field). This approach achieves significant speedups when few queries are needed, as is common in IDE tools and incremental analysis.

<div class="fstarcode">

(\* KEY INSIGHT: Instead of computing points-to for ALL variables (expensive), compute on-demand for specific queries.

Query: "What does variable x point to at program point p?"

Algorithm: 1. Start backward search from (p, x) 2. Follow assign edges backward: x = y means search for y 3. Follow load/store with matched fields: x = y.f, z.f = w 4. Stop at allocation sites: new T

This is CFL-reachability with grammar: FlowsTo -\> new \| assign FlowsTo \| load_f PointsTo store_f FlowsTo \*)

type pts_edge_label = \| New : alloc_site:node_id -\> pts_edge_label \| Assign : pts_edge_label \| Load : field:string -\> pts_edge_label \| Store : field:string -\> pts_edge_label

(\* Demand-driven points-to query \*) val points_to_query : cpg -\> node_id -\> string -\> set node_id let points_to_query cpg point var = (\* Build PEG (Pointer Expression Graph) \*) let peg = build_peg cpg in (\* Backward CFL-reachability from (point, var) \*) let rec search visited (node, var) = if Set.mem (node, var) visited then Set.empty else let visited’ = Set.add (node, var) visited in (\* Check incoming edges \*) let edges = get_incoming_peg_edges peg (node, var) in Set.concat_map (fun edge -\> match edge.label with \| New alloc_site -\> Set.singleton alloc_site (\* Found an allocation! \*) \| Assign -\> search visited’ edge.source \| Load field -\> (\* Need to find matching store \*) let base = get_base_var edge in let base_pts = search visited’ (edge.source_node, base) in Set.concat_map (fun alloc -\> (\* Find stores to this field on this allocation \*) let stores = find_stores peg alloc field in Set.concat_map (fun store_edge -\> search visited’ store_edge.source ) stores ) base_pts \| Store \_ -\> Set.empty (\* Stores are targets, not sources \*) ) edges in search Set.empty (point, var)

</div>

### Interleaved Dyck and the Combined Context+Field Problem

**Sources**: Reps 2000, Conrado et al. 2025

<span class="sans-serif">CFL</span>-reachability (Section <a href="#sec:cfl-framework" data-reference-type="ref" data-reference="sec:cfl-framework">2.1</a>) uses a **single Dyck language** for matched call/return. Field-sensitive alias analysis uses another Dyck language for matched load/store. **Combining both** requires **interleaved Dyck reachability**, which is **undecidable**.

<div class="pillarbox">

**Context sensitivity**: $`D_\alpha`$ = matched parentheses $`(\quad)`$

**Field sensitivity**: $`D_\beta`$ = matched brackets $`[\quad]`$

**Interleaved Language** $`I_{\alpha,\beta}`$: A string $`s`$ is in $`I_{\alpha,\beta}`$ iff:

- $`s`$ restricted to $`(\,)`$ is in $`D_\alpha`$ (valid call/return)

- $`s`$ restricted to $`[\,]`$ is in $`D_\beta`$ (valid load/store)

**Example valid string**: $`(_{1}\, [_f\, (_{2}\, ]_f\, )_{2}\, [_g\, ]_g\, )_{1}`$

Projections: $`(_{1} (_{2} )_{2} )_{1}`$ = valid $`D_\alpha`$; $`[_f ]_f [_g ]_g`$ = valid $`D_\beta`$

<div class="theorem">

**Theorem 2.2** (Reps 2000 — Undecidability). *Interleaved Dyck reachability is UNDECIDABLE. Reduction from Post Correspondence Problem.*

</div>

**Practical Consequence**: Cannot have BOTH context-sensitivity AND field-sensitivity with: (1) Soundness (no false negatives), (2) Completeness (no false positives), (3) Decidability (termination). Must sacrifice ONE property.

</div>

Two principled solutions exist: **MCFL underapproximation** (Section <a href="#sec:mcfl" data-reference-type="ref" data-reference="sec:mcfl">2.4</a>) sacrifices completeness, while **SPDS overapproximation** (Section <a href="#sec:spds" data-reference-type="ref" data-reference="sec:spds">2.5</a>) sacrifices soundness of the precision claim.

### Multiple Context-Free Language Reachability (MCFL)

**Paper**: Conrado, Kjelstrom, Pavlogiannis, van de Pol 2025

MCFL provides a **decidable underapproximation** of interleaved Dyck via a hierarchy of increasingly expressive languages.

<div class="pillarbox">

- 1-MCFL = CFL (context-free languages, $`\mathcal{O}{n^3}`$ reachability)

- 2-MCFL strictly contains 1-MCFL ($`\mathcal{O}{n^4}`$ reachability)

- $`d`$-MCFL strictly contains $`(d-1)`$-MCFL ($`\mathcal{O}{n^{2d}}`$ reachability)

**Completeness in Limit**: For any string $`s \in I_{\alpha,\beta}`$, there exists $`d`$ such that $`s`$ is in $`d`$-MCFL. Union over all $`d`$ gives exactly $`I_{\alpha,\beta}`$.

**Practical Finding (Conrado 2025)**: 2-MCFL matches overapproximation in 8/11 benchmarks. 2-MCFL confirms 94.3% of paths on remaining benchmarks. $`d=2`$ suffices for most real taint analysis problems.

</div>

The following F\* code formalizes MCFGs. The key insight is that MCFG nonterminals have *arity*: they derive *tuples* of strings rather than single strings. A $`d`$-MCFG(r) grammar has dimension $`d`$ (maximum arity) and rank $`r`$ (maximum nonterminals per rule). The type signatures enforce well-formedness invariants: `wf_arity` ensures each production produces the correct tuple size, `wf_rank` bounds rule complexity, and `wf_dimension` bounds nonterminal arity.

<div class="fstarcode">

module BrrrMachine.MCFG

(\* A nonterminal with arity k derives k-tuples of strings \*) type mcfg_nonterminal = name: string; arity: pos (\* Number of string components it derives, \>= 1 \*)

(\* Right-hand side element: terminal or variable reference \*) type mcfg_rhs_elem = \| MCFGTerminal : char -\> mcfg_rhs_elem \| MCFGVariable : nt_idx:nat -\> component:nat -\> mcfg_rhs_elem

(\* Production rule in d-MCFG(r) \*) type mcfg_rule (d r: pos) =

lhs: mcfg_nonterminal; lhs_patterns: list (list mcfg_rhs_elem); (\* arity-many patterns \*) rhs_nts: list mcfg_nonterminal; (\* up to r nonterminals \*)

(\* Well-formedness invariants \*) wf_arity: List.length lhs_patterns = lhs.arity; wf_rank: List.length rhs_nts \<= r; wf_dimension: lhs.arity \<= d

(\* d-MCFG(r): dimension d, rank r \*) type mcfg (d r: pos) = nonterminals: list mcfg_nonterminal; terminals: set char; rules: list (mcfg_rule d r); start: mcfg_nonterminal; start_arity_1: start.arity = 1

(\* The G_d^+ grammar for approximating interleaved Dyck \*) val construct_interleaved_grammar : d:pos -\> k:pos -\> mcfg d 2

(\* Soundness: derived strings are in interleaved Dyck \*) val grammar_sound : d:pos -\> k:pos -\> s:string -\> Lemma (requires s ‘in_language‘ (construct_interleaved_grammar d k)) (ensures is_interleaved_dyck s)

(\* Completeness in limit: every interleaved Dyck string is captured \*) val grammar_complete_limit : k:pos -\> s:string -\> Lemma (requires is_interleaved_dyck s) (ensures exists d. s ‘in_language‘ (construct_interleaved_grammar d k))

</div>

**Complexity Bounds (Conrado 2025)**:

<div class="center">

| **Language Class** | **Reachability Complexity** | **Lower Bound** |
|:-------------------|:----------------------------|:----------------|
| CFL / 1-MCFL       | $`\mathcal{O}{n^3}`$        | BMM ($`n^3`$)   |
| $`d`$-MCFL(1)      | $`\mathcal{O}{n^{2d}}`$     | SETH            |
| $`d`$-MCFL($`r`$)  | $`\mathcal{O}{n^{d(r+1)}}`$ | SETH            |
| Interleaved        | Undecidable                 | PCP             |

</div>

The SETH lower bound shows $`\mathcal{O}{n^{2d}}`$ is essentially optimal for $`d`$-MCFL(1).

### Synchronized Pushdown Systems (SPDS)

**Paper**: Spath, Ali, Bodden 2019

SPDS provides an **automaton-based approach** to combined context+field sensitivity via synchronized pushdown systems, achieving polynomial complexity under a **precision hypothesis**.

<div class="pillarbox">

**Idea**: Use TWO pushdown automata that operate synchronously:

- $`P_F`$: Field PDS (stack encodes field access sequence)

- $`P_S`$: Stack PDS (stack encodes calling context)

- Sync: Synchronization predicate on transitions

**Encoding**:

- Field store `x.f = y`: push rule $`(l, \varepsilon) \longrightarrow (l', f)`$

- Field load `y = x.f`: pop rule $`(l, f) \longrightarrow (l', \varepsilon)`$

- Method call: push rule $`(l, \varepsilon) \longrightarrow (l', c)`$

- Method return: pop rule $`(l, c) \longrightarrow (l', \varepsilon)`$

**Key Innovation**: Field automaton $`A_F`$ represents set of valid access paths IMPLICITLY. Avoids exponential enumeration of access paths. $`\mathcal{O}{N}`$ automaton states vs $`\mathcal{O}{2^N}`$ explicit access paths.

**Precision Hypothesis (Spath 2019)**: An improperly matched call site does not induce a properly matched field access, and vice versa. Under this hypothesis, SPDS = infinite access path precision.

</div>

<div class="fstarcode">

module BrrrMachine.SPDS

(\* Stack symbols \*) type field_symbol = \| FieldSym : string -\> field_symbol \| FEps type call_symbol = \| CallSiteSym : nat -\> call_symbol \| CEps

(\* Pushdown configuration \*) type field_config = f_state: nat; f_stack: list field_symbol type call_config = c_state: nat; c_stack: list call_symbol type spds_config = field_config \* call_config

(\* Pushdown rules \*) type field_rule = \| FPush : nat -\> field_symbol -\> nat -\> field_symbol -\> field_rule \| FPop : nat -\> field_symbol -\> nat -\> field_rule

type call_rule = \| CPush : nat -\> call_symbol -\> nat -\> call_symbol -\> call_rule \| CPop : nat -\> call_symbol -\> nat -\> call_rule

(\* Synchronization predicate \*) type sync_spec = field_rule -\> call_rule -\> bool

(\* The full SPDS \*) type spds = field_rules: set field_rule; call_rules: set call_rule; sync: sync_spec; initial: spds_config

(\* Field automaton: implicitly represents set of valid access paths \*) type field_automaton = fa_states: set nat; fa_initial: nat; fa_final: set nat; fa_transitions: map (nat \* field_symbol) (set nat)

(\* Post\* computation via saturation \*) val post_star : set field_rule -\> field_config -\> field_automaton

(\* SPDS synchronized reachability \*) val spds_post_star : spds -\> set spds_config let spds_post_star sys = let fa = post_star sys.field_rules (fst sys.initial) in let ca = post_star sys.call_rules (snd sys.initial) in (\* Intersect and filter by synchronization \*) filter_by_sync sys.sync (cartesian fa ca)

(\* Complexity: O(\|P_F\|^3 \* \|P_S\|^3 \* \|Q\|^3) \*) val spds_complexity : sys:spds -\> Lemma (time_complexity (spds_post_star sys) \<= pow3 (size sys.field_rules) \* pow3 (size sys.call_rules))

</div>

**Performance Results (Spath 2019, DaCapo Benchmark)**:

<div class="center">

| **Analysis**      | **SPDS Speedup** | **Timeouts (Before/After)** |
|:------------------|:-----------------|:----------------------------|
| IO Property       | 64x              | 160 $`\rightarrow`$ 28      |
| Iterator Property | 83x              | 137 $`\rightarrow`$ 3       |
| Vector Property   | 1.8x             | 57 $`\rightarrow`$ 25       |

</div>

SPDS matches access-graph precision on all benchmarks with no false positives.

<div class="pillarbox">

**MCFL (Conrado 2025)**:

- Grammar-based formalism

- Underapproximation (sound)

- First decidable interleaved Dyck

- Tight complexity bounds (SETH)

- Optimal for verification

**SPDS (Spath 2019)**:

- Automaton-based formalism

- Overapproximation under hypothesis

- Practical WPDS encoding

- Proven speedups (64–83x)

- Optimal for bug finding

**Combined Use**:

- Run MCFL underapprox: paths found are DEFINITELY reachable

- Run SPDS overapprox: paths NOT found are DEFINITELY unreachable

- Intersection: sound AND complete when they agree

- Conrado 2025 shows agreement in 8/11 benchmarks

**Recommendation**:

- For taint verification (need soundness): Use MCFL

- For bug finding (need precision): Use SPDS

- For maximum precision: Combine both

</div>

### Cross-Reference: Sparse Value-Flow Analysis (SVF)

For **source-sink reachability** problems (memory leaks, use-after-free, taint-to-sink), an alternative to <span class="sans-serif">IFDS</span>-based dataflow is **Sparse Value-Flow Analysis** (Section 5.6).

<div class="pillarbox">

Both use <span class="sans-serif">CFL</span>-reachability for context-sensitivity (Section <a href="#sec:cfl-framework" data-reference-type="ref" data-reference="sec:cfl-framework">2.1</a>). The difference is in REPRESENTATION and PREREQUISITES:

**<span class="sans-serif">IFDS</span> (Section <a href="#sec:ifds" data-reference-type="ref" data-reference="sec:ifds">1</a>)**:

- Exploded supergraph: (program_point, dataflow_fact) pairs

- Self-contained: no pre-analysis required

- Best for: general dataflow, small finite domains

**SVF (Section 5.6)**:

- Sparse VFG: nodes are variable definitions, edges are def-use chains

- REQUIRES pointer analysis for Memory SSA construction

- Best for: source-sink problems, address-taken variable tracking

For leak detection in C/C++, SVF typically achieves 10–50x speedup over dense <span class="sans-serif">IFDS</span> approaches (ISSTA 2012) due to sparse traversal.

</div>

**See Section 5.6** for complete SVF formalization including Memory SSA, SVFG construction rules, leak detection algorithms, and comparison with <span class="sans-serif">IFDS</span>.

## Under-Approximate Analysis (Bug Finding)

**Sources**: Le et al. 2022 (ISL), Vanegue 2025 (Pulse-infinity), O’Hearn 2020

<div class="pillarbox">

**Over-Approximation (<span class="sans-serif">IFDS</span>, Section <a href="#sec:ifds" data-reference-type="ref" data-reference="sec:ifds">1</a>)**:

- Sound for ABSENCE: “No taint found” means truly safe

- May have FALSE POSITIVES: Reports bugs that don’t exist

**Under-Approximation (This section)**:

- Sound for PRESENCE: “Bug found” means truly a bug

- May have FALSE NEGATIVES: Misses some bugs

**Combined**: Use <span class="sans-serif">IFDS</span> to find candidates, then under-approx to verify.

</div>

### The Eval Algorithm (Pulse/Infer)

**Paper**: Calcagno et al. 2009, 2011 (Bi-Abduction, Infer)

The Eval algorithm performs forward symbolic execution with bi-abduction to discover both bugs AND missing preconditions.

<div class="fstarcode">

(\* ————————————————– Unlike IFDS which propagates dataflow facts, Eval propagates separation logic assertions through the program. ————————————————– \*)

type eval_state = pre : assertion; (\* Discovered precondition (accumulated anti-frame) \*) post : assertion; (\* Current symbolic state \*) path : list node_id; (\* Execution path for witness \*) exit : exit_condition; (\* Ok or Er \*)

val eval_stmt : eval_state -\> ir_stmt -\> list eval_state (\* May return multiple states for conditionals/loops \*)

let eval_stmt state stmt = match stmt with \| Assign (x, e) -\> (\* x := e — update symbolic state \*) let new_post = substitute state.post x (eval_expr e state.post) in \[ state with post = new_post \]

\| Load (x, ptr, field) -\> (\* x := ptr-\>field — need ptr \|-\> field: v \*) let required = points_to ptr field (fresh_var "v") in match biabduct state.post required with \| None -\> (\* NULL DEREFERENCE: can’t satisfy requirement \*) \[ state with exit = Er; post = state.post ‘star‘ error "null_deref" \] \| Some anti_frame = m; frame = f -\> (\* Add anti-frame to precondition, update post \*) let state’ = state with pre = state.pre ‘star‘ m; post = f ‘star‘ (x \|-\> fresh_var "v"); in \[state’\]

\| Store (ptr, field, v) -\> (\* ptr-\>field := v \*) let required = points_to ptr field (fresh_var "\_") in match biabduct state.post required with \| None -\> \[ state with exit = Er; post = state.post ‘star‘ error "null_deref" \] \| Some anti_frame = m; frame = f -\> \[ state with pre = state.pre ‘star‘ m; post = f ‘star‘ points_to ptr field v; \]

\| Free ptr -\> (\* free(ptr) — need ptr \|-\> \_ with UNIQUE capability \*) let required = points_to ptr "\_" (fresh_var "\_") in match biabduct state.post required with \| None -\> \[ state with exit = Er; post = error "double_free_or_invalid" \] \| Some anti_frame = m; frame = f -\> (\* Remove the freed memory from post \*) \[ state with pre = state.pre ‘star‘ m; post = f \]

\| If (cond, then_branch, else_branch) -\> (\* Fork execution for both branches \*) let then_state = state with post = state.post ‘star‘ (cond = true) in let else_state = state with post = state.post ‘star‘ (cond = false) in (\* Prune infeasible paths \*) let then_results = if sat then_state.post then eval_block then_state then_branch else \[\] in let else_results = if sat else_state.post then eval_block else_state else_branch else \[\] in then_results @ else_results

\| While (cond, body) -\> (\* UNDER-APPROXIMATE: Bounded unrolling, NOT widening \*) eval_loop_bounded state cond body 3 (\* k=3 unrollings \*)

\| Call (ret, func, args) -\> (\* Use function summary if available \*) match get_summary func with \| Some summary -\> apply_summary state summary args ret \| None -\> (\* Inline or skip with havoc \*) \[ state with post = havoc ret state.post \]

(\* ————————————————– BOUNDED LOOP UNROLLING (UNDER-APPROXIMATE) For BUG FINDING: unroll k times, then cut off. This is SOUND for finding bugs: any bug found is real. May MISS bugs that require more iterations. ————————————————– \*)

val eval_loop_bounded : eval_state -\> ir_expr -\> ir_block -\> nat -\> list eval_state let eval_loop_bounded init_state cond body max_unroll = let rec unroll fuel state = if fuel = 0 then (\* Cut off: return current state as "exited loop" \*) \[ state with post = state.post ‘star‘ (cond = false) \] else (\* Check loop condition \*) let continue_state = state with post = state.post ‘star‘ (cond = true) in let exit_state = state with post = state.post ‘star‘ (cond = false) in let continue_results = if sat continue_state.post then let body_results = eval_block continue_state body in List.concat_map (fun s -\> unroll (fuel - 1) s) body_results else \[\] in let exit_results = if sat exit_state.post then \[exit_state\] else \[\] in continue_results @ exit_results in unroll max_unroll init_state

</div>

### ISL Triple Semantics

**Paper**: Le et al. 2022 — “Finding Real Bugs in Big Programs with ISL”

Incorrectness Separation Logic (ISL) inverts the meaning of Hoare triples. While Hoare logic’s $`\{P\} C \{Q\}`$ means “ALL executions from $`P`$ end in $`Q`$” (over-approximation), ISL’s $`[p]\ C\ [q]`$ means “SOME execution from $`p`$ ends in $`q`$” (under-approximation). This makes ISL ideal for bug finding: if we prove $`[p]\ C\ [\texttt{error}]`$, we have demonstrated that an error is *actually reachable*.

The following code shows how to convert Eval’s exploration results into ISL triples. The function `eval_to_isl_triple` runs Eval on a function and collects all states that terminate with errors. The `presumption` field captures what conditions were required (accumulated anti-frames), and the `result` field describes the error state.

<div class="fstarcode">

(\* ————————————————– ISL Triple: \[p\] C \[q; exit\] Meaning: If execution starts in state satisfying p, and terminates with exit condition, then final state satisfies q.

Key difference from Hoare logic: - Hoare: P C Q — ALL executions from P end in Q (over-approx) - ISL: \[p\] C \[q\] — SOME execution from p ends in q (under-approx) ————————————————– \*)

(\* ISL triple computed by Eval \*) val eval_to_isl_triple : cpg -\> func_id -\> isl_triple let eval_to_isl_triple cpg func = let init_state = pre = emp; (\* Start with empty precondition \*) post = emp; (\* Start with empty postcondition \*) path = \[\]; exit = Ok; in let final_states = eval_func cpg func init_state in (\* Collect error states \*) let error_states = List.filter (fun s -\> s.exit = Er) final_states in match error_states with \| \[\] -\> (\* No bugs found \*) presumption = emp; code = func; result = emp; exit_cond = Ok \| errors -\> (\* Combine error states \*) let combined_pre = List.fold_left star emp (List.map (fun s -\> s.pre) errors) in let combined_post = List.fold_left disj bot (List.map (fun s -\> s.post) errors) in presumption = combined_pre; code = func; result = combined_post; exit_cond = Er

</div>

### Latent vs Manifest Errors

**Paper**: Le, Raad, Villard, Berdine, Dreyer, O’Hearn 2022 “Finding Real Bugs in Big Programs with ISL”

<div class="pillarbox">

**Problem**: ISL generates many error specs. Which to report?

**Example**: `deref(x) { *x = 10; }`

ISL triple: $`[\texttt{x = NULL}]\ \texttt{deref(x)}\ [\texttt{er : x = NULL}]`$

Is this a bug? Only if called with NULL. But we don’t know callers.

**Solution**: Distinguish MANIFEST from LATENT errors.

- **Manifest**: Bug reachable from ANY calling context

- **Latent**: Bug only reachable under specific preconditions

**Policy**: Report manifest bugs unconditionally. Use latent specs compositionally to find manifest bugs in callers.

</div>

<div class="definition">

**Definition 3.1** (Manifest Error (Le 2022, Section 3.2)). An error triple $`\models [p]\ C\ [\texttt{er} : q]`$ denotes a MANIFEST error if:

1.  The presumption is trivial: $`p \equiv \texttt{emp} \land \texttt{true}`$

2.  The result is satisfiable: $`\texttt{sat}(q)`$

3.  All heap locations in $`q`$ are existentially quantified (fresh)

4.  All pure constraints in $`q`$ are satisfiable under any valuation

Formally, for $`q = \exists X_q.\, \kappa_q \land \pi_q`$:

1.  $`p \equiv \texttt{emp} \land \texttt{true}`$ — No precondition requirements

2.  $`\texttt{sat}(q)`$ holds — Error state is reachable

3.  $`\texttt{locs}(\kappa_q) \subseteq X_q`$ — Heap is fresh (not from caller)

4.  $`\forall v.\, \texttt{sat}(\pi_q[v / Y \cup \texttt{locs}(\kappa_q)])`$ — Pure part always satisfiable

</div>

**Intuition**: Manifest = error reachable regardless of how function is called. The heap portion is freshly allocated (condition 3), so caller can’t prevent error by providing different inputs.

<div class="theorem">

**Theorem 3.2** (True Positives Property (Le 2022, Theorem 3.4)). *If procedure `f()` in a complete program has a MANIFEST error, then either:*

1.  *`f()` is dead code (not reachable from `main()`), OR*

2.  *There exists a concrete trace from `main()` to the error.*

***Consequence**: Manifest errors have 0% false positive rate. If we report it, it’s a real bug (unless dead code).*

</div>

<div class="fstarcode">

type error_classification = \| Manifest : error_classification (\* Report unconditionally \*) \| Latent : precondition:assertion -\> error_classification (\* Report at call site \*) \| LatentLeak : error_classification (\* Report anyway - leaks are caller’s fault rarely \*)

val classify_error : isl_triple -\> error_classification let classify_error triple = match triple.exit_cond with \| Ok -\> failwith "Not an error triple" \| Er -\> let p = triple.presumption in let q = triple.result in (\* Condition 1: Trivial presumption \*) let trivial_pre = is_emp_and_true p in (\* Condition 2: Satisfiable result \*) let sat_result = sat q in (\* Condition 3: Fresh heap locations \*) let fresh_heap = all_locs_existentially_quantified q in (\* Condition 4: Pure constraints satisfiable \*) let pure_sat = pure_always_satisfiable q in

if trivial_pre && sat_result && fresh_heap && pure_sat then Manifest else if is_memory_leak_error triple then LatentLeak (\* Report leaks even when latent \*) else Latent triple.presumption

(\* BUG REPORTING POLICY (Le 2022 Section 2.3) \*) val should_report : func_id -\> isl_triple -\> bool let should_report func triple = match classify_error triple with \| Manifest -\> true \| LatentLeak -\> true (\* Always report leaks \*) \| Latent \_ -\> (\* Report latent NPE only in main() \*) is_main_function func && is_null_deref_error triple

</div>

**Practical Results (Pulse-X tool)**:

*OpenSSL-1.0.1h* (2015, 2.83M bytes IR, 8,658 procedures):

- Pulse-X: 26 bugs reported, 19 fixed (73% fix rate)

- Infer: 80 bugs reported, 39 fixed (49% fix rate)

- Pulse-X has HIGHER fix rate due to manifest filtering

*OpenSSL-3.0.0* (2021):

- Pulse-X found 15 NEW bugs, all confirmed and fixed by maintainers

- Including NPEs and memory leaks in core crypto code

**Comparison to Infer**:

<div class="center">

| **INFER (Over-Approximate)**  | **PULSE-X (Under-Approximate)** |
|:------------------------------|:--------------------------------|
| Based on separation logic     | Based on ISL                    |
| Proves ABSENCE of bugs        | Proves PRESENCE of bugs         |
| Uses heuristics for reporting | Uses manifest criterion         |
| May have false positives      | 0% FP for manifest bugs         |
| Wider coverage (all paths)    | Targeted (bounded paths)        |
| Needs widening for loops      | Simple bounded unrolling        |

</div>

**Algorithmic Simplification**: Le 2022 Remark 1: The ISL-based algorithm is “strikingly simple” compared to over-approximate biabduction. Key reason: NO LOOP INVARIANTS NEEDED. Under-approximation uses bounded unrolling—sound for finding bugs that trigger within $`k`$ iterations.

### Integration: IFDS + Eval Hybrid

<div class="center">

**Hybrid Analysis Architecture**

</div>

**Phase 1: <span class="sans-serif">IFDS</span> (Fast, Over-Approximate)**

Run <span class="sans-serif">IFDS</span> with complexity $`\mathcal{O}{ED^3}`$ to produce candidate bugs (may include false positives), e.g., “taint reaches sink at line 42”.

**Phase 2: Eval (Precise, Under-Approximate)**

Run Eval on backward slice to produce ISL triples with paths, e.g., $`[\texttt{emp}]\ \text{slice}\ [\texttt{err; Er}]`$.

**Phase 3: Classification**

For each <span class="sans-serif">IFDS</span> finding:

1.  Compute backward slice from finding

2.  Run Eval on slice

3.  Check manifest conditions (Le 2022 Definition 3.3)

4.  Classify as Manifest/Latent/Refuted

Results:

- **Manifest** $`\rightarrow`$ TRUE BUG (0% FP by theorem)

- **Latent** $`\rightarrow`$ TRUE BUG with required context

- **Refuted** $`\rightarrow`$ FALSE POSITIVE (don’t report)

<div class="fstarcode">

type hybrid_result = \| Confirmed : classification:bug_classification -\> witness:list node_id -\> hybrid_result \| Refuted : reason:string -\> hybrid_result \| Timeout : partial:option eval_state -\> hybrid_result

val verify_ifds_finding_with_eval : cpg:cpg -\> finding:dataflow_fact -\> timeout_ms:nat -\> hybrid_result

let verify_ifds_finding_with_eval cpg finding timeout = (\* Step 1: Compute backward slice \*) let slice = backward_slice cpg finding.sink_node in (\* Step 2: Run Eval on slice with timeout \*) match eval_with_timeout slice timeout with \| Timeout partial -\> Timeout partial \| Complete final_states -\> (\* Step 3: Check if any state confirms the finding \*) let confirming = List.filter (confirms_finding finding) final_states in match confirming with \| \[\] -\> Refuted "Eval found no path to error" \| states -\> (\* Step 4: Build ISL triple and classify \*) let triple = states_to_isl_triple states in let classification = classify_bug triple in Confirmed classification (extract_witness states)

</div>

## Symbolic Execution (Path-Sensitive)

**Source**: King 1976 – “Symbolic Execution and Program Testing”

<div class="pillarbox">

**<span class="sans-serif">IFDS</span>** (Section <a href="#ch:ifds" data-reference-type="ref" data-reference="ch:ifds">[ch:ifds]</a>):

- Path-*insensitive* (merges at join points)

- Tracks <span class="smallcaps">dataflow facts</span> (finite domain)

- $`\mathcal{O}(ED^3)`$ complexity

**Eval** (Section <a href="#sec:eval-algorithm" data-reference-type="ref" data-reference="sec:eval-algorithm">3.1</a>):

- Path-*sensitive* (explores paths separately)

- Tracks <span class="smallcaps">separation logic assertions</span>

- Bi-abduction for compositionality

**Symbolic Execution** (This chapter):

- Path-*sensitive* with **path condition**

- Tracks <span class="smallcaps">symbolic values</span> (expressions over inputs)

- SMT solver for path feasibility

**Use Case Guidance**:

- Taint analysis $`\to`$ <span class="sans-serif">IFDS</span>(fast, whole-program)

- Memory bugs $`\to`$ Eval (separation logic)

- Numeric bugs $`\to`$ Symbolic Execution (precise arithmetic)

</div>

<div class="pillarbox">

**1976: King** – Pure Symbolic Execution

- Original formulation: execute with symbolic values

- Path conditions track branch decisions

- **Limitation**: Path explosion, constraint complexity

**2005: DART (Godefroid) & CUTE (Sen)** – Concolic Execution

- **Key insight**: Run concrete + symbolic *simultaneously*

- Concrete guides symbolic; fallback when symbolic fails

- **Result**: Practical symbolic execution for real programs

**2008: KLEE (Cadar)** – Scalable Symbolic Execution

- LLVM bitcode as symbolic target (language-independent)

- Constraint solver optimizations: 95% query reduction

- **Result**: 90%+ coverage on COREUTILS, 56 bugs found

**2018: QSYM (Yun)** – Optimistic Concolic for Hybrid Fuzzing

- Native instrumentation (2–5$`\times`$ vs 10–100$`\times`$ IR interpretation)

- Optimistic solving: drop hard constraints, fuzzer validates

- **Result**: 13 new bugs in heavily-fuzzed software

</div>

### Execution Tree vs CPG

<div class="center">

| **<span class="sans-serif">CPG</span>(Static)** | **Execution Tree (Dynamic)** |
|:---|:---|
| Static structure | Dynamic exploration |
| All edges exist | Forking at branches |
| No path condition | PC at each node |
| Finite | Potentially infinite |

</div>

**Relationship**: Execution tree is *computed from* <span class="sans-serif">CPG</span>on-demand. <span class="sans-serif">CPG</span>provides structure; execution tree provides path-sensitive state.

### On-Demand Symbolic Execution

Unlike IFDS which merges states at join points, symbolic execution maintains *path-sensitive* state by exploring each execution path separately. The key data structure is a `symbolic_state` containing: (1) a symbolic environment mapping variables to symbolic expressions, (2) a path condition (PC) accumulating branch decisions, and (3) the current program location.

The algorithm uses a worklist of symbolic states. At each branch, it *forks* into two states with extended path conditions. The SMT solver checks path feasibility: infeasible paths (unsatisfiable PC) are pruned. This approach is *exact* for reachability but may not terminate due to loops.

The following code demonstrates on-demand symbolic execution from a CPG. The function `symbolic_execute_from_cpg` explores paths from `entry` to `target`, returning all feasible symbolic states that reach the target. The depth bound prevents infinite exploration of loops.

<div class="fstarcode">

(\* We don’t store the execution tree. We compute it lazily when needed to verify specific findings or explore specific paths. \*) val symbolic_execute_from_cpg : cpg:cpg -\> entry:node_id -\> target:node_id -\> (\* Stop when we reach target \*) max_depth:nat -\> list symbolic_state (\* All paths that reach target \*)

let symbolic_execute_from_cpg cpg entry target max_depth = let init = env = init_symbolic_env cpg entry; pc = \[\]; (\* Empty path condition \*) stmt = entry; depth = 0; in let rec explore worklist results = match worklist with \| \[\] -\> results \| state :: rest -\> if state.stmt = target then (\* Reached target - add to results if path is feasible \*) if pc_satisfiable state.pc then explore rest (state :: results) else explore rest results else if state.depth \>= max_depth then (\* Depth bound - cut off \*) explore rest results else (\* Get successors from CPG \*) let succs = cpg_successors cpg state.stmt in let new_states = List.concat_map (step_symbolic state) succs in explore (new_states @ rest) results in explore \[init\] \[\]

</div>

<div class="fstarcode">

(\* King 1976: "The symbolic execution of IF statements requires theorem proving which, even for modest programming languages, is mechanically impossible."

REALITY: SMT solvers (Z3, CVC5) are incomplete but practical. We accept Unknown as a valid result. \*) type smt_result = Sat of model \| Unsat \| Unknown of string

val pc_satisfiable : path_condition -\> bool let pc_satisfiable pc = match smt_check pc with \| Sat \_ -\> true \| Unsat -\> false \| Unknown \_ -\> true (\* Conservatively assume satisfiable \*)

val pc_implies : path_condition -\> symbolic_expr -\> trilean let pc_implies pc expr = (\* Check if pc ==\> expr \*) match smt_check (pc @ \[NegAtom expr\]) with \| Unsat -\> Definitely (\* pc / not expr is unsat ==\> pc ==\> expr \*) \| Sat \_ -\> DefinitelyNot (\* Found counterexample \*) \| Unknown \_ -\> Unknown (\* SMT timeout or incomplete \*)

</div>

<div class="fstarcode">

(\* Source: Guarnieri et al. 2020

For detecting speculative execution vulnerabilities (Spectre), we need a variant that forks on BOTH branch outcomes, modeling misprediction.

KEY DIFFERENCES from standard symbolic execution: 1. At branches, fork on BOTH outcomes (not just feasible paths) 2. Track speculation depth (bounded by hardware window  200) 3. Collect observation traces (memory accesses, jumps) 4. Check Speculative Non-Interference (SNI) property \*)

type symex_mode = \| StandardSymex (\* Fork on feasible paths only \*) \| SpeculativeSymex (\* Fork on BOTH outcomes up to window \*)

val symbolic_execute_speculative : cpg:cpg -\> entry:node_id -\> spec_window:nat -\> list (symbolic_state \* obs_trace)

let symbolic_execute_speculative cpg entry spec_window = let init = env = init_symbolic_env cpg entry; pc = \[\]; stmt = entry; depth = 0; spec_depth = 0; obs_trace = \[\]; in let rec explore worklist results = match worklist with \| \[\] -\> results \| state :: rest -\> match get_node_kind cpg state.stmt with \| NBranch cond t_target f_target -\> if state.spec_depth \< spec_window then (\* SPECULATIVE: fork on BOTH outcomes \*) let obs = ObsJumpTarget state.stmt in let state_true = state with stmt = t_target; obs_trace = obs :: state.obs_trace in let state_false = state with stmt = f_target; spec_depth = state.spec_depth + 1; obs_trace = obs :: state.obs_trace in explore (state_true :: state_false :: rest) results else (\* Speculation window exhausted \*) explore rest ((state, state.obs_trace) :: results) \| NLoad addr \_ -\> let obs = ObsMemAccess (eval_addr state addr) in let state’ = state with obs_trace = obs :: state.obs_trace in explore (advance state’ :: rest) results \| NTerminal -\> explore rest ((state, state.obs_trace) :: results) \| \_ -\> explore (advance state :: rest) results in explore \[init\] \[\]

</div>

### Witness Generation

A critical capability of symbolic execution is *witness generation*: converting a symbolic execution path into a concrete test case that triggers the bug. When symbolic execution finds a path to an error state, the path condition (PC) encodes exactly which inputs lead there. By asking the SMT solver for a satisfying assignment to the PC, we obtain concrete input values.

The following code defines `concrete_witness` containing the input assignments, execution path, and final variable values. The function `generate_witness` queries the SMT solver for a model satisfying the path condition, then extracts concrete values for all input variables. The function `confirm_bug_with_witness` integrates witness generation with bug classification: manifest bugs should always yield witnesses, while latent bugs require the specified preconditions to be satisfied.

<div class="fstarcode">

type concrete_witness = inputs : map string int; (\* Variable assignments \*) path : list node_id; (\* Execution path \*) final_state : map string int; (\* Final variable values \*)

val generate_witness : symbolic_state -\> option concrete_witness let generate_witness sym_state = match smt_check sym_state.pc with \| Sat model -\> (\* Extract concrete values from model \*) let inputs = extract_input_values model sym_state.env in let final = instantiate_env sym_state.env model in Some inputs = inputs; path = sym_state.path; final_state = final \| Unsat -\> None \| Unknown \_ -\> None

(\* Integration with bug classification \*) val confirm_bug_with_witness : cpg:cpg -\> finding:bug_classification -\> option concrete_witness let confirm_bug_with_witness cpg finding = match finding with \| Manifest proof -\> (\* Manifest bugs should always have witnesses \*) let sym_states = symbolic_execute_from_cpg cpg proof.triple.entry proof.triple.error_loc 100 in List.find_map generate_witness sym_states \| Latent ctx -\> (\* Latent bugs need context satisfied \*) let sym_states = symbolic_execute_with_precond cpg ctx in List.find_map generate_witness sym_states \| \_ -\> None

</div>

### Constraint Solver Optimizations

**Source**: Cadar, Dunbar, Engler 2008 – “KLEE: Unassisted and Automatic Generation of High-Coverage Tests”

<div class="pillarbox">

SMT solving is the **bottleneck** of symbolic execution. KLEE demonstrates that simple optimizations can dramatically reduce solver load:

**Optimization Stack** (in order of application):

1.  **Expression rewriting** – Simplify before sending to solver

2.  **Constraint independence** – Partition unrelated constraints

3.  **Counter-example cache** – Reuse previous solver results

4.  **Implied value concretization** – Extract concrete values when possible

**Empirical Results** (from KLEE paper, Table 3):

- STP queries reduced by 95% on COREUTILS

- 10$`\times`$ speedup in overall analysis time

- Counter-example cache hit rate: 92–98%

</div>

<div class="fstarcode">

(\* Simplify symbolic expressions before sending to SMT solver. Many expressions simplify to constants or simpler forms. \*) val simplify_expr : symbolic_expr -\> symbolic_expr let rec simplify_expr e = match e with (\* Additive identity: x + 0 = 0 + x = x \*) \| SymAdd (e1, SymConst 0) -\> simplify_expr e1 \| SymAdd (SymConst 0, e2) -\> simplify_expr e2

(\* Multiplicative identity: x \* 1 = 1 \* x = x \*) \| SymMul (e1, SymConst 1) -\> simplify_expr e1 \| SymMul (SymConst 1, e2) -\> simplify_expr e2

(\* Multiplicative zero: x \* 0 = 0 \* x = 0 \*) \| SymMul (\_, SymConst 0) -\> SymConst 0 \| SymMul (SymConst 0, \_) -\> SymConst 0

(\* Power-of-two multiplication to shift: x \* 2^n = x \<\< n \*) \| SymMul (e1, SymConst c) when is_power_of_two c -\> SymShl (simplify_expr e1, SymConst (log2 c))

(\* Constant folding: combine adjacent constants \*) \| SymAdd (SymConst c1, SymConst c2) -\> SymConst (c1 + c2) \| SymMul (SymConst c1, SymConst c2) -\> SymConst (c1 \* c2) \| SymSub (SymConst c1, SymConst c2) -\> SymConst (c1 - c2)

(\* Self-subtraction: x - x = 0 \*) \| SymSub (e1, e2) when expr_equal e1 e2 -\> SymConst 0

(\* Boolean simplifications \*) \| SymAnd (SymTrue, e2) -\> simplify_expr e2 \| SymAnd (e1, SymTrue) -\> simplify_expr e1 \| SymAnd (SymFalse, \_) -\> SymFalse \| SymOr (SymTrue, \_) -\> SymTrue \| SymNot (SymNot e1) -\> simplify_expr e1

(\* Recursive simplification \*) \| SymAdd (e1, e2) -\> SymAdd (simplify_expr e1, simplify_expr e2) \| \_ -\> e

</div>

<div class="fstarcode">

(\* Partition constraints by shared variables. If constraints share no variables, they can be solved INDEPENDENTLY. This dramatically reduces solver complexity since SAT is exponential in variable count.

KLEE INSIGHT: Most path conditions decompose into independent subsets. A 50-constraint problem might split into 10 independent 5-constraint problems — exponentially easier to solve. \*)

type constraint_set = constraints : list symbolic_expr; variables : set var_id;

(\* Extract variables from a symbolic expression \*) val get_variables : symbolic_expr -\> set var_id let rec get_variables e = match e with \| SymVar v -\> Set.singleton v \| SymConst \_ -\> Set.empty \| SymAdd (e1, e2) \| SymSub (e1, e2) \| SymMul (e1, e2) -\> Set.union (get_variables e1) (get_variables e2) \| SymIte (c, t, f) -\> Set.union (get_variables c) (Set.union (get_variables t) (get_variables f)) \| SymRead (arr, idx) -\> Set.add arr (get_variables idx) \| \_ -\> Set.empty

(\* Partition constraints into independent sets using union-find \*) val partition_independent : list symbolic_expr -\> list constraint_set let partition_independent constraints = (\* Build union-find over variables \*) let uf = UnionFind.create () in

(\* For each constraint, union all its variables together \*) List.iter (fun c -\> let vars = Set.to_list (get_variables c) in match vars with \| \[\] -\> () \| v :: rest -\> List.iter (fun v’ -\> UnionFind.union uf v v’) rest ) constraints;

(\* Group constraints by representative variable \*) (\* ... partition logic ... \*)

</div>

<div class="fstarcode">

(\* Cache solver results and exploit subset/superset relationships:

KEY INSIGHTS (from KLEE): 1. If constraint set S is UNSATISFIABLE, any SUPERSET S’ is also unsat 2. If constraint set S is SATISFIABLE with model M, any SUBSET S’ is also satisfiable (possibly with M) 3. If S has solution M, try M on superset S’ — it might work!

This exploits the incremental nature of symbolic execution where path conditions grow by adding constraints one at a time. \*)

type cex_cache = (\* Map from constraint hash to satisfying assignment \*) sat_cache : map constraint_hash (model \* set constraint_hash); (\* Set of unsatisfiable constraint set hashes \*) unsat_cache : set constraint_hash; (\* Statistics \*) mutable hits : nat; mutable misses : nat;

val check_with_cache : cex_cache -\> list symbolic_expr -\> smt_result let check_with_cache cache constraints = let constraint_set = Set.of_list (List.map hash_constraint constraints) in

(\* CHECK 1: Is any subset unsatisfiable? \*) let subset_unsat = Set.exists (fun h -\> Set.mem h cache.unsat_cache ) (power_set constraint_set) in if subset_unsat then begin cache.hits \<- cache.hits + 1; Unsat end else (\* CHECK 2: Does a superset have a solution that works? \*) (\* ... cache lookup logic ... \*)

(\* CACHE MISS: Call actual SMT solver \*) cache.misses \<- cache.misses + 1; let result = smt_check constraints in (\* Update cache with result \*) result

</div>

<div class="fstarcode">

(\* When a constraint directly implies a variable’s value, extract and substitute it. This reduces symbolic expression complexity.

EXAMPLE: If path condition contains (x + 1 = 10), we can derive x = 9 and replace all occurrences of x with concrete value 9.

KLEE: Writes concretized values back to memory, avoiding future symbolic operations on those values entirely. \*)

val extract_implied_values : list symbolic_expr -\> map var_id int let extract_implied_values constraints = List.fold_left (fun implied c -\> match c with (\* Direct equality: x = c \*) \| SymEq (SymVar v, SymConst c) -\> Map.add v c implied \| SymEq (SymConst c, SymVar v) -\> Map.add v c implied

(\* Linear equation: x + c1 = c2 ==\> x = c2 - c1 \*) \| SymEq (SymAdd (SymVar v, SymConst c1), SymConst c2) -\> Map.add v (c2 - c1) implied

\| \_ -\> implied ) Map.empty constraints

</div>

### Concolic Testing Optimizations

**Sources**:

- Godefroid, Klarlund, Sen 2005 – “DART: Directed Automated Random Testing” (PLDI)

- Sen, Marinov, Agha 2005 – “CUTE: A Concolic Unit Testing Engine for C” (ESEC/FSE)

<div class="pillarbox">

**Key Insight**: Run concrete execution *alongside* symbolic tracking. When symbolic reasoning becomes intractable (e.g., pointers to arrays), **substitute** concrete values and continue.

**CUTE Optimizations**:

1.  **Logical input map** – Decouple memory layout from logical structure

2.  **Constraint separation** – Pointer vs arithmetic solved *separately*

3.  **Fast unsat check** – Syntactic contradiction detection (60–95% skip)

4.  **Incremental solving** – Only re-solve dependent constraints

**Empirical Results** (from CUTE paper):

- Fast unsat check eliminates 60–95% of SMT calls

- Common sub-constraint elimination: 64–90% reduction

- Incremental solving: only 1/8 constraints re-solved on average

</div>

<div class="fstarcode">

(\* Decouples LOGICAL structure from PHYSICAL memory layout.

Traditional approach: symbolic pointers are PHYSICAL addresses Problem: Address arithmetic is undecidable with arrays

CUTE approach: logical input map I : N -\> N U V - Maps LOGICAL addresses to values or other logical addresses - Pointer operations become SIMPLE equality/disequality - Memory allocation creates NEW logical addresses

EXAMPLE: For input struct int x; Node\* next; I = 1 \|-\> 42, // p-\>x = 42 2 \|-\> 3, // p-\>next points to logical addr 3 3 \|-\> 17, // p-\>next-\>x = 17 4 \|-\> NULL // p-\>next-\>next = NULL \*)

type logical_address = nat type primitive_value = int

type logical_value = \| LVPrimitive : v:primitive_value -\> logical_value \| LVPointer : addr:logical_address -\> logical_value \| LVNull : logical_value

type logical_input_map = mapping : map logical_address logical_value; next_addr : logical_address; type_info : map logical_address ir_type;

</div>

<div class="fstarcode">

(\* CUTE separates constraints into TWO independent classes:

POINTER CONSTRAINTS: p = q, p \<\> q, p = NULL, p \<\> NULL - Solved via EQUIVALENCE GRAPH (union-find + diseq edges) - Decidable in near-linear time - No SMT solver needed!

ARITHMETIC CONSTRAINTS: linear arithmetic over integers - a1\*x1 + a2\*x2 + ... + an\*xn + c \<cmp\> 0 where \<cmp\> in =,\<\>,\<,\<=,\>,\>= - Solved via ILP solver (lp_solve) or SMT

KEY INSIGHT: Pointer and arithmetic constraints share NO variables (pointers are logical addresses, arithmetic is over values). They can be solved COMPLETELY INDEPENDENTLY. \*)

type ptr_constraint = \| PCEq : p1:symbolic_ptr -\> p2:symbolic_ptr -\> ptr_constraint \| PCNeq : p1:symbolic_ptr -\> p2:symbolic_ptr -\> ptr_constraint

type arith_constraint = (\* Linear: sum(ai \* xi) + c \<cmp\> 0 \*) \| ACLinear : coeffs:list (int \* var_id) -\> constant:int -\> cmp:comparison -\> arith_constraint

type separated_constraints = ptr_constraints : list ptr_constraint; arith_constraints : list arith_constraint;

(\* Solve pointer constraints via equivalence graph \*) val solve_ptr_constraints : list ptr_constraint -\> option (map var_id logical_address) (\* Near-linear time via union-find! \*)

</div>

<div class="fstarcode">

(\* Before calling the SMT solver, check for SYNTACTIC contradictions. If we can detect unsatisfiability cheaply, skip the expensive SMT call.

CUTE reports: 60-95

CHECK 1: Negated constraint equals existing constraint Path: \[x \> 0, y \< 5\] Negate x \> 0 to get x \<= 0 If path contains x \<= 0, trivially unsat

CHECK 2: Implied contradiction Path: \[x = 5\] Negate to get x \<\> 5 Combined with x = 5, trivially unsat \*)

val fast_unsat_check : list symbolic_expr -\> symbolic_expr -\> bool let fast_unsat_check path_condition negated = (\* CHECK 1: Is negated the negation of something in path? \*) let syntactic_contradiction = List.exists (fun c -\> is_negation_of c negated \|\| is_negation_of negated c ) path_condition in

if syntactic_contradiction then true else (\* CHECK 2: Simple propagation for equality chains \*) match negated with \| SymNeq (SymVar v, SymConst c) -\> List.exists (fun pc -\> match pc with \| SymEq (SymVar v’, SymConst c’) when v = v’ && c = c’ -\> true \| \_ -\> false ) path_condition \| \_ -\> false

</div>

<div class="fstarcode">

(\* When we negate a constraint to explore a new path, we only need to re-solve the constraints that DEPEND on the changed predicate.

DEPENDENCY: Constraint C1 depends on C2 if they share variables.

INCREMENTAL ALGORITHM: 1. Find constraints dependent on negated predicate 2. Solve ONLY the dependent subset 3. If satisfiable, try to extend old solution 4. Only full re-solve if extension fails

CUTE reports: On average, only 1/8 of constraints need re-solving! \*)

type incremental_solver_state = constraints : list symbolic_expr; solution : option model; dependency_graph : map var_id (set nat);

val solve_incremental : incremental_solver_state -\> negated_idx:nat -\> (incremental_solver_state \* option model) (\* Try old model first, only re-solve dependent subset \*)

</div>

### Chopped Symbolic Execution

**Source**: Trabish, Mattavelli, Rinetzky, Cadar 2018 – “Chopped Symbolic Execution”

<div class="pillarbox">

**The Problem**: Despite KLEE optimizations (Section <a href="#sec:klee-optimizations" data-reference-type="ref" data-reference="sec:klee-optimizations">4.4</a>), symbolic execution still suffers from exponential path explosion. Many explored paths are *irrelevant* to the analysis goal.

**Example**: Hunting heap overflow in libtasn1 requires traversing 2,945 calls to 98 functions (386,727 instructions) – most unrelated to the vulnerability.

**The Insight**: Allow *skipping* irrelevant code, but don’t simply ignore it. Execute skipped code **lazily** when side effects become *observable*.

**Results**: 10–100$`\times`$ speedup over baseline KLEE on failure reproduction. CVE-2012-1569: KLEE runs out of memory; Chopper reproduces in $`<`$ 4 minutes.

</div>

#### The Path Explosion Problem

KLEE (Section <a href="#sec:klee-optimizations" data-reference-type="ref" data-reference="sec:klee-optimizations">4.4</a>) reduces *constraint solving* cost. CSE addresses *number of paths*:

- Exponential growth: $`2^N`$ paths for $`N`$ branches

- Most paths are *irrelevant* to analysis goal

- Skip irrelevant code, recover on-demand

**Complementary**: Use *both* for maximum effectiveness:

1.  CSE to skip irrelevant code regions

2.  KLEE optimizations for constraint solving in relevant regions

#### The Four State Types

<div class="fstarcode">

(\* STATE KIND HIERARCHY: Normal -\> \[call skip\] -\> Snapshot created Normal -\> \[load mayMod\] -\> Dependent (suspended) Snapshot -\> \[recovery initiated\] -\> Recovery Recovery -\> \[return\] -\> Dependent resumes as Normal \*)

type cse_state_kind = \| CseNormal (\* Standard symbolic execution state \*) \| CseSnapshot (\* Clone saved before entering skip region \*) \| CseDependent (\* Suspended state awaiting recovery \*) \| CseRecovery (\* Executing sliced skipped function \*)

type skip_entry = skipped_func : func_id; snapshot : cse_snapshot_state; call_context : path_condition;

type cse_state =

store : symbolic_store; heap : symbolic_heap; pc : path_condition; kind : cse_state_kind; skipped : list skip_entry;

(\* For Dependent states \*) snapshot_ref : option cse_snapshot_state; guiding_constraints : path_condition; dep_addr : option address;

(\* For Recovery states \*) recovery_link : option cse_state; target_addr : option address; sliced_func : option (list ir_stmt);

(\* Optimization: track addresses written after skip \*) overwritten_set : set address;

</div>

###### Normal States:

Standard symbolic execution state (King 1976). No special handling, code executed symbolically.

###### Snapshot States:

Clone of Normal state created *before* entering skip region. Preserves complete symbolic state at call site boundary. Immutable after creation.

###### Dependent States:

Suspended Normal state that encountered a load where the address *may* have been modified by a skipped function.

###### Recovery States:

State forked from Snapshot with guiding constraints, executing a *statically sliced* version of skipped function to compute value at dependent load address.

#### Guiding Constraints

**Problem**: Recovery state forked from snapshot might explore paths *inconsistent* with the dependent state’s execution context.

**Solution**: Add path constraints accumulated *since snapshot* to Recovery state.
``` math
\mathsf{guiding\_constraints} = \mathsf{dependent.pc} - \mathsf{snapshot.pc}
```
``` math
\mathsf{recovery.pc} = \mathsf{snapshot.pc} \land \mathsf{guiding\_constraints}
```

<div class="fstarcode">

val get_guiding_constraints : current:cse_statecurrent.kind = CseDependent -\> snapshot:cse_snapshot_state -\> path_condition let get_guiding_constraints current snapshot = (\* Filter: constraints in current.pc but not in snapshot.snap_pc \*) List.filter (fun c -\> not (List.mem c snapshot.snap_pc)) current.pc

</div>

#### The Recovery Mechanism

**When Recovery Triggers**:

1.  Normal state executes load instruction

2.  $`\mathsf{mayMod}(\mathsf{state}, \mathsf{state.skipped}, \mathsf{addr})`$ returns true

3.  Recovery initiated for dependent load

**May-Mod Analysis**: Uses Andersen-style pointer analysis to compute set of allocation sites each function may modify.

**Backward Slicing**: Recovery executes *sliced* version of skipped function using PDG.

<div class="fstarcode">

type recovery_result = \| RecoveryCreated : recovery:cse_state -\> recovery_result \| RecoveryInfeasible : recovery_result

val create_recovery_state : dependent:cse_statedependent.kind = CseDependent -\> addr:address -\> entry:skip_entry -\> recovery_result let create_recovery_state dependent addr entry = let gc = get_guiding_constraints dependent entry.snapshot in let recovery_pc = concat entry.snapshot.snap_pc gc in

(\* Check feasibility: guiding constraints must not contradict snapshot \*) if not (smt_satisfiable recovery_pc) then RecoveryInfeasible else let sliced = static_backward_slice entry.skipped_func addr in RecoveryCreated store = entry.snapshot.snap_store; heap = entry.snapshot.snap_heap; pc = recovery_pc; kind = CseRecovery; skipped = entry.snapshot.snap_skipped; recovery_link = Some dependent; target_addr = Some addr; sliced_func = Some sliced; (\* ... other fields ... \*)

</div>

#### Correctness Properties

<div class="theorem">

**Theorem 4.1** (CSE Soundness). *All paths explored by CSE are feasible (satisfiable PC). Exception: Non-termination in skipped functions not detected.*

</div>

<div class="theorem">

**Theorem 4.2** (Guiding Constraints Correctness). *For all models $`M`$: $`M \models \mathsf{recovery.pc} \Rightarrow M \models \mathsf{dependent.pc}`$*

</div>

<div class="theorem">

**Theorem 4.3** (Recovery Equivalence). *After recovery completes, dependent has correct value at target address:
``` math
\mathsf{lookup}(s'.\mathsf{heap}, \mathsf{addr}) = \mathsf{lookup}(s''.\mathsf{heap}, \mathsf{addr})
```
where $`s'`$ is full execution and $`s''`$ is skip-and-recover execution.*

</div>

<div class="theorem">

**Theorem 4.4** (Relative Completeness). *For bugs not depending on skip internals, CSE will find them (assuming skipped functions terminate).*

</div>

#### Integration with Outcome Logic

<div class="center">

| **Aspect** | **Outcome Logic** | **Chopped SE** |
|:---|:---|:---|
| Goal | Find bugs (under-approx) | Find bugs (directed) |
| Path selection | Witness paths to bugs | Skip irrelevant code |
| Completeness | Partial (by design) | Relative to skip regions |
| False positives | None (manifest bugs) | None (concrete execution) |
| Recovery | N/A (permanent drop) | On-demand when relevant |
| *Both focus on relevant paths, not exhaustive exploration* |  |  |

</div>

### Optimistic Concolic Execution (QSYM)

**Source**: Yun, Lee, Xu, Jang, Kim 2018 – “QSYM: A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing” (USENIX Security)

<div class="pillarbox">

**QSYM Key Insight**: In hybrid fuzzing, the fuzzer provides *validation*. The concolic executor doesn’t need to be perfectly sound – generate candidate inputs optimistically, let the fuzzer validate them by execution.

**Scalability Wall** (why traditional concolic fails):

1.  Symbolic emulation overhead: IR translation (VEX/LLVM) is 10–100$`\times`$ slow

2.  Environment modeling: syscalls, libraries often unsupported

3.  Constraint explosion: full soundness = massive constraint sets

4.  Path explosion: even with fuzzer help, too many paths

**QSYM Solutions**:

1.  Native execution with Intel Pin instrumentation (2–5$`\times`$ vs 10–100$`\times`$)

2.  Optimistic constraint solving with fuzzer validation

3.  Basic block pruning by CFG distance to targets

4.  Concrete fallback for complex operations (FP, syscalls)

**Empirical Results**: 13 new bugs in heavily-fuzzed software (ffmpeg, OpenJPEG)

</div>

<div class="fstarcode">

(\* Traditional (KLEE, Driller): Binary -\> IR -\> Interpret symbolically (SLOW) QSYM: Binary -\> Native execution + selective instrumentation (FAST) \*)

type execution_model = \| IRInterpretation (\* KLEE-style: lift to IR, interpret symbolically \*) \| NativeInstrumented (\* QSYM-style: native exec + symbolic tracking \*) \| FullEmulation (\* S2E-style: full system emulation \*)

type instrumentation_granularity = \| AllInstructions (\* Every instruction runs symbolic \*) \| SymbolicDataOnly (\* Only when symbolic data touched - QSYM default \*) \| CFGRelevantOnly (\* Only at branch points affecting targets \*)

(\* QSYM instruments ONLY instructions touching symbolic data. Non-symbolic code runs at near-native speed. \*)

</div>

<div class="fstarcode">

(\* QSYM drops constraints that are expensive to solve, relying on the fuzzer to validate generated inputs by concrete execution. \*)

type simplification_strategy = \| DropFloatingPoint (\* FP operations are hard for SMT \*) \| DropUnmodeledCalls (\* Use return value from concrete exec \*) \| ConcreteSymbolicPointers (\* Resolve symbolic ptrs to concrete addrs \*) \| PruneDistantBlocks (\* Drop constraints from far-away code \*)

let qsym_optimistic_config : optimistic_config = strategies = \[ DropFloatingPoint; DropUnmodeledCalls; ConcreteSymbolicPointers; PruneDistantBlocks; \]; max_constraint_size = 1000; solver_timeout_ms = 1000; (\* 1 second per query - fail fast \*) validation_required = true;

val simplify_optimistically : config:optimistic_config -\> constraints:list symbolic_expr -\> concrete:concrete_state -\> list symbolic_expr (\* Apply strategies, drop oversized constraints \*)

</div>

<div class="fstarcode">

(\* Not all code is equally relevant. QSYM prioritizes blocks CLOSE to uncovered branches, pruning constraints from distant code. \*)

type block_relevance = cfg_distance : nat; (\* Hops to nearest uncovered branch \*) data_flow_score : float; (\* How much data flows to branches \*) execution_count : nat; (\* Times hit in concrete trace \*)

val compute_block_relevance : cfg:cfg -\> block:basic_block_id -\> uncovered:set basic_block_id -\> block_relevance

type pruning_config = max_cfg_distance : nat; (\* Prune if further than this \*) prune_unexecuted : bool; (\* Prune blocks not in concrete trace \*)

val prune_constraints_by_distance : constraints:list (basic_block_id \* symbolic_expr) -\> cfg:cfg -\> targets:set basic_block_id -\> config:pruning_config -\> list symbolic_expr

</div>

<div class="fstarcode">

(\* Fuzzer and concolic executor work cooperatively: - Fuzzer finds easy paths quickly (loose constraints) - Concolic solves hard constraints (tight like x == 0xdeadbeef) - Fuzzer validates concolic-generated inputs \*)

type hybrid_result = \| NewCoverage : inputs:list bytes -\> new_blocks:set basic_block_id -\> hybrid_result \| CrashFound : input:bytes -\> crash_info:crash_info -\> hybrid_result \| NoProgress : hybrid_result

val hybrid_iteration : program:string -\> inputs:list bytes -\> coverage:set basic_block_id -\> hybrid_result

let hybrid_iteration program inputs coverage = (\* 1. Run fuzzer for a while \*) let (fuzz_inputs, fuzz_coverage) = run_fuzzer program inputs in

(\* 2. Find branches where fuzzer is stuck \*) let uncovered = Set.diff (get_all_branches program) fuzz_coverage in let stuck = find_most_promising uncovered 5 in

(\* 3. Run optimistic concolic on stuck branches \*) let concolic_inputs = List.concat_map (fun target -\> let trace = get_concrete_trace program inputs target in let constraints = collect_path_constraints trace in let simplified = simplify_optimistically qsym_optimistic_config constraints trace in match optimistic_solve simplified 1000 with \| Sat model -\> \[model_to_input model\] \| \_ -\> \[\] ) stuck in

(\* 4. Validate concolic inputs with fuzzer \*) let validated = List.filter_map (fun input -\> match run_with_coverage program input with \| (cov, NormalExit) when not (Set.is_empty (Set.diff cov coverage)) -\> Some input (\* New coverage! \*) \| (\_, Crash crash) -\> Some input (\* Crashes are valuable! \*) \| \_ -\> None (\* Didn’t help - optimistic solving produced junk \*) ) concolic_inputs in (\* ... return result ... \*)

</div>

#### Soundness Model: Validation-Based

QSYM is *not* sound for path reachability (may generate invalid inputs). QSYM *is* sound for bug finding: every reported crash is validated.

<div class="center">

|  | **Sound (KLEE)** | **Best-Effort (CUTE)** | **Optimistic (QSYM)** |
|:---|:--:|:--:|:--:|
| Guarantees | Correct + Complete | Correctness | Neither (validated) |
| False positives | None | Rare | Frequent but validated away |
| Scalability | $`\sim`$<!-- -->100K LOC | Medium | Millions of LOC |
| Use case | Verification | Testing | Bug hunting |

</div>

**Complementary Use**:

- KLEE for small, critical components (crypto, parsers) – proof quality

- QSYM for large codebases (ffmpeg, Chrome) – bug hunting at scale

**Cross-References**:

- Section <a href="#sec:klee-optimizations" data-reference-type="ref" data-reference="sec:klee-optimizations">4.4</a>: KLEE optimizations (sound – apply first)

- Section <a href="#sec:concolic-optimizations" data-reference-type="ref" data-reference="sec:concolic-optimizations">4.5</a>: CUTE optimizations (constraint separation)

- Section <a href="#sec:chopped-se" data-reference-type="ref" data-reference="sec:chopped-se">4.6</a>: Chopped Symbolic Execution (function-level pruning)

- Section <a href="#sec:ifds-eval-hybrid" data-reference-type="ref" data-reference="sec:ifds-eval-hybrid">3.4</a>: Hybrid <span class="sans-serif">IFDS</span>+ Eval architecture (add fuzzer cooperation)

# Pointer Analysis — The Precision Foundation

<div class="contributionbox">

**Source**: **\[Rupta24\]**, **\[Lattner07\]** DSA. See Appendix D.10.4 for full analysis.

**OLD RECOMMENDATION**: “Steensgaard for speed, Andersen for precision”

**UPDATED**: Language AND codebase size matter!

**Language-Specific Recommendations**:

- **<span class="sans-serif">Rust</span>**: 1-callsite-sensitive + stack filtering (Section 12.22) — *FASTER AND more precise than Steensgaard for Rust!*

- **<span class="sans-serif">C</span>/<span class="sans-serif">C++</span> ($`<`$ 50K LOC)**: Andersen (most precise, subset-based)

- **<span class="sans-serif">C</span>/<span class="sans-serif">C++</span> ($`>`$ 100K LOC)**: DSA (Section 5.2.5) — unification + heap cloning. Linux kernel (355K LOC) in 3.1 seconds, $`<`$ 46MB memory. Precision *comparable* to Andersen via context-sensitivity.

- **<span class="sans-serif">C</span>/<span class="sans-serif">C++</span> source-sink**: SVF (Section 5.6) — uses DSA/Andersen as input

- **<span class="sans-serif">C</span>/<span class="sans-serif">C++</span> flow-sensitive**: Demand-driven (**\[Sridharan05\]**)

- **<span class="sans-serif">Java</span>/OOP**: Qilin + ZIPPER (Sections 5.3, 5.3.2) for selective CS

- **<span class="sans-serif">Python</span>/<span class="sans-serif">JavaScript</span>**: Type-based + dynamic traces

**Key Insight**: Rust’s ownership model changes aliasing patterns dramatically. Stack filtering (Section 5.3, 12.22) eliminates spurious stack aliasing. DSA’s heap cloning distinguishes data structure *instances* at scale.

</div>

<div class="contributionbox">

**Source**: **\[Rupta24\]**. See Appendix D.10.3 for full analysis.

**This Section** uses field <span class="smallcaps">index</span>: `FieldLoad(dst, base, field_index)`

**Rupta 2024** uses <span class="smallcaps">projection path</span>: `(base, [field1, field2, field3])`

**Trade-off**:

- **Index-based**: Simpler, sufficient for flat structs

- **Projection-based**: More precise for *nested* structs — `x.a.b.c` and `y.a.b.c` distinguished by full path. Required for Rust where nested borrows are common.

**Recommendation**: Use projection-based for Rust/<span class="sans-serif">C++</span> with deep nesting. Section 12.22 provides `typed_loc` with `type_view` for cast handling.

</div>

<div class="contributionbox">

**Source**: **\[TAJ09\]**

When tracking taint through field accesses (e.g., `x.a.b.c`), how deep should analysis go? TAJ found an empirical bound:

<div class="center">

</div>

Meaning: Most taint flows involve at most 2 levels of field dereference:

- **COMMON**: `x.field` ($`k=1`$)

- **COMMON**: `x.outer.inner` ($`k=2`$)

- **RARE**: `x.a.b.c.d` ($`k=4`$) — diminishing returns

**Recommendation**:

- Default to $`k=2`$ for field-dereference depth

- Make configurable for specific analyses requiring deeper tracking

- Cost grows exponentially with $`k`$; keep it small

Cross-reference: TAJ taint analysis (Section 8.1.3, 8.1.5)

</div>

## Andersen’s Analysis

<div class="pillarbox">

**\[Andersen94\]** L. O. Andersen. *Program Analysis and Specialization for the C Programming Language*. PhD thesis, DIKU, University of Copenhagen, 1994.

Andersen’s analysis is the gold standard for precise points-to analysis. It models pointer operations as set constraints.

</div>

### The Constraint Language

Pointer operations translate to set inclusion constraints:

<div class="center">

| **Statement** | **Constraint** | **Meaning** |
|:---|:---|:---|
| `x = &y` | $`\{y\} \subseteq \mathit{pts}(x)`$ | $`x`$ points to $`y`$’s location |
| `x = y` | $`\mathit{pts}(y) \subseteq \mathit{pts}(x)`$ | $`x`$ points to everything $`y`$ points to |
| `x = *y` | $`\forall v \in \mathit{pts}(y).\; \mathit{pts}(v) \subseteq \mathit{pts}(x)`$ | $`x`$ points to what $`y`$’s targets point to |
| `*x = y` | $`\forall v \in \mathit{pts}(x).\; \mathit{pts}(y) \subseteq \mathit{pts}(v)`$ | $`y`$’s targets added to $`x`$’s targets’ pts |

</div>

<div class="remark">

**Remark 1.1** (Example Constraint Solving). Consider:

      p = &x           {x} <= pts(p)
      q = &y           {y} <= pts(q)
      r = p            pts(p) <= pts(r)
      s = *r           forall v in pts(r). pts(v) <= pts(s)

Solution:
``` math
\begin{aligned}
\mathit{pts}(p) &= \{x\} \\
\mathit{pts}(q) &= \{y\} \\
\mathit{pts}(r) &= \{x\} \quad \text{(from } \mathit{pts}(p)\text{)} \\
\mathit{pts}(s) &= \mathit{pts}(x) \quad \text{(from dereferencing } r \text{ which points to } x\text{)}
\end{aligned}
```

</div>

### Constraint Solving

The F\* formalization below defines the core types for Andersen’s analysis. The `abstract_loc` type distinguishes between stack, heap, and global memory locations—this stratification is essential for precision, as different storage classes have different aliasing behaviors. The `constraint_` type directly encodes the four fundamental pointer operations (address-of, copy, load, store) plus field operations for struct handling.

<div class="fstarcode">

(\* ================================================== ANDERSEN’S POINTS-TO ANALYSIS Source: Andersen 1994 ================================================== \*) module BrrrMachine.PointerAnalysis.Andersen

type abstract_loc = \| StackLoc : func:string -\> var:string -\> abstract_loc \| HeapLoc : alloc_site:node_id -\> abstract_loc \| GlobalLoc : var:string -\> abstract_loc \| UnknownLoc : abstract_loc

type constraint\_ = \| AddrOf : lhs:string -\> rhs:abstract_loc -\> constraint\_ (\* lhs = &rhs: rhs \<= pts(lhs) \*) \| Copy : lhs:string -\> rhs:string -\> constraint\_ (\* lhs = rhs: pts(rhs) \<= pts(lhs) \*) \| Load : lhs:string -\> rhs:string -\> constraint\_ (\* lhs = \*rhs: forall v in pts(rhs). pts(v) \<= pts(lhs) \*) \| Store : lhs:string -\> rhs:string -\> constraint\_ (\* \*lhs = rhs: forall v in pts(lhs). pts(rhs) \<= pts(v) \*) \| FieldLoad : lhs:string -\> base:string -\> field:string -\> constraint\_ (\* lhs = base-\>field \*) \| FieldStore : base:string -\> field:string -\> rhs:string -\> constraint\_ (\* base-\>field = rhs \*)

type pts_solution = map string (set abstract_loc)

</div>

The `pts_solution` type maps each variable name to its points-to set—a set of abstract locations that the variable may reference at runtime. This is the central data structure computed by the analysis; all client analyses (taint tracking, use-after-free detection, etc.) query this map to determine aliasing relationships.

The following code demonstrates how constraints are extracted from the Code Property Graph (<span class="sans-serif">CPG</span>). Each assignment node is classified by its pointer operation type, and the corresponding constraint is generated. This extraction is language-independent; the <span class="sans-serif">CPG</span> abstraction (Section 3.1) normalizes language-specific syntax into a uniform representation.

<div class="fstarcode">

val extract_constraints : cpg -\> list constraint\_ let extract_constraints cpg = fold_nodes cpg (fun constraints node -\> match node.kind with (\* x = &y \*) \| NAssign when is_addr_of cpg node.id -\> let lhs = get_lhs cpg node.id in let rhs = get_addr_of_target cpg node.id in AddrOf lhs rhs :: constraints (\* x = y \*) \| NAssign when is_simple_copy cpg node.id -\> let lhs = get_lhs cpg node.id in let rhs = get_rhs_var cpg node.id in Copy lhs rhs :: constraints (\* x = \*y \*) \| NAssign when is_load cpg node.id -\> let lhs = get_lhs cpg node.id in let rhs = get_deref_target cpg node.id in Load lhs rhs :: constraints (\* \*x = y \*) \| NAssign when is_store cpg node.id -\> let lhs = get_store_target cpg node.id in let rhs = get_rhs_var cpg node.id in Store lhs rhs :: constraints (\* x = y-\>f \*) \| NAssign when is_field_load cpg node.id -\> let (lhs, base, field) = get_field_load_info cpg node.id in FieldLoad lhs base field :: constraints (\* x-\>f = y \*) \| NAssign when is_field_store cpg node.id -\> let (base, field, rhs) = get_field_store_info cpg node.id in FieldStore base field rhs :: constraints \| \_ -\> constraints ) \[\]

</div>

The constraint extraction function `extract_constraints` performs a single pass over the <span class="sans-serif">CPG</span>, producing a list of constraints. The helper functions (`is_addr_of`, `get_lhs`, etc.) are <span class="sans-serif">CPG</span> query operations that inspect node attributes. This design separates the constraint *language* from the constraint *extraction*, enabling the same solver to work with constraints from different front-ends.

The worklist algorithm below is the heart of Andersen’s analysis. It iteratively propagates points-to information until a fixpoint is reached. The key insight is that only variables whose points-to sets have *changed* need to be re-examined—the worklist tracks these “dirty” variables. For load/store constraints, the algorithm must handle the indirection: when `pts(y)` changes, any constraint of the form `x = *y` must be re-evaluated.

<div class="fstarcode">

val solve_andersen : list constraint\_ -\> pts_solution let solve_andersen constraints = (\* Initialize with base constraints (AddrOf) \*) let init = List.fold_left (fun sol c -\> match c with \| AddrOf lhs rhs -\> Map.update lhs (fun s -\> Set.add rhs (Option.value s  default:Set.empty)) sol \| \_ -\> sol ) Map.empty constraints in

(\* Build constraint graph for worklist \*) let copy_edges = List.filter_map (fun c -\> match c with Copy lhs rhs -\> Some (rhs, lhs) \| \_ -\> None ) constraints in let load_constraints = List.filter (fun c -\> match c with Load \_ \_ -\> true \| \_ -\> false ) constraints in let store_constraints = List.filter (fun c -\> match c with Store \_ \_ -\> true \| \_ -\> false ) constraints in

(\* Worklist: variables whose pts changed \*) let worklist = ref (Map.keys init) in let solution = ref init in

while not (List.is_empty !worklist) do let var = List.hd !worklist in worklist := List.tl !worklist; let pts_var = Map.find_default var Set.empty !solution in

(\* Propagate through copy edges \*) List.iter (fun (src, dst) -\> if src = var then let old_pts = Map.find_default dst Set.empty !solution in let new_pts = Set.union old_pts pts_var in if not (Set.equal old_pts new_pts) then begin solution := Map.add dst new_pts !solution; worklist := dst :: !worklist end ) copy_edges;

(\* Handle complex constraints \*) List.iter (fun c -\> match c with \| Load lhs rhs when rhs = var -\> Set.iter (fun loc -\> let loc_pts = Map.find_default (loc_to_var loc) Set.empty !solution in let old_pts = Map.find_default lhs Set.empty !solution in let new_pts = Set.union old_pts loc_pts in if not (Set.equal old_pts new_pts) then begin solution := Map.add lhs new_pts !solution; worklist := lhs :: !worklist end ) pts_var \| Store lhs rhs when lhs = var -\> let rhs_pts = Map.find_default rhs Set.empty !solution in Set.iter (fun loc -\> let loc_var = loc_to_var loc in let old_pts = Map.find_default loc_var Set.empty !solution in let new_pts = Set.union old_pts rhs_pts in if not (Set.equal old_pts new_pts) then begin solution := Map.add loc_var new_pts !solution; worklist := loc_var :: !worklist end ) pts_var \| \_ -\> () ) (load_constraints @ store_constraints) done; !solution

</div>

<div class="remark">

**Remark 1.2** (Complexity Analysis). Let $`n`$ = number of variables/locations.

**Worst Case**: $`\mathcal{O}{n^3}`$

- Each variable can point to $`\mathcal{O}{n}`$ locations

- Each change propagates to $`\mathcal{O}{n}`$ dependents

- $`\mathcal{O}{n}`$ variables can change

**Practical**: Much better on real programs due to sparse constraint graphs.

**Comparison** (language-dependent — see Part V tension box):

- **Andersen**: $`\mathcal{O}{n^3}`$, precise (inclusion-based)

- **Steensgaard**: $`\mathcal{O}{n \cdot \alpha(n)}`$, less precise (unification-based)

- **Rust**: 1-callsite + stack filtering (Section 12.22) — *faster* than Steensgaard!

The traditional “Steensgaard=fast, Andersen=precise” trade-off does **not** hold for all languages. Rust’s ownership model changes aliasing patterns.

</div>

### Limitations from Andersen 1994

<div class="contributionbox">

The paper explicitly identifies problems with no good solutions:

**1. Flow-Sensitive Constraint-Based Analysis**

“Currently, we have no good solution to this problem.” (Section 4.10)

*Problem*: When `x = y` updates abstract location $`p`$ at point 3, but $`p`$ does not syntactically appear in `*q = &y` at point 4, flow-sensitive constraints cannot express this update.

*Recommendation*: Use flow-<span class="smallcaps">insensitive</span> analysis. For most programs with many small functions, this is sufficient.

**2. Must-Alias Analysis**

“We shall only consider may point-to analysis in this chapter.”

Must-alias requires flow-sensitive reasoning to determine when two pointers *definitely* alias. This is significantly more expensive.

*Recommendation*: Start with may-alias. Add must-alias only when needed for specific optimizations.

**3. Unknown Explosion from LHS Store Dereferences**

“If the analysis reveals that an Unknown pointer may be dereferenced in the left hand side of an assignment, the analysis stops with ‘worst-case’ message.” (Section 4.3.3)

*Problem*: `*x = y` where $`\mathit{pts}(x)`$ contains Unknown is <span class="smallcaps">catastrophic</span>. Everything becomes Unknown transitively.

*Recommendation*:

- Track which constraints come from LHS of stores

- Only abort when LHS dereference involves Unknown

- RHS dereferences with Unknown are safe (just propagate Unknown)

**4. Full Array Element Tracking**

“Our analysis treats arrays as aggregates.” (Section 4.12.1)

Tracking individual array elements requires array dependence analysis from parallelizing compilers.

**5. Recursive Data Structure Unrolling**

“In our experience elements in a recursive data structure are used ‘the same way’” — merge all nodes into single abstract location. For precise tracking, use Shape Analysis (Section 5.4).

</div>

### Inter-Procedural Context Sensitivity

<div class="definition">

**Definition 1.3** (Context-Sensitive Extension via Constraint Vectors). **Source**: **\[Andersen94\]**, Section 4.6

Instead of copying functions for each call site (exponential blowup), use *vectors* of type variables indexed by call-site variants:
``` math
\langle T^0, T^1, \ldots, T^n \rangle
```
where:

- $`T^0`$ = summary (context-insensitive)

- $`T^i`$ = specific call context $`i`$

At each call site, the Static Call Graph determines which variant index to use. This achieves context sensitivity **without** exponential cloning of constraints.

</div>

    IMPLEMENTATION:
      struct ContextSensitiveVar {
        base: VarId,
        variant: u32,  // 0 = summary, 1..n = specific call contexts
      }

      fn get_callee_variant(
        call_site: u32,
        caller_variant: u32,
        scg: &StaticCallGraph
      ) -> u32 {
        scg.lookup(call_site, caller_variant)
      }

**Trade-off**:

- More precise than context-insensitive (fewer spurious flows)

- Less precise than full context cloning (variants merge at recursion)

- Polynomial complexity preserved

### Cross-References: Pointer Analysis Dependencies

<div class="artifactbox">

**What Depends on Pointer Analysis ($`\mathit{pts}()`$)**:

1.  **Effect Resolution** (Section 6.1): `ERead(loc)`, `EWrite(loc)` need $`\mathit{pts}()`$ to determine which locations are accessed. Without $`\mathit{pts}()`$, `Read(*p)` affects “everything”.

2.  **Use-After-Free Detection** (Section 6.1.4): `Free(ptr)` marks $`\mathit{pts}(\mathit{ptr})`$ as freed. Read/Write checks if any target in $`\mathit{pts}()`$ is in freed set.

3.  **Null Dereference Detection** (Section 2.1.7): Track nullable sources through pointer assignments. $`\mathit{pts}()`$ determines which pointers may contain null.

4.  **Data Race Detection** (Section 6.1.4): Two accesses conflict if $`\mathit{pts}()`$ sets overlap AND at least one is a write AND not synchronized.

5.  **Taint Analysis** (Section 8.1): Source(tainted) $`\to^*`$ Sink(tainted) through pointer indirection. `*p = tainted_value` taints $`\mathit{pts}(p)`$.

6.  **Call Graph Construction** (Section 5.3): Virtual/indirect calls: target $`= \mathit{pts}(\text{function\_pointer})`$.

7.  **IFDS Precision** (Section 4.1): <span class="sans-serif">IFDS</span> is **not** applicable to pointer analysis (non-distributive), but <span class="sans-serif">IFDS</span> analyses (taint, uninit) *use* $`\mathit{pts}()`$ for precision.

8.  **Sparse Value-Flow Analysis** (Section 5.6): SVF <span class="smallcaps">requires</span> $`\mathit{pts}()`$ for Memory SSA construction. $`\mu`$/$`\chi`$ annotations use $`\mathit{pts}()`$ to determine affected regions.

9.  **Alternative for Large <span class="sans-serif">C</span>/<span class="sans-serif">C++</span> Codebases** (Section 5.2.5): DSA provides $`\mathcal{O}{n \cdot \alpha(n)}`$ pointer analysis with heap cloning. For $`>`$ 100K LOC: DSA faster than Andersen, precision via context-sensitivity.

</div>

## Steensgaard’s Analysis

<div class="pillarbox">

**\[Steensgaard96\]** B. Steensgaard. *Points-to Analysis in Almost Linear Time*. POPL 1996.

Steensgaard’s analysis trades precision for speed using unification. Key insight: Use **equality** instead of **subset**. `x = y` means $`\mathit{pts}(x) = \mathit{pts}(y)`$, not $`\mathit{pts}(y) \subseteq \mathit{pts}(x)`$. This enables union-find for $`\mathcal{O}{n \cdot \alpha(n)}`$ complexity.

</div>

<div class="remark">

**Remark 2.1** (Language-Dependent Trade-off). This trade-off is **language-dependent**. For Rust, context-sensitive analysis with stack filtering (Section 12.22) is often *faster* due to reduced spurious aliasing. See tension resolution at start of Part V.

</div>

The F\* formalization below implements Steensgaard’s unification-based analysis using the classic union-find data structure. The `uf_node` type stores both the union-find parent pointer and the points-to set for each equivalence class representative. Path compression (`find`) and union-by-rank (`union`) ensure near-linear time complexity.

The key difference from Andersen is in `union`: when two variables are assigned (`x = y`), their equivalence classes are *merged*, causing all members to share the same points-to set. This loses precision but enables the $`\mathcal{O}{n \cdot \alpha(n)}`$ complexity that makes Steensgaard practical for very large codebases.

<div class="fstarcode">

(\* ================================================== STEENSGAARD’S POINTS-TO ANALYSIS Source: Steensgaard 1996 ================================================== \*) module BrrrMachine.PointerAnalysis.Steensgaard

(\* Union-find with path compression \*) type uf_node = mutable parent : int; mutable rank : int; mutable pts : set abstract_loc;

type union_find = array uf_node

val find : union_find -\> int -\> int let rec find uf i = if uf.(i).parent = i then i else begin uf.(i).parent \<- find uf uf.(i).parent; (\* Path compression \*) uf.(i).parent end

val union : union_find -\> int -\> int -\> unit let union uf i j = let ri = find uf i in let rj = find uf j in if ri \<\> rj then begin (\* Union by rank \*) if uf.(ri).rank \< uf.(rj).rank then uf.(ri).parent \<- rj else if uf.(ri).rank \> uf.(rj).rank then uf.(rj).parent \<- ri else begin uf.(rj).parent \<- ri; uf.(ri).rank \<- uf.(ri).rank + 1 end; (\* Merge points-to sets \*) let root = find uf i in uf.(root).pts \<- Set.union uf.(ri).pts uf.(rj).pts end

</div>

The union-find operations above maintain the invariant that equivalent variables share a single representative. When points-to sets are merged during `union`, the combined set is stored only at the new root—this avoids duplicating storage across the equivalence class.

The solving algorithm below processes each constraint type differently. `AddrOf` constraints directly add to points-to sets. `Copy` constraints trigger unification. `Load` and `Store` constraints are more complex: they unify through the indirection, which can cause significant precision loss when many variables flow together. The final solution is extracted by finding the root of each variable’s equivalence class.

<div class="fstarcode">

val solve_steensgaard : list constraint\_ -\> pts_solution let solve_steensgaard constraints = let vars = collect_all_vars constraints in let var_to_id = Map.of_list (List.mapi (fun i v -\> (v, i)) vars) in let n = List.length vars in

(\* Initialize union-find \*) let uf = Array.init n (fun i -\> parent = i; rank = 0; pts = Set.empty ) in

(\* Process constraints \*) List.iter (fun c -\> match c with \| AddrOf lhs rhs -\> let id = Map.find lhs var_to_id in uf.(id).pts \<- Set.add rhs uf.(id).pts \| Copy lhs rhs -\> (\* Key difference from Andersen: UNIFY instead of subset \*) let id_lhs = Map.find lhs var_to_id in let id_rhs = Map.find rhs var_to_id in union uf id_lhs id_rhs \| Load lhs rhs -\> let id_lhs = Map.find lhs var_to_id in let id_rhs = find uf (Map.find rhs var_to_id) in Set.iter (fun loc -\> let id_loc = Map.find (loc_to_var loc) var_to_id in union uf id_lhs id_loc ) uf.(id_rhs).pts \| Store lhs rhs -\> let id_lhs = find uf (Map.find lhs var_to_id) in let id_rhs = Map.find rhs var_to_id in Set.iter (fun loc -\> let id_loc = Map.find (loc_to_var loc) var_to_id in union uf id_loc id_rhs ) uf.(id_lhs).pts \| \_ -\> () ) constraints;

(\* Extract solution \*) Map.mapi (fun var id -\> let root = find uf id in uf.(root).pts ) var_to_id

</div>

<div class="remark">

**Remark 2.2** (Complexity and Precision Loss). **Complexity**: $`\mathcal{O}{n \cdot \alpha(n)} \approx \mathcal{O}{n}`$ where $`\alpha`$ is the inverse Ackermann function (effectively constant).

**Precision Loss Example**:

        p = &x
        q = &y
        r = p
        s = q

**Andersen**: $`\mathit{pts}(p) = \{x\}`$, $`\mathit{pts}(q) = \{y\}`$, $`\mathit{pts}(r) = \{x\}`$, $`\mathit{pts}(s) = \{y\}`$

**Steensgaard** (with `r = q` somewhere): All unified: $`\mathit{pts}(p) = \mathit{pts}(q) = \mathit{pts}(r) = \mathit{pts}(s) = \{x, y\}`$

**Use Cases**:

- Fast pre-analysis to identify “interesting” pointers

- Scalability-first scenarios (millions of LOC)

- When precision loss is acceptable

</div>

### Conditional Join (cjoin)

<div class="definition">

**Definition 2.3** (Conditional Join). **Source**: **\[Steensgaard96\]**, Figures 5 & 6

Key insight: When joining two ECRs where one has type $`\bot`$ (unknown), **defer** the join until the type becomes known. This prevents premature merging that loses precision.

- **Without cjoin**: `x = y` immediately unifies $`\mathit{pts}(x)`$ and $`\mathit{pts}(y)`$

- **With cjoin**: If $`\mathit{pts}(y) = \bot`$, record “join with $`x`$ when $`y`$ gets a type”

This is critical for handling uninitialized pointers and forward references.

</div>

The implementation below extends the basic Steensgaard analysis with conditional joins. The `steens_type` distinguishes between `TBot` (type unknown) and `TRef` (pointer with known target). The `pending` map tracks deferred joins: when we try to join with a variable whose type is still $`\bot`$, we record the pending join and execute it later when `settype` assigns a concrete type.

This deferred execution pattern is essential for handling forward references in the program—a pointer may be used before its allocation site is analyzed. The `cjoin` function implements the conditional logic: if the target type is known, join immediately; otherwise, record the pending join.

<div class="fstarcode">

(\* Steensgaard type: tau (location) x lambda (function signature) \*) type steens_type = \| TBot (\* Bottom: type unknown \*) \| TRef : tau:ecr_id -\> lam:ecr_id -\> steens_type (\* Pointer to tau, function lam \*)

(\* Extended state with pending conditional joins \*) type steens_state = uf : union_find; types : map ecr_id steens_type; pending : map ecr_id (list ecr_id); (\* Pending joins: ecr -\> waiting ECRs \*)

(\* Conditional join: defer if target type is bottom \*) val cjoin : steens_state -\> ecr_id -\> ecr_id -\> steens_state let cjoin st ecr1 ecr2 = let ecr2’ = find st.uf ecr2 in match Map.find ecr2’ st.types with \| None \| Some TBot -\> (\* Type unknown: DEFER the join until type is set \*) let waiting = Map.find_default ecr2’ \[\] st.pending in let pending’ = Map.add ecr2’ (ecr1 :: waiting) st.pending in st with pending = pending’ \| Some \_ -\> (\* Type known: perform immediate join via union \*) st with uf = union st.uf ecr1 ecr2

(\* Set type and trigger all pending conditional joins \*) val settype : steens_state -\> ecr_id -\> steens_type -\> steens_state let settype st ecr ty = let ecr’ = find st.uf ecr in let st’ = st with types = Map.add ecr’ ty st.types in let waiting = Map.find_default ecr’ \[\] st’.pending in let st” = List.fold_left (fun s pending_ecr -\> s with uf = union s.uf ecr’ pending_ecr ) st’ waiting in st” with pending = Map.remove ecr’ st”.pending

</div>

### Data Structure Analysis (DSA)

<div class="pillarbox">

**\[Lattner07\]** C. Lattner, A. Lenharth, V. Adve. *Making Context-sensitive Points-to Analysis with Heap Cloning Practical For The Real World*. PLDI 2007.

</div>

<div class="contributionbox">

**Key Insight**: The “fast vs precise” dichotomy is **false** at scale.

DSA combines:

- **Unification-based** (like Steensgaard) for $`\mathcal{O}{n \cdot \alpha(n)}`$ base complexity

- **Context-sensitivity** to recover precision lost by unification

- **Heap cloning by acyclic call paths** to distinguish data structure *instances*

**Result**: Precision comparable to Andersen, speed of Steensgaard

- Linux kernel (355K LOC): 3.1 seconds, $`<`$ 46MB memory

- Analysis time $`<`$ 5% of GCC compile time

**When to Use DSA** (vs Andersen):

- Codebase $`>`$ 100K LOC where Andersen is too slow

- Heavy use of wrapper functions around malloc

- Generic data structure libraries (`std::vector`, linked lists)

- Need to distinguish instances created through same allocator

</div>

#### DS Graph Structure

DSA represents points-to information using **Data Structure Graphs (DS Graphs)**:

    CROSS-REFERENCE: The F* type definitions for DSA are in Section 12.33.
    See Section 12.33.1 for:
      - ds_flag: Node flags (Heap, Stack, Global, Unknown, Array, Modified,
                 Read, Complete, Collapsed)
      - ds_node: Memory object representation with fields and flags
      - ds_graph: Per-function graph with nodes, edges, and variable mapping
      - call_node: Unresolved indirect call representation
      - dsa_state: Full analysis state across all functions

    CONCEPTUAL SUMMARY:
      DS Graphs use disjoint-set data structures where:
      - Nodes represent abstract memory locations (heap, stack, global)
      - Fields provide field-sensitive pointer edges
      - Flags track provenance (H/S/G/U) and analysis state (M/R/C/O/A)
      - The Complete (C) flag is CRITICAL - see below

#### The Completeness Flag

<div class="definition">

**Definition 2.4** (Completeness Flag Semantics). A node is **Complete** when all operations on it have been analyzed:

- All callers/callees have been incorporated

- No unknown external code can modify it

- Node will **never** be merged with other nodes in subsequent phases

**Aliasing Rules**:

- Two nodes **without** C flag may represent common objects $`\Rightarrow`$ must assume they may alias (conservative)

- Two nodes **with** C flag $`\Rightarrow`$ guaranteed distinct (precise)

**Use Cases**:

1.  **Incomplete Programs**: External functions create incomplete nodes

2.  **Incremental Analysis**: Only reanalyze incomplete portions

3.  **Speculative Field-Sensitivity**: Assume type-safe until proven wrong

4.  **Library Analysis**: Mark library boundaries as incomplete

</div>

#### Three-Phase Algorithm

    DSA ALGORITHM STRUCTURE:

    PHASE 1: LOCAL ANALYSIS (per function)
      - Build DS graph using only intraprocedural information
      - Track loads, stores, allocations, direct calls
      - Create call nodes for unresolved indirect calls
      - Mark all nodes as INCOMPLETE (no C flag)

    PHASE 2: BOTTOM-UP (BU) ANALYSIS
      - Traverse call graph SCCs in POSTORDER
      - Clone callee graphs into caller at call sites
      - Merge argument/return cells
      - Incrementally resolve indirect calls as function pointers become known
      - Use extended Tarjan's SCC algorithm with revisitation

    PHASE 3: TOP-DOWN (TD) ANALYSIS
      - Traverse call graph SCCs in REVERSE POSTORDER
      - Merge caller context into callees
      - Mark nodes COMPLETE when all callers incorporated
      - Final completeness determination

    COMPLEXITY:
      Time:  O(n*alpha(n) + K*alpha(K)*e)
      Space: O(f*K)
      where:
        n = number of instructions
        K = maximum DS graph size
        e = number of call graph edges
        f = number of functions
        alpha = inverse Ackermann function (effectively constant)

#### Heap Cloning: Distinguishing Data Structure Instances

DSA’s **heap cloning by acyclic call paths** distinguishes instances of data structures:

    HEAP CLONING EXAMPLE:

    // Two disjoint linked lists created via same wrapper
    list *X = makeList(10);   // Call context: main -> makeList
    list *Y = makeList(100);  // Call context: main -> makeList (different site)

    void addToList(list *L) {
      // Without heap cloning: X and Y conflated (same malloc site)
      // With heap cloning: X and Y DISTINGUISHED by call context
    }

    DSA proves X and Y are DISJOINT because:
      - They reach malloc through DIFFERENT acyclic call paths
      - Each path creates a separate DS node
      - No aliasing between nodes with different call contexts

    COMPARISON WITH ANDERSEN:
      - Andersen uses allocation-site naming: ONE abstract object per malloc
      - DSA uses call-path naming: SEPARATE objects per call context
      - For wrapper-heavy code: DSA more precise AND faster

#### Engineering Optimizations

    O(N^2) ELIMINATION TECHNIQUES:

    1. GLOBALS GRAPH
       Problem: Propagating globals through call tree is O(N^2)
       Solution: Separate graph for global-reachable nodes
         - Functions reference the globals graph, not copy it
         - Speedup: 2-3.5x on large programs

    2. GLOBAL EQUIVALENCE CLASSES
       Problem: Many globals point to same DS node
       Solution: Keep only representative global per DS node
         - Reduces EV entries from O(N^2) to O(N)
         - Speedup: up to 21.8x on Linux kernel

    3. EFFICIENT GRAPH INLINING
       Problem: Cloning allocates nodes only to discard them
       Solution: Recursive traversal from common pointers only
         - Only visit nodes that will be reflected in target
         - Avoid allocate-then-discard pattern

    GENERALIZABLE PATTERNS:
      - Separate representation for global entities
      - Representative-based compression
      - Lazy/demand-driven cloning

#### Precision Comparison with Andersen

<div class="center">

| **Benchmark** | **DSA May-Alias %** | **Andersen May-Alias %** | **Winner** |
|:--------------|:--------------------|:-------------------------|:-----------|
| 181.mcf       | 1.8%                | 20.5%                    | DSA        |
| 175.vpr       | 3.2%                | 18.1%                    | DSA        |
| 186.crafty    | 8.2%                | 23.4%                    | DSA        |
| 300.twolf     | 4.1%                | 15.8%                    | DSA        |
| 197.parser    | 22.1%               | 18.5%                    | Andersen   |
| 255.vortex    | 19.8%               | 16.2%                    | Andersen   |

</div>

**Key Finding**:

- **DSA better when**: wrapper functions, data structure libraries

- **Andersen better when**: no wrappers, direct malloc usage

- Context-sensitivity + heap cloning compensates for unification

**Hybrid Strategy**:

- Use DSA for initial fast analysis

- Demand-driven refinement for specific queries where DSA is imprecise

## On-the-Fly Call Graph Construction

<div class="pillarbox">

**\[Qilin22\]** D. He, J. Lu, J. Xue. *Qilin: A New Framework For Supporting Fine-Grained Context-Sensitivity in Java Pointer Analysis*. ECOOP 2022.

</div>

<div class="contributionbox">

For OOP languages with virtual dispatch:

- Call graph construction needs pointer analysis (to resolve receivers)

- Pointer analysis needs call graph (to know which methods to analyze)

**Solution**: Solve them <span class="smallcaps">together</span>, on-the-fly.

**Result**: Qilin (single-threaded) is 2.4x faster than Doop (8-threaded) while achieving identical precision.

</div>

The Qilin framework parameterizes pointer analysis over three orthogonal dimensions, enabling fine-grained control over the precision/performance trade-off. The `context_constructor` determines *how* calling contexts are represented: call-site strings ($`k`$-CFA), receiver objects (object-sensitivity), or receiver types (type-sensitivity). The `context_selector` determines *which* methods receive context-sensitive treatment—ZIPPER (Section <a href="#sec:zipper" data-reference-type="ref" data-reference="sec:zipper">3.2</a>) uses this to selectively apply context-sensitivity only where it improves precision. The `heap_abstractor` controls allocation-site naming granularity.

<div class="fstarcode">

(\* ================================================== ON-THE-FLY CALL GRAPH + POINTER ANALYSIS Source: Qilin (He, Lu, Xue 2022) Three parameters control precision/performance tradeoff: 1. Context Constructor (Cons): HOW to build contexts 2. Context Selector (Sel): WHEN to apply context-sensitivity 3. Heap Abstractor (HeapAbs): HOW to abstract allocation sites ================================================== \*) module BrrrMachine.PointerAnalysis.OnTheFly

type context_constructor = \| ConsInsens (\* No context - fastest, least precise \*) \| ConsCallsite (\* k-CFA: use call sites as context \*) \| ConsObject (\* Object-sensitivity: use receiver objects \*) \| ConsType (\* Type-sensitivity: use receiver types \*) \| ConsHybrid (\* Selective based on heuristics \*)

type context_selector = \| SelUniform : k:nat -\> hk:nat -\> context_selector \| SelSelective : cs_methods:set method_id -\> context_selector \| SelPartial : cs_vars:set var_id -\> context_selector

type heap_abstractor = \| HeapAllocationSite (\* Standard: one abstract loc per new \*) \| HeapTypeConsistent (\* Merge by type \*) \| HeapHeuristic (\* Exclude certain types like String, StringBuilder \*)

</div>

These type definitions encode the configuration space for context-sensitive analysis. The `SelSelective` variant enables ZIPPER-style selective context-sensitivity, where only methods in `cs_methods` receive context-sensitive treatment. The `SelPartial` variant provides even finer granularity at the variable level.

The core algorithm below interleaves call graph construction with points-to propagation. The key insight is maintaining *old* and *new* partitions of both reachable methods and points-to facts. When a virtual call’s receiver type becomes more precise, new callees are discovered and added to `new_part`. The algorithm continues until both partitions stabilize (fixpoint).

<div class="fstarcode">

val solve_on_the_fly : cpg -\> cons:context_constructor -\> sel:context_selector -\> heap:heap_abstractor -\> (call_graph \* pts_solution) let solve_on_the_fly cpg cons sel heap = let state = pts = Map.empty; call_graph = old_part = Set.empty; new_part = Set.empty ; reachable = old_part = Set.empty; new_part = Set.singleton (entry_method cpg, empty_ctx) ; worklist = \[\]; in

let rec process state = (\* Step 1: Process newly reachable methods \*) let state = process_new_reachable state cons sel heap in (\* Step 2: Propagate points-to through worklist \*) match pop_worklist state with \| None -\> state (\* Fixpoint reached \*) \| Some ((v, ctx), state’) -\> let pts_new = get_new_pts state’ v ctx in (\* Step 3: Propagate through direct constraints \*) let state’ = propagate_direct state’ v ctx pts_new in (\* Step 4: Propagate through indirect constraints \*) let state’ = propagate_indirect state’ v ctx pts_new in (\* Step 5: Resolve virtual calls based on new receiver types \*) let state’ = resolve_virtual_calls state’ v ctx pts_new cons sel in (\* Step 6: Flush and continue \*) let state’ = flush_pts state’ v ctx in process state’ in let final = process state in (extract_call_graph final, extract_pts final)

</div>

<div class="theorem">

**Theorem 3.1** (Variable-Level CS Subtlety). ***Source**: Qilin Theorem 1*

*Under method-level CS: When method $`m`$ is analyzed under new context $`c`$, $`\mathit{PTS}(v, \mathit{Sel}(v, c))_{\text{old}} = \emptyset`$ always holds.*

*Under variable-level CS: $`\mathit{PTS}(v, \mathit{Sel}(v, c))_{\text{old}} \neq \emptyset`$ may hold because $`\mathit{Sel}(v, c) = \mathit{Sel}(v, c')`$ can be true for $`c \neq c'`$.*

***Implication**: Must propagate through <span class="smallcaps">new</span> edges using <span class="smallcaps">old</span> points-to facts.*

</div>

### When to Use Each Approach

    DECISION TREE: Phased vs On-the-Fly

    Is the language...
      |
      +-- Purely procedural (C, Fortran)?
      |   -> Use PHASED (Section 3.1.7) - simpler, sufficient
      |
      +-- Has virtual dispatch (Java, Python, C++, JS)?
      |   -> Use ON-THE-FLY (Section 5.3) - REQUIRED for soundness
      |
      +-- Mixed (Rust, Go)?
          -> Use ON-THE-FLY for trait objects / interfaces
             Phased is OK for static dispatch portions

### Selective Context Sensitivity (ZIPPER)

<div class="pillarbox">

**\[ZIPPER20\]** Y. Li, T. Tan, A. Møller, Y. Smaragdakis. *A Principled Approach to Selective Context Sensitivity for Pointer Analysis*. OOPSLA 2020.

</div>

Context sensitivity dramatically improves pointer analysis precision but is expensive. The fundamental insight from ZIPPER is that **only $`\sim`$<!-- -->38% of methods actually benefit from context sensitivity**. The remaining 62% receive context-sensitive treatment that consumes analysis time without improving results.

#### The Uniform Context Sensitivity Problem

    OBSERVATION: Applying context sensitivity uniformly is wasteful.

    Traditional approaches:
    - 2-object-sensitive: ALL methods get 2-level context
    - 2-callsite-sensitive: ALL methods get 2-level call string

    Reality:
    - ~62% of methods: CS provides NO precision benefit
    - ~38% of methods: CS is precision-critical
    - Wasted analysis time on the 62% can be 3-25x the useful work

#### The Three Precision-Loss Patterns

ZIPPER identifies **why** context-insensitive analysis loses precision through three observable value-flow patterns:

<div class="definition">

**Definition 3.2** (Pattern 1: Direct Flow). Object $`O`$ enters via <span class="smallcaps">In</span> method parameter, flows through assignments and field operations, exits via <span class="smallcaps">Out</span> method return of <span class="smallcaps">same</span> class.

    class Person {
      String name;
      void setName(String nm) { this.name = nm; }  // In method
      String getName() { return this.name; }        // Out method
    }

**Without CS**: Objects from different `setName()` calls merge in `nm` parameter.

**With CS**: Each call site distinguished, precise points-to.

</div>

<div class="definition">

**Definition 3.3** (Pattern 2: Wrapped Flow). Object $`O`$ enters via <span class="smallcaps">In</span> method, gets stored in wrapper object $`W`$, wrapper $`W`$ flows out via <span class="smallcaps">Out</span> method return.

    void add(Object el) { this.elem = el; }           // Object wrapped
    Iterator iterator() { return new Iterator(elem); } // Wrapper flows out

Imprecision manifests when wrapper is later accessed.

</div>

<div class="definition">

**Definition 3.4** (Pattern 3: Unwrapped Flow). Object $`O`$ (a carrier) enters via <span class="smallcaps">In</span> method, contents are loaded from $`O`$, loaded contents flow out via <span class="smallcaps">Out</span> method return.

    SyncBox(Box box) { this.box = box; }
    Object getItem() {
      Box b = this.box;       // Carrier loaded
      return b.getItem();     // Unwrapped content flows out
    }

</div>

#### The ZIPPER Algorithm

    ALGORITHM: Zipper(program, OFG)
    INPUT:  program - Source program
            OFG     - Object Flow Graph from CI pre-analysis

    1. PCM <- {}  // Precision-Critical Methods

    2. FOR EACH class c in program:
    3.   PFG_c <- build_pfg(c, OFG)  // Add wrap/unwrap edges
    4.
    5.   // Backward reachability from Out method returns
    6.   FlowNodes <- {}
    7.   FOR EACH Out method m of c:
    8.     FOR EACH return var r of m:
    9.       FlowNodes <- FlowNodes U backward_reach(PFG_c, r)
    10.
    11.  // Extract methods containing flow nodes
    12.  FOR EACH node n in FlowNodes:
    13.    PCM <- PCM U {method_of(n)}

    14. RETURN PCM

    COMPLEXITY: O(|classes| * |OFG|)
                In practice: ~11 seconds for large Java programs

#### Empirical Results

<div class="center">

| **Configuration** | **Precision Preserved** | **Speedup** | **PCM Coverage** |
|:---|:---|:---|:---|
| Zipper-2obj | 98.8% | 3.4x (avg), 9.4x (max) | $`\sim`$<!-- -->38% methods |
| Zipper$`_e`$-2obj (PV=5%) | 94.7% | 25.5x (avg), 88x (max) | $`\sim`$<!-- -->14% methods |

</div>

**Surprising Result**: For 5 programs, Zipper$`_e`$-guided CS is *faster* than context-insensitive analysis while being *more precise*. This occurs because the reduced method set enables better cache locality.

### Python-Specific Call Graph via Type Inference

<div class="pillarbox">

**\[JARVIS23\]** M. Huang et al. *JARVIS: Scalable and Precise Application-Centered Call Graph Construction for Python*. ICSE 2023.

</div>

<div class="contributionbox">

Qilin (Section 5.3) is designed for **class-based OOP** with:

- Explicit type declarations (<span class="sans-serif">Java</span>, <span class="sans-serif">C++</span>)

- Receiver-based dispatch (`obj.method(args)`)

- Static method signatures

<span class="sans-serif">Python</span> requires **different handling** for:

- Duck typing (no static type at receiver)

- Module-level functions (no receiver)

- First-class functions and closures

- Magic methods (`__getattr__`, `__call__`, descriptors)

- MRO (Method Resolution Order) via C3 linearization

- Decorator wrapping

**Solution**: Function Type Graph (FTG) — per-function type inference with flow-sensitive strong updates.

**Results**: 84% higher precision than flow-insensitive (PyCG), 67% faster on exhaustive whole-program analysis. Handles 200k+ LOC codebases (PyCG runs OOM).

</div>

The Function Type Graph (FTG) represents Python’s type inference results. Unlike Java where types are declared, Python types must be inferred from usage patterns. The `python_type_element` discriminates between modules, classes, functions, variables, and builtins—each has different scoping and attribute lookup rules. The `python_namespace` captures the name-to-type bindings visible at each scope level.

The `ftg_relation` type encodes three kinds of type facts: `FTGPointsTo` for pointer-like relationships, `FTGTypeOf` for expression types, and `FTGFieldOf` for attribute access. These relations form a graph that can be queried to resolve method calls.

<div class="fstarcode">

module BrrrMachine.Python.FTG

type python_type_element = \| PTEModule : name:string -\> is_external:bool -\> python_type_element \| PTEClass : name:string -\> module:string -\> python_type_element \| PTEFunction : name:string -\> module:string -\> python_type_element \| PTEVariable : name:string -\> scope:string -\> python_type_element \| PTEBuiltin : name:string -\> python_type_element

type python_namespace = list (string \* python_type_element)

type python_type = element : python_type_element; namespace : python_namespace; name : string;

type ftg_relation = \| FTGPointsTo : src:python_type -\> dst:python_type -\> ftg_relation \| FTGTypeOf : expr:ir_expr -\> ty:python_type -\> ftg_relation \| FTGFieldOf : base:python_type -\> field:string -\> value:python_type -\> ftg_relation

type function_type_graph = types : set python_type; exprs : set ir_expr; relations : set ftg_relation;

</div>

#### C3 Linearization for Python MRO

Python’s multiple inheritance uses the C3 linearization algorithm to compute the Method Resolution Order (MRO). This determines the order in which base classes are searched when resolving a method call. The algorithm must satisfy three properties: (1) children come before parents, (2) the order respects the left-to-right ordering in the class definition, and (3) the result is consistent across all classes. If no linearization satisfying these properties exists, Python raises a `TypeError` at class definition time.

The `c3_merge` function implements the merge step: it finds a class that appears at the head of some list but not in the tail of any other list (the “good head” property). The recursion terminates when all lists are empty. A `None` result indicates an inconsistent hierarchy that Python would reject.

<div class="fstarcode">

(\* Python uses C3 linearization for method lookup in multiple inheritance. Example: class A: pass class B(A): pass class C(A): pass class D(B, C): pass \# MRO: D -\> B -\> C -\> A -\> object \*)

val c3_merge : list (list class_id) -\> option (list class_id) let rec c3_merge seqs = let non_empty = List.filter (fun l -\> not (List.is_empty l)) seqs in if List.is_empty non_empty then Some \[\] else let find_candidate () = List.find_opt (fun seq -\> let head = List.hd seq in List.for_all (fun other_seq -\> not (List.mem head (List.tl other_seq)) ) non_empty ) non_empty in match find_candidate () with \| None -\> None (\* C3 merge failure - inconsistent hierarchy \*) \| Some winner_seq -\> let head = List.hd winner_seq in let remaining = List.map (fun seq -\> List.filter (fun c -\> c \<\> head) seq ) non_empty in match c3_merge remaining with \| None -\> None \| Some rest -\> Some (head :: rest)

val compute_mro : class_summary -\> class_id -\> list class_id let compute_mro summ cls = let parents = get_direct_parents summ cls in if List.is_empty parents then \[cls; "object"\] else let parent_mros = List.map (compute_mro summ) parents in match c3_merge (parent_mros @ \[parents\]) with \| Some mro -\> cls :: mro \| None -\> cls :: List.concat parent_mros (\* Fallback \*)

</div>

#### Call Graph Algorithm Selection

                            Language Type?
                                 |
             +-------------------+-------------------+
             |                   |                   |
        Static Types      Gradual Types       Dynamic Types
       (Java, C++, Rust)   (TypeScript)      (Python, JS)
             |                   |                   |
             v                   v                   v
        Use Qilin          Use Qilin           Use JARVIS-style
        (Section 5.3)      with type hints     FTG (Section 5.3.3)

      SUMMARY:
        - Java/C++:     Qilin + ZIPPER (object-sensitive, selective CS)
        - Rust:         Qilin + Stack Filtering (Section 12.22)
        - TypeScript:   Qilin with type annotations
        - Python:       JARVIS FTG (flow-sensitive type inference)
        - JavaScript:   Hybrid FTG + prototype chain analysis
        - Mixed:        Use appropriate algorithm per module

## Shape Analysis via Three-Valued Logic

<div class="pillarbox">

**\[SRW02\]** M. Sagiv, T. Reps, R. Wilhelm. *Parametric Shape Analysis via 3-Valued Logic*. TOPLAS 2002.

</div>

Shape analysis determines structural properties of heap data: lists, trees, DAGs, cycles. Unlike pointer analysis which tracks aliasing, shape analysis tracks *data structure invariants*.

### Three-Valued Foundation

<div class="definition">

**Definition 4.1** (Three-Valued Logic). Two-valued logic is **too imprecise** for heap analysis. Consider: after `x = y->next` in a list traversal, $`x`$ might point to a summary node (represents multiple cells). Does `x->next = y`? Unknown! Neither yes nor no is safe.

**Three-Valued Logic**:

- $`1`$ = definitely true

- $`0`$ = definitely false

- $`\frac{1}{2}`$ = unknown/indefinite

Information ordering: $`\frac{1}{2} \sqsubseteq 0`$ and $`\frac{1}{2} \sqsubseteq 1`$ (Unknown is *less* informative than definite).

</div>

### Shape Structures

Shape structures represent abstract heaps using three-valued predicates. The `universe` contains abstract locations, some of which are *summary nodes* representing multiple concrete heap cells. The `predicates` map assigns three-valued truth values to predicate applications—for example, `n(u,v)` might be $`1`$ (definitely an edge), $`0`$ (definitely no edge), or $`\frac{1}{2}`$ (unknown).

The predicates below are standard for linked data structure analysis. Core predicates like `x` (variable points here) and `n` (next-pointer edge) describe the heap graph structure. Instrumentation predicates like `r_x` (reachable from x), `c` (on a cycle), and `is` (shared/aliased) are derived predicates that dramatically improve precision by encoding properties that would otherwise be lost to abstraction.

<div class="fstarcode">

type shape_structure = universe : set abstract_loc; predicates : map pred_name (list loc -\> tv); is_summary : loc -\> tv;

(\* Core predicates \*) val x : loc -\> tv (\* Variable x points here \*) val n : loc -\> loc -\> tv (\* n(u,v) = u.next points to v \*) val sm : loc -\> tv (\* Summary: represents multiple concrete \*)

(\* Derived instrumentation predicates - CRITICAL for precision \*) val r_x : loc -\> tv (\* Reachable from x: r_x(v) = TC(x,v) via n \*) val c : loc -\> tv (\* On cycle: c(v) = exists u. n\*(v,u) / n(u,v) \*) val is : loc -\> tv (\* Shared: is(v) = exists u,u’. u!=u’ / n(u,v) / n(u’,v) \*)

</div>

### Embedding Theorem

<div class="theorem">

**Theorem 4.2** (Embedding Theorem). ***Source**: **\[SRW02\]**, Theorem 3.7*

*If $`S \sqsubseteq^f S'`$ ($`S`$ embeds in $`S'`$ via $`f`$), then for every formula $`\varphi`$ and assignment $`Z`$:
``` math
\llbracket \varphi \rrbracket^3_S(Z) \sqsubseteq \llbracket \varphi \rrbracket^3_{S'}(f \circ Z)
```
*

***Meaning**: If concrete heap satisfies $`\varphi`$ (value 1), abstract shape doesn’t refute it (value $`\neq 0`$).*

</div>

### Focus-Transform-Coerce Algorithm

    ABSTRACT INTERPRETATION STEP:

    1. FOCUS: Make relevant predicates definite
       - If n(x,_) = 1/2, split structure into cases
       - "Materialize" concrete nodes from summaries
       - Essential for loop analysis precision

    2. TRANSFORM: Apply statement semantics
       - Update predicates according to operation
       - Standard abstract interpretation

    3. COERCE: Apply compatibility constraints
       - Propagate implications: phi_1 => phi_2
       - Detect inconsistencies (return bottom)
       - Sharpen indefinite predicates

    LOOP TERMINATION:
      - Bounded structures have finite domain
      - Canonical abstraction: merge by predicate values
      - No explicit widening needed!

### Integration with Brrr-Machine

<div class="artifactbox">

**Shape Analysis Track**:

1.  Build initial shape from <span class="sans-serif">CPG</span>:

    - Stack/global vars $`\to`$ individual nodes

    - Allocations $`\to`$ summary nodes ($`\mathit{sm} = \frac{1}{2}`$)

2.  Run focus-transform-coerce at each <span class="sans-serif">CPG</span> node

3.  Extract properties:

    - $`\mathit{list}(x, \mathit{null})`$ = valid list from $`x`$

    - $`\mathit{tree}(x)`$ = valid tree rooted at $`x`$

    - $`\mathit{shared}(v) = 0 \Longrightarrow`$ unique ownership

    - $`\mathit{cycle}(v) = 0 \Longrightarrow`$ acyclic

**Detected Bugs**:

- Use-after-free: $`r_x(v) = 1`$ but $`\mathit{freed}(v) = 1`$

- Dangling pointer: $`n(u,v) = 1`$ but $`\mathit{freed}(v) = 1`$

- Memory leak: $`\neg(\exists x.\, r_x(v) = 1)`$ but $`\mathit{freed}(v) = 0`$

- List corruption: was list, now has cycle

</div>

## Symbolic Heap Shape Analysis

<div class="pillarbox">

**\[DOY06\]** D. Distefano, P. O’Hearn, H. Yang. *A Local Shape Analysis Based on Separation Logic*. TACAS 2006.

</div>

<div class="contributionbox">

An **alternative** to TVLA (Section 5.4) for shape analysis based on separation logic.

**Key Advantages**:

- Natural integration with frame rule (Section 7.4)

- Direct memory leak detection via “junk” predicate

- Simpler abstraction via canonicalization rules

**Trade-off vs TVLA**:

- **TVLA**: better for materialization/summarization

- **Symbolic Heaps**: better for compositional (per-procedure) analysis

Both approaches are <span class="smallcaps">sound</span>; choice depends on analysis goals.

</div>

### Symbolic Heap Structure

<div class="definition">

**Definition 5.1** (Symbolic Heap). A symbolic heap has form $`\Pi \vdash \Sigma`$ where:

- $`\Pi`$ (pure part): equalities and disequalities between variables

- $`\Sigma`$ (spatial part): heap predicates in separation logic

This forms a <span class="smallcaps">finite</span> abstract domain when combined with canonicalization, bounded by $`2^{129n^2 + 18n + 2}`$ for $`n`$ program variables (Theorem 14).

</div>

The F\* types below encode symbolic heaps in separation logic style. Variables come in three flavors: program variables (`SHProgVar`), existentially-quantified primed variables (`SHPrimedVar`) introduced during bi-abduction, and the distinguished null value (`SHNil`). The pure formula captures equalities and disequalities between variables, while the spatial formula describes heap structure.

The key spatial connectives are: `SPointsTo` for a single heap cell, `SListSeg` for an inductively-defined list segment, `SJunk` for unreachable garbage (memory leaks), and `SSep` for the separating conjunction. The `abstract_state` is a *set* of symbolic heaps, representing disjunctive information from different execution paths.

<div class="fstarcode">

module BrrrMachine.ShapeAnalysis.SymbolicHeap

type sh_var = \| SHProgVar : v:var_id -\> sh_var \| SHPrimedVar : v:nat -\> sh_var (\* Existentially quantified x’ \*) \| SHNil : sh_var

type pure_formula = \| PEq : sh_var -\> sh_var -\> pure_formula \| PNeq : sh_var -\> sh_var -\> pure_formula \| PAnd : pure_formula -\> pure_formula -\> pure_formula \| PTrue

type spatial_formula = \| SEmpty (\* emp \*) \| SPointsTo : src:sh_var -\> field:string -\> dst:sh_var -\> spatial_formula \| SListSeg : src:sh_var -\> dst:sh_var -\> spatial_formula (\* ls(x, y) \*) \| SJunk (\* junk - unreachable garbage \*) \| SSep : spatial_formula -\> spatial_formula -\> spatial_formula (\* \* \*)

type symbolic_heap = pure : pure_formula; spatial : spatial_formula;

type abstract_state = set symbolic_heap

</div>

### Heap Predicates

<div class="definition">

**Definition 5.2** (Points-To Predicate). The cell at address $`(x + \mathit{offset}(f))`$ contains value $`y`$. This is the **atomic** spatial predicate.

**Critical**: Points-to is <span class="smallcaps">exclusive</span> — it *owns* the cell. Two points-to on same address cannot be separated.

</div>

<div class="definition">

**Definition 5.3** (List Segment Predicate). $`\mathit{ls}(x, y)`$: There is a well-formed singly-linked list from $`x`$ to $`y`$.

Defined **inductively**:

- $`\mathit{ls}(x, x) = \mathit{emp}`$ (empty list — base case)

- $`\mathit{ls}(x, y) = \exists z.\, x.\mathit{next} \mapsto z * \mathit{ls}(z, y)`$ (cons cell)

**Critical**: The list segment *owns* all intermediate nodes.

</div>

<div class="definition">

**Definition 5.4** (Junk Predicate). Represents heap cells that are <span class="smallcaps">unreachable</span> from program variables. This is garbage — a **memory leak indicator**.

**Critical Property**: $`\mathit{junk} * \mathit{junk} = \mathit{junk}`$ (idempotent under separation)

This allows <span class="smallcaps">unbounded</span> accumulation of leaks without state explosion.

</div>

### Canonicalization as Widening

<div class="theorem">

**Theorem 5.5** (Termination Guarantee). ***Source**: **\[DOY06\]**, Theorem 14*

*The number of <span class="smallcaps">canonical</span> symbolic heaps is bounded by:
``` math
2^{129n^2 + 18n + 2}
```
where $`n`$ = number of program variables.*

*This is <span class="smallcaps">finite</span>, ensuring:*

1.  *Canonicalization always terminates*

2.  *The abstract domain is finite*

3.  *Fixpoint iteration terminates without explicit widening*

</div>

    CANONICALIZATION RULES:

    SubstEq:   x' = E  =>  substitute E for x' everywhere
    Garbage1:  P(x', E) where x' unreachable => junk
    Garbage2:  P1(x', y') * P2(y', x') cycle => junk
    Abs1:      P1(E, x') * P2(x', F) => ls(E, nil) when Pi |- F = nil
    Abs2:      P1(E, x') * P2(x', F) * P3(G, H) => ls(E, F) * P3(G, H)
               (when F != G)

### Memory Leak Detection via Junk

The `SJunk` predicate makes leak detection trivial: after canonicalization, any heap cells that become unreachable from program variables are folded into `junk`. The idempotent property ($`\mathit{junk} * \mathit{junk} = \mathit{junk}`$) means the abstract state remains bounded even with unbounded leaks.

The leak classification below distinguishes three cases: `DefiniteLeak` when all abstract states contain junk (the leak is unavoidable), `PossibleLeak` when some but not all states contain junk (path-dependent), and `NoLeak` when no states contain junk. This maps directly to the manifest/latent bug classification in incorrectness logic (Section 12.3).

<div class="fstarcode">

type leak_result = \| NoLeak \| PossibleLeak \| DefiniteLeak of leaked_cells : nat

val detect_leaks : abstract_state -\> leak_result let detect_leaks final_states = let has_junk sh = contains_junk sh.spatial in let all_have_junk = Set.for_all has_junk final_states in let some_have_junk = Set.exists has_junk final_states in if all_have_junk then DefiniteLeak (min_junk_size final_states) else if some_have_junk then PossibleLeak else NoLeak

</div>

<div class="remark">

**Remark 5.6** (Integration with Manifest Bug Classification). Cross-reference: Section 12.3 (Manifest/Latent bug classification)

**DefiniteLeak** is a <span class="smallcaps">manifest</span> bug because:

1.  Junk appears on <span class="smallcaps">all</span> execution paths

2.  The leak is <span class="smallcaps">provable</span> — not a false positive

3.  It satisfies the “reachability condition”

**PossibleLeak** is a <span class="smallcaps">latent</span> bug because:

1.  Junk appears on <span class="smallcaps">some</span> but not all paths

2.  The leak depends on which path is taken

3.  Caller context determines if leak manifests

</div>

### Frame Rule for Compositional Analysis

<div class="theorem">

**Theorem 5.7** (Abstract Frame Rule). ***Source**: **\[DOY06\]**, Theorem 11*

*If $`\{P\}\, C\, \{Q\}`$ and $`C`$ doesn’t modify variables in $`R`$, then $`\{P * R\}\, C\, \{Q * R\}`$.*

*For all $`X, Y \in \mathcal{P}(\mathit{CSH})`$, if $`\mathit{Vars}(Y) \cap \mathit{Mod}(C) = \emptyset`$ then:
``` math
\gamma(\mathcal{A}\llbracket c \rrbracket(X * Y)) \subseteq \gamma((\mathcal{A}\llbracket c \rrbracket X) * Y)
```
*

*This enables <span class="smallcaps">compositional</span> interprocedural analysis:*

- *Analyze each procedure in isolation on its <span class="smallcaps">footprint</span>*

- *Combine via frame rule*

- *No need to re-analyze callee for each calling context*

</div>

<div class="definition">

**Definition 5.8** (Procedure Footprint). A function’s <span class="smallcaps">footprint</span> is the <span class="smallcaps">minimal</span> heap needed to execute safely. By Theorem 12 of **\[DOY06\]**, this footprint soundly approximates the full semantics:
``` math
\mathit{foot}(c)\sigma = \begin{cases} \mathcal{A}\llbracket c \rrbracket\{\sigma\} & \text{if } \mathit{onlyaccesses}(c, \sigma) \\ \text{undefined} & \text{otherwise} \end{cases}
```

</div>

The footprint representation below captures a procedure’s memory behavior independently of its calling context. The `input_footprint` is the minimal heap the procedure needs to execute (discovered via bi-abduction), and `output_footprints` are the possible resulting heaps. The `modified_vars` track which variables may be changed.

The `apply_footprint` function shows how footprints enable compositional analysis: at a call site, we match the caller’s state against the callee’s footprint, extracting the *frame*—the part of the heap untouched by the call. After the call, we combine the callee’s output with the frame. This is the abstract frame rule in action.

<div class="fstarcode">

type procedure_footprint = input_footprint : symbolic_heap; output_footprints : set symbolic_heap; modified_vars : set var_id;

val compute_footprint : cpg -\> func_id -\> procedure_footprint let compute_footprint cpg func = let init = pure = PTrue; spatial = SEmpty in let results = symbolic_exec_function cpg func init in let minimal_input = collect_anti_frames results in let outputs = Set.map (fun r -\> r.post) results in input_footprint = canonicalize minimal_input; output_footprints = Set.map canonicalize outputs; modified_vars = compute_mod cpg func

val apply_footprint : caller_state : symbolic_heap -\> callee_footprint : procedure_footprint -\> call_site : node_id -\> set symbolic_heap let apply_footprint caller_state footprint call_site = match find_matching_substate caller_state footprint.input_footprint with \| Some (matching, frame) -\> Set.map (fun output -\> canonicalize output with spatial = SSep output.spatial frame ) footprint.output_footprints \| None -\> Set.singleton pure = PTrue; spatial = SSep SJunk (error_marker "footprint_mismatch")

</div>

### Symbolic Execution for Symbolic Heaps

Symbolic execution on symbolic heaps discovers preconditions “on demand” via bi-abduction. When a statement requires a heap cell that is not currently available, the analysis does not fail—instead, it *abduces* (infers) the missing cell as part of the required precondition. This is captured in the `anti_frame` field of `exec_result`.

The code below handles key memory operations. For loads (`SLoad`), if the required cell is available, we extract its value; if not, we create a fresh existential and record the needed cell in `anti_frame`. For stores (`SStore`), we update the heap or abduce the target cell. For allocation (`SAlloc`), we extend the heap with a fresh cell. For free (`SFree`), we remove the cell. The `exit` field tracks whether the operation succeeded (`Ok`) or errored (`Er`).

<div class="fstarcode">

(\* Execute commands on symbolic heaps, discovering preconditions via bi-abduction when heap access requires unavailable cells. \*)

type exec_result = post : symbolic_heap; anti_frame : spatial_formula; (\* Discovered precondition \*) exit : exit_condition;

val symbolic_exec_stmt : symbolic_heap -\> ir_stmt -\> set exec_result let symbolic_exec_stmt sh stmt = match stmt with (\* LOAD: x := ptr-\>field \*) \| SLoad x ptr field -\> (match expose_points_to sh ptr field with \| Found (sh’, v) -\> Set.singleton post = assign_var sh’ x v; anti_frame = SEmpty; exit = Ok \| NeedCell -\> let fresh_v = fresh_primed_var () in Set.singleton post = assign_var sh x fresh_v; anti_frame = SPointsTo (var_to_sh ptr) field fresh_v; exit = Ok \| Contradiction -\> Set.singleton post = sh; anti_frame = SEmpty; exit = Er )

(\* STORE: ptr-\>field := v \*) \| SStore ptr field v -\> (match expose_points_to sh ptr field with \| Found (sh’, \_) -\> Set.singleton post = update_points_to sh’ ptr field v; anti_frame = SEmpty; exit = Ok \| NeedCell -\> let fresh_v = fresh_primed_var () in Set.singleton post = update_points_to sh ptr field v; anti_frame = SPointsTo (var_to_sh ptr) field fresh_v; exit = Ok \| Contradiction -\> Set.singleton post = sh; anti_frame = SEmpty; exit = Er )

(\* ALLOC: x := malloc(size) \*) \| SAlloc x size -\> let fresh_val = fresh_primed_var () in Set.singleton post = sh with spatial = SSep sh.spatial (SPointsTo (SHProgVar x) "data" fresh_val) ; anti_frame = SEmpty; exit = Ok

(\* FREE: free(ptr) \*) \| SFree ptr -\> (match remove_points_to sh ptr with \| Some sh’ -\> Set.singleton post = sh’; anti_frame = SEmpty; exit = Ok \| None -\> Set.singleton post = sh; anti_frame = SEmpty; exit = Er )

(\* ASSIGN: x := y – may cause LEAK \*) \| SAssign x y -\> let sh’ = update_var sh x y in Set.singleton post = canonicalize sh’; (\* Detect leaks via Garbage rules \*) anti_frame = SEmpty; exit = Ok

</div>

### Comparison: TVLA vs Symbolic Heaps

<div class="center">

| **Aspect** | **TVLA (Section 5.4)** | **Symbolic Heaps (Section 5.5)** |
|:---|:---|:---|
| Foundation | Three-valued logic | Separation logic |
| Abstraction | Instrumentation predicates | Canonicalization rules |
| Unknown | $`\frac{1}{2}`$ with Kleene semantics | Disjunction of heaps |
| Composition | Focus-transform-coerce | Frame rule |
| Leak Detection | Separate predicate needed | Built-in (junk) |
| Termination | Canonical abstraction | Theorem 14 bound |
| Best For | Materialization, complex shapes | Local reasoning, per-procedure |

</div>

**Recommendation**:

- Use TVLA when analyzing complex data structures where materialization of summary nodes is critical

- Use Symbolic Heaps when doing compositional analysis where frame rule enables procedure summaries

- Hybrid approach: Use symbolic heaps for interprocedural, TVLA for intraprocedural shape reasoning

## Sparse Value-Flow Analysis (SVF)

<div class="pillarbox">

**\[SYX12\]** Y. Sui, D. Ye, J. Xue. *Static Memory Leak Detection Using Full-Sparse Value-Flow Analysis*. ISSTA 2012.

**\[SX16\]** Y. Sui, J. Xue. *SVF: Interprocedural Static Value-Flow Analysis in LLVM*. CC 2016.

**\[CWZ96\]** F. Chow, S. Chan, R. Kennedy, S.-M. Liu, R. Lo, P. Tu. *A New Algorithm for Partial Redundancy Elimination Based on SSA Form*. CC 1996.

</div>

<div class="contributionbox">

SVF is a **complementary** technique to <span class="sans-serif">IFDS</span> (Section 4.1), not an optimization. Both share <span class="sans-serif">CFL</span>-reachability for context-sensitivity, but differ in:

- **Representation**: SVFG vs Exploded Supergraph

- **Prerequisites**: Requires pointer analysis vs self-contained

- **Applicability**: Source-sink problems vs general dataflow

**Key Insight**: <span class="smallcaps">Sparsity</span> — most statements affect few memory locations. By pre-computing def-use chains via Memory SSA, analysis propagates only along actual value flows, not all control-flow edges.

**Pointer Analysis Input**: SVF <span class="smallcaps">requires</span> pointer analysis ($`\mathit{pts}`$) for Memory SSA construction. For large <span class="sans-serif">C</span>/<span class="sans-serif">C++</span> codebases ($`>`$ 100K LOC):

- Use DSA (Section 5.2.5) for fast $`\mathcal{O}{n \cdot \alpha(n)}`$ pointer analysis

- DSA’s heap cloning provides **better** precision than allocation-site only

For smaller codebases: Use Andersen (Section 5.1) for maximum precision.

</div>

### Motivation: Dense vs Sparse Dataflow

    THE DENSITY PROBLEM:
    Traditional IFDS (Section 4.1) propagates dataflow facts along EVERY control-
    flow edge. For N statements and D dataflow facts, this is O(E * D^3).

    For source-sink problems (memory leaks, use-after-free, taint-to-sink):
      - We only care about reachability from SOURCE to SINK
      - Most statements don't affect the tracked values
      - Dense iteration wastes work on irrelevant paths

    SVF INSIGHT:
    Pre-compute def-use chains, then analyze ONLY along those chains.
    This leverages the SPARSITY of actual value flows.

    COMPARISON:
      Dense (IFDS):   Propagate facts at EVERY CFG edge
      Sparse (SVF):   Propagate facts only along DEF-USE edges

    When to use each:
      - IFDS: When you need ALL facts at each program point (reaching defs)
      - SVF:  When you need source-sink REACHABILITY (leak detection)

### Memory SSA

Memory SSA extends classical SSA to handle indirect memory accesses using $`\mu`$/$`\chi`$ annotations from **\[CWZ96\]**.

<div class="definition">

**Definition 6.1** ($`\mu`$ Annotation). Represents a potential <span class="smallcaps">use</span> of an address-taken variable. For load statement `x = *y`, annotate with $`\mu(o)`$ for each $`o \in \mathit{pts}(y)`$.

</div>

<div class="definition">

**Definition 6.2** ($`\chi`$ Annotation). Represents a potential <span class="smallcaps">def</span> (and use) of an address-taken variable. For store statement `*x = y`, annotate with $`o = \chi(o)`$ for each $`o \in \mathit{pts}(x)`$.

</div>

<div class="definition">

**Definition 6.3** (Strong vs Weak Update). **Strong Update**: If `ptr` uniquely points to $`o`$ (singleton points-to set and $`o`$ is a concrete location), then new value <span class="smallcaps">kills</span> old contents.

**Weak Update**: If `ptr` may point to multiple locations or $`o`$ is abstract (summary node), new value must <span class="smallcaps">join</span> with old value.

This distinction is **critical** for precision: strong update enables <span class="smallcaps">kill</span>, improving precision.

</div>

    ANNOTATION EXAMPLES:

    1. LOAD: x = *y (where pts(y) = {o1, o2})

       Before:  x = *y
       After:   x = *y [mu(o1_3), mu(o2_5)]

       Meaning: x's value MAY come from o1 version 3 or o2 version 5

    2. STORE: *x = y (where pts(x) = {o}, singleton)

       Before:  *x = y
       After:   *x = y [o_4 = chi(o_3)]  (STRONG update)

       Meaning: o gets new version 4, old value killed

    3. STORE: *x = y (where pts(x) = {o1, o2}, multiple)

       Before:  *x = y
       After:   *x = y [o1_4 = chi(o1_3), o2_6 = chi(o2_5)]  (WEAK update)

       Meaning: Both regions updated but old values preserved in join

### SVFG Construction

The Sparse Value-Flow Graph (SVFG) is constructed from Memory SSA by connecting def-use pairs. The F\* types below encode SVFG nodes and edges. Nodes come in several flavors: `CopyNode` and `PhiNode` handle direct value-flow; `LoadNode` and `StoreNode` handle indirect value-flow via memory with $`\mu`$/$`\chi`$ annotations; `DParamNode`/`IParamNode` and `DRetNode`/`IRetNode` handle interprocedural value-flow for direct and indirect parameters/returns; `SourceNode` and `SinkNode` mark allocation and deallocation sites for leak detection.

Edge labels distinguish between direct flow (register assignments), indirect flow (through memory), and interprocedural edges (calls and returns with callsite/callee information for context-sensitivity).

<div class="fstarcode">

module BrrrMachine.SVFG

type svfg_node = (\* Direct value-flow nodes \*) \| CopyNode : def_site:node_id -\> dst:var_id -\> src:var_id -\> svfg_node \| PhiNode : def_site:node_id -\> dst:var_id -\> srcs:list var_id -\> svfg_node

(\* Indirect value-flow nodes \*) \| LoadNode : def_site:node_id -\> dst:var_id -\> ptr:var_id -\> mu:mu_annotation -\> svfg_node \| StoreNode : def_site:node_id -\> ptr:var_id -\> val\_:var_id -\> chi:chi_annotation -\> svfg_node

(\* Interprocedural nodes \*) \| DParamNode : func:func_id -\> param:var_id -\> svfg_node \| IParamNode : func:func_id -\> region:versioned_region -\> svfg_node \| DRetNode : callsite:node_id -\> result:var_id -\> svfg_node \| IRetNode : callsite:node_id -\> region:versioned_region -\> svfg_node

(\* Source/sink markers \*) \| SourceNode : alloc_site:node_id -\> svfg_node \| SinkNode : free_site:node_id -\> svfg_node

type svfg_edge_label = \| DirectFlow \| IndirectFlow \| CallEdge : callsite:node_id -\> callee:func_id -\> svfg_edge_label \| ReturnEdge : callsite:node_id -\> callee:func_id -\> svfg_edge_label

type svfg = nodes : map svfg_node_id svfg_node; edges : list svfg_edge; sources : set svfg_node_id; sinks : set svfg_node_id;

</div>

### Context-Sensitivity via CFL-Reachability

Context-sensitivity uses the **same** approach as <span class="sans-serif">IFDS</span> (Section 4.2):

    CFL FOR MATCHED CALLS AND RETURNS:

    Edge labels form a Dyck language (matched parentheses):
      - Call edge:   (c_g  (open paren for callsite c calling g)
      - Return edge: )c_g  (close paren)

    Valid interprocedural path: balanced parentheses

    Grammar:
      S -> S S | (i S )i | epsilon    for each callsite i

    This is IDENTICAL to IFDS context-sensitivity.
    The difference is WHAT is tracked (value-flows vs dataflow facts).

### IFDS vs SVF Comparison

<div class="center">

| **Aspect** | **IFDS (**\[Reps95\]**)** | **SVF (**\[SX16\]**)** |
|:---|:---|:---|
| Problem Class | Distributive dataflow | Source-sink value-flow |
| Domain | Finite set $`D`$ of facts | Def-use chains for memory |
| Graph Structure | Exploded supergraph ($`N \times D`$) | Sparse VFG ($`N`$ nodes) |
| Context Sensitivity | <span class="sans-serif">CFL</span>-reachability | <span class="sans-serif">CFL</span>-reachability |
| Distributivity | <span class="smallcaps">Required</span> | Not required |
| Pointer Handling | Requires separate $`\mathit{pts}`$ | Integrated via Memory SSA |
| Pre-analysis | None required | Pointer analysis <span class="smallcaps">required</span> |
| Time Complexity | $`\mathcal{O}{E \cdot D^3}`$ | $`\mathcal{O}{\text{Andersen}} + \mathcal{O}{N \cdot R^2}`$ |
| Best Use Cases | Taint, reaching defs, uninit | Leak detection, UAF, DFree |

</div>

**Key Insight**: Context-sensitivity mechanism is <span class="smallcaps">identical</span> (<span class="sans-serif">CFL</span>-reachability). The difference is <span class="smallcaps">representation</span> and <span class="smallcaps">problem class</span>.

### When to Use Each Approach

    USE IFDS WHEN:
      - Transfer functions are naturally distributive
      - Domain D is small and finite (taint labels, null states)
      - No pointer analysis available or needed
      - Need proven O(E * D^3) complexity bound
      - Need complete coverage of ALL reachable facts
      - Problem is NOT source-sink specific

    USE SVF WHEN:
      - Tracking value-flows through memory (leak detection)
      - Pointer analysis is already available (from Section 5.1-5.3)
      - Need to handle address-taken variables precisely
      - Analysis is source-sink reachability problem
      - Program has many loads/stores
      - Memory regions are relatively sparse (R << D)

    CRITICAL: SVF REQUIRES pointer analysis as pre-requisite.
             IFDS is self-contained (no pre-analysis needed).

### Leak Detection Algorithm

The SVFG enables precise leak detection through a two-phase algorithm. The first phase (`some_path_analysis`) performs a forward slice from each allocation site to find which free sites are reachable—if no free is reachable, the allocation definitely leaks. The second phase (`all_path_analysis`) computes path guards: under what conditions does the allocation reach a free? If the guard is a tautology (always true), there is no leak; if it is satisfiable but not a tautology, the leak is conditional.

The `leak_result` type distinguishes four cases: `DefinitelyLeaks` (no path to free), `ConditionalLeak` (some paths leak, some don’t), `NoLeak` (all paths reach free), and `ReachesGlobal` (the pointer escapes to global state, so leak detection is inconclusive).

<div class="fstarcode">

module BrrrMachine.LeakDetection

type leak_result = \| DefinitelyLeaks : source:svfg_node_id -\> leak_result \| ConditionalLeak : source:svfg_node_id -\> condition:guard -\> leak_result \| NoLeak : source:svfg_node_id -\> leak_result \| ReachesGlobal : source:svfg_node_id -\> leak_result

(\* Phase 1: SOME-PATH ANALYSIS (Forward Slice) \*) val some_path_analysis : svfg -\> svfg_node_id -\> forward_slice_result let some_path_analysis svfg source = let rec explore visited call_stack worklist = match worklist with \| \[\] -\> visited \| (node, stack) :: rest -\> if Set.mem (node, stack) visited then explore visited call_stack rest else let visited’ = Set.add (node, stack) visited in let successors = get_cfl_successors svfg node stack in explore visited’ call_stack (successors @ rest) in let slice = explore Set.empty \[\] \[(source, \[\])\] in let sinks = Set.filter (fun (n, \_) -\> is_sink n svfg) slice in let reaches_global = Set.exists (fun (n, \_) -\> is_global_store n) slice in forward_slice = Set.map fst slice; reachable_sinks = Set.map fst sinks; reaches_global

(\* Phase 2: ALL-PATH ANALYSIS (Guard Computation) \*) val all_path_analysis : svfg -\> svfg_node_id -\> set svfg_node_id -\> guard let all_path_analysis svfg source sinks = let path_guards = Set.fold (fun sink acc -\> let paths = enumerate_vf_paths svfg source sink in let guards = List.map (compute_path_guard svfg) paths in guards @ acc ) sinks \[\] in List.fold_left (fun acc g -\> GOr acc g) GFalse path_guards

(\* Main Leak Detection \*) val detect_leak : svfg -\> svfg_node_id -\> leak_result let detect_leak svfg source = let fsr = some_path_analysis svfg source in if Set.is_empty fsr.reachable_sinks then DefinitelyLeaks source else if fsr.reaches_global then ReachesGlobal source else let freed = all_path_analysis svfg source fsr.reachable_sinks in if guard_is_tautology freed then NoLeak source else ConditionalLeak source (GNot freed)

</div>

<div class="remark">

**Remark 6.4** (Practical Performance). **Source**: **\[SYX12\]**

On average, only 5.75% of functions and 4.31% of SVFG nodes are traversed during leak detection. The sparse representation enables demand-driven analysis that skips irrelevant program regions.

**Speedup over dense approaches**: 10–50x faster than iterative dense dataflow.

</div>

### Cross-References

<div class="artifactbox">

**SVF Integration Points**:

**Prerequisites**:

- Pointer Analysis (Section 5.1–5.3): <span class="smallcaps">required</span> for Memory SSA

- <span class="sans-serif">CPG</span> (Section 3.1): SVFG nodes correspond to <span class="sans-serif">CPG</span> statement nodes

**Related Techniques**:

- <span class="sans-serif">IFDS</span> (Section 4.1): Alternative for distributive problems

- <span class="sans-serif">CFL</span>-Reachability (Section 4.2): Shared context-sensitivity approach

- Interleaved Dyck (Section 4.2.3): Combined context+field problem

- Shape Analysis (Section 5.4–5.5): Can use SVFG for heap queries

**Client Analyses**:

- Memory leak detection (primary use case)

- Use-after-free detection

- Double-free detection

- Null dereference (if null is tracked as taint)

</div>

# Effect Systems and Coeffects—Tracking Computational Behavior

## Effects as First-Class Citizens

<div class="pillarbox">

**\[Moggi91\]** Notions of computation and monads.  
**\[Plotkin03\]** The semantics of algebraic effects.  
**\[Plotkin09\]** Handlers of algebraic effects.  
**\[Leijen14\]** Koka: programming with row polymorphic effect types.  
**\[Leijen17\]** Type directed compilation of row-typed algebraic effects.

</div>

Effects are central to the brrr-machine. An effect is anything a computation does besides returning a value. Types tell you what values exist; **effects tell you what happens**.

### The Moggi Foundation

<div class="pillarbox">

Programs compute **computations**, not just **values**.

A computation of type $`T`$ may:

- Return a value of type $`T`$

- Perform effects before returning

- Never return (diverge)

</div>

<div class="definition">

**Definition 1.1** (Monad). The **monad** captures this notion:
``` math
\begin{aligned}
\mathsf{return} &: T \to M\,T && \text{(Pure value, no effect)} \\
\mathsf{bind} &: M\,T \to (T \to M\,U) \to M\,U && \text{(Sequence computations)}
\end{aligned}
```
subject to the monad laws:
``` math
\begin{aligned}
\mathsf{return}\,a \mathbin{\gg\!=}f &= f\,a && \text{(Left identity)} \\
m \mathbin{\gg\!=}\mathsf{return} &= m && \text{(Right identity)} \\
(m \mathbin{\gg\!=}f) \mathbin{\gg\!=}g &= m \mathbin{\gg\!=}(\lambda x.\, f\,x \mathbin{\gg\!=}g) && \text{(Associativity)}
\end{aligned}
```

</div>

<div class="definition">

**Definition 1.2** (Effects as Monads). Common effects have monadic interpretations:
``` math
\begin{aligned}
\mathsf{State}\,S\,T &= S \to (T \times S) && \text{(Read/write state)} \\
\mathsf{Error}\,E\,T &= T + E && \text{(May fail with error)} \\
\mathsf{Writer}\,W\,T &= T \times W && \text{(Accumulate output)} \\
\mathsf{Reader}\,R\,T &= R \to T && \text{(Read from environment)} \\
\mathsf{IO}\,T &= \mathsf{World} \to (T \times \mathsf{World}) && \text{(External effects)} \\
\mathsf{Cont}\,R\,T &= (T \to R) \to R && \text{(Continuations)}
\end{aligned}
```

</div>

### Algebraic Effects

<div class="pillarbox">

**\[Plotkin03\]**, **\[Plotkin09\]** for theory.  
**\[Sivaramakrishnan21\]** for production implementation in Multicore OCaml.

</div>

<div class="pillarbox">

Monads are powerful but don’t compose well. Algebraic effects separate **what** from **how**:

- **Effect operations**: Abstract interface (what can happen)

- **Effect handlers**: Concrete implementation (how it happens)

</div>

<div class="pillarbox">

Multicore OCaml demonstrates that algebraic effects can be added to a production language with:

- 1% mean overhead for non-effect code (validated on 54 benchmarks)

- Full DWARF debugging support (GDB, perf work correctly)

- C FFI compatibility (with boundary limitations—see Section <a href="#sec:effect-handler-limitations" data-reference-type="ref" data-reference="sec:effect-handler-limitations">[sec:effect-handler-limitations]</a>)

- Runtime-enforced continuation linearity (one-shot by default)

</div>

<div class="definition">

**Definition 1.3** (Effect Signature). Effect signatures declare the operations an effect provides:
``` math
\begin{aligned}
\mathbf{effect}\;\mathsf{State}(S) &\{\\
    &\mathsf{get} : () \to S \\
    &\mathsf{put} : S \to () \\
\} \\[1ex]
\mathbf{effect}\;\mathsf{Fail} &\{\\
    &\mathsf{fail} : () \to \bot \\
\} \\[1ex]
\mathbf{effect}\;\mathsf{IO} &\{\\
    &\mathsf{read} : () \to \mathsf{String} \\
    &\mathsf{write} : \mathsf{String} \to () \\
\}
\end{aligned}
```

</div>

<div class="definition">

**Definition 1.4** (Effect Handler). Effect handlers give semantics to effect operations:
``` math
\begin{aligned}
\mathbf{handler}\;\mathsf{state\_handler}(\mathit{init} : S) &\{ \\
    &\mathsf{return}(x) \mapsto \lambda s.\,(x, s) \\
    &\mathsf{get}((), k) \mapsto \lambda s.\,k(s)(s) && \text{($k$ is continuation)} \\
    &\mathsf{put}(s', k) \mapsto \lambda s.\,k(())(s') \\
\} \\[1ex]
\mathbf{handler}\;\mathsf{maybe\_handler} &\{ \\
    &\mathsf{return}(x) \mapsto \mathsf{Some}(x) \\
    &\mathsf{fail}((), k) \mapsto \mathsf{None} && \text{(Don't invoke $k$)} \\
\}
\end{aligned}
```

</div>

<div class="pillarbox">

1.  **Every bug is an effect violation**:

    - Use-after-free: $`\mathsf{Read}`$ effect after $`\mathsf{EFree}`$ effect on same location

    - Race condition: Concurrent $`\mathsf{Write}`$ effects without synchronization

    - SQL injection: $`\mathsf{IO}`$ effect (query) with tainted data

2.  **Effect tracking enables**:

    - Purity analysis (function has no effects)

    - Effect-based optimization (pure functions can be cached)

    - Cross-language boundary analysis (effect mismatch = risk)

</div>

### Row-Polymorphic Effects

<div class="pillarbox">

</div>

<div class="pillarbox">

Consider: $`\mathsf{read\_file} : \mathsf{String} \to \mathsf{IO}\,\mathsf{String}`$

This says “$`\mathsf{read\_file}`$ has $`\mathsf{IO}`$ effect” but:

- Can’t compose with $`\mathsf{State}`$ functions

- Can’t be polymorphic over additional effects

- Forces monomorphic effect typing

</div>

<div class="definition">

**Definition 1.5** (Leijen’s Solution: Row Polymorphism). Effect types are **rows**—unordered collections with a “tail variable”:
``` math
\mathsf{read\_file} : \mathsf{String} \to \langle \mathsf{IO}\mid \varepsilon \rangle\,\mathsf{String}
```
This says:

- $`\mathsf{read\_file}`$ definitely has $`\mathsf{IO}`$ effect

- It may have other effects ($`\varepsilon`$ is the “rest”)

- Callers can instantiate $`\varepsilon`$ with their effects

</div>

<div class="definition">

**Definition 1.6** (Row Operations).
``` math
\begin{aligned}
\text{Empty row:} \quad & \langle\rangle \\
\text{Extend:} \quad & \langle E \mid \rho \rangle && \text{($E$ plus whatever is in $\rho$)} \\
\text{Variable:} \quad & \varepsilon && \text{(Unknown effects)}
\end{aligned}
```

</div>

<div class="definition">

**Definition 1.7** (Subtyping via Row Extension). If $`f : T \to \langle E_1 \mid \varepsilon \rangle\,U`$ and we call $`f`$ in context with effects $`\langle E_1, E_2 \rangle`$, then $`\varepsilon = \langle E_2 \rangle`$ and the call has type $`\langle E_1, E_2 \rangle`$.

</div>

<div class="definition">

**Definition 1.8** (Effect-Polymorphic Map).
``` math
\mathsf{map} : \forall \alpha\,\beta\,\varepsilon.\; (\alpha \to \langle\varepsilon\rangle\,\beta) \to \mathsf{List}\,\alpha \to \langle\varepsilon\rangle\,\mathsf{List}\,\beta
```
This map is effect-polymorphic:

- $`\mathsf{map}\;\mathsf{pure\_f}\;\mathit{xs}`$ has no effects

- $`\mathsf{map}\;\mathsf{io\_f}\;\mathit{xs}`$ has $`\mathsf{IO}`$ effect

- Effect inferred from the function argument

</div>

### F\* Specification: Effect System

The following F\* specification formalizes the effect system described above. The key components are:

1.  **Effect kinds**: A comprehensive taxonomy of effects organized by category (memory, control, I/O, concurrency, channels, resources). Each constructor captures one primitive operation that has observable side effects.

2.  **Effect rows**: Row-polymorphic effect types following Leijen’s design, where effects are accumulated in extensible rows. The `RowVar` constructor enables effect polymorphism.

3.  **Session types**: Following Honda 1998/2008, channel effects include session type operations (select, branch, delegate) for typed communication protocols.

<div class="fstarcode">

module BrrrMachine.Effects

(\* Effect kinds - The taxonomy of effects \*) type effect_kind = (\* Memory effects \*) \| ERead : loc:abstract_loc -\> effect_kind \| EWrite : loc:abstract_loc -\> effect_kind \| EAlloc : effect_kind \| EFree : loc:abstract_loc -\> effect_kind (\* Control effects \*) \| EDiverge : effect_kind \| EThrow : exn_type:string -\> effect_kind \| ECatch : exn_type:string -\> effect_kind (\* I/O effects \*) \| EInput : source:io_source -\> effect_kind \| EOutput : sink:io_sink -\> effect_kind \| EFileRead : path:string -\> effect_kind \| EFileWrite : path:string -\> effect_kind \| ENetwork : effect_kind \| ERandom : effect_kind \| EClock : effect_kind (\* Concurrency effects \*) \| ESpawn : effect_kind \| EJoin : effect_kind \| ELock : lock_id:nat -\> effect_kind \| EUnlock : lock_id:nat -\> effect_kind \| ESend : chan:nat -\> payload_type:ir_type -\> effect_kind \| ERecv : chan:nat -\> payload_type:ir_type -\> effect_kind \| EAtomic : effect_kind (\* Channel effects - Honda 1998/2008 session type operations \*) \| EChanCreate : chan:nat -\> elem_type:ir_type -\> buffer_size:nat -\> effect_kind \| EChanClose : chan:nat -\> effect_kind \| ESelect : chan:nat -\> label:string -\> effect_kind \| EBranch : chan:nat -\> labels:list string -\> effect_kind \| EDelegate : chan:nat -\> delegated_chan:nat -\> effect_kind (\* Resource effects \*) \| EAcquire : resource_type:string -\> effect_kind \| ERelease : resource_type:string -\> effect_kind \| EUse : resource_type:string -\> effect_kind

and io_source = Stdin \| EnvVar of string \| FileInput \| NetworkInput \| UserInput and io_sink = Stdout \| Stderr \| FileOutput \| NetworkOutput \| DatabaseOutput

</div>

<div class="fstarcode">

type effect_row = \| RowEmpty : effect_row (\* \<\> - no effects \*) \| RowExtend : effect:effect_kind -\> tail:effect_row -\> effect_row (\* \<E \| rho\> - effect E plus tail \*) \| RowVar : var:effect_var -\> effect_row (\* epsilon - unknown effects \*)

and effect_var = nat (\* De Bruijn index or unique ID \*)

</div>

<div class="pillarbox">

This is NOT a set! The following are DISTINCT:

- $`\langle \mathsf{exn} \mid \mu \rangle`$ — one exception effect

- $`\langle \mathsf{exn}, \mathsf{exn} \mid \mu \rangle`$ — TWO exception effects

Why this matters: Effect **handlers** consume labels one at a time. A single catch handler removes ONE $`\mathsf{exn}`$ from the row:
``` math
\mathsf{catch} : \langle \mathsf{exn} \mid \varepsilon \rangle\,\tau \to (\mathsf{exn} \to \tau) \to \langle\varepsilon\rangle\,\tau
```

If you have $`\langle \mathsf{exn}, \mathsf{exn} \mid \mu \rangle`$, after one catch you still have $`\langle \mathsf{exn} \mid \mu \rangle`$!

</div>

<div class="fstarcode">

(\* Effect elimination removes ONE instance \*) val effect_handle : effect_kind -\> effect_row -\> option effect_row let rec effect_handle eff row = match row with \| RowEmpty -\> None (\* Effect not present \*) \| RowExtend e tail -\> if e = eff then Some tail (\* Remove ONE instance \*) else (match effect_handle eff tail with \| Some tail’ -\> Some (RowExtend e tail’) \| None -\> None) \| RowVar \_ -\> None (\* Can’t remove from variable \*)

(\* Effect row membership \*) val row_contains : effect_row -\> effect_kind -\> bool let rec row_contains row eff = match row with \| RowEmpty -\> false \| RowExtend e tail -\> e = eff \|\| row_contains tail eff \| RowVar \_ -\> true (\* Conservative: assume present \*)

(\* Effect row union \*) val row_union : effect_row -\> effect_row -\> effect_row let rec row_union r1 r2 = match r1 with \| RowEmpty -\> r2 \| RowExtend e tail -\> RowExtend e (row_union tail r2) \| RowVar v -\> RowVar v (\* Variable absorbs other row \*)

(\* Effect row subsumption \*) val row_subsumes : effect_row -\> effect_row -\> bool let rec row_subsumes larger smaller = match smaller with \| RowEmpty -\> true \| RowExtend e tail -\> row_contains larger e && row_subsumes larger tail \| RowVar \_ -\> true (\* Variable matches anything \*)

</div>

The following F\* code models session type semantics for channel effects. Key concepts:

- **Channel linearity**: Each channel use consumes the current session type prefix

- **Commutativity**: Operations on different channels commute (can be reordered)

- **Delegation**: Transfers channel ownership to another endpoint (linear transfer)

*Note*: `List.assoc_opt` is a helper function (not standard F\*) returning `Some v` if key found, `None` otherwise.

<div class="fstarcode">

(\* Channel effects are LINEAR: each channel use must be accounted for exactly once. This models session type linearity where each communication action consumes the current type prefix and advances to the continuation. \*)

(\* Channel effect commutativity: ESend(k, T); ERecv(k’, T’) commutes if k \<\> k’ Operations on DIFFERENT channels are independent (parallel composition) \*) val channel_effects_commute : effect_kind -\> effect_kind -\> bool let channel_effects_commute e1 e2 = match e1, e2 with \| ESend k1 \_, ESend k2 \_ -\> k1 \<\> k2 \| ESend k1 \_, ERecv k2 \_ -\> k1 \<\> k2 \| ERecv k1 \_, ESend k2 \_ -\> k1 \<\> k2 \| ERecv k1 \_, ERecv k2 \_ -\> k1 \<\> k2 \| ESend k1 \_, ESelect k2 \_ -\> k1 \<\> k2 \| ESend k1 \_, EBranch k2 \_ -\> k1 \<\> k2 \| ERecv k1 \_, ESelect k2 \_ -\> k1 \<\> k2 \| ERecv k1 \_, EBranch k2 \_ -\> k1 \<\> k2 \| ESelect k1 \_, ESelect k2 \_ -\> k1 \<\> k2 \| ESelect k1 \_, EBranch k2 \_ -\> k1 \<\> k2 \| EBranch k1 \_, EBranch k2 \_ -\> k1 \<\> k2 \| EChanClose k1, EChanClose k2 -\> k1 \<\> k2 \| EChanClose k1, ESend k2 \_ -\> k1 \<\> k2 \| EChanClose k1, ERecv k2 \_ -\> k1 \<\> k2 \| \_, \_ -\> true (\* Non-channel effects always commute with channel effects \*)

(\* Channel linearity check: A channel cannot be used after being closed. Closing a channel CONSUMES it (linear resource). \*) type channel_state = \| ChanOpen : elem_type:ir_type -\> buffer_size:nat -\> channel_state \| ChanClosed : channel_state

type channel_context = list (nat \* channel_state)

val check_channel_linearity : channel_context -\> effect_kind -\> option channel_context let check_channel_linearity ctx eff = match eff with \| EChanCreate k elem_ty buf_sz -\> if List.assoc_opt k ctx = None then Some ((k, ChanOpen elem_ty buf_sz) :: ctx) else None (\* Channel already exists \*) \| ESend k \_ \| ERecv k \_ \| ESelect k \_ \| EBranch k \_ -\> (match List.assoc_opt k ctx with \| Some (ChanOpen \_ \_) -\> Some ctx (\* Channel is open, use allowed \*) \| \_ -\> None) (\* Channel closed or doesn’t exist \*) \| EChanClose k -\> (match List.assoc_opt k ctx with \| Some (ChanOpen \_ \_) -\> Some (List.map (fun (k’, s) -\> if k’ = k then (k’, ChanClosed) else (k’, s)) ctx) \| \_ -\> None) (\* Already closed or doesn’t exist \*) \| EDelegate k delegated_k -\> (\* Delegation transfers ownership: delegated channel must be open, becomes unavailable in current context (transferred to receiver) \*) (match List.assoc_opt k ctx, List.assoc_opt delegated_k ctx with \| Some (ChanOpen \_ \_), Some (ChanOpen \_ \_) -\> Some (List.filter (fun (k’, \_) -\> k’ \<\> delegated_k) ctx) \| \_, \_ -\> None) \| \_ -\> Some ctx (\* Non-channel effect doesn’t affect channel context \*)

</div>

<div class="definition">

**Definition 1.9** (Channel Effect Typing Rules (Row-Polymorphic Style)).
``` math
\begin{gathered}
\frac{}{\Gamma \vdash \mathsf{chan\_create}\langle T, n\rangle() : \mathsf{Chan}\langle T\rangle \;\&\; \langle \mathsf{EChanCreate}(k, T, n) \mid \varepsilon \rangle}
\tag{Chan-Create} \\[2ex]
\frac{\Gamma \vdash \mathit{ch} : \mathsf{Chan}\langle T\rangle \quad \Gamma \vdash v : T}{\Gamma \vdash \mathit{ch}.\mathsf{send}(v) : () \;\&\; \langle \mathsf{ESend}(k, T) \mid \varepsilon \rangle}
\tag{Chan-Send} \\[2ex]
\frac{\Gamma \vdash \mathit{ch} : \mathsf{Chan}\langle T\rangle}{\Gamma \vdash \mathit{ch}.\mathsf{recv}() : T \;\&\; \langle \mathsf{ERecv}(k, T) \mid \varepsilon \rangle}
\tag{Chan-Recv} \\[2ex]
\frac{\Gamma \vdash \mathit{ch} : \mathsf{Chan}\langle T\rangle}{\Gamma \vdash \mathit{ch}.\mathsf{close}() : () \;\&\; \langle \mathsf{EChanClose}(k) \mid \varepsilon \rangle}
\tag{Chan-Close} \\[2ex]
\frac{\Gamma \vdash \mathit{ch} : \mathsf{Chan}\langle T\rangle \quad l \in \{l_1, \ldots, l_n\}}{\Gamma \vdash \mathit{ch}.\mathsf{select}(l) : () \;\&\; \langle \mathsf{ESelect}(k, l) \mid \varepsilon \rangle}
\tag{Chan-Select} \\[2ex]
\frac{\Gamma \vdash \mathit{ch} : \mathsf{Chan}\langle T\rangle \quad \Gamma, x:l_i \vdash e_i : U \;\&\; \langle\varepsilon_i\rangle \;\text{for each}\; i}{\Gamma \vdash \mathit{ch}.\mathsf{branch}\{l_1 \Rightarrow e_1, \ldots, l_n \Rightarrow e_n\} : U \;\&\; \langle \mathsf{EBranch}(k, \{l_1,\ldots,l_n\}) \mid \varepsilon_1 \sqcup \cdots \sqcup \varepsilon_n \rangle}
\tag{Chan-Branch} \\[2ex]
\frac{\Gamma \vdash \mathit{ch} : \mathsf{Chan}\langle S\rangle \quad \Gamma \vdash \mathit{delegated\_ch} : \mathsf{Chan}\langle T\rangle}{\Gamma \vdash \mathit{ch}.\mathsf{delegate}(\mathit{delegated\_ch}) : () \;\&\; \langle \mathsf{EDelegate}(k, k') \mid \varepsilon \rangle}
\tag{Delegate}
\end{gathered}
```
Note: In Delegate, $`\mathit{delegated\_ch}`$ is consumed—linear ownership transfer.

</div>

<div class="fstarcode">

type effect_signature = (\* Effect row for this function \*) effects : effect_row; (\* Specific categories for quick checks \*) may_read : bool; may_write : bool; may_alloc : bool; may_free : bool; may_throw : bool; may_diverge : bool; may_io : bool; may_spawn : bool; (\* Channel effects - Honda 1998/2008 session operations \*) may_send : bool; may_recv : bool; may_create_chan : bool; may_close_chan : bool; may_select : bool; may_branch : bool; may_delegate : bool; (\* Channel context: maps channel IDs to their current session types \*) channel_types : list (nat \* local_session_type); is_pure : bool; (\* No effects at all \*)

let empty_effect_sig : effect_signature = effects = RowEmpty; may_read = false; may_write = false; may_alloc = false; may_free = false; may_throw = false; may_diverge = false; may_io = false; may_spawn = false; may_send = false; may_recv = false; may_create_chan = false; may_close_chan = false; may_select = false; may_branch = false; may_delegate = false; channel_types = \[\]; is_pure = true;

</div>

<div class="fstarcode">

type effect_event = kind : effect_kind; location : node_id; timestamp : nat; thread : option nat;

type effect_trace = list effect_event

</div>

<div class="fstarcode">

(\* Source: Lamport 1978 (basic), extended per Batty 2011 (C11 memory model)

This is a SIMPLIFIED version for effect_event analysis. For the FULL C11 memory model with consume semantics, release sequences, and coherence axioms, see Section 6.5.2 happens_before_c11. \*)

val happens_before : effect_event -\> effect_event -\> bool let happens_before e1 e2 = (\* Case 1: Sequenced-before (same thread program order) \*) (e1.thread = e2.thread && e1.timestamp \< e2.timestamp) \|\| (\* Case 2: Synchronizes-with (cross-thread via release/acquire) \*) (is_release e1 && is_acquire e2 && synchronizes e1 e2) \|\| (\* Case 3: Dependency-ordered-before (consume semantics) \*) (is_release e1 && is_consume e2 && synchronizes e1 e2)

(\* Helper: Check if effect has release semantics \*) val is_release : effect_event -\> bool let is_release e = match e.kind with \| EUnlock \_ -\> true (\* Mutex unlock has release semantics \*) \| EAtomic -\> true (\* Conservative: treat atomics as release \*) \| ESend \_ -\> true (\* Channel send has release semantics \*) \| \_ -\> false

(\* Helper: Check if effect has acquire semantics \*) val is_acquire : effect_event -\> bool let is_acquire e = match e.kind with \| ELock \_ -\> true (\* Mutex lock has acquire semantics \*) \| EAtomic -\> true (\* Conservative: treat atomics as acquire \*) \| ERecv \_ -\> true (\* Channel receive has acquire semantics \*) \| \_ -\> false

(\* Helper: Check if two events synchronize \*) val synchronizes : effect_event -\> effect_event -\> bool let synchronizes e1 e2 = match e1.kind, e2.kind with \| EUnlock l1, ELock l2 -\> l1 = l2 (\* Unlock/Lock on same mutex \*) \| ESend c1, ERecv c2 -\> c1 = c2 (\* Send/Recv on same channel \*) \| \_, EJoin -\> true (\* Thread join synchronizes \*) \| EAtomic, EAtomic -\> true (\* Atomic operations may synchronize \*) \| \_ -\> false

</div>

<div class="fstarcode">

type effect_violation = \| UseAfterFree : loc:abstract_loc -\> free_site:node_id -\> use_site:node_id -\> effect_violation \| DoubleFree : loc:abstract_loc -\> first_free:node_id -\> second_free:node_id -\> effect_violation \| DataRace : loc:abstract_loc -\> access1:effect_event -\> access2:effect_event -\> effect_violation \| Deadlock : locks:list nat -\> threads:list nat -\> effect_violation \| ResourceLeak : resource:string -\> acquire_site:node_id -\> effect_violation \| UnhandledEffect : effect:effect_kind -\> site:node_id -\> effect_violation

(\* Detect violations in effect trace \*) val detect_violations : effect_trace -\> list effect_violation let detect_violations trace = let violations = ref \[\] in (\* Track memory state \*) let freed : set abstract_loc ref = ref Set.empty in List.iter (fun event -\> match event.kind with \| EFree loc -\> if Set.mem loc !freed then violations := DoubleFree loc event.location event.location :: !violations; freed := Set.add loc !freed \| ERead loc \| EWrite loc -\> if Set.mem loc !freed then violations := UseAfterFree loc 0 event.location :: !violations \| \_ -\> () ) trace; (\* Detect data races \*) let pairs = all_pairs trace in List.iter (fun (e1, e2) -\> if same_location e1 e2 && (is_write e1 \|\| is_write e2) && not (happens_before e1 e2) && not (happens_before e2 e1) then violations := DataRace (get_loc e1) e1 e2 :: !violations ) pairs; !violations

</div>

## Effect Handler Limitations

<div class="pillarbox">

Plotkin–Pretnar 2009 proves that effect handlers are **homomorphisms** from free algebraic models. This has important consequences:

1.  **Continuations are NOT algebraic**
    ``` math
    \mathsf{call/cc} : ((a \to b) \to a) \to a
    ```
    Problem: Captured continuation escapes the handler scope. The homomorphism property does NOT hold.

2.  **Parallel composition CANNOT be handled**
    ``` math
    \mathsf{parallel} : m\,a \to m\,b \to m\,(a \times b)
    ```
    Problem: Requires BINARY handler (handling two computations). Plotkin–Pretnar: “it seems that parallel cannot be \[implemented\]”

3.  **C FFI boundaries BLOCK effect propagation** (Sivaramakrishnan 2021)

    Effects CANNOT propagate across C stack frames!

    <div class="center">

    OCaml (handler) $`\to`$ C function $`\to`$ OCaml (perform effect)

    </div>

    The C frames don’t know about effect handlers. An effect performed in the inner OCaml code cannot be handled by the outer OCaml handler. This is a **fundamental limitation** of retrofitting effects onto existing languages with C FFI.

**Consequence**: Use different mechanisms for these effects:

- Continuations: Delimited control operators (shift/reset)

- Parallelism: Concurrency model with explicit threads

- Cross-FFI: Exception-style handling only (no continuation capture)

</div>

<div class="fstarcode">

(\* A handler is CORRECT iff it preserves the equations of the effect theory. This is the key condition from Plotkin-Pretnar 2009. \*)

type effect_equation = lhs : effect_term; rhs : effect_term;

type effect_theory = signature : effect_signature; equations : list effect_equation;

val handler_correct : \#e:effect_theory -\> \#m:Type -\> handler e.signature m -\> bool let handler_correct \#e \#m h = (\* Handler must preserve ALL equations \*) List.for_all (fun eq -\> interpret_with_handler h eq.lhs == interpret_with_handler h eq.rhs ) e.equations

</div>

<div class="fstarcode">

(\* WRONG: Trying to model call/cc as an effect This DOES NOT WORK because the continuation (cont b) can escape the handler scope, violating the homomorphism property. \*)

(\* CORRECT: Model call/cc with delimited control \*) type delimited_control = \| Shift : ((a -\> answer) -\> answer) -\> delimited_control \| Reset : (() -\> answer) -\> answer -\> delimited_control

(\* WRONG: Trying to handle parallel composition Can’t handle TWO computations at once! \*)

(\* CORRECT: Model parallelism with explicit concurrency \*) type concurrent_effect = \| Spawn : (() -\> unit) -\> thread_id -\> concurrent_effect \| Join : thread_id -\> unit -\> concurrent_effect \| Yield : unit -\> concurrent_effect

</div>

<div class="fstarcode">

(\* State: get/put - ALGEBRAIC \*) type state_eff (s:Type) = \| Get : s \| Put : s -\> unit

(\* Exceptions: raise - ALGEBRAIC (arity 0, no continuation) \*) type exception_eff (e:Type) = \| Raise : e -\> ’a

(\* Nondeterminism: fail/choose - ALGEBRAIC \*) type nondet_eff = \| Fail : ’a \| Choose : unit -\> bool

(\* Reader: ask - ALGEBRAIC \*) type reader_eff (r:Type) = \| Ask : r

(\* Writer: tell - ALGEBRAIC \*) type writer_eff (w:Type) = \| Tell : w -\> unit

(\* SUMMARY: Effect handlers work for sequential, single-continuation effects. For parallelism and first-class continuations, use other mechanisms. \*)

</div>

### Production Effect Handler Implementation

<div class="pillarbox">

</div>

<div class="pillarbox">

**Key Insight**: Separate OCaml frames into “fibers”—heap-allocated stack segments that can be captured and resumed independently.

<div class="center">

<table>
<tbody>
<tr>
<td style="text-align: center;"><strong>Traditional Stack</strong></td>
<td style="text-align: center;"><strong>Multicore OCaml</strong></td>
</tr>
<tr>
<td style="text-align: center;"><table>
<thead>
<tr>
<th style="text-align: center;">C frames</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OCaml frames</td>
</tr>
<tr>
<td style="text-align: center;">C frames</td>
</tr>
<tr>
<td style="text-align: center;">OCaml frames</td>
</tr>
</tbody>
</table></td>
<td style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Fiber Stack (one per handler)</td>
</tr>
<tr>
<td style="text-align: center;">OCaml frames</td>
</tr>
<tr>
<td style="text-align: center;">C stack (unchanged)</td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>

</div>

**Fiber Operations**:

1.  Handler installation = Allocate new fiber (8KB default)

2.  Effect perform = Capture fiber state as continuation

3.  Continue = Restore fiber and resume execution

4.  Discontinue = Restore and raise exception in captured context

**Stack Overflow Checking (Red Zone Technique)**: Every function checks: $`\mathit{sp} < \mathit{fiber\_end} + \mathit{red\_zone\_size}`$ (16 words). If true: grow fiber stack ($`2\times`$) or signal overflow. Trade-off: 19% text section size increase for 1% runtime overhead.

**Continuation Linearity (Runtime Enforced)**: Continuations are ONE-SHOT by default. Resuming a captured continuation twice = runtime error. Trade-off: Runtime check vs compile-time linear types.

**Performance Characteristics**:

- Mean overhead (no effects): 1%

- Median overhead: $`<`$<!-- -->5% for 32/54 benchmarks

- Worst case overhead: $`\sim`$<!-- -->15% (allocation-heavy code)

- Text section size increase: 19% (with 16-word red zone)

- Exception handling: No measurable change

- External C calls: No measurable change

</div>

<div class="fstarcode">

module BrrrMachine.Effects.FiberImplementation

(\* Fiber: heap-allocated stack segment for effect handlers \*) type fiber_id = nat

type fiber = id : fiber_id; frames : list stack_frame; stack_size : nat; stack_capacity : nat; (\* Default 1024 words = 8KB \*) handler : option handler_closure; parent : option fiber_id;

type handler_clause = \| ReturnClause : (value -\> computation) -\> handler_clause \| EffectClause : effect_label:string -\> (value -\> continuation -\> computation) -\> handler_clause \| ExceptionClause : exn_label:string -\> (value -\> computation) -\> handler_clause

type handler_closure = clauses : list handler_clause;

(\* Continuation: captured fiber state (linear by default) \*) type continuation_state = \| Fresh \| Resumed \| Finalized

type continuation = captured_fiber : fiber; captured_handlers : list handler_closure; state : ref continuation_state; (\* Runtime linearity tracking \*)

(\* CRITICAL: C FFI boundary detection \*) type stack_segment = \| OCamlSegment : fiber_id -\> stack_segment \| CSegment : list c_frame -\> stack_segment

type mixed_stack = list stack_segment

(\* Effects CANNOT cross C segments \*) val effect_can_reach_handler : effect_label:string -\> mixed_stack -\> bool let rec effect_can_reach_handler label stack = match stack with \| \[\] -\> false \| OCamlSegment fid :: rest -\> if fiber_handles_effect fid label then true else effect_can_reach_handler label rest \| CSegment \_ :: \_ -\> (\* C frames BLOCK effect propagation! \*) false

(\* Continue with linearity check \*) val continue_checked : continuation -\> value -\> result computation string let continue_checked k v = match !(k.state) with \| Fresh -\> k.state := Resumed; Ok (resume_fiber k.captured_fiber v) \| Resumed -\> Error "Continuation already resumed (linearity violation)" \| Finalized -\> Error "Continuation was finalized by GC"

</div>

## C11/C++ Memory Model

<div class="pillarbox">

**Primary Source**: **\[Batty11\]** — “Mathematizing C++ Concurrency”  
**Foundational**: **\[Boehm08\]** — “Foundations of the C++ Concurrency Memory Model”

**Relationship**: Boehm 2008 defines the C++ memory model informally and proves the DRF-SC theorem. Batty 2011 provides the **rigorous mathematical formalization** we use here. Both papers are essential:

- Boehm 2008: Conceptual foundation, race definitions, SC-DRF theorem

- Batty 2011: Formal execution model, coherence axioms, proof framework

**Key Insight**: Concurrent program semantics require **execution-based** models, not just sequential effect traces. The C11 memory model defines:

1.  Memory orderings (relaxed to sequentially consistent)

2.  Execution witnesses (reads-from, modification order)

3.  Happens-before relation with release/acquire synchronization

4.  Coherence axioms ensuring per-location consistency

**Race Definitions** (Boehm 2008 terminology):

- **Type 1 Race**: Conflicting actions adjacent in a total interleaving order (used in operational/interleaving semantics)

- **Type 2 Race**: Conflicting data accesses unordered by happens-before (used in axiomatic/execution-based semantics)

**Theorem 8.1–8.2** (Boehm): Type 1 and Type 2 races are **equivalent** for programs without undefined behavior from other sources.

**Integration with Effect System**:

- $`\mathsf{Atomic}`$ effect kind is REPLACED by memory-order-parameterized atomics

- Effect traces are EXTENDED to execution witnesses

- Happens-before is EXTENDED with consume/dependency

</div>

<div class="pillarbox">

The C11 axiomatic model (Batty 2011) permits “out-of-thin-air” values in programs using relaxed atomics. This is a **soundness bug** that violates:

- Basic invariant reasoning (cannot prove $`x=y=0`$ is maintained)

- DRF-SC guarantees (race-free programs may behave non-SC)

- Type safety (Java-like languages cannot guarantee type-correct values)

**Litmus Test (LBd—Thin Air)**:

<div class="center">

Initial: $`x = y = 0`$ (relaxed atomics)  
Thread 1: $`a := x;\; y := a;`$  
Thread 2: $`x := y;`$  
Outcome $`a=1`$: **IMPOSSIBLE** (no source) but **ALLOWED** by C11 axiomatic model!

</div>

**Safe Subset**: Use ONLY release/acquire synchronization (no relaxed atomics) to avoid thin-air issues.

**Recommended**: For sound relaxed atomic reasoning, use Promising Semantics 2.0 (**\[Lee20\]**) formalized in Section <a href="#sec:promising-semantics" data-reference-type="ref" data-reference="sec:promising-semantics">3.7</a>.

**References**:

- **\[Kang17\]**: “A Promising Semantics for Relaxed-Memory Concurrency”

- **\[Lee20\]**: “Promising 2.0” (enables global opts, fixes ARMv8)

- **\[Podkopaev19\]**: IMM intermediate model

- **\[Vafeiadis15\]**: “Common Compiler Optimisations are Invalid in C11”

- **\[Batty11\]**: “Mathematizing C++ Concurrency”

</div>

### Memory Orderings and Actions

<div class="fstarcode">

module BrrrMachine.Effects.C11MemoryModel

(\* Memory order enumeration Partial ordering: seq_cst \> acq_rel \> acquire/release \> consume \> relaxed \*) type memory_order = \| MO_relaxed (\* No ordering constraints - only atomicity guaranteed \*) \| MO_consume (\* Data dependency ordering - weaker than acquire \*) \| MO_acquire (\* Acquire semantics - synchronizes with release \*) \| MO_release (\* Release semantics - synchronizes with acquire \*) \| MO_acq_rel (\* Both acquire and release - for read-modify-write \*) \| MO_seq_cst (\* Sequentially consistent - total order of all SC ops \*)

(\* Memory ordering strength comparison \*) val mo_stronger : memory_order -\> memory_order -\> bool let mo_stronger mo1 mo2 = match mo1, mo2 with \| MO_seq_cst, \_ -\> true \| \_, MO_seq_cst -\> false \| MO_acq_rel, (MO_acquire \| MO_release \| MO_consume \| MO_relaxed) -\> true \| (MO_acquire \| MO_release \| MO_consume \| MO_relaxed), MO_acq_rel -\> false \| MO_acquire, (MO_consume \| MO_relaxed) -\> true \| MO_release, (MO_consume \| MO_relaxed) -\> true \| (MO_consume \| MO_relaxed), MO_acquire -\> false \| (MO_consume \| MO_relaxed), MO_release -\> false \| MO_consume, MO_relaxed -\> true \| MO_relaxed, MO_consume -\> false \| MO_acquire, MO_release -\> false (\* Incomparable \*) \| MO_release, MO_acquire -\> false (\* Incomparable \*) \| mo1, mo2 -\> mo1 = mo2

(\* Check if ordering provides acquire/release/consume semantics \*) val is_acquire : memory_order -\> bool let is_acquire mo = match mo with \| MO_acquire \| MO_acq_rel \| MO_seq_cst -\> true \| \_ -\> false

val is_release : memory_order -\> bool let is_release mo = match mo with \| MO_release \| MO_acq_rel \| MO_seq_cst -\> true \| \_ -\> false

val is_consume : memory_order -\> bool let is_consume mo = match mo with \| MO_consume \| MO_acquire \| MO_acq_rel \| MO_seq_cst -\> true \| \_ -\> false

</div>

<div class="fstarcode">

type action_id = nat type thread_id = nat type location = abstract_loc type value = int type mutex_id = nat

type memory_action = (\* Non-atomic memory operations - subject to data race UB \*) \| Rna : loc:location -\> val_read:value -\> memory_action \| Wna : loc:location -\> val_written:value -\> memory_action

(\* Atomic memory operations - race-free by construction \*) \| R : loc:location -\> mo:memory_order -\> val_read:value -\> memory_action \| W : loc:location -\> mo:memory_order -\> val_written:value -\> memory_action \| RMW : loc:location -\> mo:memory_order -\> val_read:value -\> val_written:value -\> memory_action

(\* Fence operations \*) \| Fence : mo:memory_order -\> memory_action

(\* Lock operations - implicit acquire/release semantics \*) \| Lock : mutex:mutex_id -\> memory_action \| Unlock : mutex:mutex_id -\> memory_action

(\* Helper functions \*) val action_location : memory_action -\> option location val action_memory_order : memory_action -\> option memory_order val is_write : memory_action -\> bool val is_read : memory_action -\> bool val is_atomic_action : memory_action -\> bool val same_location : memory_action -\> memory_action -\> bool

</div>

### Execution Witnesses and Candidate Executions

<div class="fstarcode">

type relation (a : Type) = a -\> a -\> bool

(\* The witness contains relations NOT determined by program text \*) type execution_witness =

(\* Reads-from: maps each read action to the write it reads from \*) reads_from : map action_id action_id;

(\* Modification order: per-location total order of all writes \*) modification_order : map location (list action_id);

(\* SC order: total order of all sequentially consistent operations \*) sc_order : list action_id;

(\* Candidate execution combines actions, relations, and witness \*) type candidate_execution = actions : set memory_action; action_ids : map memory_action action_id; id_to_action : map action_id memory_action; thread_of : map action_id thread_id; sequenced_before : relation action_id; additional_synchronized_with : relation action_id; data_dependency : relation action_id; witness : execution_witness;

</div>

<div class="fstarcode">

val synchronizes_with : candidate_execution -\> action_id -\> action_id -\> bool let synchronizes_with exec a b = match get_action exec a, get_action exec b with \| Some act_a, Some act_b -\> (\* Case 1: Release write synchronizes with acquire read via release sequence \*) (is_write act_a && is_release (Option.get (action_memory_order act_a)) && is_read act_b && is_acquire (Option.get (action_memory_order act_b)) && reads_from_release_sequence exec b a) \|\|

(\* Case 2: Release fence synchronizes with acquire fence \*) (match act_a, act_b with \| Fence mo_a, Fence mo_b -\> is_release mo_a && is_acquire mo_b && fence_synchronizes exec a b \| \_, \_ -\> false) \|\|

(\* Case 3: Unlock synchronizes with subsequent lock on same mutex \*) (match act_a, act_b with \| Unlock m1, Lock m2 -\> m1 = m2 && unlock_before_lock exec a b \| \_, \_ -\> false) \|\|

(\* Case 4: Additional synchronization from thread operations \*) exec.additional_synchronized_with a b \| \_, \_ -\> false

</div>

<div class="fstarcode">

(\* This EXTENDS the simple happens_before with full C11 semantics. Happens-before is the transitive closure of: 1. Sequenced-before (program order within thread) 2. Synchronizes-with (cross-thread via release/acquire) 3. Dependency-ordered-before (consume semantics) \*)

val happens_before_c11 : candidate_execution -\> action_id -\> action_id -\> bool let rec happens_before_c11 exec a b = exec.sequenced_before a b \|\| synchronizes_with exec a b \|\| dependency_ordered_before exec a b \|\| Set.exists (fun c -\> happens_before_c11 exec a c && happens_before_c11 exec c b ) (all_action_ids exec)

</div>

### Release Sequences

<div class="definition">

**Definition 3.1** (Release Sequence (Batty 2011 Section 3.2)). A release sequence headed by a release write $`A`$ consists of:

- $`A`$ itself

- Subsequent writes to the same location by the same thread

- RMW operations that read from the sequence

This enables lock-free algorithms to work correctly by allowing synchronization to “chain” through multiple atomic operations.

</div>

<div class="fstarcode">

val in_release_sequence_batty : candidate_execution -\> action_id -\> action_id -\> bool let rec in_release_sequence_batty exec head action = match get_action exec head with \| None -\> false \| Some head_act -\> if not (is_release_write head_act) then false else if not (same_location_id exec head action) then false else (\* Case 1: action IS the head \*) action = head \|\| (\* Case 2: Same thread, sequenced after, and is a write \*) (same_thread exec head action && exec.sequenced_before head action && (match get_action exec action with \| Some act -\> is_write act \| None -\> false)) \|\| (\* Case 3: RMW that reads from something in the sequence \*) (match get_action exec action with \| Some act when is_rmw act -\> (match Map.find action exec.witness.reads_from with \| Some prev_write -\> in_release_sequence_batty exec head prev_write \| None -\> false) \| \_ -\> false)

</div>

<div class="theorem">

**Theorem 3.2** (Release Sequence RMW Extension). *If $`A`$ is in the release sequence of $`H`$, and $`B`$ reads from $`A`$, and $`B`$ is an RMW, then $`B`$ is also in the release sequence of $`H`$.*

</div>

### Coherence Axioms

<div class="pillarbox">

These four axioms ensure per-location consistency between the happens-before relation and the modification order. They are the key soundness requirements for the C11 memory model.

**Intuition**: Even though different threads may observe writes in different orders (relaxed atomics), the four coherence axioms guarantee that:

- Causally related observations are consistent

- No thread observes “time travel” (reading older values after newer)

</div>

<div class="definition">

**Definition 3.3** (CoRR: Coherent Read-Read). If two reads are ordered by happens-before and both read from writes to the same location, the later read cannot see an earlier write (in modification order) than the earlier read saw.

*Intuition*: If I see value $`V`$, then anyone who observes my read (via happens-before) cannot see a value from before $`V`$.

</div>

<div class="definition">

**Definition 3.4** (CoWR: Coherent Write-Read). A read cannot observe a write that happens-after the read.

*Intuition*: You cannot read from the future.

</div>

<div class="definition">

**Definition 3.5** (CoWW: Coherent Write-Write). If two writes are ordered by happens-before, they must be in the same order in the modification order.

*Intuition*: Causality is respected in the write history.

</div>

<div class="definition">

**Definition 3.6** (CoRW: Coherent Read-Write (Acyclicity)). The combined relation $`(\mathit{hb} \cup \mathit{rf} \cup \mathit{mo} \cup \mathit{rb})`$ must be acyclic, where $`\mathit{rb}`$ (reads-before) $`= \mathit{mo} \mathbin{;} \mathit{rf}^{-1}`$.

This prevents causal cycles that would make the execution internally inconsistent.

</div>

<div class="fstarcode">

val coherent_read_read_batty : candidate_execution -\> bool val coherent_write_read_batty : candidate_execution -\> bool val coherent_write_write_batty : candidate_execution -\> bool val coherent_read_write_batty : candidate_execution -\> bool

val is_coherent_batty : candidate_execution -\> bool let is_coherent_batty exec = coherent_read_read_batty exec && coherent_write_read_batty exec && coherent_write_write_batty exec && coherent_read_write_batty exec

</div>

<div class="theorem">

**Theorem 3.7** (Coherence Implies Per-Location SC). *Under coherence, each individual location behaves as if accessed sequentially consistently, even if the overall execution is not SC.*

</div>

### Data Race Detection

<div class="definition">

**Definition 3.8** (Data Race (Batty 2011 Section 2.3)). A **data race** is the fundamental source of undefined behavior in C11. Two memory operations **race** if they:

1.  Access the same location

2.  At least one is a write

3.  At least one is non-atomic

4.  They are not ordered by happens-before

**Key Insight**: If a program has NO data races under SC semantics, then ALL its behaviors are sequentially consistent (DRF-SC guarantee).

</div>

<div class="definition">

**Definition 3.9** (Conflict Relation). Two actions **conflict** if they access the same location and at least one is a write. This is a necessary but not sufficient condition for a data race.

</div>

<div class="fstarcode">

val is_data_race_batty : candidate_execution -\> action_id -\> action_id -\> bool let is_data_race_batty exec a b = match get_action exec a, get_action exec b with \| Some act_a, Some act_b -\> conflicts act_a act_b && not (both_atomic act_a act_b) && not (happens_before_c11 exec a b) && not (happens_before_c11 exec b a) \| \_, \_ -\> false

val no_data_races_batty : candidate_execution -\> bool let no_data_races_batty exec = Set.for_all (fun a -\> Set.for_all (fun b -\> not (is_data_race_batty exec a b) ) (all_action_ids exec) ) (all_action_ids exec)

</div>

<div class="definition">

**Definition 3.10** (Unsequenced Race (C-specific)). In addition to data races, C has “unsequenced races”—conflicting accesses within a **single expression** that are unsequenced. Example: `i = i++` has unsequenced race on `i`.

</div>

<div class="definition">

**Definition 3.11** (Undefined Behavior Detection). A program has undefined behavior if ANY consistent execution has:

- A data race, OR

- An unsequenced race, OR

- An indeterminate read

</div>

### Sequential Consistency and SC-DRF

<div class="pillarbox">

The DRF-SC (Data Race Freedom implies Sequential Consistency) theorem is the **fundamental guarantee** of the C11 memory model.

**Informal Statement**: If your program has no data races when run with SC semantics, then all its behaviors under C11 will also be SC.

This allows programmers to reason about correctly synchronized code using the simple SC model, while the compiler/hardware can still optimize based on the relaxed C11 model.

</div>

<div class="definition">

**Definition 3.12** (Sequentially Consistent Execution). An execution is SC if there exists a total order of all memory accesses that:

1.  Respects program order (sequenced-before)

2.  Each read sees the most recent write in the total order

</div>

<div class="definition">

**Definition 3.13** (DRF Program). A program is DRF (Data Race Free) if, when executed under SC semantics, no execution has a data race.

</div>

<div class="theorem">

**Theorem 3.14** (DRF-SC Theorem (Main Result)). ***The fundamental guarantee of the C11 memory model.***

*If a program has no data races under SC semantics, then:*

1.  *All its C11-consistent executions are also SC, OR*

2.  *All its C11-consistent executions have the same observable behavior as some SC execution.*

</div>

<div class="fstarcode">

val drf_sc_theorem : is_program_execution:(candidate_execution -\> bool) -\> Lemma (requires is_drf_program is_program_execution) (ensures forall exec. is_program_execution exec && is_consistent exec ==\> exists sc_exec. is_sc_execution sc_exec && same_observable_behavior exec sc_exec)

</div>

<div class="corollary">

**Corollary 3.15** (Safe Concurrent Programming Model). *For DRF programs, programmers can reason using simple SC semantics, even though the implementation uses relaxed memory.*

</div>

### Promising Semantics 2.0 for Sound Relaxed Atomics

<div class="pillarbox">

**Sources**:

- **\[Kang17\]**: “A Promising Semantics for Relaxed-Memory Concurrency”

- **\[Lee20\]**: “Promising 2.0: Global Optimizations in Relaxed Memory”

Coq Development: <http://sf.snu.ac.kr/promise-concurrency> ($`\sim`$<!-- -->30K lines)

**Why This Is Needed**: The C11 axiomatic model has a fundamental flaw: it permits “out-of-thin-air” values in programs using relaxed atomics. The Promising Semantics provides an **operational model** that:

1.  **Prevents** thin-air values via thread-local certification

2.  **Validates** all standard compiler optimizations

3.  **Proves** correct compilation to x86-TSO, ARMv8, and POWER

4.  **Preserves** DRF-SC and other key theorems

**PS 2.0 Improvements** (Lee et al. PLDI 2020):

- Enables global value-range analysis soundness

- Enables register promotion for single-threaded locations

- Fixes ARMv8 compilation (no extra fence for relaxed RMWs)

- Replaces universal quantification with **capped memory** construction

**Key Insight**: Use **operational semantics** (step-by-step execution) with a **promise mechanism**, rather than **axiomatic** (check complete graphs). Promises commit to future writes but must be **certified**—the thread must be able to fulfill the promise against **capped memory** (PS 2.0), not all futures.

</div>

<div class="fstarcode">

module BrrrMachine.PromisingSemanticsTyped

(\* Timestamps for totally ordering writes per location \*) type timestamp = nat

(\* Views track which messages a thread has observed \*) type timemap = location -\> timestamp type view = v_pln : timemap; v_rlx : timemap;

(\* Messages represent writes to memory with timestamp intervals \*) type message = msg_loc : location; msg_val : value; msg_from : timestamp; (\* Exclusive lower bound \*) msg_to : timestamp; (\* Inclusive upper bound = "real" timestamp \*) msg_view : view; (\* Release view attached to message \*)

(\* Memory is a set of pairwise disjoint messages \*) type memory = set message

</div>

<div class="fstarcode">

(\* The key innovation of PS 2.0: replace universal quantification over ALL future memories with quantification over CAPPED memories. Capped memory represents realistic interference patterns without over-approximation.

Construction: 1. Add RESERVATIONS in gaps between consecutive messages (blocks promises) 2. Add CAP MESSAGES at end of each location’s chain (unless promised)

This enables GLOBAL optimizations (value-range, register promotion) that were unsound under original PS. \*)

val capped_memory : memory -\> set message -\> memory let capped_memory mem promises = let promised_ends = Set.filter (fun m -\> match max_timestamp_message mem m.msg_loc with \| Some m_max -\> m.msg_from = m_max.msg_to \| None -\> false) promises in let cap_locs = Set.diff (all_locations mem) (Set.map (fun m -\> m.msg_loc) promised_ends) in Set.union mem (Set.map (cap_message mem) cap_locs)

</div>

<div class="pillarbox">

**The Key Innovation**: A promise is only allowed if thread-local certification succeeds against **capped memory** (not all futures).

**Original PS** (Kang 2017): Certify against ALL future memories

- Too conservative: blocks valid global optimizations

- Problem: future could contain impossible values

**PS 2.0** (Lee 2020): Certify against CAPPED memory only

- Capped memory bounds realistic interference

- Enables value-range analysis and register promotion

- Fixes ARMv8 compilation issue

**Definition**: Thread configuration is **consistent** iff thread can fulfill ALL its promises against capped memory.

**Why Certification Prevents Thin-Air** (LBd Example):

1.  T1 attempts to promise $`y := 1`$

2.  Certification: T1 must execute alone to fulfill promise

3.  T1 reads $`x`$: only value available is 0 (initial)

4.  T1 computes $`y := a`$ where $`a = 0`$

5.  T1 would write $`y := 0`$, not $`y := 1`$

6.  Certification **fails**—promise rejected

7.  Result: $`a = 1`$ is **forbidden** (correct!)

</div>

<div class="theorem">

**Theorem 3.16** (No Thin-Air Values). *Under Promising Semantics, if value $`v \neq 0`$ is readable at location $`\mathit{loc}`$, then some thread wrote it with justified certification.*

</div>

<div class="theorem">

**Theorem 3.17** (DRF-SC for Promise-Free Machine). *If all reachable states under promise-free execution are race-free, then full machine equals promise-free machine (behaviors identical).*

</div>

<div class="theorem">

**Theorem 3.18** (Release-Acquire Programs Are Safe). *Programs using only release/acquire synchronization have no thin-air values.*

</div>

<div class="theorem">

**Theorem 3.19** (Well-Locked Programs Are SC). *If a program is well-locked and SC-reachable states have no races on normal locations, then full machine equals SC machine.*

</div>

<div class="theorem">

**Theorem 3.20** (Compiler Optimizations Sound). *All standard optimizations that preserve sequential traces are sound under Promising Semantics.*

</div>

<div class="theorem">

**Theorem 3.21** (Compilation Correctness). *Promising behaviors are subset of hardware behaviors for x86-TSO, POWER, and ARMv8 (PS 2.0 fixes ARMv8—no extra fence for relaxed RMWs).*

</div>

<div class="theorem">

**Theorem 3.22** (Value-Range Analysis Soundness (PS 2.0)). *Sound value-range analysis ensures all thread states and messages satisfy the analysis in reachable states.*

</div>

<div class="theorem">

**Theorem 3.23** (Register Promotion Soundness (PS 2.0)). *For single-threaded locations, register promotion preserves Promising behaviors.*

</div>

### Cross-Reference Summary for Memory Models

| **Model/Section** | **Description** |
|:---|:---|
| Section 6.5.1–6.5.6 (C11 Axiomatic) | Memory orderings, execution witnesses, coherence axioms. **Limitation**: Allows thin-air values with relaxed atomics. **Safe for**: release/acquire only programs. |
| Section 6.5.7 (Promising 2.0) | Operational model with promise mechanism. Thread-local certification prevents thin-air. PS 2.0: Capped memory enables global optimizations. **Safe for**: all programs including relaxed atomics. |
| Section 12.26 (C11 Theorems) | DRF-SC theorem (valid), Release-acquire soundness (valid), Thin-air warning (critical). |
| **When to Use Which Model (Updated for PS 2.0):** |  |
| Release/acquire only | C11 axiomatic is sufficient |
| Relaxed atomics present | Use PS 2.0 (not original PS) |
| Global optimizations needed | MUST use PS 2.0 |
| Register promotion analysis | MUST use PS 2.0 |
| ARMv8 compilation verification | MUST use PS 2.0 |

### IMM: Intermediate Memory Model for Compilation Proofs

<div class="pillarbox">

**Source**: Podkopaev, Lahav, Vafeiadis. POPL 2019. “Bridging the Gap between Programming Languages and Hardware Weak Memory Models”

Coq Development: <http://plv.mpi-sws.org/imm/> ($`\sim`$<!-- -->33K lines)

**Purpose**: IMM serves as a **common target** for source language memory models and a **common source** for hardware models, reducing proof complexity:

<div class="center">

</div>

Proof Burden WITHOUT IMM: $`O(n \times m)`$—each source to each hardware

Proof Burden WITH IMM: $`O(n + m)`$—source$`\to`$IMM + IMM$`\to`$hardware

**Key Result**: First **mechanized** compilation correctness proofs for models weaker than x86-TSO. Corrects error in Kang 2017 POWER proof.

</div>

<div class="fstarcode">

module BrrrMachine.IMM

type event_label = \| LRead : loc:location -\> val:value -\> mode:read_mode -\> event_label \| LWrite : loc:location -\> val:value -\> mode:write_mode -\> event_label \| LFence : mode:fence_mode -\> event_label \| LInit : loc:location -\> event_label

type execution_graph = events : set event_id; lab : event_id -\> event_label; po : relation event_id; (\* Program order \*) rf : relation event_id; (\* Reads-from \*) mo : relation event_id; (\* Modification order \*) data : relation event_id; (\* Data dependency \*) addr : relation event_id; (\* Address dependency \*) ctrl : relation event_id; (\* Control dependency \*) rmw : relation event_id; (\* RMW pairing \*)

(\* The main acyclicity relation \*) val ar : execution_graph -\> relation event_id let ar g = union_all \[rfe g; bob g; ppo g; detour g; fre g; coe g\]

val imm_consistent : execution_graph -\> bool let imm_consistent g = wf_execution g && complete_execution g && coherent g && atomic_rmw g && acyclic (ar g) && acyclic (union (ar g) g.rf)

</div>

<div class="pillarbox">

ARMv8/RISC-V exclusive ops are **weaker** than POWER/ARMv7!

Solution: IMM distinguishes “strong” and “normal” RMW compilation.

<div class="center">

| **Hardware** | **RMW Strength** | **Notes**                          |
|:-------------|:-----------------|:-----------------------------------|
| x86-TSO      | Strong           | LOCK prefix is strong              |
| POWER        | Strong           | LWARX/STWCX is strong              |
| ARMv7        | Strong           | LDREX/STREX is strong              |
| ARMv8        | **Normal**       | LDXR/STXR is WEAKER—needs barrier! |
| RISC-V       | **Normal**       | LR/SC is WEAKER—needs barrier!     |

</div>

**Warning**: Promising RMWs must compile to “strong” IMM RMWs with extra barrier on ARMv8/RISC-V.

</div>

| **Source**   | **x86-TSO**  | **ARMv8**     | **POWER**        | **RISC-V**    |
|:-------------|:-------------|:--------------|:-----------------|:--------------|
| rlx read     | MOV          | LDR           | LD               | LW            |
| rlx write    | MOV          | STR           | ST               | SW            |
| acq read     | MOV          | LDAR          | LD;CMPW;BC;ISYNC | LW;FENCE R,RW |
| rel write    | MOV          | STLR          | LWSYNC;ST        | FENCE RW,W;SW |
| sc read      | MOV          | LDAR;DMB ISH  | SYNC;LD          | ...           |
| RMW (strong) | LOCK CMPXCHG | LDXR;STXR;DMB | LWARX;STWCX      | LR;SC;FENCE   |
| RMW (normal) | LOCK CMPXCHG | LDXR;STXR     | LWARX;STWCX      | LR;SC         |

<div class="theorem">

**Theorem 3.24** (IMM Compilation Correctness).

1.  *IMM $`\to`$ Hardware: For all hardware targets, IMM-consistent executions compile to hardware-consistent executions.*

2.  *Promising $`\to`$ IMM: Promising behaviors have corresponding IMM-consistent graphs.*

3.  *C11 $`\to`$ IMM: C11-consistent executions are IMM-consistent.*

4.  *End-to-End: Promising $`\to`$ Hardware via IMM.*

</div>

## Coeffect Systems: Context Requirements

<div class="pillarbox">

</div>

<div class="pillarbox">

<div class="center">

| **Effects (Section 6.1)** | **Coeffects (This Section)** |
|:---|:---|
| What computation **produces** | What computation **requires** |
| Monadic structure ($`T^\varepsilon\,A`$) | Comonadic structure ($`D^r\,A`$) |
| Effect rows: $`\langle E \mid \varepsilon \rangle`$ | Coeffect annotations: $`\Gamma @ r`$ |
| Examples: $`\mathsf{Write}`$, $`\mathsf{Throw}[]`$, $`\mathsf{IO}`$ | Examples: liveness, usage, capabilities |
| $`\mathsf{return} : A \to M^\varepsilon\,A`$ | $`\mathsf{extract} : D^r\,A \to A`$ |

</div>

**Typing with Both**:
``` math
\Gamma @ r \vdash e : \tau \;\&\; \varepsilon
```
“Under context $`\Gamma`$ with requirements $`r`$, $`e`$ has type $`\tau`$ and effects $`\varepsilon`$”

**Why This Matters for Brrr-Machine**:

1.  Effects tell us what code **does** (mutations, I/O, exceptions)

2.  Coeffects tell us what code **needs** (liveness, resources, capabilities)

3.  Complete analysis requires **both** for full specification

</div>

### Flat vs Structural Coeffects

<div class="definition">

**Definition 4.1** (Flat Coeffects—Whole-Context Properties). **Shape**: Single coeffect value for entire context.

**Example**: $`\Gamma @ \{\mathsf{network}, \mathsf{gps}\} \vdash e : \tau`$

**Meaning**: Expression $`e`$ requires network AND GPS capabilities.

**Use cases**:

- Platform/capability requirements

- Resource access requirements (files, database)

- Runtime feature detection

- Cross-platform compatibility checking

</div>

<div class="definition">

**Definition 4.2** (Structural Coeffects—Per-Variable Properties). **Shape**: Coeffect annotation per variable in context.

**Example**: $`(x:\mathsf{Int} @ 2, y:\mathsf{Bool} @ 1) \vdash e : \tau`$

**Meaning**: Expression $`e`$ uses $`x`$ twice and $`y`$ once (bounded linear logic).

**Use cases**:

- Liveness analysis (is variable accessed?)

- Usage counting (how many times is variable used?)

- Dataflow dependencies (which past values needed?)

- Linear resource management

</div>

<div class="definition">

**Definition 4.3** (Coeffect Algebraic Structure). Both shapes use:
``` math
\begin{aligned}
+ \text{ (addition)} &: \text{Sequencing---using both contexts} \\
\times \text{ (multiplication)} &: \text{Nesting---context for inner computation} \\
\sqcap \text{ (meet)} &: \text{Branching---required by at least one branch} \\
1 \text{ (unit)} &: \text{Empty requirement} \\
0 \text{ (zero)} &: \text{Unused (for structural coeffects)}
\end{aligned}
```

</div>

### Liveness as a Structural Coeffect

<div class="pillarbox">

Liveness is naturally a **coeffect**: it describes which variables a computation **requires** from its context.

</div>

<div class="definition">

**Definition 4.4** (Liveness Coeffect Algebra (Semiring Structure)). $`L = \{ \mathsf{LDead}, \mathsf{LLive} \}`$

``` math
\begin{aligned}
\mathsf{LDead} + \mathsf{LDead} &= \mathsf{LDead} && \text{(neither uses $\to$ not used)} \\
\mathsf{LDead} + \mathsf{LLive} &= \mathsf{LLive} && \text{(one uses $\to$ used)} \\
\mathsf{LLive} + \mathsf{LLive} &= \mathsf{LLive} && \text{(both use $\to$ used)} \\[1ex]
\mathsf{LDead} \times \_ &= \mathsf{LDead} && \text{(not used in outer $\to$ dead)} \\
\mathsf{LLive} \times l &= l && \text{(used in outer $\to$ propagate inner)} \\[1ex]
\mathsf{LDead} \sqcap \mathsf{LDead} &= \mathsf{LDead} && \text{(dead in both branches $\to$ dead)} \\
\_ \sqcap \_ &= \mathsf{LLive} && \text{(live in either branch $\to$ live)}
\end{aligned}
```

</div>

<div class="pillarbox">

Liveness is a GEN/KILL problem, hence IFDS-applicable. Coeffect formulation provides:

1.  Compositionality via semiring structure

2.  Typing guarantee: well-typed = correct liveness

3.  Connection to linear logic via usage bounds

</div>

<div class="definition">

**Definition 4.5** (Liveness Inference Rules).
``` math
\begin{aligned}
\text{VAR:} \quad & x \text{ is live at } (x:\tau @ \mathsf{LLive}) \\
\text{APP:} \quad & f\,e \text{ has liveness } (\mathsf{liveness}(f) + \mathsf{liveness}(e)) \\
\text{LAMBDA:} \quad & \lambda x.\,e \text{ has liveness } (\mathsf{liveness}(e) \setminus x) \\
\text{IF:} \quad & \mathbf{if}\;c\;\mathbf{then}\;t\;\mathbf{else}\;e \text{ has liveness } (c + (t \sqcap e))
\end{aligned}
```

</div>

### Usage Coeffects and Linear Types

<div class="definition">

**Definition 4.6** (Bounded Linear Types via Coeffects (Petricek 2014, Section 2.1)). Track how many times each variable is used:
``` math
\begin{aligned}
\mathsf{UZero} &: \text{Never used} \\
\mathsf{UOne} &: \text{Used exactly once} \\
\mathsf{UBounded}\,n &: \text{Used at most } n \text{ times} \\
\mathsf{UMany} &: \text{Used arbitrarily many times}
\end{aligned}
```

</div>

<div class="definition">

**Definition 4.7** (Usage Coefficient Algebra).
``` math
\begin{aligned}
\mathsf{UZero} + u &= u \\
\mathsf{UOne} + \mathsf{UOne} &= \mathsf{UBounded}\,2 \\
\mathsf{UBounded}\,n + \mathsf{UBounded}\,m &= \mathsf{UBounded}\,(n + m) \\
\mathsf{UMany} + \_ &= \mathsf{UMany} \\[1ex]
\mathsf{UZero} \times \_ &= \mathsf{UZero} \\
\mathsf{UOne} \times u &= u \\
\mathsf{UBounded}\,n \times \mathsf{UBounded}\,m &= \mathsf{UBounded}\,(n \times m)
\end{aligned}
```

</div>

<div class="pillarbox">

Rust-style ownership maps to usage coeffects:

<div class="center">

|                  |                                       |
|:-----------------|:--------------------------------------|
| Move semantics   | usage $`\leq 1`$ (affine)             |
| Borrow semantics | usage $`\geq 0`$ (unrestricted reads) |
| Mut borrow       | usage $`= 1`$ (linear)                |

</div>

The semiring structure gives composition:

- $`x`$ moved $`+ x`$ moved $`=`$ error (usage $`> 1`$ violates affine)

- $`x`$ borrowed $`+ x`$ borrowed $`=`$ ok (unrestricted composes)

</div>

### Platform Capability Coeffects

<div class="definition">

**Definition 4.8** (Flat Coeffects for Cross-Platform Analysis). Track which platform capabilities code requires:
``` math
\begin{aligned}
\mathsf{CapNetwork} &: \text{Requires network access} \\
\mathsf{CapFileSystem} &: \text{Requires filesystem access} \\
\mathsf{CapGPS} &: \text{Requires location services} \\
\mathsf{CapDatabase} &: \text{Requires database connection} \\
\mathsf{CapFFI}(\mathit{lang}) &: \text{Requires FFI to specific language}
\end{aligned}
```

</div>

<div class="definition">

**Definition 4.9** (Capability Algebra). Capability algebra is simple set union:
``` math
\begin{aligned}
c_{\mathsf{zero}} &= \{\} && \text{(no requirements)} \\
c_{\mathsf{one}} &= \{\} && \text{(unit doesn't add requirements)} \\
c_{\mathsf{plus}} &= \mathsf{Set.union} && \text{(sequence needs both)} \\
c_{\mathsf{times}} &= \mathsf{Set.union} && \text{(nesting preserves requirements)} \\
c_{\mathsf{meet}} &= \mathsf{Set.union} && \text{(either branch might run)}
\end{aligned}
```

</div>

<div class="pillarbox">

1.  Cross-platform compatibility checking

2.  Graceful degradation (fallback when capability missing)

3.  Permission documentation generation

4.  FFI boundary analysis (Section 6.4: C FFI limitations)

</div>

### F\* Specification: Coeffect System

The following F\* specification formalizes the coeffect system. The key insight from Petricek et al. is that coeffects form an algebraic structure (a semiring) that enables compositional reasoning about context requirements. Each coeffect instance (liveness, usage, capabilities) instantiates this algebra with domain-specific operations:

- **coeffect_algebra**: The type class defining the semiring interface. The operations `c_plus` (sequential composition), `c_times` (nesting), and `c_meet` (branching) correspond directly to the algebraic operations from Section <a href="#sec:flat-structural-coeffects" data-reference-type="ref" data-reference="sec:flat-structural-coeffects">4.1</a>.

- **Liveness instance**: A simple two-element semiring tracking whether variables are live or dead. This connects directly to IFDS-based liveness analysis (Part IV).

- **Usage instance**: Tracks bounded usage counts, connecting to linear types and ownership (Part VII).

- **Capability instance**: Flat coeffects using set union, for platform/resource tracking.

<div class="fstarcode">

module BrrrMachine.Coeffects

(\* Coeffect algebra - Operations for composing coeffects \*) class coeffect_algebra (c : Type) =

c_zero : c; (\* No requirement \*) c_one : c; (\* Basic requirement \*) c_plus : c -\> c -\> c; (\* Sequential: both needed \*) c_times : c -\> c -\> c; (\* Nesting: inner within outer \*) c_meet : c -\> c -\> c; (\* Branching: at least one needs it \*)

(\* Semiring laws \*) plus_comm : forall a b. c_plus a b = c_plus b a; plus_assoc : forall a b c. c_plus (c_plus a b) c = c_plus a (c_plus b c); times_assoc : forall a b c. c_times (c_times a b) c = c_times a (c_times b c); zero_plus : forall a. c_plus c_zero a = a; one_times : forall a. c_times c_one a = a;

</div>

<div class="fstarcode">

type liveness = \| LDead \| LLive

instance liveness_algebra : coeffect_algebra liveness = c_zero = LDead; c_one = LLive; c_plus = fun l1 l2 -\> if l1 = LDead && l2 = LDead then LDead else LLive; c_times = fun l1 l2 -\> if l1 = LDead then LDead else l2; c_meet = fun l1 l2 -\> if l1 = LDead && l2 = LDead then LDead else LLive; ...

type liveness_coeffect = map var_id liveness

</div>

<div class="fstarcode">

type usage_bound = \| UZero : usage_bound \| UOne : usage_bound \| UBounded : n:nat -\> usage_bound \| UMany : usage_bound

instance usage_algebra : coeffect_algebra usage_bound = c_zero = UZero; c_one = UOne; c_plus = fun u1 u2 -\> match u1, u2 with \| UZero, u -\> u \| u, UZero -\> u \| UOne, UOne -\> UBounded 2 \| UOne, UBounded n -\> UBounded (n + 1) \| UBounded n, UOne -\> UBounded (n + 1) \| UBounded n, UBounded m -\> UBounded (n + m) \| UMany, \_ \| \_, UMany -\> UMany; c_times = fun u1 u2 -\> match u1, u2 with \| UZero, \_ \| \_, UZero -\> UZero \| UOne, u -\> u \| u, UOne -\> u \| UBounded n, UBounded m -\> UBounded (n \* m) \| UMany, \_ \| \_, UMany -\> UMany; c_meet = fun u1 u2 -\> max_usage u1 u2; ...

type usage_coeffect = map var_id usage_bound

</div>

<div class="fstarcode">

type capability = \| CapNetwork \| CapFileSystem \| CapGPS \| CapCamera \| CapDatabase : db_type:string -\> capability \| CapFFI : language:string -\> capability \| CapPlatform : platform:string -\> min_version:nat -\> capability

type capability_set = set capability

instance capability_algebra : coeffect_algebra capability_set = c_zero = Set.empty; c_one = Set.empty; c_plus = Set.union; c_times = Set.union; c_meet = Set.union; (\* Either branch might run, need both capabilities \*) ...

</div>

<div class="fstarcode">

type full_type_judgment = context : typing_context; coeffects : coeffect_annotation; (\* What it requires \*) effects : effect_row; (\* What it produces - from Section 6.1 \*) result_type : ir_type;

(\* Well-typed expression with both effects and coeffects \*) val has_full_type : typing_context -\> coeffect_annotation -\> ir_expr -\> ir_type -\> effect_row -\> bool

(\* Coeffect safety: no access beyond annotations \*) val coeffect_safety : gamma:typing_context -\> r:coeffect_annotation -\> e:ir_expr -\> tau:ir_type -\> eff:effect_row -\> has_full_type gamma r e tau eff -\> Lemma (accessed_variables e ‘subset‘ live_in_coeffect r)

</div>

# Ownership and Resources

<div class="pillarbox">

**Strom & Yemini 1986 — Original Typestate:** The foundational paper introducing typestate as “a refinement of type.” While *type* determines operations ever permitted on an object, *typestate* determines the subset permitted in a particular context.

Key insights that persist to modern systems:

- State machine semantics: Objects have typestates forming lower semilattice

- Transitions via operations: Each op has pre and post typestate conditions

- Compile-time tracking: Typestate as static invariant at each point

- Greatest lower bound at merge: Conservative approximation at branches

- Finalization guarantee: All objects returned to bottom state at exit

**Citation chain:** Strom 1986 $`\rightarrow`$ DeLine/Fahndrich Vault 2004 $`\rightarrow`$ Bierhoff 2007 $`\rightarrow`$ Prusti pledges (Astrauskas 2022) $`\rightarrow`$ Modern Rust unsafe

**Boyapati, Lee, Rinard 2003 — Ownership Types for Safe Programming:** Introduced *owner-as-dominator* discipline: “All paths in the heap from root to object $`x`$ must pass through $`x`$’s owner.”

Key contributions:

- Ownership types for data race and deadlock prevention

- Parameterized classes by owners (precursor to Rust generics + lifetime)

- Lock level partial ordering for deadlock freedom

- Tree-based lock ordering for nested data structures

- Unique pointers combined with ownership (first system to do so)

**Citation chain:** Boyapati 2003 $`\rightarrow`$ Cyclone (Grossman 2006) $`\rightarrow`$ Rust ownership $`\rightarrow`$ RustBelt (Jung 2018) $`\rightarrow`$ RefinedRust (Gaher 2024)

**Modern Synthesis:** Rust’s ownership = Boyapati’s owner discipline + Girard’s linearity. Prusti pledges = Strom’s typestate + separation logic permissions. Iris cameras = Semantic model for Boyapati’s ownership hierarchy.

</div>

<div class="pillarbox">

**Cameras (Section <a href="#sec:resource-algebras" data-reference-type="ref" data-reference="sec:resource-algebras">1</a>) — From Iris/RustBelt:**

- Use for: Rust-like ownership, move semantics, borrow checking

- Strengths: Expressive, handles complex aliasing patterns

- Complexity: Step-indexed, requires frame preservation proofs

- Best for: Systems languages (Rust, C++ with ownership annotations)

**Access Permissions (Section <a href="#sec:access-permissions" data-reference-type="ref" data-reference="sec:access-permissions">2.1</a>) — From Bierhoff:**

- Use for: Java-like typestate, resource protocols, API contracts

- Strengths: Decidable, practical for static analysis tools

- Complexity: 5 fixed permission kinds, simpler composition rules

- Best for: Managed languages (Java, C#, Go), protocol verification

**Guidance:** Start with Access Permissions for decidability. Upgrade to Cameras only when permission splitting is insufficient.

</div>

<div class="pillarbox">

Ownership and linear types connect to usage coeffects (Section <a href="#sec:coeffects" data-reference-type="ref" data-reference="sec:coeffects">[sec:coeffects]</a>):

<div class="center">

| **Ownership Style** | **Usage Coeffect** | **Constraint** |
|:---|:---|:---|
| Rust move semantics | $`\mathsf{UBounded}~1`$ (affine) | usage $`\leq 1`$ |
| Rust borrow (`&T`) | $`\mathsf{UMany}`$ (unrestricted) | unlimited reads |
| Rust mut borrow (`&mut`) | $`\mathsf{UOne}`$ (linear) | usage $`= 1`$ exactly |
| Linear **\[Girard87\]** | $`\mathsf{UOne}`$ | must use exactly once |
| Relevant | $`\mathsf{UAtLeast}~1`$ | must use at least once |

</div>

**Composition via Semiring:**

- $`x`$ moved $`+`$ $`x`$ moved $`=`$ error ($`\mathsf{UBounded}~1 + \mathsf{UBounded}~1 = \mathsf{UBounded}~2 > 1`$)

- $`x`$ borrowed $`+`$ $`x`$ borrowed $`=`$ ok ($`\mathsf{UMany} + \mathsf{UMany} = \mathsf{UMany}`$)

Coeffects provide compositional reasoning; Cameras provide semantic model.

</div>

<div class="pillarbox">

**Source:** Girard 1987 (Linear Logic), Walker 2005 (Substructural Type Systems)

Substructural type systems restrict the *structural rules* of the lambda calculus:

- **Contraction:** Can a variable be used more than once?

- **Weakening:** Can a variable be ignored (not used)?

**The Substructural Hierarchy:**

<div class="center">

| **Type System** | **Contraction** | **Weakening** | **Property** |
|:---|:--:|:--:|:---|
| Unrestricted | Yes | Yes | Normal types (can copy/discard) |
| Affine | No | Yes | Use *at most once* (Rust ownership) |
| Linear | No | No | Use *exactly once* (Girard) |
| Relevant | Yes | No | Use *at least once* |

</div>

**Why This Matters for Resource Management:**

- **Linear types** guarantee resources are neither leaked nor double-used. A file handle with linear type *must* be closed exactly once.

- **Affine types** allow discarding but prevent double-use. Rust’s ownership is affine: values can be dropped implicitly but cannot be used after move.

- **Relevant types** ensure resources are used at least once, preventing “acquire but never use” patterns.

**Rust’s Borrow Checker as Affine + Borrowing:**

1.  Owned values are **affine**: use at most once (move semantics)

2.  Shared borrows (`&T`) are **unrestricted**: can copy freely

3.  Mutable borrows (`&mut T`) are **linear during lifetime**: exactly one active mutable borrow

4.  The borrow checker enforces these constraints statically via lifetime analysis

**Connection to Separation Logic:** Linear types and separation logic share the “resources cannot be duplicated” principle:

- Linear $`A \otimes B`$ (tensor) $`\sim`$ Separation $`A * B`$ (separating conjunction)

- Linear $`A \multimap B`$ (lollipop) $`\sim`$ “consume $`A`$ to produce $`B`$”

- Both enforce disjointness of resource usage

</div>

<div class="pillarbox">

**Reynolds’ Warning **\[Reynolds02\]**:** “GC interaction is problematic” for separation logic.

- Separation logic assumes explicit deallocation

- GC may collect memory mid-analysis (unpredictable timing)

- Cannot model “disconnected garbage” in standard separation logic

**Resolution by Language Class:**

*GC Languages (Python, Go, Java, JavaScript):*

- Use separation logic for *resources only* (files, connections, locks)

- Not for memory—grant MemSafe axiom from runtime

- Track ownership for API resources, not heap cells

- Frame Rule still applies to resource footprints

*Manual Memory (C, C++, unsafe Rust):*

- Full separation logic applies

- Frame rule essential for scalability

- Shape Analysis uses separating conjunction

</div>

## Resource Algebras from Iris

**Papers:** **\[Girard87\]**, **\[Reynolds02\]**, **\[Boyapati03\]**, **\[Jung18\]**

<div class="pillarbox">

Boyapati, Lee, Rinard (OOPSLA 2002) introduced ownership types for safe concurrent programming in Java. Their key insight: ownership encapsulation enables modular reasoning about locking.

**Owner-as-Dominator:** All heap paths to object $`x`$ pass through $`x`$’s owner.

- Owner’s lock protects all transitively owned objects

- Parameterized classes: different instances can have different owners

- `thisThread` owner: thread-local objects need no synchronization

- `self` owner: objects protected by their own lock

**Evolution to modern systems:**

<div class="center">

|  |  |  |
|:---|:--:|:---|
| Boyapati 2003: | `Owner<T>` | $`\rightarrow`$ explicit owner parameter |
| Cyclone 2006: | `region<T>` | $`\rightarrow`$ region-based memory management |
| Rust 2015: | `T`, `&’a T`, `&’a mut T` | $`\rightarrow`$ lifetime-annotated borrows |
| RustBelt 2018: | $`\mathsf{own}(t,v)`$, $`\mathsf{shr}(\kappa,t,l)`$ | $`\rightarrow`$ semantic model via Iris cameras |

</div>

Iris cameras provide the semantic foundation for ownership:

- Exclusive camera $`\mathsf{Ex}(T)`$ $`\sim`$ Boyapati’s `thisThread`/`self` owner

- Fractional camera $`\mathsf{Frac}`$ $`\sim`$ Boyapati’s shared read access

- Agreement camera $`\mathsf{Ag}(T)`$ $`\sim`$ Boyapati’s readonly owner

- Authoritative camera $`\mathsf{Auth}`$ $`\sim`$ Boyapati’s ownership transfer

</div>

The brrr-machine uses resource algebras (cameras) from Iris to track ownership. This provides a unified framework for reasoning about:

- Memory ownership (Rust-style)

- Reference counting

- Shared state with synchronization

- File handles and connections

### The Camera Abstraction

<div class="definition">

**Definition 1.1** (Camera (from Iris)). A *camera* is a step-indexed partial commutative monoid (PCM):
``` math
\text{Camera } M = (M, V : M \to \mathrm{Prop}, |\cdot| : M \to M?, (\cdot) : M \times M \to M)
```
where:

- $`M`$ is the carrier set (possible ownership states)

- $`V`$ is the validity predicate (which states are coherent)

- $`|\cdot|`$ is the core (shareable part)

- $`(\cdot)`$ is composition (combining ownerships)

**Axioms:**
``` math
\begin{aligned}
\textsc{Assoc:} & \quad (a \cdot b) \cdot c = a \cdot (b \cdot c) \\
\textsc{Comm:} & \quad a \cdot b = b \cdot a \\
\textsc{Core-Id:} & \quad |a| \text{ defined} \implies |a| \cdot a = a \\
\textsc{Valid-Op:} & \quad V(a \cdot b) \implies V(a)
\end{aligned}
```

</div>

**Cameras capture different ownership disciplines:**

Exclusive Camera $`\mathsf{Ex}(T)`$:  
Elements are $`T`$ or “invalid”. Composition always invalid (cannot share exclusive). Use: unique ownership, like `Box<T>` in Rust.

Agreement Camera $`\mathsf{Ag}(T)`$:  
Elements are sets of $`T`$ values. Composition is union (must agree on values). Valid if all elements in set are equal. Use: read-only shared state.

Fractional Camera $`\mathsf{Frac}`$:  
Elements are rationals in $`(0, 1]`$. Composition is addition (must sum to $`\leq 1`$). Use: fractional permissions, shared borrows.

Authoritative Camera $`\mathsf{Auth}(M)`$:  
Elements are (full?, fragment) pairs. Full owns the “truth”, fragments are views. Use: ghost state, invariants.

**Connection to Girard 1987 Exponentials:** Cameras implement Girard’s exponential modality ($`!A`$, $`?A`$) via resource structure:

- Exclusive $`\mathsf{Ex}(T)`$ $`\sim`$ bare $`A`$ (linear: exactly-once use, can free)

- Fractional $`\mathsf{Frac}`$ $`\sim`$ $`!A`$ (of-course: shareable, copyable borrows)

- Agreement $`\mathsf{Ag}(T)`$ $`\sim`$ $`!A`$ readonly (unlimited read-only copies)

The exponential $`!A`$ in linear logic makes $`A`$ freely copyable and discardable. Cameras achieve this via the “core” operation: $`|a|`$ extracts the shareable part.

- $`|\mathsf{Ex}(v)| = \bot`$ (exclusive has no shareable part)

- $`|\mathsf{Frac}(q)| = \mathsf{Frac}(q)`$ (fractions are their own core—duplicable)

- $`|\mathsf{Ag}(s)| = \mathsf{Ag}(s)`$ (agreement is duplicable)

#### Ghost State and Invariants

Ghost state enables strong specifications for concurrent programs without runtime cost:

**Ghost State Rules:**
``` math
\begin{aligned}
\textsc{Allocation:} & \quad \vdash \Rrightarrow\exists \gamma.\, \mathsf{own}(\gamma, a) \\
\textsc{Interaction:} & \quad \mathsf{own}(\gamma, a) * \mathsf{own}(\gamma, b) \vdash \mathsf{valid}(a \cdot b) \\
\textsc{Update:} & \quad \mathsf{own}(\gamma, a) \vdash \Rrightarrow\mathsf{own}(\gamma, b) \quad \text{[frame-preserving]}
\end{aligned}
```

**Invariant Protocol:** $`\mathsf{inv}~N~P`$ is duplicable—multiple threads can hold reference. But can only be opened during atomic operations. Must close before atomic operation completes. This enables safe sharing of logical resources.

#### View Shifts and Step-Indexing

<div class="definition">

**Definition 1.2** (View Shift). The update modality $`\Rrightarrow P`$ asserts ownership of resources that can be updated to satisfy $`P`$. Key primitive for ghost state manipulation.

**Rules:**
``` math
\begin{aligned}
& \Rrightarrow\Rrightarrow P \Leftrightarrow \Rrightarrow P \quad \text{(idempotent)} \\
& P \vdash \Rrightarrow P \quad \text{(identity)} \\
& (\Rrightarrow P) * Q \vdash \Rrightarrow(P * Q) \quad \text{(frame preserving)} \\
& \mathsf{own}(\gamma, a) \vdash \Rrightarrow\mathsf{own}(\gamma, b) \quad \text{(when $a \rightsquigarrow b$ frame-preserving)}
\end{aligned}
```

</div>

**Step-Indexing (why cameras are step-indexed):**

- **Problem:** Higher-order ghost state creates circularity. Propositions can contain ghost state, ghost state validates propositions.

- **Solution:** Stratify by “steps remaining” (approximation index $`n`$).

  - At step 0: all propositions equivalent (base case)

  - At step $`n+1`$: can distinguish based on $`n`$-step behavior

  - Cameras are step-indexed: $`\mathsf{valid}_n(a)`$ means valid for $`n`$ more steps

- **Enables:** Impredicative invariants, recursive protocols, later modality ($`\triangleright`$) for guarded recursion

### Separation Logic for Memory

**Paper:** **\[Reynolds02\]**

<div class="definition">

**Definition 1.3** (Separating Conjunction). $`P * Q`$ means: $`P`$ and $`Q`$ hold on *disjoint* portions of memory.

</div>

<div class="theorem">

**Theorem 1.4** (Frame Rule). *
``` math
\frac{\{P\}\; C\; \{Q\}}{\{P * R\}\; C\; \{Q * R\}}
```
If $`C`$ only needs $`P`$, then $`R`$ is preserved. This enables local reasoning.*

</div>

<div class="definition">

**Definition 1.5** (Points-To Assertion). $`x \mapsto v`$ means: $`x`$ points to location containing $`v`$, and we have *exclusive* access.

</div>

**Example (swap):**
``` math
\begin{aligned}
& \{x \mapsto a * y \mapsto b\} \\
& t := *x; \\
& *x := *y; \\
& *y := t \\
& \{x \mapsto b * y \mapsto a\}
\end{aligned}
```
The frame rule lets us add any $`R`$ that does not mention $`x`$, $`y`$.

**Why this matters for brrr-machine:** We can express memory safety as separation logic formulas:

- No use-after-free: freed locations removed from valid heap

- No races: concurrent accesses require disjoint permissions

- Ownership transfer: moving $`x \mapsto v`$ from one context to another

**GC Language Caveat:** For GC languages (Python, Java, Go, JS), use separation logic for *resources* (files, connections) only—not for memory. Memory safety is granted axiomatically by the runtime.

### F\* Specification: Ownership System

The following F\* specifications formalize the ownership concepts from Iris cameras in a form suitable for static analysis implementation. These type definitions provide the foundation for tracking resource ownership, permissions, and state transitions throughout the analysis.

**Note:** F\* code using numeric multiplication requires `open FStar.Mul` at the module level. All code blocks assume a module preamble with appropriate imports.

#### Resource Algebra Typeclass

The `resource_algebra` typeclass captures the essential structure of Iris cameras: a partial commutative monoid with a validity predicate and a core operation for extracting shareable components. The four axioms ensure compositional reasoning about resource ownership.

<div class="fstarcode">

(\* Option bind for composing optional values \*) let option_bind \#a \#b (x : option a) (f : a -\> option b) : option b = match x with \| Some v -\> f v \| None -\> None

(\* Resource algebra operations as a record type \*) noeq type resource_algebra_ops (a : Type) = (\* Composition – partial, may fail \*) compose : a -\> a -\> option a; (\* Validity predicate \*) valid : a -\> bool; (\* Core – the shareable/duplicable part \*) core : a -\> option a;

(\* Axiom predicates – instances must satisfy these \*) let ra_compose_comm \#a (ops : resource_algebra_ops a) = forall (x y : a). ops.compose x y == ops.compose y x

let ra_compose_assoc \#a (ops : resource_algebra_ops a) = forall (x y z : a). option_bind (ops.compose x y) (fun xy -\> ops.compose xy z) == option_bind (ops.compose y z) (fun yz -\> ops.compose x yz)

let ra_valid_compose \#a (ops : resource_algebra_ops a) = forall (x y r : a). ops.compose x y == Some r ==\> ops.valid r ==\> ops.valid x

let ra_core_id \#a (ops : resource_algebra_ops a) = forall (x : a) (c : a). ops.core x == Some c ==\> ops.compose c x == Some x

(\* A valid resource algebra satisfies all axioms \*) type resource_algebra (a : Type) = ops:(resource_algebra_ops a) ra_compose_comm ops / ra_compose_assoc ops / ra_valid_compose ops / ra_core_id ops

</div>

The `exclusive` type models unique ownership—the “Box” pattern from Rust. When you own something exclusively, composition with any other claim invalidates both. This captures Rust’s fundamental rule: a `Box<T>` cannot be aliased. The `core` returns `None` because exclusive resources have no shareable fragment.

<div class="fstarcode">

type exclusive (a : Type) = \| ExOwned : value:a -\> exclusive a \| ExInvalid : exclusive a

(\* Operations for exclusive resources \*) let exclusive_ops (#a : Type) : resource_algebra_ops (exclusive a) = compose = (fun x y -\> (\* Exclusive resources cannot be composed \*) Some ExInvalid); valid = (fun x -\> match x with \| ExOwned \_ -\> true \| ExInvalid -\> false); core = (fun x -\> None); (\* No shareable part \*)

(\* NOTE: To form a valid resource_algebra, one must prove the axioms hold \*)

</div>

Fractional permissions enable shared borrows—Rust’s `&T` references. Instead of binary ownership, we track fractions: a full ownership is $`1`$, and multiple shared references each hold a fraction summing to at most $`1`$. Key properties:

- Fractions can be *composed* (combined) as long as the sum stays $`\leq 1`$

- Unlike exclusive, fractions have a `core`—they are duplicable (within limits)

- When all fractions reassemble to $`1`$, we can recover full ownership

This directly models Rust’s rule: multiple `&T` or one `&mut T`, never both.

<div class="fstarcode">

type fraction = num : nat; denom : pos (\* num/denom in (0,1\] \*)

let frac_add (f1 f2 : fraction) : option fraction = let num = f1.num \* f2.denom + f2.num \* f1.denom in let denom = f1.denom \* f2.denom in if num \<= denom then Some num; denom else None (\* Sum \> 1, invalid \*)

type fractional (a : Type) = \| FracOwn : frac:fraction -\> value:a -\> fractional a \| FracInvalid : fractional a

(\* Operations for fractional resources \*) let fractional_ops (#a : Type) : resource_algebra_ops (fractional a) = compose = (fun x y -\> match x, y with \| FracOwn f1 v1, FracOwn f2 v2 -\> if v1 = v2 then match frac_add f1 f2 with \| Some f -\> Some (FracOwn f v1) \| None -\> Some FracInvalid else Some FracInvalid (\* Disagreement on value \*) \| \_, \_ -\> Some FracInvalid); valid = (fun x -\> match x with \| FracOwn f v -\> f.num \> 0 && f.num \<= f.denom \| FracInvalid -\> false); core = (fun x -\> match x with \| FracOwn f v -\> Some (FracOwn f v) (\* Fractions are duplicable \*) \| \_ -\> None);

(\* NOTE: To form a valid resource_algebra, one must prove the axioms hold \*)

</div>

The `ownership_state` type below models the lifecycle of a memory location through the lens of Rust-style ownership semantics. Each location transitions through states from allocation to deallocation, with borrowing as intermediate states. The `valid_transition` function encodes which state changes are legal, directly corresponding to Rust’s borrow checker rules.

Key type signatures:

- `OsOwned owner`: Location owned by scope `owner`, can be moved or freed

- `OsBorrowedShared borrowers`: Shared borrows active, read-only access by multiple scopes

- `OsBorrowedMut borrower`: Exclusive mutable borrow, single scope has write access

- `OsMoved`: Value moved out, location is “poisoned” (use-after-move error if accessed)

<div class="fstarcode">

type ownership_state = \| OsUnallocated : ownership_state \| OsOwned : owner:scope_id -\> ownership_state \| OsBorrowedShared : borrowers:set scope_id -\> ownership_state \| OsBorrowedMut : borrower:scope_id -\> ownership_state \| OsMoved : ownership_state \| OsFreed : ownership_state

(\* Valid transitions \*) let valid_transition (from to\_ : ownership_state) : bool = match from, to\_ with \| OsUnallocated, OsOwned \_ -\> true (\* Allocation \*) \| OsOwned \_, OsOwned \_ -\> true (\* Transfer \*) \| OsOwned o, OsBorrowedShared bs -\> Set.for_all (fun b -\> b \<\> o) bs (\* Borrow \*) \| OsOwned o, OsBorrowedMut b -\> b \<\> o (\* Mut borrow \*) \| OsBorrowedShared bs, OsOwned \_ -\> Set.is_empty bs (\* Borrows ended \*) \| OsBorrowedMut \_, OsOwned \_ -\> true (\* Mut borrow ended \*) \| OsOwned \_, OsMoved -\> true (\* Move \*) \| OsOwned \_, OsFreed -\> true (\* Deallocation \*) \| \_, \_ -\> false

</div>

### Capability Multiplicities

**Paper:** Crary, Walker, Morrisett 1999 (Typed Memory Management)

<div class="pillarbox">

The `ownership_state` type above tracks **lifecycle**:
``` math
\text{Unallocated} \rightarrow \text{Owned} \rightarrow \text{Borrowed} \rightarrow \text{Moved} \rightarrow \text{Freed}
```

But this misses **aliasing information**: “Is this pointer unique, or might other pointers exist?”

**Crary’s insight:** Only *unique* capabilities can safely free memory. An “owned” but aliased pointer *cannot* safely free—dangling refs remain.

</div>

<div class="definition">

**Definition 1.6** (Multiplicity (from Crary 1999)).
``` math
\begin{aligned}
\mathsf{MUnique}~(\{r^1\}) & \quad \text{Provably no aliases exist. Can free.} \\
\mathsf{MDup}~(\{r^+\}) & \quad \text{May have aliases. Can only access, not free.}
\end{aligned}
```

This is **stronger** than “owned”:

- Owned + Aliased $`\Rightarrow`$ Cannot free safely

- Unique $`\Rightarrow`$ Can free (no dangling pointers possible)

</div>

**Capability Algebra:**
``` math
\begin{aligned}
\text{Join rules:} \quad & \{r^1\} + \{r^1\} = \text{Invalid} \quad \text{(can't have two uniques!)} \\
& \{r^1\} + \{r^+\} = \{r^+\} \quad \text{(unique + dup = dup)} \\
& \{r^+\} + \{r^+\} = \{r^+\} \quad \text{(dup + dup = dup)} \\[1ex]
\text{Strip (lending):} \quad & \mathsf{strip}(\{r^1\}) = \{r^+\} \quad \text{(lend as duplicatable)} \\
& \mathsf{strip}(\{r^+\}) = \{r^+\} \quad \text{(already dup)}
\end{aligned}
```

**Bounded Quantification for Capability Recovery:**

- **Problem:** How to get uniqueness back after lending?

- **Solution:** $`\forall[\varepsilon \leq \{r^+\}].\ (\varepsilon, \ldots, (\varepsilon, \ldots) \to 0) \to 0`$

- Caller instantiates $`\varepsilon = \{r^1\}`$ (passes unique)

- Callee sees only $`\varepsilon \leq \{r^+\}`$ (can’t free, can only access)

- Continuation gets $`\{r^1\}`$ back!

- This models Rust’s borrow-and-return pattern.

<div class="theorem">

**Theorem 1.7** (Complete Collection (Crary 1999)). *Well-typed terminating programs return all memory to the system.
``` math
\text{If } \vdash (M, e) \text{ well-typed, then either:}
\begin{cases}
(M, e) \text{ diverges}, & \text{or} \\
(M, e) \to^* (\emptyset, \mathsf{halt}~i) & \text{heap is empty at termination}
\end{cases}
```
*

***Proof sketch:***

1.  *Typing rule for <span class="sans-serif">halt</span> requires capability $`= \emptyset`$ (empty)*

2.  *Empty capability means no regions owned*

3.  *Subject reduction preserves typing*

4.  *Progress ensures termination reaches <span class="sans-serif">halt</span>*

5.  *Therefore: terminating programs have empty heap*

*This is **leak-freedom by construction**.*

</div>

## Access Permissions and Typestate

### Access Permissions (Bierhoff)

**Paper:** Bierhoff 2007 (Modular Typestate Checking)

**Historical Foundation:** Strom & Yemini 1986 (Original Typestate)

<div class="pillarbox">

**Strom & Yemini 1986 — Original Typestate Concept:** “Typestate is a refinement of the concept of type. Whereas the type of a data object determines the set of operations ever permitted on the object, typestate determines the subset of these operations which is permitted in a particular context.”

**Core Principles (still valid today):**

1.  State machine semantics: Each type has associated typestate lattice

2.  Preconditions: Each operation requires specific typestate

3.  Postconditions: Operations produce new typestate (possibly per outcome)

4.  Static tracking: Typestate is compile-time invariant at each point

5.  Merge rule: At branches, take greatest lower bound of typestates

6.  Finalization: All objects must reach bottom typestate at exit

**Key Evolution:**

<div class="center">

|                |                                                         |
|:---------------|:--------------------------------------------------------|
| Strom 1986:    | Typestate as static invariant, no aliasing handling     |
| Vault 2004:    | Adoption (permanent) and Focus (temporary) for aliasing |
| Bierhoff 2007: | 5 permission kinds integrate aliasing with typestate    |

</div>

Modern typestate = Strom’s state machines + Bierhoff’s permission aliasing

</div>

<div class="pillarbox">

Iris cameras (Section <a href="#sec:resource-algebras" data-reference-type="ref" data-reference="sec:resource-algebras">1</a>) are **general** but **undecidable**. Bierhoff’s 5 permission kinds are **specific** but **decidable**.

**Five Permission Kinds:**

<div class="center">

| **Permission**                | **Description**                            |
|:------------------------------|:-------------------------------------------|
| $`\mathsf{unique}(r,n,g)`$    | Full ownership, can free, no aliases exist |
| $`\mathsf{full}(r,n,g)`$      | Full access, may have pure aliases         |
| $`\mathsf{share}(r,n,g,k)`$   | Shared mutable, fraction $`k`$             |
| $`\mathsf{immutable}(r,n,g)`$ | Read-only, unlimited copies                |
| $`\mathsf{pure}(r,n,g)`$      | Read-only, guarantees no modification      |

</div>

**Key Insight:** Permissions bound to state guarantees via root nodes. A permission can be confined to a state subtree, guaranteeing the object stays in that region of the typestate.

</div>

<div class="fstarcode">

type access_permission = \| APUnique : root:root_node -\> node:state_node -\> guarantee:state_node -\> access_permission (\* Full ownership, can free, guaranteed no aliases \*) \| APFull : root:root_node -\> node:state_node -\> guarantee:state_node -\> access_permission (\* Full access, may have pure aliases observing \*) \| APShare : root:root_node -\> node:state_node -\> guarantee:state_node -\> frac:fraction -\> access_permission (\* Shared mutable access with fraction \*) \| APImmutable : root:root_node -\> node:state_node -\> guarantee:state_node -\> access_permission (\* Read-only, can duplicate freely \*) \| APPure : root:root_node -\> node:state_node -\> guarantee:state_node -\> access_permission (\* Read-only, guarantees object won’t be modified \*)

</div>

Permission splitting and joining form the core algebra of access control. These operations implement Rust’s borrow semantics at the permission level:

- `split_unique`: Creates a mutable reference from an owned value (like `&mut` from `Box`)

- `split_full`: Creates multiple shared references from full access

- `join_shares`: Recombines all shared references back to full access

- `join_full_pure`: Restores unique ownership when all borrows end

These operations are *linear*—permissions cannot be duplicated or discarded, only split and rejoined.

<div class="fstarcode">

(\* Split unique into full + pure \*) val split_unique : APUnique r n g -\> (APFull r n g \* APPure r n g)

(\* Split full into share + share \*) val split_full : APFull r n g -\> (APShare r n g (half_frac) \* APShare r n g (half_frac))

(\* Join shares back to full \*) val join_shares : APShare r n g f1 -\> APShare r n g f2 -\> requires (add_fracs f1 f2 = full_frac) -\> APFull r n g

(\* Join full + pure back to unique \*) val join_full_pure : APFull r n g -\> APPure r n g -\> APUnique r n g

</div>

#### Per-Node Fraction Functions (Bierhoff 2007 Extended)

Fraction functions generalize simple fractions to complex objects with heterogeneous access patterns. Consider a database connection object: you might have full access to the metadata but only partial (shared) access to the result cursor. The fraction function maps each state node to its permission fraction, enabling fine-grained access control over object substructure. This extends Bierhoff’s typestate checking to handle partially-locked composite objects.

<div class="fstarcode">

(\* Complex objects may have different access levels at different states. A "partially open" file has full access to metadata, partial to content. \*)

type fraction_function = state_node -\> fraction

val frac_fn_split : fraction_function -\> (fraction_function \* fraction_function) let frac_fn_split ff = (fun n -\> half_of (ff n), fun n -\> half_of (ff n))

val frac_fn_join : fraction_function -\> fraction_function -\> option fraction_function let frac_fn_join ff1 ff2 = (\* Sum must not exceed 1 at any node \*) let can_join = forall (fun n -\> can_add (ff1 n) (ff2 n)) in if can_join then Some (fun n -\> add_frac (ff1 n) (ff2 n)) else None (\* Sum exceeds 1 at some node \*)

</div>

#### Temporary State in Weak Permissions

When strong permission (unique/full) splits into weak (pure/immutable), the weak permission carries **temporary state**—the state observed at split. This is **forgotten** when permissions rejoin, ensuring:

- Observers saw consistent state at observation time

- State may have changed by rejoin time (that’s OK)

<div class="fstarcode">

(\* Split unique into full + pure WITH state capture \*) val split_unique_with_state : p:access_permission_v2APUnique_v2? p -\> (access_permission_v2 \* access_permission_v2) let split_unique_with_state (APUnique_v2 r n g) = let current_state = n in (\* Capture state at split time \*) (APFull_v2 r n g, APPure_v2 r n g (Some current_state)) (\* Pure carries observed state \*)

(\* When rejoining, temporary state is FORGOTTEN \*) val join_full_pure_forget : full:access_permission_v2APFull_v2? full -\> pure:access_permission_v2APPure_v2? pure -\> option access_permission_v2 let join_full_pure_forget (APFull_v2 r n g) (APPure_v2 r’ n’ g’ obs) = if r = r’ && g = g’ then (\* Observed state (obs) is DISCARDED \*) Some (APUnique_v2 r n g) (\* Back to unique, no memory of observation \*) else None

</div>

### MALL Specifications for Method Contracts

**Source:** **\[Girard87\]** (Linear Logic), Bierhoff 2007 (Modular Typestate)

<div class="pillarbox">

Multiplicative-Additive Linear Logic (**\[Girard87\]**) provides precise permission contracts. MALL is the fragment without Par ($`\mathbin{\rotatebox[origin=c]{180}{\&}}`$) or exponentials.

**Girard’s Two Conjunctions (multiplicative vs additive):**
``` math
\begin{aligned}
P \otimes Q &= \text{``have both $P$ and $Q$''} \quad \text{(tensor/times) --- both consumed} \\
P \mathbin{\&}Q &= \text{``can use as $P$ or as $Q$''} \quad \text{(with) --- choice by consumer}
\end{aligned}
```

**Girard’s Two Disjunctions:**
``` math
\begin{aligned}
P \oplus Q &= \text{``provides $P$ or provides $Q$''} \quad \text{(plus) --- choice by provider} \\
P \mathbin{\rotatebox[origin=c]{180}{\&}}Q &= \text{Par (excluded from MALL)}
\end{aligned}
```

**Linear Implication:**
``` math
P \multimap Q = \text{``consume $P$ to produce $Q$''}
```

**Effect Composition Mapping (**\[Girard87\]** Section 1.1.4):**

- Sequential composition $`(e_1; e_2)`$ maps to tensor $`(E_1 \otimes E_2)`$

- Choice (if-then-else) maps to plus $`(E_1 \oplus E_2)`$

</div>

The `mall_formula` type encodes MALL (Multiplicative-Additive Linear Logic) formulas for specifying method contracts. Each constructor corresponds to a linear logic connective:

- `MAtom`: A single permission (the atomic propositions of our logic)

- `MTensor` ($`\otimes`$): “Have both”—consumes both resources

- `MLolli` ($`\multimap`$): “Consume to produce”—linear implication

- `MWith` ($`\mathbin{\&}`$): “Choose one”—internal choice by consumer

- `MPlus` ($`\oplus`$): “Provides one”—external choice by provider

<div class="fstarcode">

type mall_formula = \| MAtom : perm:access_permission_v2 -\> mall_formula \| MTensor : mall_formula -\> mall_formula -\> mall_formula (\* P tensor Q: have both \*) \| MLolli : mall_formula -\> mall_formula -\> mall_formula (\* P -o Q: consume to produce \*) \| MWith : mall_formula -\> mall_formula -\> mall_formula (\* P & Q: choose one \*) \| MPlus : mall_formula -\> mall_formula -\> mall_formula (\* P + Q: provides one \*) \| MOne : mall_formula (\* Unit for tensor \*) \| MTop : mall_formula (\* Unit for with \*) \| MZero : mall_formula (\* Unit for plus \*) \| MBot : mall_formula (\* Unit for par \*)

</div>

Method contracts specify resource protocols using MALL. A contract has:

- **Precondition**: Permissions the caller must provide (these are *consumed*)

- **Postcondition**: Permissions the method returns (these are *produced*)

The contract itself is a linear implication: $`\mathsf{pre} \multimap\mathsf{post}`$. Calling a method *consumes* the precondition and *produces* the postcondition—this is the essence of substructural reasoning for APIs. The examples below show file operations where state transitions are tracked through permission transformations.

<div class="fstarcode">

type method_contract = method_name : string; precondition : mall_formula; (\* Required permissions - CONSUMED \*) postcondition : mall_formula; (\* Produced permissions - RETURNED \*)

(\* File.open(): unique(closed) -o unique(open) \*) let file_open_contract : method_contract = method_name = "open"; precondition = MAtom (APUnique_v2 "file" "closed" "closed"); postcondition = MAtom (APUnique_v2 "file" "open" "open");

(\* File.read(): can use full OR immutable - caller chooses \*) let file_read_contract : method_contract = method_name = "read"; precondition = MWith (MAtom (APFull_v2 "file" "open" "open")) (MAtom (APImmutable_v2 "file" "open" "open")); postcondition = MWith (MAtom (APFull_v2 "file" "open" "open")) (MAtom (APImmutable_v2 "file" "open" "open"));

</div>

### Vault Adoption and Focus Mechanisms

**Papers:** DeLine & Fahndrich 2001, 2002, 2004 (Vault, Fugue)

<div class="pillarbox">

Bierhoff (Section <a href="#sec:access-permissions" data-reference-type="ref" data-reference="sec:access-permissions">2.1</a>) provides 5 permission kinds for typestate. Vault **complements** this with two critical operations for aliasing:

**Adoption:** Permanently transition from unique to shared-frozen state.
``` math
\mathsf{unique}(\mathsf{obj}) \xrightarrow{\mathsf{adopt}} \mathsf{pure}(\mathsf{obj}) \quad \text{[Permanent, typestate frozen]}
```
Use case: Publishing object to shared memory, `builder.build()`

**Focus:** Temporarily upgrade shared to unique within a scope.
``` math
\mathsf{pure}(\mathsf{obj}) \xrightarrow{\mathsf{focus}} \{\mathsf{unique}(\mathsf{obj})\} \xrightarrow{\mathsf{unfocus}} \mathsf{pure}(\mathsf{obj}) \quad \text{[Temporary]}
```
Use case: Critical section, exclusive state change on shared object

**Key Difference from Rust:**

- Rust borrows are **temporary** and return ownership

- Vault adoption is **permanent**—once adopted, never unique again

- Use adoption for “publish once” patterns (builders, caches)

- Use Rust-style borrowing (Section <a href="#sec:resource-algebras" data-reference-type="ref" data-reference="sec:resource-algebras">1</a>) for temporary access

</div>

The `adopt` function below demonstrates the key operation that transitions from unique ownership to permanent sharing. Once an object is adopted, its typestate becomes frozen—it cannot be modified further. This is useful for “publish once” patterns like builders that produce immutable results.

<div class="fstarcode">

(\* Adopt operation: unique -\> pure (frozen) \*) val adopt : perm:access_permission_v2APUnique_v2? perm -\> adoption_result let adopt (APUnique_v2 r n g) = (\* Typestate at adoption time becomes the permanent frozen state \*) AdoptionSuccess (APPure_v2 r n g (Some n))

(\* Adoption is idempotent on already-adopted (pure) permissions \*) val adopt_idempotent : perm:access_permission_v2 -\> Lemma (APPure_v2? perm ==\> adopt_safe perm = perm)

</div>

The `focus` operation provides the inverse capability: temporarily upgrading a shared (pure) permission back to unique within a protected scope. This requires lock acquisition to ensure no concurrent access occurs during the focus period. The key insight is that focus is *scoped*—upon exiting the focus block, the permission reverts to pure with the updated typestate visible to all aliases.

<div class="fstarcode">

(\* Enter focus: pure -\> unique (temporarily) \*) val enter_focus : perm:access_permission_v2APPure_v2? perm -\> acquired_lock:bool -\> focus_result let enter_focus (APPure_v2 r n g obs) acquired_lock = let token = target_root = r; target_node = n; target_guarantee = g; original_mode = AMMayBeAliased; scope_id = fresh_scope_id (); requires_lock = true; (\* Concurrent access requires lock \*) in if acquired_lock \|\| not token.requires_lock then FocusSuccess token (APUnique_v2 r n g) else FocusFailure "Focus requires lock acquisition for concurrent access"

(\* Exit focus: unique -\> pure (restore aliased state) \*) val exit_focus : token:focus_token -\> current_perm:access_permission_v2APUnique_v2? current_perm -\> access_permission_v2 let exit_focus token (APUnique_v2 r n g) = (\* New typestate (n) becomes the frozen state for all aliases \*) APPure_v2 r n g (Some n)

</div>

### Frame-Based OOP Inheritance

**Source:** DeLine & Fahndrich 2004 (Vault, Fugue)

<div class="pillarbox">

When typestate meets inheritance, a challenge arises: subclasses may have their own state machines, and virtual method calls must handle all possible subclass states.

**Key Insight:** Decompose object typestate into *per-class frames*. Each class in the inheritance hierarchy maintains its own frame typestate. Virtual calls use “sliding” signatures that abstract over unknown subclass frames.

**Frame Typestate Structure:**

- Each class has a frame with its own state machine

- Object typestate = composition of all frame typestates

- Abstract references include a “rest” component for unknown subclasses

- Virtual dispatch slides all frames including rest

</div>

The following F\* code defines the frame typestate structure. The `frame_typestate` type captures per-class state, while `object_typestate` composes frames for the full inheritance chain. The distinction between `OTSConcrete` and `OTSAbstract` is critical: concrete typestates know all frames (no virtual dispatch needed), while abstract typestates include a “rest” state for unknown subclass behavior.

<div class="fstarcode">

(\* Per-class frame typestate \*) type frame_typestate = class_name : string; state_name : state_node; fields : option (list (string \* symbolic_name)); (\* None = packed \*)

(\* Object typestate: collection of frame typestates \*) type object_typestate = \| OTSConcrete : frames:list frame_typestate -\> object_typestate (\* Known concrete type - all frames specified \*) \| OTSAbstract : frames:list frame_typestate -\> rest:state_node -\> object_typestate (\* Abstract reference - rest represents unknown subclass frames \*)

(\* Get frame for a specific class \*) val get_frame : object_typestate -\> string -\> option frame_typestate let get_frame ots cls = let frames = match ots with \| OTSConcrete fs -\> fs \| OTSAbstract fs \_ -\> fs in List.find (fun f -\> f.class_name = cls) frames

(\* Upcast: convert concrete to abstract (lose subclass info) \*) val upcast : object_typestate -\> string -\> object_typestate let upcast ots target_class = match ots with \| OTSConcrete frames -\> let (above, below) = List.partition (fun f -\> is_superclass_or_equal f.class_name target_class ) frames in let rest_state = match below with \| \[\] -\> "Unknown" \| f :: \_ -\> f.state_name in OTSAbstract above rest_state \| OTSAbstract \_ \_ -\> ots (\* Already abstract \*)

</div>

Virtual methods that change typestate require two signatures: an *implementation signature* (implSig) that specifies what the implementing class does, and a *virtual signature* (virtSig) that specifies behavior for all possible dynamic types. The key difference is that virtSig changes the “rest” state, affecting all unknown subclass frames.

<div class="fstarcode">

type method_signature = method_name : string; declaring_class : string; this_pre : object_typestate; this_post : object_typestate;

(\* Implementation signature: changes frames up to implementing class \*) val impl_sig : string -\> string -\> state_node -\> state_node -\> method_signature let impl_sig decl impl pre_s post_s = let pre_ts = OTSAbstract \[ class_name = decl; state_name = pre_s; fields = None \] pre_s (\* rest unchanged \*) in let post_ts = OTSAbstract \[ class_name = decl; state_name = post_s; fields = None ; class_name = impl; state_name = post_s; fields = None \] pre_s (\* rest UNCHANGED in impl sig \*) in method_name = ""; declaring_class = decl; this_pre = pre_ts; this_post = post_ts

(\* Virtual signature: changes all frames INCLUDING rest \*) val virt_sig : string -\> state_node -\> state_node -\> method_signature let virt_sig decl pre_s post_s = let pre_ts = OTSAbstract \[ class_name = decl; state_name = pre_s; fields = None \] pre_s in let post_ts = OTSAbstract \[ class_name = decl; state_name = post_s; fields = None \] post_s (\* rest = post_state - ALL frames change! \*) in method_name = ""; declaring_class = decl; this_pre = pre_ts; this_post = post_ts

</div>

#### API Pattern Examples with Adoption and Focus

The following examples demonstrate how Vault’s adoption and focus mechanisms apply to common design patterns. These patterns show the practical application of typestate and permission tracking.

<div class="fstarcode">

(\* Builder Pattern: unique -\> unique -\> ... -\> adopt -\> pure + result Builder starts unique, accumulates state, adopts at build(). \*)

type builder_state = \| BEmpty \| BHasName \| BHasAge \| BComplete

(\* Builder.new() -\> unique(Empty) \*) let builder_new_post = APUnique_v2 "builder" "Empty" "alive"

(\* Builder.withName() : unique(Empty) -\> unique(HasName) \*) let builder_withName_contract : method_contract = method_name = "withName"; precondition = MAtom (APUnique_v2 "builder" "Empty" "alive"); postcondition = MAtom (APUnique_v2 "builder" "HasName" "alive");

(\* Builder.build() : unique(HasAge) -\> ADOPT -\> pure(Complete) + unique(Person) \*) let builder_build_contract : method_contract = method_name = "build"; precondition = MAtom (APUnique_v2 "builder" "HasAge" "alive"); postcondition = MTensor (MAtom (APPure_v2 "builder" "Complete" "Complete" (Some "Complete"))) (MAtom (APUnique_v2 "person" "Valid" "alive")); (\* Builder is ADOPTED - frozen at Complete, cannot be reused \*)

</div>

<div class="fstarcode">

(\* Socket lifecycle demonstrates state transitions with focus for closing a shared socket. Multiple readers can share, but close requires focus. \*)

type socket_state = \| SRaw \| SBound \| SConnected \| SClosed

let socket_bind_contract : method_contract = method_name = "bind"; precondition = MAtom (APUnique_v2 "socket" "Raw" "alive"); postcondition = MAtom (APUnique_v2 "socket" "Bound" "alive");

(\* receive() : pure(Connected) -\> pure(Connected) - read-only, aliased OK \*) let socket_receive_contract : method_contract = method_name = "receive"; precondition = MAtom (APPure_v2 "socket" "Connected" "alive" None); postcondition = MAtom (APPure_v2 "socket" "Connected" "alive" None);

(\* close() requires focus if socket was shared: focus socket with lock – temporarily unique(Connected) socket.close(); – unique(Closed) – back to pure(Closed) \*)

</div>

<div class="pillarbox">

**Bierhoff Provides (Section <a href="#sec:access-permissions" data-reference-type="ref" data-reference="sec:access-permissions">2.1</a>):**

- 5 permission kinds: unique, full, share, immutable, pure

- Permission splitting and joining

- State guarantees via root nodes

- Per-node fraction functions

**Vault Adds (This Section):**

- Adoption: permanent sharing with frozen typestate

- Focus: temporary unique upgrade for aliased objects

- Frame typestate decomposition for OOP

- Sliding methods for virtual dispatch

**Relationship:**

<div class="center">

|  |  |  |
|:---|:--:|:---|
| Vault NotAliased | $`=`$ | Bierhoff unique |
| Vault MayBeAliased | $`=`$ | Bierhoff pure with frozen state |
| Adoption | $`:`$ | unique $`\to`$ pure (permanent) |
| Focus | $`:`$ | pure $`\to`$ unique $`\to`$ pure (temporary, scoped) |

</div>

**vs Rust Borrowing:**

- Rust borrow: temporary, lifetime-tracked, ownership returns

- Vault adoption: permanent, no restoration

- Vault focus: temporary exclusive access with explicit lock

Use Rust-style borrowing (Section <a href="#sec:resource-algebras" data-reference-type="ref" data-reference="sec:resource-algebras">1</a>) for most cases. Use Vault adoption for “publish once” patterns (builders, caches).

</div>

### Rust Verification: RefinedRust and Prusti

**Papers:** **\[Gaher24\]** (RefinedRust); Astrauskas, Poli, Mueller, Summers, Leymann 2022 (Prusti)

<div class="pillarbox">

The `ownership_state` in Section <a href="#sec:fstar-ownership" data-reference-type="ref" data-reference="sec:fstar-ownership">1.3</a> tracks lifecycle (owned/borrowed/moved) but lacks:

1.  **Functional correctness** (`Vec::push` adds element correctly)

2.  **Place structure** (nested field access `x.f.g.h`)

3.  **Unsafe code verification** (preconditions for raw pointers)

4.  **Temporal reasoning** (what holds when a borrow expires)

**RefinedRust (Foundational, Coq-verified):**

- Refinement types: $`T~@~\mathit{spec}`$ where spec is mathematical

- Place types: $`\mathsf{place}(T)`$, $`\mathsf{blocked}_{'\!a}(T)`$ for borrow tracking

- Unsafe verification with explicit preconditions

- Layout-generic (correct for any valid layout algorithm)

**Prusti (Practical, automated via Viper):**

- Pledges: `assert_on_expiry` for reborrowing contracts

- Automatic core proof from type information

- Pure functions for specification use

- Type-conditional specifications (`refine_spec`)

**Selection Guidance:**

<div class="center">

|  |  |
|:---|:---|
| Safe Rust, simple lifetimes | $`\rightarrow`$ Prusti (faster, less annotation) |
| Unsafe Rust, libraries | $`\rightarrow`$ RefinedRust (foundational proofs) |
| Functional correctness | $`\rightarrow`$ Either (both support refinements) |
| Machine-checked proofs | $`\rightarrow`$ RefinedRust (Coq output) |

</div>

</div>

#### Place Types and Blocked Types (RefinedRust)

RefinedRust introduces *place types* to make Rust’s lvalue structure first-class in the type system. The key insight is that a “place” (an lvalue like `x.field.subfield`) has two components:

1.  The **refinement**: the mathematical/specification value for verification

2.  The **place type**: the ownership structure (owned, blocked, uninitialized, dead)

The `PlaceBlocked` constructor is critical: when a place is borrowed, it becomes “blocked” until the lifetime ends. The inner type records what will be *restored* when the borrow expires. This enables precise tracking of what ownership state returns after borrowing.

<div class="fstarcode">

type place_type = \| PlaceOwned : inner:rust_type -\> place_type (\* place(T) - full ownership of a place containing value of type T \*) \| PlaceBlocked : lifetime:lifetime_id -\> inner:rust_type -\> place_type (\* blocked\_’a(T) - place is borrowed until lifetime ’a ends The inner type T records what will be restored when borrow ends \*) \| PlaceUninit : size:nat -\> place_type (\* Uninitialized memory of given size in bytes \*) \| PlaceDead : place_type (\* Place has been moved out of - cannot access \*)

(\* Refined type = refinement paired with place type \*) type refined_type = ref_val : refinement; (\* Mathematical/specification value \*) place_ty : place_type; (\* Ownership structure \*)

</div>

#### Pledge Specifications for Reborrowing (Prusti)

Prusti introduces *pledges* to handle a challenging verification problem: when a function returns a mutable reference (reborrowing), what happens when that reference expires? The key insight is that reborrowing creates a *temporal contract*:

- **Caller obligation:** ensure certain conditions hold *before* the reference expires

- **Callee guarantee:** certain conditions will hold *after* the reference expires

This is expressed via `assert_on_expiry` annotations that specify both the pre-expiry constraint and the post-expiry guarantee. The pledge below demonstrates this for a BST’s `get_root_value` method, which returns a mutable reference to the root value.

<div class="fstarcode">

(\* MOTIVATION: When a function returns a mutable reference (reborrowing), we need to specify: 1. What the CALLER must ensure before the reference expires 2. What the CALLEE guarantees after the reference expires \*)

type pledge =

(\* The reborrowed reference this pledge applies to \*) reborrowed_ref : string;

(\* Constraint that MUST hold before expiry (caller obligation) \*) pre_expiry_constraint : temporal_expr -\> prop;

(\* Guarantee that WILL hold after expiry (callee guarantee) \*) post_expiry_guarantee : temporal_expr -\> temporal_expr -\> prop;

(\* Example: BST get_root_value specification \*) let bst_get_root_pledge : pledge = reborrowed_ref = "result"; pre_expiry_constraint = fun te -\> (\* All elements in left subtree less than result \*) forall_in_tree (fun elem -\> elem \< eval_at_before_expiry te) left_subtree; post_expiry_guarantee = fun old_self before_result -\> (\* The root value equals what was returned \*) match old_self with \| Node (root_val, \_, \_) -\> root_val = before_result \| \_ -\> false;

</div>

## Linearizability for Concurrent Objects

**Paper:** Herlihy & Wing 1990

<div class="pillarbox">

The synthesis has: race detection, ownership, happens-before. The synthesis **lacks**: concurrent data structure *correctness*.

Linearizability answers: “Does this concurrent queue behave like a queue?” (Not just “are there races?” but “is the behavior correct?”)

**Key Theorem (Locality):** $`H`$ is linearizable $`\Longleftrightarrow`$ $`\forall x.\, H|_x`$ is linearizable.

This enables **modular verification**—verify each object independently.

</div>

<div class="definition">

**Definition 3.1** (Linearizability). $`H`$ is linearizable w.r.t. spec $`S`$ iff:

1.  $`H`$ can be extended to $`H'`$ (completing pending operations)

2.  $`\mathsf{complete}(H')`$ is equivalent to some legal sequential history $`S`$

3.  Precedence in $`H`$ is preserved in $`S`$

</div>

The formalization below captures concurrent execution histories as sequences of events. Each event is either an `Invocation` (a process starting an operation) or a `Response` (the operation completing). The `sequential` predicate checks if a history represents serial execution (invocations immediately followed by responses), while `precedes` captures the real-time ordering that linearizability must preserve.

<div class="fstarcode">

type event = \| Invocation : obj:string -\> op:string -\> args:list value -\> proc:nat -\> event \| Response : obj:string -\> result:value -\> proc:nat -\> event

type history = list event

(\* A history is sequential if invocations immediately followed by responses \*) let rec sequential (h : history) : bool = match h with \| \[\] -\> true \| Invocation \_ \_ \_ \_ :: Response \_ \_ \_ :: rest -\> sequential rest \| \_ -\> false

(\* Precedence: response r precedes invocation i if r appears before i \*) let precedes (e1 e2 : event) (h : history) : bool = match e1, e2 with \| Response \_ \_ \_, Invocation \_ \_ \_ \_ -\> index_of e1 h \< index_of e2 h \| \_ -\> false

</div>

<div class="pillarbox">

**Sequential Objects:** $`\alpha : \mathsf{rep} \to \mathsf{abs}`$ (Single abstract value)

**Concurrent Objects:** $`\alpha : \mathsf{rep} \to \mathcal{P}(\mathsf{abs})`$ (Set of abstract values)

**Why a set?** Concurrent execution introduces nondeterminism. Multiple threads may interleave operations, and multiple linearizations of the same history may all be valid. Each linearization produces a (potentially different) abstract state.

**Example:** Concurrent queue with `enqueue(1)`, `enqueue(2)` overlapping:

- Linearization 1: `enqueue(1)` then `enqueue(2)` $`\rightarrow`$ $`[1, 2]`$

- Linearization 2: `enqueue(2)` then `enqueue(1)` $`\rightarrow`$ $`[2, 1]`$

- $`\alpha(\mathsf{rep}) = \{[1,2], [2,1]\}`$ — both are valid abstract states

</div>

### Progress Properties for Concurrent Objects

**Source:** Herlihy & Shavit 2008 — “The Art of Multiprocessor Programming”

Linearizability is a *safety* property (“nothing bad happens”). Progress properties ensure *liveness* (“something good eventually happens”).

<div class="definition">

**Definition 3.2** (Progress Guarantees (Hierarchy from strongest to weakest)).

Wait-Free:  
Every operation completes in bounded steps, regardless of other threads. No thread can be starved.

Lock-Free:  
System-wide progress: some operation completes in finite time. Individual threads may starve, but system as a whole progresses.

Obstruction-Free:  
Progress guaranteed if thread runs in isolation (no contention). May livelock under contention.

Blocking:  
May use locks; susceptible to deadlock, priority inversion, and indefinite blocking if lock holder fails.

</div>

<div class="fstarcode">

type progress_guarantee = \| WaitFree : bound:nat -\> progress_guarantee \| LockFree : progress_guarantee \| ObstructionFree : progress_guarantee \| Blocking : progress_guarantee

(\* Well-known classifications \*) let michael_scott_queue : progress_guarantee = LockFree let harris_linked_list : progress_guarantee = LockFree let herlihy_snapshot : progress_guarantee = WaitFree 2 (\* 2-scan algorithm \*) let treiber_stack : progress_guarantee = LockFree let mutex_protected : progress_guarantee = Blocking

</div>

### Temporal Logic for Liveness

**Source:** Pnueli 1977 — “The Temporal Logic of Programs”

<div class="pillarbox">

Progress properties describe *what* guarantees a concurrent object provides. Temporal logic provides the formal *language* to specify and verify these properties.

**Key Insight (Pnueli):** Program properties split into two classes:

- **Safety:** “Nothing bad ever happens” (invariance)

- **Liveness:** “Something good eventually happens” (eventuality)

The temporal operators $`\Box`$ (always) and $`\Diamond`$ (eventually) provide:

- Precise specification of concurrent behavior

- Proof principles for verification (P1 for safety, P2 for liveness)

- Foundation for fairness assumptions in concurrent systems

</div>

<div class="fstarcode">

type temporal_formula = (\* State formula lifted to path \*) \| TFState : state_formula -\> temporal_formula

(\* Next: holds in the next state \*) \| TFNext : temporal_formula -\> temporal_formula (\* X phi \*)

(\* Globally/Always: holds in all future states \*) \| TFAlways : temporal_formula -\> temporal_formula (\* G phi or Box phi \*)

(\* Finally/Eventually: holds in some future state \*) \| TFEventually : temporal_formula -\> temporal_formula (\* F phi or Diamond phi \*)

(\* Until: phi holds until psi becomes true \*) \| TFUntil : temporal_formula -\> temporal_formula -\> temporal_formula (\* phi U psi \*)

(\* Leads-to: if phi holds, psi eventually holds \*) \| TFLeadsTo : temporal_formula -\> temporal_formula -\> temporal_formula (\* phi  \> psi \*)

</div>

<div class="theorem">

**Theorem 3.3** (Invariance Principle (P1) — Pnueli 1977). *To prove $`\Box P`$ ($`P`$ always holds):*

1.  *Show $`P`$ holds initially*

2.  *Show every transition preserves $`P`$*

*This is the foundation for verifying **safety** properties.*

</div>

<div class="theorem">

**Theorem 3.4** (Eventuality Principle (P2) — Pnueli 1977). *To prove $`P \leadsto Q`$ (if $`P`$ holds, $`Q`$ eventually holds): Find a ranking function $`r : \mathsf{state} \to \mathbb{N}`$ such that:*

- *$`r`$ decreases on each step while $`Q`$ does not hold*

- *When $`r`$ reaches 0, $`Q`$ must hold*

*This is the foundation for verifying **liveness** properties.*

</div>

### CTL Model Checking

**Source:** Clarke, Emerson, Sistla 1986 — “Automatic Verification of Finite-State Concurrent Systems Using Temporal Logic”

<div class="pillarbox">

Pnueli’s LTL (Section <a href="#sec:temporal-logic" data-reference-type="ref" data-reference="sec:temporal-logic">3.2</a>) uses **linear time**—formulas describe properties of single execution paths. CTL uses **branching time**—formulas can distinguish between “all paths” and “some path”.

**Key Distinction:**

- LTL: $`\Box(\mathit{request} \Rightarrow \Diamond \mathit{response})`$ — on this path, responses follow requests

- CTL: $`\mathsf{AG}(\mathit{request} \Rightarrow \mathsf{AF}\ \mathit{response})`$ — on all paths from all states

- CTL: $`\mathsf{AG}(\mathit{request} \Rightarrow \mathsf{EF}\ \mathit{response})`$ — on some path from all states

**Why CTL Matters for Security:**

- Use-after-free: $`\mathsf{AG}(\mathit{freed}(p) \Rightarrow \mathsf{AX}\ \mathsf{AG}(\neg \mathit{deref}(p)))`$

- Resource leak: $`\mathsf{AG}(\mathit{acquire}(r) \Rightarrow \mathsf{AF}\ \mathit{release}(r))`$

- Deadlock-free: $`\mathsf{AG}(\mathsf{EF}\ \mathit{progress})`$

**Complexity:** $`O(|f| \cdot (|S| + |R|))`$ — **linear** in formula and state graph. This is much better than LTL model checking (PSPACE-complete)!

</div>

<div class="fstarcode">

type ctl_formula = \| CTLAtom : pred:(node_id -\> bool) -\> ctl_formula \| CTLNot : ctl_formula -\> ctl_formula \| CTLAnd : ctl_formula -\> ctl_formula -\> ctl_formula

(\* Path quantifier + Next \*) \| CTLAX : ctl_formula -\> ctl_formula (\* On ALL paths, phi holds NEXT \*) \| CTLEX : ctl_formula -\> ctl_formula (\* On SOME path, phi holds NEXT \*)

(\* Path quantifier + Until \*) \| CTLAU : ctl_formula -\> ctl_formula -\> ctl_formula (\* A\[phi U psi\] \*) \| CTLEU : ctl_formula -\> ctl_formula -\> ctl_formula (\* E\[phi U psi\] \*)

(\* Derived operators \*) let ctl_af phi = CTLAU ctl_true phi (\* AF: inevitable \*) let ctl_ef phi = CTLEU ctl_true phi (\* EF: reachable \*) let ctl_ag phi = CTLNot (ctl_ef (CTLNot phi)) (\* AG: invariant \*) let ctl_eg phi = CTLNot (ctl_af (CTLNot phi)) (\* EG: persistent \*)

</div>

<div class="fstarcode">

(\* Use-after-free prevention: AG(freed(p) -\> AX AG(not deref(p))) \*) val use_after_free_safe : (node_id -\> bool) -\> (node_id -\> bool) -\> ctl_formula let use_after_free_safe is_free is_deref = let freed = CTLAtom is_free in let deref = CTLAtom is_deref in ctl_ag (CTLImplies freed (CTLAX (ctl_ag (CTLNot deref))))

(\* Resource leak detection: AG(acquire(r) -\> AF release(r)) \*) val resource_must_release : (node_id -\> bool) -\> (node_id -\> bool) -\> ctl_formula let resource_must_release is_acquire is_release = let acquired = CTLAtom is_acquire in let released = CTLAtom is_release in ctl_ag (CTLImplies acquired (ctl_af released))

(\* Deadlock freedom: AG(EF progress) \*) val deadlock_free : (node_id -\> bool) -\> ctl_formula let deadlock_free is_progress = ctl_ag (ctl_ef (CTLAtom is_progress))

</div>

## Frame Rule and Footprint Computation

**Source:** **\[Reynolds02\]** — “Separation Logic: A Logic for Shared Mutable Data Structures”

### The Frame Rule

<div class="theorem">

**Theorem 4.1** (Frame Rule — The Most Important Theorem for Compositional Analysis). *
``` math
\frac{\{P\}\; C\; \{Q\}}{\{P * R\}\; C\; \{Q * R\}}
```
*

***Provided:***

- *$`R`$ shares no modified variables with $`C`$*

- *$`R`$ does not mention locations freed by $`C`$*

***Meaning:** If $`C`$ is correct with precondition $`P`$ and postcondition $`Q`$, then $`C`$ is **also** correct when extra resources $`R`$ exist.*

***Why This Matters:***

- *Analyze functions in **isolation***

- *Caller’s resources automatically preserved*

- *Enables true **compositional** verification*

</div>

### Footprint Computation

<div class="fstarcode">

type footprint = reads : set loc; (\* Locations read \*) writes : set loc; (\* Locations written \*) allocates : set loc; (\* Locations allocated \*) frees : set loc; (\* Locations freed \*)

val compute_footprint : ir_stmt -\> footprint let rec compute_footprint stmt = match stmt with \| SRead dst ptr -\> empty_footprint with reads = singleton (resolve_loc ptr) \| SWrite ptr val\_ -\> empty_footprint with writes = singleton (resolve_loc ptr) \| SAlloc dst size -\> empty_footprint with allocates = fresh_abstract_loc () \| SFree ptr -\> empty_footprint with frees = singleton (resolve_loc ptr) \| SSeq s1 s2 -\> merge_footprint (compute_footprint s1) (compute_footprint s2) \| SIf cond s1 s2 -\> merge_footprint (compute_footprint s1) (compute_footprint s2) \| SWhile cond body -\> compute_footprint body \| SCall dst func args -\> lookup_footprint_summary func \| SPure \_ -\> empty_footprint

</div>

### Compositional Analysis Algorithm

**Algorithm: Analyze function with frame rule**

1.  **Compute Footprint:** $`\mathsf{footprint}(f) = \bigcup \{ \mathsf{footprint}(\mathit{stmt}) \mid \mathit{stmt} \in f.\mathit{body} \}`$

2.  **Generate Specification:**

    - Pre: resources needed by footprint

    - Post: resources produced/modified

3.  **At Call Sites:**

    - Caller has resources: $`P_{\mathit{caller}}`$

    - Callee needs: $`P_{\mathit{callee}}`$ (from spec)

    - Frame: $`R = P_{\mathit{caller}} - P_{\mathit{callee}}`$ (what’s left over)

    - Apply frame rule: $`\{P_{\mathit{callee}} * R\}\; \mathit{call}\; \{Q_{\mathit{callee}} * R\}`$

4.  **Verify Disjointness:**

    - Callee footprint $`\cap`$ $`R = \emptyset`$

    - No aliasing between callee access and frame

**Benefit:** Analyze callee **once**, reuse at all call sites!

### Magic Wands for Partial Data Structures

**Source:** Muller et al. 2016 (Viper), **\[Reynolds02\]**

<div class="definition">

**Definition 4.2** (Magic Wand (Separating Implication)). Magic wand $`A \mathbin{-\!\!*}B`$ means: “providing $`A`$ yields $`B`$”

**Semantics:** $`(A \mathbin{-\!\!*}B)`$ holds in state $`s`$ if:
``` math
\forall s'.\, s' \text{ disjoint from } s \implies A \text{ holds in } s' \implies B \text{ holds in } (s * s')
```

**Use Cases:**

- Partial data structure traversal: $`\mathsf{lseg}(\mathit{ptr}, \mathit{null}) \mathbin{-\!\!*}\mathsf{lseg}(\mathit{hd}, \mathit{null})`$

- Reborrowing specifications (Prusti)

- Modular verification of iterative algorithms

</div>

**Viper Syntax:**

      acc(lseg(ptr, null)) --* acc(lseg(hd, null))

    Operations:
      package (A --* B)   -- Create wand
      apply (A --* B)     -- Use wand: exchange A for B

**Complexity Warning:** Full magic wand reasoning is PSPACE-complete (Reynolds). Use for *specification* only. Prefer frame rule for implementation.

### Quantified Permissions for Arrays

**Source:** Muller et al. 2016 (Viper)

<div class="fstarcode">

(\* Enables pointwise permission specification for arrays and graphs: forall i: Int :: 0 \<= i && i \< len ==\> acc(loc(arr, i).val) \*)

type quantified_permission = index_var : string; index_type : ir_type; range_constraint : ir_expr; (\* e.g., 0 \<= i && i \< len \*) permission_body : sl_assertion; (\* e.g., acc(loc(arr, i).val) \*)

(\* Array permission: all elements accessible \*) val array_perm : arr:var_id -\> len:nat -\> quantified_permission let array_perm arr len = index_var = "i"; index_type = TInt; range_constraint = EBinOp OpAnd (EBinOp OpLeq (EInt 0) (EVar "i")) (EBinOp OpLt (EVar "i") (EInt len)); permission_body = SLPointsTo (EArrayAccess (EVar arr) (EVar "i")) PermFull;

</div>

**Advantages over Recursive Predicates:**

- Direct random access (no fold/unfold chains)

- Works for cyclic structures

- Efficient SMT encoding

### Automated Proof Search for Separation Logic

**Source:** Mulder et al. 2022 (Diaframe), Calcagno et al. 2009 (Bi-Abduction)

<div class="pillarbox">

**Key Insight:** Treat separation logic connectives as proof search instructions. Inspired by linear logic programming (Hodas & Miller).

**Automation Strategy:**

1.  Interpret connectives as search instructions:

    - $`*`$ (star) $`\rightarrow`$ ISplit: split goal into two

    - $`\mathbin{-\!\!*}`$ (wand) $`\rightarrow`$ IIntro: introduce hypothesis

    - $`\exists x`$ $`\rightarrow`$ IExists: postpone instantiation

    - $`\Rrightarrow`$ (update) $`\rightarrow`$ IUpdate: perform ghost state update

2.  Use bi-abduction for resource matching

3.  Apply domain-specific hints when stuck

**Diaframe Achieves:**

- Foundational soundness (proofs checked by Coq kernel)

- Practical automation (10x less manual proof than raw Iris)

- Extensibility via user-defined hints

</div>

## Representation Predicates

**Source:** VeriFFI (Wang et al. 2025) — “A Verified Foreign Function Interface between Coq and C”

<div class="pillarbox">

Representation predicates connect **high-level types** to **low-level memory**. This is critical for:

- FFI boundary verification

- Data structure correctness

- Garbage collection safety

**Key Insight:** Without representation predicates, we cannot prove that foreign function calls preserve type invariants across language boundaries.

</div>

<div class="definition">

**Definition 5.1** (Representation Predicate (InGraph)). $`\mathsf{InGraph}(g, x, p, t)`$ means:

- $`x`$ is a value of type $`t`$

- $`p`$ points to $`x`$’s representation in memory graph $`g`$

- The representation is **valid** according to $`t`$’s layout

</div>

<div class="fstarcode">

class rep_predicate (t : Type) =

(\* The predicate itself: graph -\> value -\> address -\> proposition \*) in_graph : memory_graph -\> t -\> address -\> bool;

(\* MONOTONICITY: Adding nodes doesn’t invalidate existing predicates. Critical for GC correctness. \*) monotone : squash (forall g g’ x p. extends g g’ -\> in_graph g x p -\> in_graph g’ x p);

(\* INJECTIVITY: Same address, same type =\> same value. A representation is deterministic. \*) injective : squash (forall g x y p. in_graph g x p -\> in_graph g y p -\> x == y);

</div>

<div class="theorem">

**Theorem 5.2** (GC Preservation (VeriFFI Lemma 3.2)). *Representation predicates are preserved under GC isomorphism. If $`x`$ is represented at $`p`$ in $`\mathit{old\_graph}`$, and GC maps $`p`$ to $`p'`$, then $`x`$ is represented at $`p'`$ in $`\mathit{new\_graph}`$.*

</div>

## GC-Aware Ownership Analysis

**Papers:** VeriFFI (Wang et al. 2025), RustBelt (**\[Jung18\]**)

<div class="pillarbox">

For languages with garbage collection (Java, Python, Go, OCaml), ownership analysis must account for:

- GC can **move** objects (address changes)

- GC can **collect** unreachable objects (lifetime shorter than scope)

- GC provides **implicit** memory safety

The key property: **representation preserved under GC**.

</div>

### GC-Isomorphism Definition

<div class="fstarcode">

type gc_state = heap : memory_graph; roots : set address; generation : nat; (\* GC generation/epoch \*)

type gc_isomorphism = source : gc_state; target : gc_state; address_map : map address address; (\* Old addr -\> new addr \*)

(\* Validity: bijection on reachable, preserves structure and values \*) val is_valid_gc_iso : gc_isomorphism -\> bool let is_valid_gc_iso iso = is_bijection iso.address_map (reachable iso.source) (reachable iso.target) && (\* Preserves pointer structure \*) (forall a b. points_to iso.source.heap a b ==\> points_to iso.target.heap (Map.find a iso.address_map) (Map.find b iso.address_map)) && (\* Preserves values \*) (forall a. iso.source.heap.values a = iso.target.heap.values (Map.find a iso.address_map))

</div>

### Ownership in GC Languages

<div class="fstarcode">

(\* For GC languages, we DON’T track deallocation (GC handles it). Instead, we track reachability, mutation, and finalization. \*)

type gc_ownership_state = \| GCRooted (\* Definitely reachable from a GC root \*) \| GCReachable (\* Reachable from some rooted object \*) \| GCMaybeCollected (\* May have been collected \*) \| GCFinalized (\* Finalizer has run - object is "dead" \*)

</div>

### GC at Language Boundaries

<div class="fstarcode">

(\* CRITICAL: When GC and non-GC languages interact via FFI, we must ensure GC doesn’t collect objects still referenced by the non-GC side. \*)

type boundary_gc_requirement = \| PinDuringCall (\* Object must not move during FFI call \*) \| RegisterAsRoot (\* Register pointer as GC root \*) \| CopyOut (\* Copy data out of GC heap \*) \| Opaque (\* Non-GC side cannot access GC heap \*)

val determine_gc_requirement : source_lang : language_idhas_gc source_lang -\> target_lang : language_id -\> value_type : ir_type -\> boundary_gc_requirement let determine_gc_requirement src tgt ty = if has_gc tgt then if same_gc_runtime src tgt then Opaque else RegisterAsRoot else if is_primitive ty then CopyOut else if is_small_struct ty then CopyOut else PinDuringCall

</div>

## WebAssembly Memory Safety Analysis

**Papers:** MS-Wasm (Disselkoen et al. 2019), WASM Security Survey (Perrone & Romano 2024)

<div class="pillarbox">

WebAssembly provides **isolation** from host (sandbox) but **not** memory safety *within* the sandbox. C/C++ vulnerabilities carry over when compiled to Wasm.

**MS-Wasm** extends Wasm with explicit memory safety semantics:

- **Segments:** bounded memory regions with lifetime tracking

- **Handles:** typed pointers that encapsulate bounds information

- **Progressive enforcement:** backends choose safety/performance tradeoff

**Key insight:** Capture C/C++ pointer semantics at *compile time* so backends can leverage *hardware features* (ARM MTE, PAC, CHERI) at runtime.

</div>

### WebAssembly Linear Memory Model

WebAssembly uses **linear memory**: a contiguous, mutable array of raw bytes.

- Loads and stores operate on untyped byte addresses (i32)

- No bounds metadata preserved from source language

- Stack-smashing mitigated (separate stack), but in-sandbox exploits remain

**Attack primitives that still work in WebAssembly:**

- Stack-based buffer overflow (into Wasm’s linear memory stack area)

- Heap metadata corruption

- Overwriting “constant” data in linear memory

- Redirecting indirect calls via corrupted function table indices

### MS-Wasm Segment Memory Model

<div class="fstarcode">

(\* HANDLE: A typed pointer with bounds information 4-tuple: (base, offset, bound, isCorrupted) \*)

type ms_wasm_handle = base : address; (\* Start of segment in segment memory \*) offset : int; (\* Current offset within segment \*) bound : nat; (\* Size of accessible region \*) is_corrupted : bool; (\* Set if handle integrity violated \*)

(\* Spatial safety check \*) val is_spatially_safe : ms_wasm_handle -\> bool let is_spatially_safe h = not (h = handle_null) && h.offset \>= 0 && h.offset \< h.bound && not h.is_corrupted

(\* Handle slicing for intra-object safety \*) val segment_slice : parent:ms_wasm_handle -\> new_base:nat -\> new_bound:natnew_base + new_bound \<= parent.bound -\> ms_wasm_handle let segment_slice parent new_base new_bound = base = parent.base + new_base; offset = 0; bound = new_bound; is_corrupted = parent.is_corrupted

</div>

### MS-Wasm Memory Safety Properties

**Three Memory Safety Properties** (de Amorim et al. 2017):

1.  **Spatial Safety:** no out-of-bounds reads/writes

2.  **Temporal Safety:** no use-after-free

3.  **Pointer Integrity:** no pointer forgery or corruption

### Hardware-Accelerated Memory Safety

<div class="fstarcode">

type hw_memory_safety_mechanism = \| HWSoftwareOnly (\* Pure software checks,  2x overhead \*) \| HWMemoryTagging (\* ARM MTE,  0 \| HWPointerAuth (\* ARM PAC,  20 \| HWCapabilities (\* CHERI, \<5

type safety_enforcement_level = \| EnforceFull (\* All three properties enforced \*) \| EnforceSpatialOnly (\* Bounds checking only \*) \| EnforceProbabilistic (\* MTE-style detection \*) \| EnforceNone (\* Performance mode, equivalent to plain Wasm \*)

</div>

### WebAssembly Analysis Landscape (2024 Survey)

**Source:** Perrone & Romano 2024 (121 papers surveyed)

**Analysis Categories:**

- **Static Analysis:** CFG analysis, context-sensitive data flow, CPG, symbolic semantic graphs

- **Dynamic Analysis:** Fuzzing, symbolic execution (EUNOMIA), concolic execution (SWAM), runtime instrumentation (Wasabi)

**Vulnerability Types Detected:**

- Smart Contracts: Fake transfer, reentrancy, rollback, missing auth

- Memory Safety: Buffer overflow, UAF, double-free, format string

- Side Channels: Cache timing, port contention, transient execution

**Security Enhancements:**

- CT-Wasm (Watt 2019): Type-driven constant-time for crypto

- MS-Wasm (Michael 2023): Memory safety via segment memory

- Formal verification: WasmCert-Isabelle, WasmCert-Coq

- Software Fault Isolation (SFI): VeriWasm verifier

### Integration with Brrr-Machine Analysis

**Integration Points:**

1.  **Part V (Pointer Analysis):**

    - Wasm linear memory as single abstract object

    - Function table indices as pointer-like values

    - MS-Wasm handles map directly to ownership analysis

2.  **Part VIII (Security Analysis):**

    - Taint analysis for Wasm imports (JS inputs are taint sources)

    - XSS/injection via Wasm-to-JS string passing

    - Cryptojacking detection (characteristic Wasm patterns)

3.  **Section <a href="#ch:gc-aware" data-reference-type="ref" data-reference="ch:gc-aware">6</a> (GC-Aware Analysis):**

    - JS-Wasm boundary: JS side is GC’d, Wasm side is manual

    - ArrayBuffer references must survive GC cycles

    - Similar to VeriFFI constraints

4.  **Part IX (Multi-Language):**

    - Wasm-JS boundary follows Matthews-Findler semantics

    - Type coercions at boundary (JS dynamic $`\to`$ Wasm static)

    - Gradual typing applicable to Wasm-JS interface

**Cross-References:**

- See Section <a href="#ch:gc-aware" data-reference-type="ref" data-reference="ch:gc-aware">6</a> for GC-aware ownership when Wasm interoperates with JS/Python

- See Part VIII for taint analysis applicable to Wasm imports

- See Part IX for boundary semantics (JS-Wasm follows Matthews-Findler)

# Security Analysis — Information Flow and Taint

<div class="warningbox">

**Source**: **\[Leijen14\]** (Koka), Theorems 2–4 — See Section 12.27

**Critical Insight**: Effect inference provides *mathematically proven* safety. If an effect is **absent** from the inferred effect row, the corresponding bug class is **impossible**—not just unlikely, but provably cannot occur.

**Effect-Based Bug Classification**:

<div class="center">

| **Condition** | **Guarantee** |
|:---|:---|
| $`\mathtt{exn} \notin \mathit{effects}`$ | **Proven exception-safe** (Theorem 12.27.1) |
|  | No unhandled exceptions possible—0% false positive |
| $`\mathtt{div} \notin \mathit{effects}`$ | **Proven terminating** (Theorem 12.27.2) |
|  | Guaranteed to halt—no infinite loops possible |
| $`\mathtt{st}\langle h \rangle \notin \mathit{effects}`$ | **Proven heap-isolated** (Theorem 12.27.3) |
|  | Cannot modify/observe heap $`h`$—state encapsulated |
| $`\mathtt{IO} \notin \mathit{effects}`$ | **Proven no external I/O** |
|  | Taint cannot exfiltrate via I/O sinks |

</div>

**Application to Taint Analysis**: Effect absence *strengthens* taint findings by eliminating false positives:

- Exception sinks unreachable if $`\mathtt{exn} \notin \mathit{effects}`$

- Heap-based taint flows impossible if $`\mathtt{st}\langle h \rangle \notin \mathit{effects}`$

- External data exfiltration impossible if $`\mathtt{IO} \notin \mathit{effects}`$

**Integration**: Use effect inference *before* taint analysis to prune impossible flows. Remaining flows have higher true-positive rate.

**Cross-references**: Section 12.27 for effect absence theorems and proofs; Section 12.34 for robust declassification theorems; Section 6.1.3 for effect row definitions.

</div>

## Information Flow and Taint Analysis

<div class="pillarbox">

**\[Denning77\]**, **\[Livshits05\]**, **\[Tripp09\]**

</div>

Security vulnerabilities are fundamentally about improper information flow: untrusted data reaching sensitive operations without proper validation.

### Denning’s Lattice Model

<div class="definition">

**Definition 1.1** (Security Lattice **\[Denning77\]**). Security levels form a *lattice*:

<div class="center">

</div>

</div>

<div class="definition">

**Definition 1.2** (Information Flow Rule). Information can flow from level $`L_1`$ to level $`L_2`$ *only if* $`L_1 \sqsubseteq L_2`$ ($`L_1`$ is below or equal to $`L_2`$):

- $`\mathsf{PUBLIC} \to \mathsf{SECRET}`$ (upgrading is safe)

- $`\mathsf{SECRET} \to \mathsf{PUBLIC}`$ (leaking is dangerous)

</div>

#### Taint Mapping

For taint analysis, we assign:
``` math
\begin{aligned}
\mathsf{UNTAINTED} &= \text{safe} = \text{low security} \\
\mathsf{TAINTED} &= \text{dangerous} = \text{high security}
\end{aligned}
```
Improper flow: $`\mathsf{TAINTED} \to \mathit{sensitive\_operation}`$

#### The 4-Point Lattice with Integrity

Denning’s original model tracks only **confidentiality** (secrecy). For robust declassification (Section <a href="#sec:declassification" data-reference-type="ref" data-reference="sec:declassification">1.4.3</a>, Section 12.34), we need **integrity** too.

<div class="definition">

**Definition 1.3** (Confidentiality and Integrity).

- **Confidentiality**: Can the adversary *observe* this data?

  - $`C_{\mathsf{Low}}`$ = Public (adversary can observe)

  - $`C_{\mathsf{High}}`$ = Secret (adversary cannot observe)

- **Integrity**: Can the adversary *influence* this data?

  - $`I_{\mathsf{Low}}`$ = Untrusted (adversary can control)

  - $`I_{\mathsf{High}}`$ = Trusted (adversary cannot control)

</div>

<div class="definition">

**Definition 1.4** (4-Point Product Lattice).

<div class="center">

</div>

Where:

- $`(C_{\mathsf{High}}, I_{\mathsf{High}})`$: Secret and Trusted (e.g., system password)

- $`(C_{\mathsf{High}}, I_{\mathsf{Low}})`$: Secret but Untrusted

- $`(C_{\mathsf{Low}}, I_{\mathsf{High}})`$: Public and Trusted

- $`(C_{\mathsf{Low}}, I_{\mathsf{Low}})`$: Public and Untrusted (adversary-controlled input)

**Ordering**: $`(c_1, i_1) \sqsubseteq (c_2, i_2)`$ iff $`c_1 \sqsubseteq_C c_2`$ **and** $`i_1 \sqsupseteq_I i_2`$

Note: Integrity ordering is *inverted* (higher integrity = lower risk).

</div>

<div class="warningbox">

For **robust declassification** (Section <a href="#sec:declassification" data-reference-type="ref" data-reference="sec:declassification">1.4.3</a>), we must ensure:

- Adversary cannot control *which* secret gets declassified

- Adversary cannot influence the *value* being declassified

This requires tracking integrity of control flow and data dependencies.

</div>

**Mapping Taint to 4-Point Lattice**:

- Taint source (user input) $`\Rightarrow \{ \mathit{conf} = C_{\mathsf{Low}}; \mathit{integ} = I_{\mathsf{Low}} \}`$

- Secret source (password) $`\Rightarrow \{ \mathit{conf} = C_{\mathsf{High}}; \mathit{integ} = I_{\mathsf{High}} \}`$

- Public constant $`\Rightarrow \{ \mathit{conf} = C_{\mathsf{Low}}; \mathit{integ} = I_{\mathsf{High}} \}`$

Cross-reference: Section 12.34 for complete formalization and theorems.

### The Taint Analysis Framework

The taint analysis framework provides the core infrastructure for tracking untrusted data as it flows through a program. The following F\* code defines the fundamental types used throughout the analysis:

- **Taint sources** represent entry points where untrusted data enters the system (HTTP parameters, environment variables, file reads, etc.)

- **Taint sinks** represent sensitive operations where untrusted data could cause harm (SQL queries, shell commands, HTML output, etc.)

- **Sanitizers** represent functions that transform untrusted data into safe values for specific contexts

The types are designed to be language-agnostic, supporting analysis across Python, JavaScript, Go, Rust, and other languages in the brrr-machine framework.

<div class="fstarcode">

(\* ================================================== TAINT ANALYSIS Sources: Denning 1977, Livshits 2005, Tripp 2009 ================================================== \*) module BrrrMachine.Security.Taint

(\* ————————————————– TAINT SOURCES — Where untrusted data enters ————————————————– \*) type taint_source = (\* User input \*) \| SrcHttpParam : param:string -\> taint_source \| SrcHttpHeader : header:string -\> taint_source \| SrcHttpBody : taint_source \| SrcCookie : name:string -\> taint_source \| SrcUrlPath : taint_source (\* Environment \*) \| SrcEnvVar : var:string -\> taint_source \| SrcCommandArg : index:nat -\> taint_source \| SrcStdin : taint_source (\* External data \*) \| SrcFileRead : taint_source \| SrcNetworkRead : taint_source \| SrcDatabaseRead : taint_source \| SrcDeserialize : format:string -\> taint_source (\* Language-specific \*) \| SrcEval : taint_source (\* JS eval, Python exec \*) \| SrcReflection : taint_source

</div>

<div class="fstarcode">

type taint_sink = (\* Injection vulnerabilities \*) \| SinkSqlQuery : taint_sink \| SinkSqlParam : taint_sink \| SinkShellCommand : taint_sink \| SinkShellArg : taint_sink \| SinkLdapQuery : taint_sink \| SinkXPathQuery : taint_sink \| SinkRegexPattern : taint_sink (\* XSS \*) \| SinkHtmlOutput : taint_sink \| SinkHtmlAttribute : taint_sink \| SinkJavaScript : taint_sink \| SinkCssValue : taint_sink (\* Path traversal \*) \| SinkFilePath : taint_sink \| SinkFileOpen : taint_sink (\* Deserialization \*) \| SinkDeserialize : format:string -\> taint_sink \| SinkYamlLoad : taint_sink \| SinkPickle : taint_sink (\* Code execution \*) \| SinkEval : taint_sink \| SinkExec : taint_sink \| SinkReflection : taint_sink (\* Sensitive data exposure \*) \| SinkLog : taint_sink \| SinkHttpResponse : taint_sink \| SinkErrorMessage : taint_sink (\* SSRF \*) \| SinkHttpRequest : taint_sink \| SinkUrlFetch : taint_sink

</div>

<div class="fstarcode">

(\* CRITICAL DISTINCTION (see Section 8.1.4.3, 12.34.9): - Sanitizer: Changes VALUE, preserves LABEL (e.g., HTML escape) - Declassification: Preserves VALUE, changes LABEL (policy decision)

Both may be needed: sanitize to prevent injection, declassify to allow controlled observation of secrets. \*)

type sanitizer = \| SanHtmlEscape : sanitizer \| SanUrlEncode : sanitizer \| SanSqlEscape : sanitizer \| SanSqlParameterize : sanitizer \| SanShellEscape : sanitizer \| SanPathCanonicalize : sanitizer \| SanInputValidation : pattern:string -\> sanitizer \| SanTypeCheck : expected:string -\> sanitizer \| SanLengthCheck : max_len:nat -\> sanitizer \| SanWhitelist : allowed:list string -\> sanitizer \| SanCustom : name:string -\> sanitizer

(\* Which sinks does a sanitizer protect? \*) let sanitizer_protects (san : sanitizer) (sink : taint_sink) : bool = match san, sink with \| SanHtmlEscape, (SinkHtmlOutput \| SinkHtmlAttribute) -\> true \| SanSqlEscape, (SinkSqlQuery \| SinkSqlParam) -\> true \| SanSqlParameterize, (SinkSqlQuery \| SinkSqlParam) -\> true \| SanShellEscape, (SinkShellCommand \| SinkShellArg) -\> true \| SanUrlEncode, SinkHttpRequest -\> true \| SanPathCanonicalize, (SinkFilePath \| SinkFileOpen) -\> true \| SanInputValidation \_, \_ -\> true (\* Depends on pattern \*) \| \_, \_ -\> false

</div>

<div class="fstarcode">

type vulnerability_type = \| VulnSqlInjection \| VulnCommandInjection \| VulnXss \| VulnPathTraversal \| VulnSsrf \| VulnDeserializationRce \| VulnLdapInjection \| VulnXPathInjection \| VulnLogInjection \| VulnReDoS \| VulnOpenRedirect \| VulnHeaderInjection \| VulnTemplateInjection

let sink_to_vuln (sink : taint_sink) : vulnerability_type = match sink with \| SinkSqlQuery \| SinkSqlParam -\> VulnSqlInjection \| SinkShellCommand \| SinkShellArg -\> VulnCommandInjection \| SinkHtmlOutput \| SinkHtmlAttribute \| SinkJavaScript -\> VulnXss \| SinkFilePath \| SinkFileOpen -\> VulnPathTraversal \| SinkHttpRequest \| SinkUrlFetch -\> VulnSsrf \| SinkDeserialize \_ \| SinkYamlLoad \| SinkPickle -\> VulnDeserializationRce \| SinkLdapQuery -\> VulnLdapInjection \| SinkXPathQuery -\> VulnXPathInjection \| SinkLog -\> VulnLogInjection \| SinkRegexPattern -\> VulnReDoS \| SinkEval \| SinkExec -\> VulnCommandInjection \| \_ -\> VulnXss (\* Default \*)

type taint_flow = source : taint_source; source_location : node_id; sink : taint_sink; sink_location : node_id; path : list node_id; (\* Nodes on the taint path \*) vuln_type : vulnerability_type; confidence : float; (\* 0.0 - 1.0, LEGACY - see Section 12.3 for Manifest/Latent \*)

type taint_analysis_result = flows : list taint_flow; source_count : nat; sink_count : nat; sanitizer_count : nat;

</div>

The taint analysis is formulated as an IFDS problem (see Section 4.1 for the IFDS framework). The key insight is that taint facts form a finite set (at most one fact per variable/access path), making the analysis tractable. The flow function propagates taint through assignments, kills taint at sanitizers, and generates taint at sources.

<div class="fstarcode">

val run_taint_analysis : cpg -\> sources:(node_id -\> option taint_source) -\> sinks:(node_id -\> option taint_sink) -\> sanitizers:(node_id -\> option sanitizer) -\> taint_analysis_result

let run_taint_analysis cpg sources sinks sanitizers = (\* Build IFDS problem \*) let problem = supergraph = cpg; domain = (\* all taint facts \*); zero = TaintZero; flow_function = fun edge d -\> let n = edge.source in (\* Check for source \*) begin match sources n with \| Some src -\> let var = get_assigned_var cpg n in Set.add (TaintedVar var src) (Set.singleton d) \| None -\> Set.singleton d end \|\> (\* Check for sanitizer \*) begin match sanitizers n with \| Some san -\> Set.filter (fun t -\> not (sanitizes san t)) \| None -\> identity end \|\> (\* Propagate through assignments \*) propagate_taint cpg n; (\* ... call/return flows ... \*) in (\* Solve IFDS \*) let solution = solve problem in (\* Check for tainted values at sinks \*) let flows = ref \[\] in Map.iter (fun node facts -\> match sinks node with \| Some sink -\> Set.iter (fun fact -\> match fact with \| TaintedVar var src -\> flows := source = src; source_location = 0; (\* TODO: track \*) sink = sink; sink_location = node; path = \[\]; (\* TODO: reconstruct \*) vuln_type = sink_to_vuln sink; confidence = 1.0; :: !flows \| \_ -\> () ) facts \| None -\> () ) solution; flows = !flows; source_count = count_sources cpg sources; sink_count = count_sinks cpg sinks; sanitizer_count = count_sanitizers cpg sanitizers

</div>

### Hybrid Thin Slicing

<div class="pillarbox">

Full taint analysis is expensive. Most paths are irrelevant. **Thin slicing**: Only follow dependencies that are *relevant* to the sink.

</div>

**Example**:

      x = user_input()      # Source
      y = sanitize(x)       # Sanitizer
      z = y + " suffix"     # Relevant
      w = unrelated()       # NOT relevant
      log(w)                # NOT relevant
      query(z)              # Sink

**Full backward slice** from `query(z)`:

- `query(z)` $`\leftarrow`$ `z` $`\leftarrow`$ `y` $`\leftarrow`$ `sanitize` $`\leftarrow`$ `x` $`\leftarrow`$ `user_input` $`\checkmark`$ Relevant

- `query(z)` $`\leftarrow`$ `z` $`\leftarrow`$ `" suffix"` Partially relevant

- `log(w)` $`\leftarrow`$ `w` $`\leftarrow`$ `unrelated()` $`\times`$ Not relevant!

**Thin slice**: Only follow `z` backward, ignoring `w`.

**Implementation**:

1.  Start from sink argument

2.  Track which variables are “relevant”

3.  Only follow data dependencies for relevant variables

4.  Stop at sanitizers for the sink type

This dramatically reduces analysis scope.

<div class="fstarcode">

val thin_slice_taint : cpg -\> sink_node:node_id -\> sink_arg:string -\> sanitizers:(node_id -\> option sanitizer) -\> set node_id (\* Nodes in the thin slice \*)

let thin_slice_taint cpg sink_node sink_arg sanitizers = let rec slice visited frontier relevant_vars = if Set.is_empty frontier then visited else (\* Get predecessors via data dependence \*) let edges = get_data_dep_edges cpg frontier in (\* Filter to relevant variables only \*) let relevant_edges = List.filter (fun e -\> let defined_var = get_defined_var cpg e.source in Set.mem defined_var relevant_vars ) edges in (\* Stop at sanitizers \*) let unsanitized = List.filter (fun e -\> match sanitizers e.source with \| Some san -\> not (sanitizer_protects san (get_sink_type sink_node)) \| None -\> true ) relevant_edges in let new_nodes = Set.of_list (List.map (fun e -\> e.source) unsanitized) in let new_nodes’ = Set.diff new_nodes visited in (\* Update relevant variables \*) let new_vars = Set.concat_map (fun n -\> get_used_vars cpg n ) new_nodes’ in slice (Set.union visited new_nodes’) new_nodes’ (Set.union relevant_vars new_vars) in slice (Set.singleton sink_node) (Set.singleton sink_node) (Set.singleton sink_arg)

</div>

<div class="fstarcode">

(\* Key insight: Most taint flows are LOCAL - tainted values are typically used close to where they were introduced. Processing near-source nodes first finds vulnerabilities faster and enables early termination. This is the "locality-of-taint" principle: prioritize analysis of nodes that are closer (in the PDG) to known taint sources. Cross-reference: Uses IFDS foundation from Section 4.1 (Reps 1995) \*)

module BrrrMachine.Security.PriorityTaint

type taint_priority_queue = queue : priority_queue (node_id \* taint_fact); distance_from_source : map node_id nat; source_nodes : set node_id;

(\* Initialize with sources at priority 0 \*) val init_priority_queue : cpg -\> (node_id -\> option taint_source) -\> taint_priority_queue let init_priority_queue cpg sources = let source_set = Set.filter (fun n -\> Option.is_some (sources n)) (all_nodes cpg) in let initial_dist = Set.fold (fun acc n -\> Map.add n 0 acc) Map.empty source_set in queue = PQueue.of_list (List.map (fun n -\> (n, initial_taint n, 0)) (Set.to_list source_set)); distance_from_source = initial_dist; source_nodes = source_set;

(\* Priority = distance from nearest source (lower = higher priority) \*) val priority_of_node : taint_priority_queue -\> node_id -\> nat let priority_of_node pq node = match Map.find node pq.distance_from_source with \| Some d -\> d \| None -\> max_int (\* Unknown nodes have lowest priority \*)

(\* Process taint in priority order - finds vulnerabilities faster \*) val process_priority_order : cpg -\> taint_priority_queue -\> sinks:(node_id -\> option taint_sink) -\> max_findings:nat -\> list taint_flow

let rec process_priority_order cpg pq sinks max_findings = if max_findings = 0 then \[\] else match PQueue.pop pq.queue with \| None -\> \[\] (\* Queue exhausted \*) \| Some ((node, fact), queue’) -\> let pq’ = pq with queue = queue’ in (\* Check if we reached a sink \*) match sinks node with \| Some sink -\> let flow = source = fact.source; sink = sink; path = \[\]; confidence = 1.0 in flow :: process_priority_order cpg pq’ sinks (max_findings - 1) \| None -\> (\* Propagate to successors with updated priorities \*) let successors = get_taint_successors cpg node fact in let pq” = List.fold_left (fun acc (succ_node, succ_fact) -\> let acc’ = update_distance acc node succ_node in let priority = priority_of_node acc’ succ_node in acc’ with queue = PQueue.push (succ_node, succ_fact) priority acc’.queue ) pq’ successors in process_priority_order cpg pq” sinks max_findings

</div>

<div class="pillarbox">

Priority-driven processing finds first vulnerability **10–100x faster** than breadth-first or depth-first orderings on large codebases.

</div>

#### Taint Finding Classification

Cross-reference: Section 12.3 (Manifest/Latent Framework)

After detecting taint flow via IFDS (Section 4.1) or TAJ priority-driven analysis, classify each finding using the Manifest/Latent framework:

1.  Build ISL triple from taint path:
    ``` math
    [\mathsf{emp}] \; \mathit{source\text{-}to\text{-}sink\text{-}path} \; [\mathit{tainted\_at\_sink}; E_r]
    ```

2.  Check manifest conditions (**\[Le22\]** Definition 3.3):

    - Empty presumption (any input triggers)?

    - Satisfiable result (path reachable)?

    - All heap locs existential?

    - Pure constraints universally satisfiable?

3.  Report based on classification:

    - **Manifest taint** $`\to`$ CRITICAL (0% FP guaranteed by theorem)

    - **Latent taint** $`\to`$ Report with triggering context

    - **Refuted** $`\to`$ Filter out (was false positive from IFDS)

This eliminates the heuristic “confidence score” approach. A manifest taint vulnerability is *proven* to be exploitable.

### Extended Information Flow Control

<div class="warningbox">

8.1.4.1 Implicit Flow (PC Label):  
Flows through control structure  
`if (h) { l := 1 }` — leaks `h` via branch taken

8.1.4.2 Multi-Principal (DLM):  
Multiple independent flow policies  
`{alice: alice,hospital; bob: bob,insurance}`

8.1.4.3 Declassification:  
Controlled release of secrets  
`declassify(secret, newLabel)` with robust declassification

8.1.4.4 Covert Channels:  
Timing, termination, side channels  
`while(secret){}` leaks via termination; timing attacks on crypto

8.1.4.5 Concurrent IFC:  
Thread interleavings as channels  
Scheduler-dependent flows, internal timing

</div>

#### Implicit Flow Analysis via PC Label

<div class="pillarbox">

**\[Sabelfeld03\]** (Sabelfeld & Myers 2003)

</div>

<div class="warningbox">

Explicit-only taint analysis is **insufficient** for security. Implicit flows through control structure leak secrets equally well.

</div>

**Example** — Both are insecure:

      l := h;                    // EXPLICIT flow (synthesis tracks this)

      if (h == secret) {         // IMPLICIT flow (synthesis MISSES this!)
          l := 1;
      }

In the second case, the *value* of `h` is revealed by whether `l` becomes 1. The assignment itself uses only low data, but the *branch* depends on high data.

<div class="definition">

**Definition 1.5** (PC Label). The *program counter* (pc) label tracks the security level of the current control context. It is elevated when entering high branches.

**Rule**: An assignment is secure only if:
``` math
\mathit{level}(\mathit{rhs}) \sqcup \mathit{pc} \sqsubseteq \mathit{level}(\mathit{lhs})
```

This rejects `l := 1` when `pc` is high and `l` is low.

</div>

<div class="fstarcode">

(\* ================================================== IMPLICIT FLOW ANALYSIS Source: Sabelfeld & Myers 2003 ================================================== \*) module BrrrMachine.Security.ImplicitFlow

(\* Extended taint state: includes PC label \*) type implicit_flow_state = var_label : map string security_level; pc : security_level; (\* Program counter security level \*) pc_sources : list node_id; (\* Where PC elevation came from \*)

(\* Security level operations \*) let level_join l1 l2 = match l1, l2 with \| SHigh, \_ \| \_, SHigh -\> SHigh \| SLow, SLow -\> SLow

let level_leq l1 l2 = match l1, l2 with \| SLow, \_ -\> true \| SHigh, SHigh -\> true \| \_, \_ -\> false

(\* Transfer function with implicit flow detection \*) type implicit_finding = \| ExplicitFlow : var:string -\> source:node_id -\> implicit_finding \| ImplicitFlow : var:string -\> pc_source:node_id -\> implicit_finding \| TerminationChannel : loop_loc:node_id -\> implicit_finding

val transfer_implicit : cpg_node -\> implicit_flow_state -\> (implicit_flow_state \* list implicit_finding)

let transfer_implicit node state = match node.kind with (\* Assignment: check BOTH explicit AND implicit flow \*) \| NAssign var -\> let rhs_level = compute_expr_level state (get_rhs node) in (\* CRITICAL: Include PC in effective level \*) let effective_level = level_join rhs_level state.pc in let var_declared = get_declared_label var in if not (level_leq effective_level var_declared) then let finding = if state.pc = SHigh && rhs_level = SLow then ImplicitFlow var (List.hd state.pc_sources) (\* Pure implicit \*) else ExplicitFlow var (get_source_node state) (\* Explicit \*) in (state, \[finding\]) else ( state with var_label = Map.add var effective_level state.var_label , \[\])

(\* Conditional: ELEVATE PC \*) \| NIf -\> let cond_level = compute_expr_level state (get_condition node) in let new_pc = level_join state.pc cond_level in ( state with pc = new_pc; pc_sources = if cond_level = SHigh then node.id :: state.pc_sources else state.pc_sources , \[\])

(\* Loop: ELEVATE PC + warn about termination channel \*) \| NWhile -\> let cond_level = compute_expr_level state (get_condition node) in let findings = if cond_level = SHigh then \[TerminationChannel node.id\] else \[\] in ( state with pc = level_join state.pc cond_level; pc_sources = node.id :: state.pc_sources , findings)

(\* Scope exit: RESTORE PC (static analysis advantage!) \*) \| NJoin -\> ( state with pc = get_enclosing_pc node; pc_sources = get_enclosing_sources node , \[\]) \| \_ -\> (state, \[\])

(\* Noninterference soundness theorem — See Section 12.2 for full statement implicit_analysis_sound: If analysis reports no leaks, noninterference holds. \*)

</div>

#### Multi-Principal Labels (DLM)

<div class="pillarbox">

**\[Myers97\]** (Decentralized Label Model)

</div>

<div class="warningbox">

Simple taint lattice: $`T_{\bot} < T_{\mathsf{Untainted}} < T_{\mathsf{Maybe}} < T_{\mathsf{Tainted}}`$

**Problem**: Cannot express “Alice’s data readable by Alice+Hospital, Bob’s data readable by Bob+Insurance” — two independent policies.

**DLM Solution**: Labels have *multiple owners*, each with their own reader sets.

**Example**: `{alice: alice,hospital; bob: bob,insurance}` — two independent flow policies in the same label.

DLM *subsumes* simple taint:

- $`T_{\mathsf{Tainted}} = \{\mathit{source}: \mathit{source}\}`$ (only source can read)

- $`T_{\mathsf{Untainted}} = \{\}`$ (empty label, anyone can read)

</div>

<div class="fstarcode">

(\* ================================================== DECENTRALIZED LABEL MODEL (DLM) Source: Myers 1997 ================================================== \*) module BrrrMachine.Security.DLM

type principal = string

(\* DLM LABEL: Multiple owners, each with their own reader set Label = o1: r1,r2; o2: r3,r4 means: owner o1 allows readers r1,r2 owner o2 allows readers r3,r4 \*) type dlm_label = owners : set principal; readers : principal -\> set principal; (\* readers for each owner \*)

(\* Empty label: anyone can read (public) \*) let dlm_public : dlm_label = owners = Set.empty; readers = fun \_ -\> Set.all

(\* Tainted label: only source can read \*) let dlm_tainted (source : principal) : dlm_label = owners = Set.singleton source; readers = fun o -\> if o = source then Set.singleton source else Set.all;

(\* Effective reader set: intersection of all owner’s reader sets \*) let effective_readers (l : dlm_label) : set principal = if Set.is_empty l.owners then Set.all else Set.fold (fun acc owner -\> Set.inter acc (l.readers owner) ) Set.all l.owners

(\* DLM LATTICE OPERATIONS \*)

(\* Join: union owners, intersect readers — more restrictive \*) let dlm_join (l1 l2 : dlm_label) : dlm_label = owners = Set.union l1.owners l2.owners; readers = fun o -\> let r1 = if Set.mem o l1.owners then l1.readers o else Set.all in let r2 = if Set.mem o l2.owners then l2.readers o else Set.all in Set.inter r1 r2;

(\* Restriction check: l1 \<= l2 iff l1 allows at least what l2 allows \*) let dlm_leq (l1 l2 : dlm_label) : bool = (\* l2 has subset of owners \*) Set.subset l2.owners l1.owners && (\* For each owner in l2, l2’s readers subset of l1’s readers \*) Set.for_all (fun o -\> Set.subset (l2.readers o) (l1.readers o) ) l2.owners

</div>

<div class="fstarcode">

(\* Principal hierarchy (acts-for relation) Enables groups, roles, and delegation. If p acts-for q, then p can do anything q can do. \*) type principal_hierarchy = principals : set principal; acts_for : principal -\> principal -\> bool; (\* Reflexive and transitive \*) acts_for_refl : squash (forall p. acts_for p p); acts_for_trans : squash (forall p q r. acts_for p q && acts_for q r ==\> acts_for p r);

(\* Can process acting as ’actor’ declassify data owned by ’owner’? \*) let can_declassify (ph : principal_hierarchy) (actor owner : principal) : bool = ph.acts_for actor owner

(\* DECLASSIFICATION: Authorized release of information Rule 1 (Restriction): Anyone can make data MORE restrictive Rule 2 (Declassification): Only owner (or delegate) can make LESS restrictive \*) type relabel_result = RelabelOK \| RelabelDenied of string

let check_relabel (ph : principal_hierarchy) (actor : principal) (from_label to_label : dlm_label) : relabel_result = if dlm_leq from_label to_label then (\* Restriction: always allowed (making more restrictive) \*) RelabelOK else (\* Declassification: need authority \*) let owners_losing_restriction = Set.filter (fun o -\> not (Set.subset (to_label.readers o) (from_label.readers o)) ) from_label.owners in if Set.for_all (can_declassify ph actor) owners_losing_restriction then RelabelOK else RelabelDenied ("Actor " ^ actor ^ " cannot declassify for all affected owners")

</div>

#### Declassification and Endorsement

<div class="pillarbox">

**\[Zdancewic01\]**, **\[Chong04\]** (Security Policies for Downgrading), **\[Sabelfeld09\]**

</div>

<div class="warningbox">

**Key Insight**: Declassification is not just “allow this flow” — it requires precise specification of **WHAT**, **WHERE**, **WHO**, and **WHEN** information is released. Without robustness, attackers can manipulate what gets declassified.

Cross-reference: Section 12.34 for complete F\* formalization.

</div>

##### The Fundamental Problem

Strict noninterference is **too restrictive** for real systems. Real systems *must* release some secrets:

- Password comparison results (boolean)

- Encrypted versions of data

- Statistical aggregates over private data

- Authentication tokens derived from secrets

But **uncontrolled** declassification creates “laundering” vulnerabilities where attackers can influence *what* gets declassified.

##### Semantic Security under Declassification

Traditional noninterference:
``` math
M_1 \sim_L M_2 \implies P(M_1) \sim_L P(M_2)
```
“Low-equivalent inputs produce low-equivalent outputs”

Security under declassification **\[Chong04\]**:
``` math
M_1 \sim_L M_2 \implies \mathit{visible}(P(M_1)) \sim_L \mathit{visible}(P(M_2))
```
where $`\mathit{visible}(\mathit{trace})`$ extracts *only* the declassified values.

The key change: we don’t require *all* outputs to be equivalent, only the *observable* outputs (public + explicitly declassified).

##### Delimited Release: WHAT Can Be Released

      declassify(expression, target_label)

The **expression** defines an “escape hatch” — only the information computed by that expression may be released.

- **Good**: `declassify(password == guess, public)`  
  Only releases **boolean** (one bit), not the password.

- **Bad**: `declassify(password, public)`  
  Releases entire password — too much!

##### Robust Declassification: Integrity-Based Security

**The Attack Model**: Attacker controls LOW inputs (low integrity) but should **not** be able to:

1.  Influence *which* secret gets declassified

2.  Influence *how much* of a secret gets declassified

**Non-Robust (Vulnerable)**:

      if (attacker_controlled) {
        declassify(secret1, public)
      } else {
        declassify(secret2, public)
      }
      // Attacker controls WHICH secret is released!

**Robust Declassification Condition**: For expression $`e`$ in `declassify(e, L)`:

- The PC (program counter) must have **high integrity** at declassification

- Expression $`e`$ must **not** depend on low integrity inputs

- The **path** to declassification must be attacker-independent

<div class="warningbox">

Robustness requires tracking **integrity**, not just confidentiality. The synthesis upgrades to 4-point security labels.

</div>

##### The Four Dimensions of Declassification **\[Sabelfeld09\]**

1.  **WHAT** — What information is released (delimited release)  
    Escape hatches define the *function* of secrets that is released.

2.  **WHERE** — At what program points declassification is allowed  
    Only specific code locations may perform declassification.  
    E.g., only the authentication module may release auth results.

3.  **WHO** — By whose authority  
    Principals must have ownership of the secret to authorize release.  
    Integrates with DLM (Section <a href="#sec:dlm" data-reference-type="ref" data-reference="sec:dlm">1.4.2</a>): acts-for hierarchy.

4.  **WHEN** — Under what conditions  
    State-dependent policies: “release only after encryption”  
    Temporal constraints: “release only during session”

5.  **SPECULATION-AWARE** (SPECTECTOR extension, Section <a href="#sec:speculative" data-reference-type="ref" data-reference="sec:speculative">1.4.6</a>): Declassification must **not** occur on **speculative** paths!

##### Endorsement: The Integrity Dual

Endorsement is the dual of declassification for **integrity**:

- **Declassification**: $`\mathit{high\_confidentiality} \to \mathit{low\_confidentiality}`$

- **Endorsement**: $`\mathit{low\_integrity} \to \mathit{high\_integrity}`$

<!-- -->

      endorse(untrusted_input, high_integrity)

**Robust Endorsement Condition** (dual of robust declassification): High-*confidentiality* code cannot influence *what* gets endorsed. This prevents secrets from leaking via the choice of trusted values.

##### Sanitizers vs Declassification: Critical Distinction

<div class="warningbox">

**Sanitizer**: Changes **value**, keeps **label**

    sanitize(user_input) -> safe_value

The value is *transformed* (e.g., HTML escaping). The security label remains unchanged (still “from user input”).

**Declassification**: Keeps **value**, changes **label**

    declassify(secret, public) -> secret

The value is *unchanged*. The security label is downgraded (high $`\to`$ low confidentiality).

**Both** are needed for complete security analysis:

- Sanitizers prevent injection attacks (transform dangerous values)

- Declassification allows controlled information release (change policy)

</div>

<div class="fstarcode">

(\* ================================================== DECLASSIFICATION AND ENDORSEMENT ANALYSIS Source: Chong & Myers 2004 "Security Policies for Downgrading"

KEY INSIGHT: Security under declassification requires tracking BOTH confidentiality AND integrity. Robustness is an INTEGRITY property.

Cross-reference: Section 12.34 for complete theorems and proofs. ================================================== \*) module BrrrMachine.Security.Declassification

(\* SECURITY LABELS WITH BOTH CONFIDENTIALITY AND INTEGRITY This is the critical upgrade from simple taint: 4-point lattice. \*) type confidentiality_level = CLow \| CHigh type integrity_level = ILow \| IHigh

type security_label = conf : confidentiality_level; (\* Can information be observed? \*) integ : integrity_level; (\* Can information be trusted? \*)

(\* Label ordering: (c1, i1) \<= (c2, i2) iff c1 \<= c2 AND i1 \>= i2 Higher confidentiality = more secret = higher in conf lattice Higher integrity = more trusted = LOWER risk = inverted ordering \*) val label_leq : security_label -\> security_label -\> bool let label_leq l1 l2 = (l1.conf = CLow \|\| l2.conf = CHigh) && (\* conf: Low \<= High \*) (l1.integ = IHigh \|\| l2.integ = ILow) (\* integ: High \<= Low (inverted!) \*)

val label_join : security_label -\> security_label -\> security_label let label_join l1 l2 = conf = if l1.conf = CHigh \|\| l2.conf = CHigh then CHigh else CLow; integ = if l1.integ = ILow \|\| l2.integ = ILow then ILow else IHigh;

</div>

<div class="fstarcode">

(\* DECLASSIFICATION POLICY: Full Four-Dimensional Specification \*) type declassification_policy = what : list escape_hatch; (\* WHAT can be released \*) where : set node_id; (\* WHERE declassification allowed \*) who : set principal; (\* WHO can authorize \*) when_conditions : list ir_expr; (\* WHEN conditions required \*) robust_required : bool; (\* Require integrity-based robustness \*)

(\* VIOLATION TYPES: Comprehensive Error Reporting \*) type declassify_violation = (\* Robustness violations - attacker influence detected \*) \| LowIntegrityControl : declass_node:node_id -\> influencing_node:node_id -\> declassify_violation (\* Low-integrity code path can reach declassification \*) \| LowIntegrityData : var:string -\> declass_node:node_id -\> declassify_violation (\* Low-integrity data flows into declassified expression \*)

(\* Policy violations \*) \| ExcessiveRelease : released:ir_expr -\> allowed:ir_expr -\> declassify_violation (\* Expression releases more than escape hatch allows \*) \| Unauthorized : required:principal -\> actual:principal -\> declassify_violation (\* Principal lacks authority to declassify \*) \| WrongLocation : actual:node_id -\> allowed:set node_id -\> declassify_violation (\* Declassification at unauthorized code location \*) \| ConditionNotMet : required:ir_expr -\> actual_state:ir_state -\> declassify_violation (\* When-condition not satisfied \*)

</div>

<div class="fstarcode">

(\* This is the key contribution of Chong & Myers 2004. \*) val check_robustness : cpg:cpg -\> declass_node:node_id -\> hatch:escape_hatch -\> option declassify_violation

let check_robustness cpg declass_node hatch = (\* CHECK 1: PC integrity must be HIGH at declassification point If low-integrity code can reach this point, attacker controls WHETHER declassification occurs. \*) let pc_integ = get_pc_integrity cpg declass_node in if pc_integ = ILow then let influencing = get_pc_integrity_source cpg declass_node in Some (LowIntegrityControl declass_node influencing) else (\* CHECK 2: No low-integrity data dependencies If the declassified expression depends on attacker-controlled data, attacker influences WHAT is declassified. \*) let expr = get_declassified_expr cpg declass_node in let deps = backward_slice cpg expr in let low_integ_deps = Set.filter (fun n -\> (get_label cpg n).integ = ILow ) deps in if not (Set.is_empty low_integ_deps) then let bad_var = get_var_name (Set.choose low_integ_deps) in Some (LowIntegrityData bad_var declass_node) else (\* CHECK 3: Expression matches escape hatch semantically \*) if not (expr_semantically_matches expr hatch.expr) then Some (ExcessiveRelease expr hatch.expr) else (\* CHECK 4: Authority check \*) let current_principal = get_current_principal cpg declass_node in if not (has_authority current_principal hatch.authority) then Some (Unauthorized hatch.authority current_principal) else None

</div>

#### Covert Channels

<div class="pillarbox">

**\[Agat00\]** (Timing), **\[Sabelfeld00\]** (Probabilistic)

</div>

**Covert Channels**: Information leaks through *side effects* of computation.

1.  **Termination Channel** (already in Section <a href="#sec:implicit-flow" data-reference-type="ref" data-reference="sec:implicit-flow">1.4.1</a>):

        while (secret) { loop_forever(); }

    Leaks secret via whether program terminates.

2.  **Timing Channel**:

        if (secret) { expensive_operation(); }
        else { cheap_operation(); }

    Leaks secret via execution time difference.

3.  **Cache Timing**:

        array[secret * 256]  // Cache side channel (Spectre-style)

    Memory access patterns leak secret.  
    See Section <a href="#sec:speculative" data-reference-type="ref" data-reference="sec:speculative">1.4.6</a> for speculative execution attacks (Spectre).  
    See Section <a href="#sec:constant-time" data-reference-type="ref" data-reference="sec:constant-time">1.4.7</a> for constant-time verification (CT-Verif).

4.  **Storage Channel**:  
    Allocation patterns, file sizes, etc.

<div class="definition">

**Definition 1.6** (Timing-Sensitive Noninterference **\[Agat00\]**). Programs must take *same time* regardless of secrets.

**Requirement**: All branches on secrets must be “balanced”:

    if (secret) { A } else { B }

requires: $`\mathit{time}(A) = \mathit{time}(B)`$

**Cross-copying transformation**:

    if (secret) { A; skip_B } else { skip_A; B }

where `skip_X` takes same time as `X` but does nothing.

</div>

**Timing Security Layers** (complete coverage):

1.  Instruction timing (**\[Agat00\]**) — balance branch execution

2.  Memory access patterns (CT-Verif, Section <a href="#sec:constant-time" data-reference-type="ref" data-reference="sec:constant-time">1.4.7</a>) — constant-time verification

3.  Speculative execution (SPECTECTOR, Section <a href="#sec:speculative" data-reference-type="ref" data-reference="sec:speculative">1.4.6</a>) — microarchitectural security

4.  Countermeasure verification — LFENCE, SLH placement

<div class="fstarcode">

type timing_violation = \| UnbalancedBranch : branch:node_id -\> diff:time_estimate -\> timing_violation \| SecretDependentLoop : loop:node_id -\> timing_violation \| VariableTimeOp : op:node_id -\> op_name:string -\> timing_violation

type time_estimate = \| Constant of nat \| Linear of string (\* Linear in variable \*) \| Unknown

val analyze_timing : cpg -\> list timing_violation let analyze_timing cpg = let violations = ref \[\] in (\* Check all branches on secret conditions \*) iter_nodes cpg (fun node -\> match node.kind with \| NIf when condition_is_secret cpg node -\> let then_time = estimate_time cpg (get_then_branch node) in let else_time = estimate_time cpg (get_else_branch node) in if not (times_equal then_time else_time) then violations := UnbalancedBranch node (time_diff then_time else_time) :: !violations \| NWhile when condition_is_secret cpg node -\> violations := SecretDependentLoop node :: !violations \| NCall when is_variable_time_op cpg node -\> if any_arg_is_secret cpg node then violations := VariableTimeOp node (get_func_name node) :: !violations \| \_ -\> () ); !violations

(\* Variable-time operations to flag \*) let variable_time_ops = \[ "strcmp"; "memcmp"; (\* String comparison - timing varies \*) "division"; "modulo"; (\* Some CPUs have variable-time div \*) "branch_on_secret"; (\* Any conditional on secret \*) \]

</div>

#### Concurrent Information Flow

<div class="pillarbox">

**\[Smith98\]** (Smith & Volpano 1998), **\[Russo06\]** (Russo & Sabelfeld 2006)

</div>

**Concurrent IFC Challenges**:

1.  **Internal Timing**:

        Thread 1: if (secret) { sleep(1000); } l1 := 1;
        Thread 2: sleep(500); l2 := l1;

    Thread 2 observes *whether* Thread 1 took the branch via timing.

2.  **Scheduler Channels**:  
    Thread scheduling decisions may depend on secrets. Especially problematic with priority inheritance.

3.  **Shared Memory Races**:

        Thread 1: if (secret) { x := 1; }
        Thread 2: l := x;

    Value of `l` depends on interleaving *and* secret.

**Solutions**:

1.  **Observational Determinism** (**\[McLean92\]**): Low-equivalent inputs $`\Rightarrow`$ same low observations regardless of scheduler

2.  **Strong Security** (Sabelfeld & Sands): Require timing-insensitivity *plus* scheduler-independence

3.  **Practical Approach**:

    - Ban races on shared variables with different security levels

    - Require synchronized access to shared secrets

    - Flag any shared variable accessed by threads with different clearances

<div class="fstarcode">

type concurrent_ifc_violation = \| SharedSecretRace : var:string -\> threads:list thread_id -\> concurrent_ifc_violation \| InternalTimingLeak : high_thread:thread_id -\> observer:thread_id -\> concurrent_ifc_violation \| UnsyncedCrossLevelAccess : var:string -\> writer_level:security_level -\> reader_level:security_level -\> concurrent_ifc_violation

val analyze_concurrent_ifc : cpg -\> list concurrent_ifc_violation let analyze_concurrent_ifc cpg = let violations = ref \[\] in let shared_vars = find_shared_variables cpg in Set.iter (fun var -\> let accessors = get_accessing_threads cpg var in let levels = List.map (fun t -\> get_thread_clearance t) accessors in (\* Check 1: Race on secret \*) if get_var_level var = SHigh && has_race cpg var then violations := SharedSecretRace var accessors :: !violations; (\* Check 2: Cross-level access without sync \*) let writers = get_writers cpg var in let readers = get_readers cpg var in List.iter (fun w -\> List.iter (fun r -\> if get_level w \<\> get_level r && not (synced_access cpg w r) then violations := UnsyncedCrossLevelAccess var (get_level w) (get_level r) :: !violations ) readers ) writers; ) shared_vars; !violations

</div>

#### Speculative Execution Security

<div class="pillarbox">

**\[Guarnieri20\]** (SPECTECTOR — Guarnieri, Kopf, Morales, Reineke, Sanchez 2020)

</div>

**Speculative execution** creates covert channels through *microarchitectural* state. Even with perfect constant-time code, speculative execution can leak secrets through cache side channels (Spectre family attacks).

**The Problem**: Modern CPUs speculatively execute past branches before they resolve. On misprediction, **architectural** state rolls back. But **microarchitectural** state (caches) is *not* rolled back. This creates observable side channels.

**Spectre Gadget Example**:

    if (x < array1_size) {           // Mispredicted as true when false
      y = array1[x];                 // Speculatively loads secret
      z = array2[y * 256];           // Leaks secret via cache
    }                                // Rollback - but cache state persists!

<div class="definition">

**Definition 1.7** (Speculative Non-Interference (SNI)). Two low-equivalent initial memories produce distinguishable speculative observations *only if* they produce distinguishable non-speculative observations.

Formally:
``` math
\forall M_1, M_2.\; \mathit{low\_equiv}(M_1, M_2) \implies
```
``` math
(\mathit{obs\_eq}(\mathit{run\_spec}(P, M_1), \mathit{run\_spec}(P, M_2)) \implies
 \mathit{obs\_eq}(\mathit{run\_std}(P, M_1), \mathit{run\_std}(P, M_2)))
```

</div>

**Key Insight: Worst-Case Branch Predictor Abstraction**

Instead of modeling specific CPUs, use a predictor that **always mispredicts**. Sound: secure against worst-case $`\Rightarrow`$ secure against any real predictor.

This is a Galois connection:

<div class="center">

Concrete predictors $`\{`$<!-- -->1-bit, 2-bit, neural, ...$`\}`$  
$`\downarrow`$ (abstraction)  
Abstract predictor: always-mispredict (worst case)

</div>

**Observer Model** (compatible with Section <a href="#sec:covert-channels" data-reference-type="ref" data-reference="sec:covert-channels">1.4.4</a>):

1.  Memory access addresses (cache timing)

2.  Jump targets (branch prediction)

**Countermeasure Verification**: SPECTECTOR can verify that mitigations are correctly placed:

- **LFENCE**: Serializing instruction forces rollback

- **SLH** (Speculative Load Hardening): Masks speculative values

- **Retpoline**: Return trampoline for indirect branches

<div class="fstarcode">

module BrrrMachine.Security.Speculative

(\* Observation types for cache timing attacks \*) type observation = \| ObsMemAccess : addr:nat -\> observation \| ObsJumpTarget : target:nat -\> observation

type obs_trace = list observation

(\* Speculation state \*) type spec_state = pc : nat; regs : map nat int; mem : map nat int; spec_depth : nat; (\* Current speculation depth \*) mispredict_stack : list (nat \* map nat int); (\* Recovery points \*) trace : obs_trace; (\* Accumulated observations \*)

(\* Speculation window bound (typical for Intel CPUs) \*) let max_spec_window : nat = 200

(\* Speculative step: fork on BOTH branch outcomes \*) val spec_branch : spec_state -\> nat -\> nat -\> list spec_state let spec_branch st true_target false_target = if st.spec_depth \< max_spec_window then let cond_val = eval_condition st in let (correct, wrong) = if cond_val then (true_target, false_target) else (false_target, true_target) in \[ (\* Non-speculative: take correct path \*) record_jump st with pc = correct correct; (\* Speculative: take WRONG path (misprediction) \*) record_jump st with pc = wrong; spec_depth = st.spec_depth + 1; mispredict_stack = (correct, st.regs) :: st.mispredict_stack wrong \] else (\* Speculation window exhausted - rollback \*) rollback st

</div>

<div class="fstarcode">

type spectre_gadget = secret_load : node_id; (\* Load that accesses secret \*) dependent_access : node_id; (\* Memory access dependent on secret \*) speculation_path : list node_id; (\* Path through speculative execution \*)

val detect_spectre_gadgets : cpg -\> (nat -\> security_level) -\> nat -\> list spectre_gadget let detect_spectre_gadgets cpg policy window = (\* Symbolic execution under speculative semantics \*) let spec_states = symbolic_spec_execute cpg window in List.filter_map (fun state -\> let spec_loads = get_speculative_secret_loads state policy in let dependent = get_dependent_memory_accesses state spec_loads in if List.length dependent \> 0 then Some secret_load = List.hd spec_loads; dependent_access = List.hd dependent; speculation_path = state.path else None ) spec_states

type countermeasure = \| LFENCE : loc:node_id -\> countermeasure \| SLH : loc:node_id -\> masked_reg:nat -\> countermeasure \| Retpoline : loc:node_id -\> countermeasure

val countermeasure_blocks_gadget : cpg -\> spectre_gadget -\> countermeasure -\> bool let countermeasure_blocks_gadget cpg gadget cm = match cm with \| LFENCE loc -\> (\* LFENCE must dominate the vulnerable load \*) dominates cpg loc gadget.secret_load && (\* No speculative path bypasses the fence \*) not (exists_speculative_bypass cpg loc gadget.speculation_path) \| SLH loc masked_reg -\> (\* SLH must mask the value used in dependent access \*) slh_masks_dependency cpg loc gadget.dependent_access masked_reg \| Retpoline loc -\> (\* Retpoline must replace indirect branch \*) is_indirect_branch cpg loc && loc ‘List.mem‘ gadget.speculation_path

(\* Theorem: Correctly placed countermeasure establishes SNI \*) val countermeasure_soundness : cpg:cpg -\> gadget:spectre_gadget -\> cm:countermeasure -\> policy:(nat -\> security_level) -\> window:nat -\> Lemma (requires countermeasure_blocks_gadget cpg gadget cm) (ensures sni (apply_countermeasure (cpg_to_ir cpg) cm) policy window)

</div>

#### Constant-Time Verification via Product Programs

<div class="pillarbox">

**\[Almeida16\]** (CT-Verif — Almeida, Barbosa, Barthe, Dupressoir, Emmi 2016)

</div>

**Constant-Time Security**: Execution time and memory access patterns must be independent of secret data.

**The Problem**:

1.  Execution time variations leak via branch timing

2.  Memory access patterns leak via cache timing

3.  Variable-time operations (div, modulo) leak via operand-dependent timing

4.  Compiler optimizations may introduce violations

**Constant-Time is a 2-Safety Hyperproperty**: Relates *two* executions of the same program. Cannot be verified by analyzing single executions.

<div class="definition">

**Definition 1.8** (CT-Verif Insight: Product Program Reduction). Given program $`P`$, construct product program $`Q`$ that:

- Maintains two shadow copies of state (original + shadow)

- Executes both copies in lockstep along identical control paths

- Asserts observations match at each step

$`P`$ is constant-time $`\iff`$ $`Q`$ is assertion-safe

Reduction verified in Coq.

</div>

**Leakage Models** (parameterized observer power):

1.  **PC Model**: Only control flow visible (branch-prediction attacks)

2.  **Memory Model**: PC + memory addresses (cache-timing attacks)

3.  **Operand Model**: PC + addresses + operand values (variable-time instructions)

**Output-Sensitive Security**: Allow intentional leaks bounded by public outputs. Distinguishes benign leaks from true vulnerabilities.

**Architecture-Aware Variable-Time Operations**:

<div class="center">

| **Operation** | **x86_64** | **ARM_v8** | **RISC-V** |     |
|:--------------|:----------:|:----------:|:----------:|:---:|
| `div`         |    VAR     |    VAR     |   CONST    |     |
| `mod`         |    VAR     |    VAR     |   CONST    |     |
| `fdiv`        |    VAR     |    VAR     |    VAR     |     |
| `strcmp`      |    VAR     |    VAR     |    VAR     |     |
| `memcmp`      |    VAR     |    VAR     |    VAR     |     |

</div>

<div class="fstarcode">

module BrrrMachine.Security.ConstantTime

(\* LEAKAGE MODELS \*) type leakage_model = \| LeakPC (\* Control flow only - branch prediction attacks \*) \| LeakMemory (\* PC + memory addresses - cache timing attacks \*) \| LeakOperand (\* Full operand values - variable-time instructions \*)

type ct_observation = \| ObsPC : pc:nat -\> ct_observation \| ObsMem : pc:nat -\> addr:nat -\> ct_observation \| ObsOp : pc:nat -\> addr:nat -\> operands:list nat -\> ct_observation

(\* CONSTANT-TIME SECURITY DEFINITION \*) type program_state = map string int type trace = list ct_observation

(\* Two states are i-equivalent if they agree on public inputs \*) val i_equivalent : program_state -\> program_state -\> set string -\> bool let i_equivalent s1 s2 public_vars = Set.for_all (fun v -\> s1 v = s2 v) public_vars

(\* DEFINITION: Constant-time security \*) val is_constant_time : ir_program -\> leakage_model -\> secrets:set string -\> outputs:list public_output -\> bool let is_constant_time prog model secrets outputs = let public = Set.complement secrets in forall s1 s2. i_equivalent s1 s2 public ==\> (let (t1, s1’) = execute_with_trace prog model s1 in let (t2, s2’) = execute_with_trace prog model s2 in o_equivalent s1’ s2’ outputs ==\> t1 = t2)

</div>

<div class="fstarcode">

type product_state = orig : program_state; shadow : program_state;

(\* Rename variables to shadow namespace \*) val rename_to_shadow : ir_instr -\> ir_instr let rename_to_shadow instr = map_vars (fun v -\> "shadow\_" ^ v) instr

(\* Construct observation assertion for leakage model \*) val observation_assertion : ir_instr -\> leakage_model -\> ir_instr let observation_assertion instr model = match model with \| LeakPC -\> IAssert (EBinOp Eq (EVar "pc") (EVar "shadow_pc")) \| LeakMemory -\> let addr = get_memory_addr instr in let shadow_addr = get_shadow_memory_addr instr in IAssert (EBinOp And (EBinOp Eq (EVar "pc") (EVar "shadow_pc")) (EBinOp Eq addr shadow_addr)) \| LeakOperand -\> let ops = get_operands instr in let shadow_ops = get_shadow_operands instr in IAssert (all_equal ops shadow_ops)

(\* Product program transformation \*) val construct_product_instr : ir_instr -\> leakage_model -\> list ir_instr let construct_product_instr instr model = \[ instr; (\* Original instruction \*) rename_to_shadow instr; (\* Shadow instruction \*) observation_assertion instr model (\* Assert observations match \*) \]

(\* THEOREM: Product construction is sound and complete \*) val product_reduction_correct : prog:ir_program -\> model:leakage_model -\> secrets:set string -\> outputs:list public_output -\> Lemma ( is_constant_time prog model secrets outputs \<==\> is_assertion_safe (construct_product prog model) )

</div>

<div class="fstarcode">

type ct_violation = \| SecretBranch : condition:ir_expr -\> secret_deps:set string -\> location:node_id -\> ct_violation \| SecretMemoryAccess : address:ir_expr -\> secret_deps:set string -\> location:node_id -\> ct_violation \| VariableTimeOpOnSecret : op:string -\> operand:ir_expr -\> secret_deps:set string -\> location:node_id -\> ct_violation

val analyze_constant_time : cpg -\> model:leakage_model -\> secrets:set string -\> outputs:list public_output -\> list ct_violation let analyze_constant_time cpg model secrets outputs = let prog = cpg_to_ir cpg in let product = construct_product prog model in let failed = find_failing_assertions product in List.map (fun (loc, \_) -\> let instr = get_instr_at cpg loc in match categorize_violation instr secrets with \| VBranch cond deps -\> SecretBranch cond deps loc \| VMemAccess addr deps -\> SecretMemoryAccess addr deps loc \| VVariableOp op operand deps -\> VariableTimeOpOnSecret op operand deps loc ) failed

(\* Architecture-aware variable-time operation check \*) type architecture = X86_64 \| ARM_v8 \| RISC_V \| PowerPC

val is_variable_time : string -\> architecture -\> bool let is_variable_time op arch = match (op, arch) with \| ("div", X86_64) \| ("div", ARM_v8) -\> true \| ("mod", X86_64) \| ("mod", ARM_v8) -\> true \| ("fdiv", \_) -\> true \| ("strcmp", \_) \| ("memcmp", \_) -\> true \| \_ -\> false

</div>

### Report Optimization and Taint Carriers (TAJ 2009)

<div class="pillarbox">

**\[Tripp09\]** (Tripp, Pistoia, Fink, Sridharan, Weisman 2009)

</div>

<div class="warningbox">

A large codebase may have thousands of potential taint flows. Developers cannot review them all — they need **actionable** findings.

**TAJ Solutions**:

- 8.1.5.1 Library Call Point (LCP) Grouping — reduce report volume

- 8.1.5.2 Taint Carrier Detection — find nested/indirect taint

**Cross-references**:

- Findings feed into Manifest/Latent classification (Section 12.3)

- Taint on object state relates to typestate tracking (Section 7.2.1)

</div>

#### Library Call Point (LCP) Grouping

**The Insight**: Multiple taint flows often share the *same root cause* — a single API call where tainted data enters the system.

**Example**:

    line 100: data = request.get_param("user")    # LCP: taint enters here
    line 110: query1 = "SELECT * FROM users WHERE name = '" + data + "'"
    line 120: query2 = "SELECT * FROM orders WHERE user = '" + data + "'"
    line 130: query3 = "DELETE FROM sessions WHERE user = '" + data + "'"

Raw analysis reports 3 SQL injection vulnerabilities. But the **fix** is **one** location: sanitize at line 100.

**LCP Grouping**:

- Group all findings by the Library Call Point where taint enters

- Report **one** representative finding per LCP group

- Developer fixes **one** location, addresses **all** variants

**Benefit**:

- 10–100x reduction in report volume

- Each finding is actionable (one fix location)

- Prioritize by LCP with most flows (highest impact fixes)

<div class="fstarcode">

module BrrrMachine.Security.LCPGrouping

type library_call_point = api_function : string; (\* e.g., "request.get_param" \*) call_site : node_id; (\* Location in source code \*) sink_types : set taint_sink; (\* Types of sinks this LCP reaches \*)

type lcp_grouped_report = lcp : library_call_point; representative_flow : taint_flow; (\* Shortest/clearest example \*) all_flows : list taint_flow; (\* All flows from this LCP \*) impact_score : nat; (\* Number of sinks affected \*)

(\* Extract LCP from a taint flow - the source is the LCP \*) val extract_lcp : taint_flow -\> library_call_point let extract_lcp flow = api_function = get_source_api flow.source; call_site = flow.source_location; sink_types = Set.singleton (get_sink_type flow.sink);

(\* Create deduplicated report - one entry per LCP \*) val deduplicate_reports : list taint_flow -\> list lcp_grouped_report let deduplicate_reports flows = let groups = group_by_lcp flows in Map.fold (fun lcp group acc -\> (\* Pick shortest path as representative - clearest to understand \*) let representative = List.min_by (fun f -\> List.length f.path) group in (\* Compute impact: number of distinct sinks \*) let all_sinks = List.fold_left (fun s f -\> Set.add (get_sink_type f.sink) s ) Set.empty group in lcp = lcp with sink_types = all_sinks ; representative_flow = representative; all_flows = group; impact_score = List.length group; :: acc ) groups \[\]

(\* Sort by impact - highest impact LCPs first \*) val prioritize_reports : list lcp_grouped_report -\> list lcp_grouped_report let prioritize_reports reports = List.sort (fun a b -\> compare b.impact_score a.impact_score) reports

</div>

#### Taint Carrier Detection

**The Problem**: Taint stored in **heap objects** may reach sinks indirectly. Standard taint tracking may miss these “carrier” objects.

**Example**:

    line 10: user_data = request.get_param("data")   # Taint source
    line 20: obj.field = user_data                   # Store taint in carrier
    line 30: process(obj)                            # Pass carrier
    line 40: def process(o):
    line 50:     query(o.field)                      # Taint reaches sink!

The taint flows through the **carrier object** `obj`. Without heap graph analysis, we might miss that `obj.field` is tainted when it reaches the sink.

**Solution**: Build a heap graph from pointer analysis. Track which objects have tainted fields. At sinks, check reachability from tainted fields via heap graph.

Cross-reference: This relates to typestate tracking (Section 7.2.1, **\[Bierhoff07\]**) where object state (including tainted fields) must be tracked across calls.

<div class="fstarcode">

module BrrrMachine.Security.TaintCarriers

type taint_carrier = object_var : var_id; (\* Variable holding carrier object \*) tainted_field : field_id; (\* Field that holds tainted data \*) store_site : node_id; (\* Where taint was stored \*) source : taint_source; (\* Original taint source \*)

type heap_edge = \| FieldEdge : base:abstract_loc -\> field:field_id -\> target:abstract_loc -\> heap_edge \| ArrayEdge : base:abstract_loc -\> target:abstract_loc -\> heap_edge

type heap_graph = nodes : set abstract_loc; edges : set heap_edge; tainted_locs : map abstract_loc (set (field_id \* taint_source));

(\* Build heap graph from pointer analysis + taint stores \*) val build_heap_graph : cpg -\> pts_solution -\> heap_graph let build_heap_graph cpg pts = let nodes = collect_all_locs pts in let edges = collect_heap_edges cpg pts in let tainted = find_taint_stores cpg pts in nodes; edges; tainted_locs = tainted

(\* Check if a sink argument can receive tainted data via carrier \*) val check_carrier_taint : heap_graph -\> sink_arg:abstract_loc -\> option (taint_carrier \* list heap_edge) (\* Carrier + path to sink \*)

let rec check_carrier_taint hg sink_arg = (\* Direct check: is sink_arg a tainted location? \*) match Map.find_opt sink_arg hg.tainted_locs with \| Some taints when not (Set.is_empty taints) -\> let (field, source) = Set.choose taints in Some ( object_var = loc_to_var sink_arg; tainted_field = field; store_site = 0; (\* TODO: track \*) source = source , \[\]) \| \_ -\> (\* Transitive check: can we reach a tainted location via heap edges? \*) bfs_find_taint hg sink_arg

</div>

<div class="warningbox">

1.  Run standard IFDS taint analysis (Section <a href="#sec:taint-framework" data-reference-type="ref" data-reference="sec:taint-framework">1.2</a>)

2.  Detect taint carriers via heap graph (Section 8.1.5.2)

3.  Merge direct + carrier taint flows

4.  Group by LCP (Section 8.1.5.1)

5.  Prioritize by impact score

6.  Feed into Manifest/Latent classification (Section 12.3)

**Result**: Actionable, prioritized, deduplicated vulnerability reports.

</div>

### Lifecycle-Aware Taint Analysis (FlowDroid)

<div class="pillarbox">

**\[Arzt14\]** (Arzt, Rasthofer, Fritz, Bodden, Bartel, Klein, Le Traon, Octeau, McDaniel 2014)

</div>

<div class="warningbox">

Android apps, Django views, Express handlers, etc. are **plugins** into a framework that invokes callbacks based on system events.

**Without lifecycle modeling**: Analysis misses many real leaks

**With lifecycle modeling**: 93% recall vs 25–76% for tools without it

</div>

**The Challenge**:

- Framework invokes app callbacks in arbitrary order

- Callbacks share state via instance fields

- Taint may flow: callback1 $`\to`$ field $`\to`$ callback2 $`\to`$ sink

- Without modeling *all* callbacks, we miss these flows

**Example (Android)**:

    class LeakyActivity extends Activity {
      String secret;

      void onCreate(Bundle b) {
        secret = getDeviceId();  // Source: IMEI
      }
      void onResume() {
        sendToServer(secret);    // Sink: network
      }
    }

**Standard analysis**: Sees only `onCreate` *or* `onResume`, misses the flow.

**FlowDroid**: Models that `onResume` follows `onCreate`, finds the leak.

**Solution: Dummy Main Method**

Generate a synthetic entry point that models all possible callback orderings:

    void dummyMain() {
      Activity a = new LeakyActivity();
      while (true) {  // Non-deterministic loop = all orderings
        switch (nondet()) {
          case 0: a.onCreate(null); break;
          case 1: a.onStart(); break;
          case 2: a.onResume(); break;
          case 3: a.onPause(); break;
          case 4: a.onStop(); break;
          case 5: a.onDestroy(); break;
        }
      }
    }

The loop structure allows analysis to consider all orderings without explicit enumeration ($`2^n`$ combinations).

#### Activation Statements for Flow-Sensitive Heap Taint

<div class="warningbox">

    p1.f = source();    // Line 1: taint stored in heap
    sink(p2.f);         // Line 2: should NOT report (p1 != p2 yet)
    p1 = p2;            // Line 3: now aliased!
    sink(p2.f);         // Line 4: SHOULD report (p1 == p2)

**Without flow-sensitivity**: BOTH sinks flagged (false positive at line 2)

**With activation statements**: Only line 4 flagged (precise)

</div>

**Solution: Activation Statements**

When backward alias analysis finds a potential alias, record *where* it was found. Mark the taint as **inactive** until forward analysis passes that point.

1.  Forward finds heap write: `p1.f = tainted`  
    $`\to`$ Spawn backward analysis to find aliases of `p1.f`

2.  Backward finds: at line 3, `p2` becomes aliased to `p1`  
    $`\to`$ Create fact: `(p2.f, INACTIVE, activation=line3)`

3.  Forward propagates inactive taint past line 3  
    $`\to`$ Fact becomes: `(p2.f, ACTIVE, activation=line3)`

4.  At sink: only report **active** taints  
    $`\to`$ Line 2: `p2.f` is INACTIVE $`\to`$ no report  
    $`\to`$ Line 4: `p2.f` is ACTIVE $`\to`$ **report leak**

The F\* formalization below introduces **access paths** — a representation that tracks not just variables, but entire field access chains (e.g., `x.f.g.h`). Access paths enable field-sensitive taint tracking:

- `base`: The root variable

- `fields`: A list of field identifiers forming the access chain

- `array_wildcard`: When true, matches any array index (e.g., `x.f[*]`)

The `max_access_path_length` constant (typically 5) bounds path length to ensure termination. Longer paths are approximated using array wildcards.

<div class="fstarcode">

(\* Source: Arzt 2014 (FlowDroid Section 4.3) \*) module BrrrMachine.Security.ActivationTaint

(\* Access path = variable + field chain for precise heap tracking \*) type access_path = base : var_id; fields : list field_id; (\* x.f.g.h represented as \[f; g; h\] \*) array_wildcard : bool; (\* x.f\[\*\] matches any array index \*)

(\* Maximum path length to ensure termination \*) let max_access_path_length : nat = 5

(\* Taint state with activation tracking \*) type taint_activation_state = \| TaintActive (\* Can trigger leak at sink \*) \| TaintInactive : activation:node_id -\> (\* Waiting to pass activation \*) taint_activation_state

(\* Extended taint fact with activation \*) type activation_taint_fact = path : access_path; state : taint_activation_state; source : taint_source; source_location : node_id;

(\* At sink: only report if taint is ACTIVE \*) val check_sink_with_activation : node:node_id -\> facts:set activation_taint_fact -\> sink:taint_sink -\> list taint_flow

let check_sink_with_activation node facts sink = (\* CRITICAL: Filter to ACTIVE taints only \*) let active_facts = Set.filter (fun f -\> match f.state with \| TaintActive -\> true \| TaintInactive \_ -\> false ) facts in Set.to_list active_facts \|\> List.map (fun fact -\> source = fact.source; source_location = fact.source_location; sink = sink; sink_location = node; path = \[\]; vuln_type = sink_to_vuln sink; confidence = 1.0 )

</div>

##### Bidirectional IFDS Extension

FlowDroid extends standard IFDS with **bidirectional analysis**. The forward phase propagates taint from sources, while the backward phase finds heap aliases to tainted locations. These two directions *spawn* each other:

- **Forward** $`\to`$ finds heap write $`\to`$ **spawns Backward**

- **Backward** $`\to`$ finds alias $`\to`$ **spawns Forward** (with INACTIVE taint)

<div class="warningbox">

This is *not* pure IFDS because may-alias is non-distributive. The backward phase uses on-demand alias analysis. A cleaner formalization would use IDE (the non-distributive extension of IFDS) for the backward phase.

Cross-reference: Section 4.1 for standard IFDS (Reps 1995).

</div>

<div class="fstarcode">

(\* Bidirectional analysis direction \*) type analysis_direction = \| Forward : analysis_direction \| Backward : analysis_direction

(\* Worklist for bidirectional analysis \*) noeq type bidirectional_worklist = forward : list (int \* activation_taint_fact); (\* node \* fact \*) backward : list (int \* access_path \* int); (\* node \* path \* activation_stmt \*)

(\* Propagate taint through statement, handling activation \*) val propagate_with_activation : cpg:cpg -\> node:node_id -\> fact:activation_taint_fact -\> set activation_taint_fact

let propagate_with_activation cpg node fact = (\* Check if we’re passing the activation point \*) let fact’ = match fact.state with \| TaintInactive activation when node = activation -\> fact with state = TaintActive (\* ACTIVATE! \*) \| \_ -\> fact in (\* Standard taint propagation \*) propagate_taint_fact cpg node fact’

</div>

The bidirectional worklist maintains separate queues for forward and backward work items. When forward analysis encounters a heap write of tainted data, it spawns backward analysis to find all aliases. When backward analysis discovers an alias, it spawns forward analysis with an inactive taint fact that will be activated when passing the alias point.

#### Framework Lifecycle Modeling

The FlowDroid lifecycle modeling creates a synthetic “dummy main” that captures all possible orderings of framework callbacks. The following F\* code generalizes this beyond Android to support Django, Express, and other frameworks. The key insight is that lifecycle callbacks can be modeled as non-deterministic choices within a loop, ensuring the analysis considers all possible callback orderings without explicitly enumerating $`2^n`$ combinations.

<div class="fstarcode">

(\* Generalized beyond Android to support multiple frameworks. \*) module BrrrMachine.Analysis.Lifecycle

(\* Generic component type (framework-agnostic) \*) type component_kind = \| CompActivity (\* Android Activity, Django View \*) \| CompService (\* Android Service, background worker \*) \| CompReceiver (\* Android BroadcastReceiver, event handler \*) \| CompProvider (\* Android ContentProvider, data source \*) \| CompMiddleware (\* Express/Django middleware \*) \| CompHandler (\* HTTP handler, route handler \*)

(\* Lifecycle callback specification \*) type lifecycle_callback = component_kind : component_kind; method_pattern : string; (\* e.g., "on\*", "handle\*" \*) phase : lifecycle_phase; can_access_state : bool; (\* Can read/write instance fields \*)

type lifecycle_phase = \| PhaseInit (\* Constructor, onCreate \*) \| PhaseStart (\* onStart, request received \*) \| PhaseActive (\* onResume, handling request \*) \| PhasePause (\* onPause, request complete \*) \| PhaseStop (\* onStop, cleanup \*) \| PhaseDestroy (\* onDestroy, teardown \*)

(\* Generate dummy main from detected components \*) val generate_dummy_main : components:list (component_kind \* string \* list method_id) -\> lifecycle:lifecycle_graph -\> ir_func

let generate_dummy_main components lifecycle = (\* 1. Instantiate all components \*) let instantiations = List.map (fun (kind, class_name, \_) -\> let var = fresh_var kind in SLet var (TRef (TStruct class_name)) (ENew class_name \[\]) ) components in (\* 2. Build non-deterministic callback invocation \*) let callback_calls = List.concat_map (fun (kind, class_name, methods) -\> List.map (fun method_id -\> SCall None (EMethodCall (EVar (var_for kind)) method_id) \[\] ) methods ) components in (\* 3. Wrap in while(true) for all orderings \*) let body = SWhile (EVal (VBool true)) (SNondet callback_calls) in id = "dummyMain"; name = "dummyMain"; params = \[\]; return_type = TUnit; body = SSeq (SSeq_many instantiations) body; effect_sig = effect_any; is_public = true; source_lang = "java"

</div>

#### Taint Wrappers for Library Methods

Real-world applications depend heavily on library methods whose source code may be unavailable or too expensive to analyze. FlowDroid’s **taint wrappers** provide explicit specifications of how taint propagates through these APIs. The wrapper rules specify:

- `PassThrough`: Taint flows from one parameter to another

- `Kill`: Taint is sanitized/removed at a parameter

- `Source`: The method introduces new taint

- `Sink`: The method is a security-sensitive operation

The F\* code below models these wrapper rules. The `param_spec` type allows precise specification of taint flow through the receiver (`this`), arguments, return values, and fields.

<div class="fstarcode">

(\* Library methods (whose code is unavailable) need explicit taint propagation rules. Wrappers specify how taint flows through APIs. \*) module BrrrMachine.Security.TaintWrapper

type taint_wrapper_rule = \| WrapperPassThrough : from:param_spec -\> to:param_spec -\> taint_wrapper_rule (\* Taint flows from ’from’ to ’to’ \*) \| WrapperKill : param:param_spec -\> taint_wrapper_rule (\* Taint is removed at this parameter \*) \| WrapperSource : to:param_spec -\> source:taint_source -\> taint_wrapper_rule (\* Method introduces taint \*) \| WrapperSink : from:param_spec -\> sink:taint_sink -\> taint_wrapper_rule (\* Method is a sensitive sink \*)

type param_spec = \| ParamThis (\* The ’this’ pointer \*) \| ParamArg : index:nat -\> param_spec (\* Argument by index \*) \| ParamReturn (\* Return value \*) \| ParamFieldOfThis : field:string -\> param_spec \| ParamFieldOfArg : index:nat -\> field:string -\> param_spec

type taint_wrapper = method_signature : string; rules : list taint_wrapper_rule;

(\* Standard library wrappers \*) val string_wrappers : list taint_wrapper let string_wrappers = \[ (\* String.concat: if either arg tainted, result tainted \*) method_signature = "java.lang.String.concat"; rules = \[WrapperPassThrough (ParamArg 0) ParamReturn; WrapperPassThrough ParamThis ParamReturn\] ; (\* StringBuilder.append: arg taints this and return \*) method_signature = "java.lang.StringBuilder.append"; rules = \[WrapperPassThrough (ParamArg 0) ParamThis; WrapperPassThrough ParamThis ParamReturn\] ; \]

val collection_wrappers : list taint_wrapper let collection_wrappers = \[ (\* List.add: if arg tainted, list becomes tainted \*) method_signature = "java.util.List.add"; rules = \[WrapperPassThrough (ParamArg 0) ParamThis\] ; (\* List.get: if list tainted, return tainted \*) method_signature = "java.util.List.get"; rules = \[WrapperPassThrough ParamThis ParamReturn\] ; (\* Map.put: both key and value taint the map \*) method_signature = "java.util.Map.put"; rules = \[WrapperPassThrough (ParamArg 0) ParamThis; WrapperPassThrough (ParamArg 1) ParamThis\] ; \]

</div>

<div class="warningbox">

**TAJ**: Uses heap graph BFS reachability — **fast** but flow-insensitive

**FlowDroid**: Uses activation statements — **precise** but more expensive

**Resolution**: Configurable strategy based on analysis budget

- For quick scans: TAJ carrier detection (Section 8.1.5.2)

- For precision: FlowDroid activation statements (Section 8.1.6.1)

- For hybrid: Use TAJ for initial candidates, FlowDroid to verify

</div>

<div class="fstarcode">

type heap_taint_strategy = \| StrategyTAJCarriers (\* Fast heap graph BFS - Section 8.1.5.2 \*) \| StrategyActivation (\* Precise activation statements - Section 8.1.6.1 \*) \| StrategyHybrid (\* TAJ for candidates, activation to verify \*)

</div>

### Incremental Taint Analysis

**Cross-reference**: Section 10.1.3 (DRedL), Section 10.1.4 (Infer Deployment)

**Incremental Taint Analysis for IDE/CI Integration**:

When code changes, we can incrementally update taint analysis results using the techniques from **\[Szabo18\]** (DRedL) and **\[Distefano19\]** (Infer).

**Strategy**:

1.  File change detected  
    $`\to`$ Adapton marks affected CPG thunks dirty (Section 10.1.1)

2.  Re-parse changed file via tree-sitter (fast, $`<`$ 50ms typically)

3.  Convert CPG changes to Datalog tuple changes:

    - New taint sources/sinks $`\to`$ insertions

    - Removed sources/sinks $`\to`$ deletions

    - Modified flow edges $`\to`$ increasing/decreasing replacements

4.  Use DRedL (Section 10.1.3) to update taint relations:

    - Taint lattice: $`\mathsf{UNTAINTED} < \mathsf{TAINTED}`$ (simple 2-point)

    - Adding taint source $`\to`$ increasing replacement (monotonic)

    - Removing taint source $`\to`$ anti-monotonic, needs re-derivation

5.  Only report flows involving **changed** code (diff-time principle)

**Expected Performance** (based on IncA benchmarks **\[Szabo18\]**):

- Median update time: 2–5ms

- Speedup vs from-scratch: 65x–243x

- Target: sub-second for IDE integration

**Integration with IFDS**: Taint analysis is often formulated as IFDS (Section 4.1). Encode IFDS path edges as Datalog relations:

    PathEdge(d1, n1, d2, n2) :-
      FlowFunction(n1, n2, d1, d2).
    PathEdge(d1, n1, d3, n3) :-
      PathEdge(d1, n1, d2, n2),
      PathEdge(d2, n2, d3, n3).

Then use DRedL to incrementally maintain `PathEdge` relations.

**Zoncolan Experience** **\[Distefano19\]**:

Facebook’s security taint analyzer for 100M LOC Hack codebase:

- Compositional summary-based taint propagation

- Diff-time deployment in CI pipeline

- Outperforms other security detection methods

- Thousands of security fixes deployed

<div class="fstarcode">

val incremental_taint_update : prev_solution:ifds_solution -\> cpg_changes:list cpg_change -\> (ifds_solution \* list taint_flow)

let incremental_taint_update prev cpg_changes = (\* Convert CPG changes to Datalog tuple changes \*) let datalog_changes = List.concat_map cpg_change_to_tuples cpg_changes in (\* Use DRedL to maintain IFDS relations incrementally \*) let new_solution = maintain_incrementally ifds_program prev.relations prev.support datalog_changes in (\* Extract new taint flows from changed relations \*) let new_flows = extract_taint_flows_from_solution new_solution in (new_solution, new_flows)

</div>

# Multi-Language Analysis

<div class="pillarbox">

**Matthews & Findler 2007**: See Appendix D.10.6 for full analysis.

**THIS SECTION** assumes type-directed conversion: $`\text{type\_map} : \text{Type}_1 \to \text{Type}_2`$

M&F 2007 shows: Real FFIs have **ARBITRARY** conventions.

**Real-World FFI Conventions**:

- <span class="sans-serif">C</span> returns $`-1`$ for error, $`0`$ for success (zero-for-error)

- <span class="sans-serif">Python</span> `None` $`\leftrightarrow`$ <span class="sans-serif">C</span> `NULL` (null-for-none)

- Sentinel values for special cases

- Custom serialization for complex types

**Resolution**:

- Section 12.20 adds `conversion_strategy` type

- Section 9.3 integrates guards with conversion strategies

- `CSTypeDirected`, `CSZeroForError`, `CSNullForNone`, `CSCustom` variants

**Guard Generation (Section 9.3)** handles polarity FLIP at function arguments!

</div>

**Theoretical Foundation**: The multi-language analysis is grounded in **Institution Theory** **\[Goguen92\]**. See **Section 12.25** for the formal foundation including:

- The satisfaction condition (truth invariance under translation)

- Theory colimits (combining language theories)

- Institution morphisms (cross-system theorem proving)

- Constraint institutions (freeness/initiality requirements)

- Duplex institutions (combining decidable and expressive logics)

## Crossing Language Boundaries

**Papers**: **\[Matthews07\]**, **\[Goguen92\]**

When code crosses from one language to another, safety invariants may be violated. The brrr-machine detects these risks.

### Matthews’ Boundary Semantics

<div class="pillarbox">

Multi-language programs have **EXPLICIT BOUNDARIES**. At each boundary, values must be converted (marshaled).

**The Boundary Term**:
``` math
\hat{}^{L_2}_{L_1}(e) \quad \text{means: ``embed } L_1 \text{ expression } e \text{ into } L_2\text{''}
```

**Evaluation**:

- If $`e`$ evaluates to $`v`$ in $`L_1`$

- Then $`\hat{}^{L_2}_{L_1}(e)`$ evaluates to $`\text{convert}(v, L_1, L_2)`$

</div>

**Conversion Strategies**:

1.  **Natural Embedding**: Values with direct equivalents are converted naturally.

    - `int` (<span class="sans-serif">Python</span>) $`\to`$ `i64` (<span class="sans-serif">Rust</span>): natural

    - `str` (<span class="sans-serif">Python</span>) $`\to`$ `String` (<span class="sans-serif">Rust</span>): natural

2.  **Lump Embedding**: Values without equivalents are “lumped” as opaque.

    - `object` (<span class="sans-serif">Python</span>) $`\to`$ `PyObject*` (<span class="sans-serif">C</span>): lump

3.  **Guarded Embedding**: Runtime checks enforce target language invariants.

    - `Option` (<span class="sans-serif">Python</span> `None`) $`\to`$ Non-null (<span class="sans-serif">Java</span>): requires check

**What Can Go Wrong**:

- **<span class="sans-serif">Python</span> $`\to`$ <span class="sans-serif">C</span>**: <span class="sans-serif">Python</span> has GC, <span class="sans-serif">C</span> doesn’t. If <span class="sans-serif">Python</span> frees object while <span class="sans-serif">C</span> holds pointer $`\to`$ use-after-free

- **<span class="sans-serif">JavaScript</span> $`\to`$ <span class="sans-serif">Rust</span>**: <span class="sans-serif">JavaScript</span> has `null`, <span class="sans-serif">Rust</span> has `Option`. If `null` passed where non-null expected $`\to`$ crash

- **<span class="sans-serif">Go</span> $`\to`$ <span class="sans-serif">C</span>**: <span class="sans-serif">Go</span> has goroutines, <span class="sans-serif">C</span> has raw threads. If <span class="sans-serif">Go</span> object escapes to <span class="sans-serif">C</span> $`\to`$ race condition

### Boundary Risk Analysis

The following F\* code defines the core data structures for cross-language boundary analysis. The `axiom` type enumerates the safety guarantees that different languages provide, while `language_config` captures the complete characterization of a language’s safety properties. When code crosses from one language to another, the difference in axiom sets determines what properties might be violated at the boundary.

<div class="fstarcode">

module BrrrMachine.Boundary

(\* Language Axioms — What each language guarantees \*) type axiom = \| AxMemSafe (\* No use-after-free, double-free, buffer overflow \*) \| AxNullSafe (\* No null pointer dereference \*) \| AxTypeSafe (\* No type errors at runtime \*) \| AxRaceFree (\* No data races \*) \| AxLeakFree (\* No resource leaks \*) \| AxDetDrop (\* Deterministic resource destruction \*) \| AxInitSafe (\* No uninitialized reads \*)

type language_config = name : string; axioms : set axiom; memory_mode : memory_mode; type_mode : type_mode; null_mode : null_mode; eval_mode : eval_mode; (\* Evaluation strategy - affects VC soundness \*)

type memory_mode = ModeGC \| ModeRC \| ModeOwned \| ModeManual type type_mode = TStatic \| TDynamic \| TGradual type null_mode = NNullable \| NOptional \| NNonNull

</div>

<div class="fstarcode">

(\* CRITICAL INSIGHT: VCs that are SOUND under eager evaluation may be UNSOUND under lazy evaluation! Under laziness, a binding like: let n = diverge 1 in ... means the VC can include assumptions about n that are NEVER realized because n may never be evaluated.

The "false" refinement in diverge’s output type contaminates the VC: false / y = 0 =\> v = 0 =\> v \> 0 This is VALID (contradiction in antecedent) but UNSOUND under laziness!

See Section 2.1.5b for stratified types that restore soundness. \*)

type eval_mode = \| EvalStrict (\* Call-by-value: all bindings are values, VCs are sound \*) \| EvalLazy (\* Call-by-need: bindings may be thunks, need stratification \*) \| EvalHybrid (\* Mixed: strict by default with lazy constructs \*)

</div>

<div class="fstarcode">

(\* Python requires specialized call graph construction via Function Type Graph (FTG) rather than Qilin-style object-sensitive analysis. \*)

type python_mro_algorithm = \| C3Linearization (\* Standard Python MRO - REQUIRED \*) \| SimpleDFS (\* Fallback for pathological hierarchies \*)

type python_magic_handling = \| MagicFull (\* Handle \_\_getattr\_\_, \_\_call\_\_, descriptors \*) \| MagicPartial (\* Only \_\_init\_\_ and \_\_call\_\_ \*) \| MagicNone (\* Ignore magic methods - fast but imprecise \*)

type python_decorator_mode = \| DecoratorExpand (\* Expand decorator chains for precise call graph \*) \| DecoratorIdentity (\* Treat decorators as identity - fast but imprecise \*)

type python_import_resolution = \| ImportFullPackage (\* Full package hierarchy with \_\_init\_\_.py \*) \| ImportFlat (\* Flat module resolution \*)

type python_type_graph_mode = \| FTGFlowSensitive (\* JARVIS-style with strong updates - RECOMMENDED \*) \| FTGFlowInsensitive (\* PyCG-style - faster but less precise \*)

type python_analysis_mode = \| AnalysisApplicationCentered (\* Only reachable from entry points \*) \| AnalysisWholeProgramme (\* Analyze everything - expensive \*) \| AnalysisDemandDriven (\* Only what’s queried \*)

type python_options = mro_algorithm : python_mro_algorithm; magic_handling : python_magic_handling; decorator_mode : python_decorator_mode; import_resolution : python_import_resolution; type_graph_mode : python_type_graph_mode; analysis_mode : python_analysis_mode;

let default_python_options : python_options = mro_algorithm = C3Linearization; magic_handling = MagicFull; decorator_mode = DecoratorExpand; import_resolution = ImportFullPackage; type_graph_mode = FTGFlowSensitive; analysis_mode = AnalysisApplicationCentered;

</div>

<div class="fstarcode">

let python_config : language_config = name = "Python"; axioms = set_of \[AxMemSafe; AxLeakFree\]; memory_mode = ModeGC; type_mode = TDynamic; null_mode = NNullable; eval_mode = EvalHybrid; (\* Strict by default, generators are lazy \*)

let rust_config : language_config = name = "Rust"; axioms = set_of \[AxMemSafe; AxNullSafe; AxTypeSafe; AxRaceFree; AxLeakFree; AxDetDrop; AxInitSafe\]; memory_mode = ModeOwned; type_mode = TStatic; null_mode = NOptional; eval_mode = EvalStrict;

let c_config : language_config = name = "C"; axioms = set_of \[\]; (\* C guarantees nothing! \*) memory_mode = ModeManual; type_mode = TStatic; null_mode = NNullable; eval_mode = EvalStrict;

let go_config : language_config = name = "Go"; axioms = set_of \[AxMemSafe; AxTypeSafe; AxLeakFree\]; memory_mode = ModeGC; type_mode = TStatic; null_mode = NNullable; eval_mode = EvalStrict;

let javascript_config : language_config = name = "JavaScript"; axioms = set_of \[AxMemSafe; AxLeakFree; AxRaceFree\]; (\* Single-threaded \*) memory_mode = ModeGC; type_mode = TDynamic; null_mode = NNullable; eval_mode = EvalHybrid;

let haskell_config : language_config = name = "Haskell"; axioms = set_of \[AxMemSafe; AxTypeSafe; AxLeakFree\]; memory_mode = ModeGC; type_mode = TStatic; null_mode = NOptional; eval_mode = EvalLazy; (\* CRITICAL: Lazy evaluation \*) (\* NOTE: EvalLazy requires stratified type analysis (Div/Wnf/Fin). Standard VC translation is UNSOUND. \*)

</div>

The following F\* code defines the **boundary mechanism** taxonomy, which captures the different ways code can cross language boundaries. Each mechanism type has distinct security implications: FFI calls may expose memory safety issues, IPC/RPC boundaries introduce serialization concerns, and language-specific mechanisms like `ctypes` or JNI have their own quirks. The `boundary` record type pairs the source and target language configurations with the specific mechanism and call site location.

<div class="fstarcode">

type boundary_mechanism = \| BoundaryFFI (\* Foreign function interface \*) \| BoundaryIPC (\* Inter-process communication \*) \| BoundaryRPC (\* Remote procedure call \*) \| BoundaryFile (\* File-based communication \*) \| BoundarySocket (\* Network socket \*) \| BoundarySerialization (\* JSON, protobuf, etc. \*) (\* Python-C specific FFI mechanisms (PolyCruise, Li 2022) \*) \| BoundaryFFI_Ctypes (\* Python ctypes.CDLL \*) \| BoundaryFFI_Extension (\* Python C extension module \*) \| BoundaryFFI_Callback (\* C calling back to Python \*) \| BoundaryFFI_CFFI (\* Python CFFI \*) (\* Node.js specific \*) \| BoundaryFFI_NAPI (\* Node.js N-API for native addons \*) \| BoundaryFFI_WASM (\* WebAssembly boundary \*) (\* JNI/JNA \*) \| BoundaryFFI_JNI (\* Java Native Interface \*) \| BoundaryFFI_JNA (\* Java Native Access \*)

type boundary = source_lang : language_config; target_lang : language_config; mechanism : boundary_mechanism; call_site : node_id;

</div>

<div class="fstarcode">

(\* Axioms at risk = axioms source has but target lacks \*) val boundary_risks : boundary -\> set axiom let boundary_risks b = Set.diff b.source_lang.axioms b.target_lang.axioms

(\* Risk severity \*) type risk_level = RiskCritical \| RiskHigh \| RiskMedium \| RiskLow \| RiskNone

let axiom_risk_level (ax : axiom) : risk_level = match ax with \| AxMemSafe -\> RiskCritical (\* Memory corruption = RCE \*) \| AxTypeSafe -\> RiskHigh (\* Type confusion = potential RCE \*) \| AxNullSafe -\> RiskMedium (\* Null deref = crash \*) \| AxRaceFree -\> RiskHigh (\* Races = unpredictable behavior \*) \| AxLeakFree -\> RiskLow (\* Leaks = DoS over time \*) \| AxDetDrop -\> RiskLow (\* Non-determinism = subtle bugs \*) \| AxInitSafe -\> RiskMedium (\* Uninit = info leak or crash \*)

let max_risk_level (risks : set axiom) : risk_level = Set.fold (fun ax level -\> max level (axiom_risk_level ax) ) risks RiskNone

</div>

The following F\* code implements **type consistency** for gradual typing, a key concept for handling type checking at language boundaries. Unlike traditional subtyping, type consistency is reflexive and symmetric but **NOT transitive**—this non-transitivity is essential for soundness. The `gradual_type` algebraic data type represents types that may contain the dynamic type `GTDynamic` (written as `?` in the literature), which is consistent with any other type.

<div class="fstarcode">

(\* CRITICAL: Type consistency is NOT TRANSITIVE! This is essential for soundness at language boundaries.

Example of why transitivity is unsound: int   ? (consistent) ?   string (consistent) int   string (INCONSISTENT!) If we assumed transitivity, we’d allow int -\> Any -\> string coercions. \*)

type gradual_type = \| GTGround : ground_type -\> gradual_type \| GTDynamic : gradual_type (\* The ? type - consistent with anything \*) \| GTFunc : list gradual_type -\> gradual_type -\> gradual_type \| GTRef : gradual_type -\> gradual_type

(\* Type consistency relation - reflexive, symmetric, but NOT transitive \*) val type_consistent : gradual_type -\> gradual_type -\> bool let rec type_consistent t1 t2 = match t1, t2 with \| GTDynamic, \_ -\> true (\* ?   t for any t \*) \| \_, GTDynamic -\> true (\* t   ? for any t \*) \| GTGround g1, GTGround g2 -\> g1 = g2 \| GTFunc p1 r1, GTFunc p2 r2 -\> List.length p1 = List.length p2 && List.for_all2 type_consistent p2 p1 && (\* Contravariant! \*) type_consistent r1 r2 \| GTRef t1’, GTRef t2’ -\> type_consistent t1’ t2’ \| \_, \_ -\> false

(\* Cast insertion when types are consistent but not equal \*) type cast_result = CastOK of ir_expr \| CastError of string

val insert_boundary_cast : gradual_type -\> gradual_type -\> ir_expr -\> cast_result let insert_boundary_cast src tgt e = if src = tgt then CastOK e else if type_consistent src tgt then CastOK (ECast e tgt) else CastError ("Inconsistent types: cannot cast " ^ show src ^ " to " ^ show tgt)

</div>

<div class="fstarcode">

(\* CRITICAL INSIGHT: Gradual types ARE abstract interpretations of static type sets. This connects Part IX to Part II (Abstract Interpretation).

The unknown type ? represents the SET OF ALL STATIC TYPES: gamma(?) = all static types – ? abstracts everything gamma(Int) = Int – ground types abstract themselves gamma(? -\> Int) = T -\> Int \| any T – partial knowledge

GALOIS CONNECTION (connects to Section 2.1.2): Concrete domain: P(StaticType) ordered by subset inclusion Abstract domain: GradualType ordered by precision alpha: Set of static types -\> minimal gradual type covering set gamma: Gradual type -\> set of static types it represents

TYPE CONSISTENCY DERIVED (not stipulated): G1   G2 iff gamma(G1) intersect gamma(G2) != empty

This derivation EXPLAINS why consistency is non-transitive: gamma(Int) intersect gamma(?) = Int != empty – Int   ? gamma(?) intersect gamma(String) = String != empty – ?   String gamma(Int) intersect gamma(String) = empty – Int !  String \*)

(\* Concretization: gradual type -\> set of static types it represents \*) val gamma_gradual : gradual_type -\> set static_type let rec gamma_gradual gt = match gt with \| GTDynamic -\> all_static_types (\* ? represents ALL static types \*) \| GTGround g -\> singleton (to_static g) \| GTFunc params ret -\> cartesian_func (List.map gamma_gradual params) (gamma_gradual ret) \| GTRef t -\> set_map STRef (gamma_gradual t)

(\* AGT-derived consistency: equivalent to type_consistent above \*) val agt_consistent : gradual_type -\> gradual_type -\> bool let agt_consistent g1 g2 = not (Set.is_empty (Set.intersect (gamma_gradual g1) (gamma_gradual g2)))

(\* THEOREM: AGT consistency equals operational consistency (Siek 2006) \*) val agt_equals_siek : g1:gradual_type -\> g2:gradual_type -\> Lemma (agt_consistent g1 g2 \<==\> type_consistent g1 g2)

(\* PRECISION ORDERING: G1 is more precise than G2 if gamma(G1) subset gamma(G2) \*) val precision_leq : gradual_type -\> gradual_type -\> bool let precision_leq g1 g2 = Set.subset (gamma_gradual g1) (gamma_gradual g2)

(\* GTDynamic is LEAST precise (bottom in precision order) \*) val dynamic_is_bottom : g:gradual_type -\> Lemma (precision_leq GTDynamic g g = GTDynamic)

</div>

<div class="pillarbox">

Making types **LESS precise** (more `?`) preserves semantics but may add runtime checks. Making types **MORE precise** removes runtime checks.

If $`\vdash e : G`$ and $`G'`$ is less precise than $`G`$, then $`\vdash e : G'`$ and:

- If $`e`$ evaluates to $`v`$ under $`G`$, it evaluates to $`v`$ under $`G'`$

- Under $`G'`$, more runtime checks may occur (and may fail)

This connects to the **pay-as-you-go principle** **\[Siek06\]**:

- Fully annotated code: no runtime checks, static guarantees

- Partially annotated: checks at `?` boundaries

- Unannotated (`?`): maximum runtime checking

</div>

<div class="fstarcode">

(\* For languages with record subtyping (JS objects, Python dicts, TS interfaces), we need CONSISTENT SUBTYPING, not just consistency:

G1 \<  G2 iff exists T1 in gamma(G1), T2 in gamma(G2). T1 \<: T2

Equivalently (Garcia 2016, Theorem 3): G1 \<  G2 iff exists G. (G1   G) and (G \<: G2) iff exists G. (G1 \<: G) and (G   G2) \*)

val consistent_subtype : gradual_type -\> gradual_type -\> bool let consistent_subtype g1 g2 = Set.exists (gamma_gradual g1) (fun t1 -\> Set.exists (gamma_gradual g2) (fun t2 -\> static_subtype t1 t2))

(\* Use consistent_subtype for record types at boundaries \*) val boundary_check_record : gradual_type -\> gradual_type -\> bool let boundary_check_record src tgt = match src, tgt with \| GTGround (TRecord \_), GTGround (TRecord \_) -\> consistent_subtype src tgt \| \_, \_ -\> type_consistent src tgt

</div>

In gradual typing systems, when a runtime type check fails, it is essential to provide **precise blame information** that identifies exactly which boundary caused the error. The `evidence` type below tracks *how* type consistency was established during compilation. When two types are consistent through the dynamic type (`?`), evidence records this fact. At runtime, if a cast fails, the evidence structure can be traversed to produce actionable error messages like “Type mismatch at boundary Python$`\to`$Rust: expected `Int`, got `String`.” Evidence composition (`compose_evidence`) handles chained casts across multiple boundaries.

<div class="fstarcode">

(\* Evidence tracks HOW consistency was established during type checking. When a runtime check fails, evidence pinpoints the BOUNDARY that caused it.

This improves upon simple cast insertion by providing actionable error messages: "Type mismatch at boundary Python-\>Rust: expected Int, got String" \*)

type evidence = \| EvRefl : gradual_type -\> evidence (\* G   G \*) \| EvDynL : gradual_type -\> evidence (\* ?   G \*) \| EvDynR : gradual_type -\> evidence (\* G   ? \*) \| EvFunc : list evidence -\> evidence -\> evidence (\* Function evidence \*) \| EvRecord : list (string \* evidence) -\> evidence (\* Record evidence \*)

type evidence_result = \| EvOK : evidence -\> evidence_result \| EvFail : blame:string -\> evidence_result

(\* Evidence composition - may fail at runtime \*) val compose_evidence : evidence -\> evidence -\> evidence_result let rec compose_evidence ev1 ev2 = match ev1, ev2 with \| EvRefl \_, ev -\> EvOK ev \| ev, EvRefl \_ -\> EvOK ev \| EvDynL g1, EvDynR g2 -\> if type_consistent g1 g2 then EvOK (EvRefl (meet_gradual g1 g2)) else EvFail ("Incompatible at boundary: " ^ show g1 ^ " vs " ^ show g2) \| EvDynR \_, EvDynL \_ -\> EvOK (EvDynL GTDynamic) \| EvFunc ps1 r1, EvFunc ps2 r2 -\> (\* Contravariant for params, covariant for return \*) compose_func_evidence ps1 r1 ps2 r2 \| \_, \_ -\> EvFail "Evidence structure mismatch"

(\* Enhanced cast with evidence for blame tracking \*) type cast_with_blame = CastBlameOK of ir_expr \* evidence \| CastBlameFail of string

val insert_boundary_cast_with_blame : source_lang:string -\> target_lang:string -\> gradual_type -\> gradual_type -\> ir_expr -\> cast_with_blame

</div>

<div class="fstarcode">

(\* CRITICAL INSIGHT: These two systems solve DIFFERENT problems: - Gradual typing: Handle ? types at MODULE BOUNDARIES - Occurrence typing: Refine types WITHIN modules via type tests

TOGETHER they provide a complete solution for dynamic language analysis: - At boundary: Use type consistency (Siek 2006, derived via AGT) - Within module: Use occurrence typing refinement (Tobin-Hochstadt 2008)

AGT INTERPRETATION: Occurrence typing NARROWS the concretization set. Given gradual type G with gamma(G) = T1, T2, T3, ..., a type test "if isinstance(x, T1)" narrows gamma to T1 in the true branch. \*)

type refined_gradual_type = base_type : gradual_type; (\* The declared/inferred type \*) refinements : list type_prop; (\* Active refinement propositions \*) narrowed_type : option gradual_type; (\* Refined type if different \*)

(\* Refine a gradual type based on occurrence typing propositions \*) val refine_gradual_type : gradual_type -\> type_prop -\> refined_gradual_type

(\* At module boundary: forget refinements, use only base type \*) val boundary_type : refined_gradual_type -\> gradual_type let boundary_type rgt = rgt.base_type

(\* Within module: use refined type if available \*) val effective_type : refined_gradual_type -\> gradual_type let effective_type rgt = match rgt.narrowed_type with \| Some t -\> t \| None -\> rgt.base_type

</div>

The `boundary_issue` algebraic data type enumerates the categories of problems that can arise when values cross language boundaries. Each constructor captures a specific risk class identified during boundary analysis: null values entering null-safe languages, unclear ownership semantics, type mismatches, unsanitized tainted data propagation, lifetime violations, thread-safety concerns, and serialization incompatibilities. These issues are reported by the boundary analyzer and can trigger guard generation (Section 9.3) or compilation errors.

<div class="fstarcode">

type boundary_issue = \| IssueNullCrossing : loc:node_id -\> boundary_issue (\* Nullable value enters null-free language \*) \| IssueOwnershipUnclear : loc:node_id -\> boundary_issue (\* Ownership not specified at boundary \*) \| IssueTypeMismatch : expected:string -\> actual:string -\> loc:node_id -\> boundary_issue (\* Type doesn’t match across boundary \*) \| IssueTaintedCrossing : source:taint_source -\> loc:node_id -\> boundary_issue (\* Tainted data crosses without sanitization \*) \| IssueLifetimeMismatch : loc:node_id -\> boundary_issue (\* Reference may outlive its target \*) \| IssueThreadSafety : loc:node_id -\> boundary_issue (\* Non-thread-safe value crosses to concurrent context \*) \| IssueSerializationUnsafe : type\_:string -\> loc:node_id -\> boundary_issue (\* Type may not round-trip through serialization \*)

</div>

The `analyze_boundary` function takes a CPG, a boundary specification, and the arguments being passed across the boundary. It iterates through each argument, checking for violations of the axioms that the source language guarantees but the target language does not. The `detect_boundaries` function scans the CPG for FFI and RPC call sites, constructing boundary records for each cross-language call discovered.

<div class="fstarcode">

val analyze_boundary : cpg -\> boundary -\> args:list (node_id \* abstract_value) -\> list boundary_issue

let analyze_boundary cpg boundary args = let risks = boundary_risks boundary in let issues = ref \[\] in List.iter (fun (arg_node, arg_val) -\> (\* Null safety risk \*) if Set.mem AxNullSafe risks then if may_be_null cpg arg_node then issues := IssueNullCrossing arg_node :: !issues; (\* Memory safety risk \*) if Set.mem AxMemSafe risks then if is_reference arg_val then issues := IssueOwnershipUnclear arg_node :: !issues; (\* Type safety risk \*) if Set.mem AxTypeSafe risks then let expected = get_expected_type boundary arg_node in let actual = get_actual_type cpg arg_node in if not (types_compatible expected actual) then issues := IssueTypeMismatch expected actual arg_node :: !issues; (\* Race freedom risk \*) if Set.mem AxRaceFree risks then if is_shared_mutable arg_val then issues := IssueThreadSafety arg_node :: !issues; ) args; !issues

val detect_boundaries : cpg -\> list boundary let detect_boundaries cpg = fold_nodes cpg (fun boundaries node -\> match node.kind with \| NCall when is_ffi_call cpg node.id -\> let source = get_source_language cpg node.id in let target = get_target_language cpg node.id in source_lang = source; target_lang = target; mechanism = BoundaryFFI; call_site = node.id :: boundaries \| NCall when is_rpc_call cpg node.id -\> source_lang = get_source_language cpg node.id; target_lang = infer_target_language cpg node.id; mechanism = BoundaryRPC; call_site = node.id :: boundaries \| \_ -\> boundaries ) \[\]

</div>

### Realizability Models for Semantic Soundness

**Paper**: **\[Patterson22\]** (Semantic Soundness for Language Interoperability)

<div class="pillarbox">

Matthews’ boundary terms (Section 9.1.1) provide operational semantics but don’t answer: “When is a type conversion **SEMANTICALLY SOUND**?”

**Patterson’s critique**: “Matthews-Findler-style boundaries give an elegant, abstract model but they don’t reflect reality... understanding of what datatypes should be convertible depends on how sources are **COMPILED** and how data is **REPRESENTED** in the target.”

For **REAL** soundness proofs, we need **SEMANTIC** models.

</div>

**Realizability Model** **\[Patterson22\]**:
``` math
V[\tau] = \text{set of TARGET values that ``behave as'' source type } \tau
```

Instead of syntactic conversion rules, we:

1.  Interpret types from BOTH languages as sets of target terms

2.  Define convertibility as GLUE CODE that preserves type membership

3.  Prove soundness by showing glue code maps $`V[\tau_1]`$ into $`V[\tau_2]`$

**Convertibility Judgment**:

$`\tau_A \sim \tau_B`$ means: types are interconvertible with sound glue code.

Requires:

- $`\text{glue}_{A \to B} : \text{target} \to \text{target}`$

- $`\text{glue}_{B \to A} : \text{target} \to \text{target}`$

- $`\forall v.\; v \in V[\tau_A] \Rightarrow \text{glue}_{A \to B}(v) \in V[\tau_B]`$

- $`\forall v.\; v \in V[\tau_B] \Rightarrow \text{glue}_{B \to A}(v) \in V[\tau_A]`$

**Example**: <span class="sans-serif">Python</span> int $`\leftrightarrow`$ <span class="sans-serif">C</span> long

- $`V[\text{Python int}]`$ = arbitrary-precision integers

- $`V[\text{C long}]`$ = 64-bit signed integers

- $`\text{glue}_{Py \to C}(n) = \textbf{if } |n| > 2^{63} \textbf{ then raise OverflowError else } n`$

- NOT identity! Glue code enforces target constraints.

<div class="theorem">

**Theorem 1.9** (Shared Memory Identity Constraint (Patterson 2022, Section 4.2)). *To share `ref` $`\tau_1`$ with `ref` $`\tau_2`$ via IDENTITY (no copy):
``` math
\text{glue}_{1 \to 2} = \text{id} \;\land\; \text{glue}_{2 \to 1} = \text{id}
\quad \Rightarrow \quad
V[\tau_1] = V[\tau_2]
```
Type interpretations must be **IDENTICAL**.*

</div>

**Implications**:

- <span class="sans-serif">Rust</span> `&mut i32` $`\leftrightarrow`$ <span class="sans-serif">C</span> `int*`: Both have same target representation. Identity glue is safe.

- $`\times`$ <span class="sans-serif">Python</span> `list` $`\leftrightarrow`$ <span class="sans-serif">C</span> array: Different representations. MUST copy or use opaque handle.

- $`\times`$ <span class="sans-serif">Java</span> `String` $`\leftrightarrow`$ <span class="sans-serif">Rust</span> `String`: UTF-16 vs UTF-8. MUST convert encoding + copy.

<div class="fstarcode">

(\* Target representation equivalence \*) type target_repr = \| TRWord of nat (\* n-bit word \*) \| TRPointer of target_repr (\* pointer to \*) \| TRStruct of list target_repr \| TROpaque (\* Cannot compare \*)

let get_target_repr (lang : language_config) (ty : ir_type) : target_repr = match lang.name, ty with \| "Rust", TInt 32 -\> TRWord 32 \| "C", TInt 32 -\> TRWord 32 \| "Python", TInt \_ -\> TROpaque (\* Arbitrary precision \*) \| \_, TPointer inner -\> TRPointer (get_target_repr lang inner) \| \_ -\> TROpaque

(\* Check if shared mutable reference is safe \*) let check_shared_ref_safe (b : boundary) (ty1 ty2 : ir_type) : option boundary_issue = let repr1 = get_target_repr b.source_lang ty1 in let repr2 = get_target_repr b.target_lang ty2 in if repr1 \<\> repr2 then Some (IssueSharedRefUnsafe ty1 ty2 b.call_site) else None

type boundary_issue = (\* ... existing issues ... \*) \| IssueSharedRefUnsafe : ty1:ir_type -\> ty2:ir_type -\> site:node_id -\> boundary_issue (\* NEW \*)

</div>

**GC-Linear Interoperability Asymmetry**:

**Direction matters**:

- **Linear $`\to`$ GC**: SAFE to convert directly. Linear guarantee = no aliases exist. `gcmov` instruction transfers ownership to GC. No copy needed.

- **GC $`\to`$ Linear**: REQUIRES COPY. GC reference may have unknown aliases. Cannot guarantee uniqueness. Must copy data to establish linear ownership.

This asymmetry affects:

- <span class="sans-serif">Rust</span> $`\to`$ <span class="sans-serif">Python</span>: can pass `Box<T>` directly (linear $`\to`$ GC)

- <span class="sans-serif">Python</span> $`\to`$ <span class="sans-serif">Rust</span>: must copy `PyObject` to owned `T` (GC $`\to`$ linear)

See Section 12.7 for full realizability theorems.

### Cross-Language Dynamic Information Flow Analysis (PolyCruise)

**Paper**: **\[Li22\]** — “PolyCruise: A Cross-Language Dynamic Information Flow Analysis”

<div class="pillarbox">

Single-language analyzers miss vulnerabilities at language boundaries. Even if individual language units are secure, the combined system may not be.

**Example (<span class="sans-serif">Python</span>-<span class="sans-serif">C</span>)**: Buffer overflow in NumPy

- <span class="sans-serif">Python</span>: `data = input()` \# Unconstrained user input

- <span class="sans-serif">C</span> (via FFI): `memcpy(buf, data)` \# No bounds check $`\to`$ buffer overflow

Neither <span class="sans-serif">Python</span> nor <span class="sans-serif">C</span> analysis alone sees the full flow!

PolyCruise found **14 unknown cross-language vulnerabilities** (8 CVEs).

</div>

**The Challenge**:

- Static semantic unification across languages is EXTREMELY HARD

- Different memory models, type systems, evaluation strategies

- Pure static cross-language analysis doesn’t scale

**PolyCruise Solution**: Hybrid Static + Dynamic

- **LIGHT STATIC**: Determine WHAT to instrument (cheap)

- **HEAVY DYNAMIC**: Track ACTUAL flows at runtime (precise)

Key insight: While SEMANTIC analysis across languages is hard, SYNTACTIC analysis (def/use) can be unified.

#### Language-Independent Symbolic Representation (LISR)

PolyCruise’s key innovation is the **Language-Independent Symbolic Representation (LISR)**, which provides a unified intermediate form for def/use analysis across different programming languages. While full semantic unification across languages is extremely difficult due to differences in memory models and type systems, **syntactic** def/use analysis can be unified. The following F\* code defines LISR’s core types: `symbolic_name` abstracts language-specific identifiers into a canonical form, and `lisr_stmt` represents statements in a language-agnostic way. The `LisrBoundary` constructor explicitly marks cross-language calls.

<div class="fstarcode">

module BrrrMachine.CrossLang.LISR

(\* Symbolic names abstract away language-specific identifiers \*) type symbolic_name = \| SymLocal : func_id:string -\> name:string -\> symbolic_name \| SymGlobal : module_id:string -\> name:string -\> symbolic_name \| SymParam : func_id:string -\> index:nat -\> symbolic_name \| SymReturn : func_id:string -\> symbolic_name \| SymField : base:symbolic_name -\> field:string -\> symbolic_name

(\* LISR statements - language-agnostic representation \*) type lisr_stmt = \| LisrAssign : lhs:symbolic_name -\> rhs:lisr_expr -\> lisr_stmt \| LisrCall : ret:option symbolic_name -\> callee:symbolic_name -\> args:list lisr_expr -\> lisr_stmt \| LisrOutput : sink_kind:taint_sink -\> arg:lisr_expr -\> lisr_stmt \| LisrInput : source_kind:taint_source -\> target:symbolic_name -\> lisr_stmt \| LisrBranch : cond:lisr_expr -\> lisr_stmt \| LisrBoundary : direction:boundary_dir -\> mechanism:boundary_mechanism -\> inner:lisr_stmt -\> lisr_stmt (\* Cross-language call marker \*)

type lisr_expr = \| LExprSym : symbolic_name -\> lisr_expr \| LExprConst : value -\> lisr_expr \| LExprBinop : op:binop -\> lhs:lisr_expr -\> rhs:lisr_expr -\> lisr_expr \| LExprField : base:lisr_expr -\> field:string -\> lisr_expr \| LExprIndex : base:lisr_expr -\> index:lisr_expr -\> lisr_expr

type boundary_dir = BoundaryOut \| BoundaryIn (\* Calling out / returning in \*)

</div>

<div class="fstarcode">

(\* This is the key to cross-language analysis: language-agnostic def/use. \*)

val def_set : lisr_stmt -\> set symbolic_name let def_set stmt = match stmt with \| LisrAssign lhs \_ -\> Set.singleton lhs \| LisrCall (Some ret) \_ \_ -\> Set.singleton ret \| LisrInput \_ target -\> Set.singleton target \| LisrBoundary \_ \_ inner -\> def_set inner \| \_ -\> Set.empty

let rec use_expr_set expr = match expr with \| LExprSym sym -\> Set.singleton sym \| LExprConst \_ -\> Set.empty \| LExprBinop \_ l r -\> Set.union (use_expr_set l) (use_expr_set r) \| LExprField base \_ -\> use_expr_set base \| LExprIndex base idx -\> Set.union (use_expr_set base) (use_expr_set idx)

val use_set : lisr_stmt -\> set symbolic_name let use_set stmt = match stmt with \| LisrAssign \_ rhs -\> use_expr_set rhs \| LisrCall \_ \_ args -\> Set.unions (List.map use_expr_set args) \| LisrOutput \_ arg -\> use_expr_set arg \| LisrBranch cond -\> use_expr_set cond \| LisrBoundary \_ \_ inner -\> use_set inner \| \_ -\> Set.empty

(\* Convert language-specific IR to LISR \*) val to_lisr : language_config -\> ir_func -\> list lisr_stmt

</div>

#### Symbolic Dependence Analysis (SDA)

**Symbolic Dependence Analysis (SDA)** computes which program statements are potentially dependent on security-relevant criteria (taint sources and sinks). This determines the **instrumentation scope** for dynamic analysis—only statements in the dependence set need runtime instrumentation. The key insight is to include **both** true dependencies (def flows to use) **AND** anti-dependencies (use followed by def of same symbol), which ensures soundness without requiring expensive pointer analysis. The following F\* code defines the `symbolically_dependent` relation and the `compute_sym_dep_set` function that computes the transitive closure of symbolic dependencies.

<div class="fstarcode">

(\* SDA computes which statements are symbolically dependent on criteria (taint sources/sinks). This determines the INSTRUMENTATION SCOPE for dynamic analysis - we only instrument statements in the dependence set.

KEY INSIGHT: Include BOTH true dependencies AND anti-dependencies. This ensures soundness without expensive pointer analysis. \*)

module BrrrMachine.CrossLang.SDA

(\* Symbolic dependence relation \*) val symbolically_dependent : lisr_stmt -\> lisr_stmt -\> bool let symbolically_dependent si sj = let di = def_set si in let ui = use_set si in let dj = def_set sj in let uj = use_set sj in (\* True/flow dependence: D(Si) intersect U(Sj) != empty \*) not (Set.is_empty (Set.inter di uj)) \|\| (\* Anti-dependence: U(Si) intersect D(Sj) != empty - ensures soundness without aliasing \*) not (Set.is_empty (Set.inter ui dj))

(\* Compute transitive symbolic dependence set from criteria \*) val compute_sym_dep_set : program:list lisr_stmt -\> criteria:set symbolic_name -\> (\* Source/sink symbols \*) set nat (\* Statement indices in symbolic dependence set \*)

(\* Symbolic dependence summary for interprocedural analysis \*) type sds_summary = func_id : string; return_depends_on_params : list bool; (\* Per-parameter: does return depend on it? \*) params_depend_on_criteria : list bool; (\* Per-parameter: reachable from criteria? \*)

</div>

#### Dynamic Information Flow Graph (DIFG)

The **Dynamic Information Flow Graph (DIFG)** is constructed at runtime from instrumented execution events. Unlike static data flow graphs, the DIFG captures **actual** flows that occur during program execution, including those that cross language boundaries via FFI. The following F\* code defines the runtime event types (`execution_event`), the DIFG node types (`difg_node`), and the edge types (`difg_edge`). The `is_cross_language` field on edges explicitly tracks whether a flow crossed a language boundary, enabling detection of cross-language vulnerabilities.

<div class="fstarcode">

(\* DIFG is built at RUNTIME from instrumented execution events. It captures ACTUAL flows, including those crossing language boundaries. \*)

module BrrrMachine.CrossLang.DIFG

(\* Runtime execution events (from instrumentation) \*) type execution_event = \| EventDef : stmt_id:nat -\> sym:symbolic_name -\> value:runtime_value -\> lang:language_config -\> execution_event \| EventUse : stmt_id:nat -\> sym:symbolic_name -\> value:runtime_value -\> lang:language_config -\> execution_event \| EventBoundaryCross : direction:boundary_dir -\> mechanism:boundary_mechanism -\> args:list (symbolic_name \* runtime_value) -\> execution_event \| EventCall : caller_lang:language_config -\> callee_lang:language_config -\> callee:symbolic_name -\> execution_event \| EventReturn : func:symbolic_name -\> ret_val:option runtime_value -\> execution_event

(\* DIFG node types \*) type difg_node = \| DifgSource : source:taint_source -\> stmt_id:nat -\> lang:language_config -\> difg_node \| DifgSink : sink:taint_sink -\> stmt_id:nat -\> lang:language_config -\> difg_node \| DifgIntermediate : stmt_id:nat -\> sym:symbolic_name -\> lang:language_config -\> difg_node \| DifgBoundary : boundary:boundary -\> difg_node

(\* DIFG edge types \*) type difg_edge = from_node : difg_node; to_node : difg_node; flow_type : difg_flow_type; is_cross_language : bool;

type difg_flow_type = \| FlowDirect (\* Direct assignment \*) \| FlowParameter (\* Function parameter passing \*) \| FlowReturn (\* Function return value \*) \| FlowFFI (\* Cross-language FFI boundary \*) \| FlowCallback (\* C calling back to Python \*)

(\* Extract vulnerability witnesses from completed DIFG \*) type cross_lang_vulnerability = source : taint_source; source_lang : language_config; sink : taint_sink; sink_lang : language_config; path : list difg_node; boundary_crossings : list boundary;

val find_cross_lang_flows : difg_state -\> list cross_lang_vulnerability

</div>

#### Hybrid Cross-Language Taint Analysis

<div class="fstarcode">

(\* KEY INSIGHT: Use static analysis to determine WHAT to instrument, use dynamic analysis to track ACTUAL flows.

This avoids: - Scalability issues of pure dynamic (ORBS) - Semantic disparity issues of pure static cross-language analysis

PERFORMANCE (from paper): - Static phase: \<3 seconds for 220 KSLOC, \<3 minutes for 6,419 KSLOC - Runtime overhead: 2.71x - 11.96x slowdown - Precision: 93.5

module BrrrMachine.CrossLang.HybridAnalysis

(\* Multi-language program representation \*) type language_unit = language : language_config; source_file : string; lisr : list lisr_stmt; entry_points : list symbolic_name;

type multilang_program = units : list language_unit; boundaries : list boundary; main_entry : symbolic_name;

(\* Instrumentation point - what to instrument in each language \*) type instrumentation_point = stmt_id : nat; kind : instrumentation_kind; language : language_config;

type instrumentation_kind = \| InstrDef : symbolic_name -\> instrumentation_kind \| InstrUse : symbolic_name -\> instrumentation_kind \| InstrCall : callee:symbolic_name -\> instrumentation_kind \| InstrBoundary : boundary:boundary -\> instrumentation_kind

(\* PHASE 1: Static - compute instrumentation scope via SDA \*) val compute_instrumentation_points : program:multilang_program -\> sources:set taint_source -\> sinks:set taint_sink -\> list instrumentation_point

(\* PHASE 2: Dynamic - execute with instrumentation, build DIFG \*) val execute_with_instrumentation : program:multilang_program -\> instr_points:list instrumentation_point -\> inputs:list runtime_value -\> difg_state

(\* Combined analysis \*) val cross_language_taint_analysis : program:multilang_program -\> sources:set taint_source -\> sinks:set taint_sink -\> inputs:list runtime_value -\> list cross_lang_vulnerability

</div>

<div class="pillarbox">

PolyCruise complements the primarily **STATIC** boundary analysis in 9.1.1–9.1.3:

**Static (Sections 9.1.1–9.1.3)**:

- Boundary detection and risk classification **\[Matthews07\]**

- Type consistency checking **\[Siek06\]**

- Realizability models **\[Patterson22\]**

- FFI contract verification (VeriFFI, Section 9.4)

**Dynamic (PolyCruise, Section 9.1.4)**:

- LISR for unified cross-language def/use

- SDA for instrumentation guidance

- DIFG for precise runtime flow tracking

- Actual vulnerability witness generation

**Recommended Workflow**:

1.  Static boundary analysis (9.1.1–9.1.3) for risk assessment

2.  PolyCruise SDA (9.1.4.2) to scope instrumentation

3.  Dynamic DIFG (9.1.4.3) for specific test inputs

4.  Feed results into taint classification (Section 12.3)

</div>

## Hole Tagging for Evaluation Contexts

**Source**: **\[Matthews07\]** — “Operational Semantics for Multi-Language Programs”

### The Language Bleeding Problem

**Problem**: Without hole tagging, reduction rules can “bleed” across languages.

**Example (ML expression with embedded Scheme)**:
``` math
\text{ML}[ 1 + \text{MS}(\text{SM}[\text{Scheme-expr}]) ]
```

**Without Tagging**: If we reduce “$`1 + \_`$” first, the Scheme-expr is evaluated by ML rules! This violates language semantics.

**Solution**: Tag holes with their expected language.

### Tagged Evaluation Contexts

<div class="fstarcode">

type eval_hole = \| HoleML : eval_hole \| HoleScheme : eval_hole \| HolePython : eval_hole \| HoleRust : eval_hole \| HoleBoundary : source:lang_id -\> target:lang_id -\> eval_hole

(\* Evaluation context with language annotations \*) type eval_context = \| ECHole : hole:eval_hole -\> eval_context \| ECBinOpL : op:binop -\> ctx:eval_context -\> rhs:expr -\> eval_context \| ECBinOpR : op:binop -\> lhs:value -\> ctx:eval_context -\> eval_context \| ECApp : ctx:eval_context -\> arg:expr -\> eval_context \| ECBoundary : direction:boundary_dir -\> ty:ir_type -\> ctx:eval_context -\> eval_context

(\* Fill hole with expression \*) val fill : eval_context -\> expr -\> expr let rec fill ctx e = match ctx with \| ECHole \_ -\> e \| ECBinOpL op inner rhs -\> EBinOp op (fill inner e) rhs \| ECBinOpR op lhs inner -\> EBinOp op lhs (fill inner e) \| ECApp inner arg -\> EApp (fill inner e) arg \| ECBoundary dir ty inner -\> EBoundary dir ty (fill inner e)

</div>

### Language-Respecting Reduction

**Reduction Rule Constraint**: A reduction rule for language $`L`$ can only fire when the redex is at a hole tagged with $`L`$.

**Formally**: If $`e \to e'`$ by $`L`$-rule, then in context $`C[e]`$:

- $`C`$ has hole tagged $`L`$

- All intervening boundaries are properly crossed

**Implementation**:

    step(lang, ctx, expr) =
      let hole_lang = tag_of_hole(ctx) in
      if hole_lang = lang then
        apply_rules(lang, expr)
      else
        (* Cannot reduce here - wrong language context *)
        Stuck

### Boundary Crossing Updates Tags

<div class="fstarcode">

(\* When crossing a boundary, update the hole tag \*) val cross_boundary : eval_context -\> boundary_dir -\> lang_id -\> eval_context let cross_boundary ctx dir target_lang = (\* The inner context now expects target_lang \*) update_inner_hole ctx (HoleBoundary ... target_lang)

(\* Example: ML calling Python \*) let ml_python_example = ECApp (ECHole HoleML) (\* ML context \*) (EBoundary ToPython int_ty (\* Boundary \*) (ECHole HolePython)) (\* Now Python context \*)

</div>

## Guard Generation Algorithm

**Source**: **\[Matthews07\]** Section 3.3

### Guard Polarity

**Key Insight**: Guards have **POLARITY** that determines checking direction.

**Positive polarity**: Value entering STRICTER type system

- Dynamic $`\to`$ Static (e.g., <span class="sans-serif">Python</span> $`\to`$ <span class="sans-serif">Rust</span>)

- Must CHECK at runtime (source doesn’t guarantee)

**Negative polarity**: Value from STRICTER type system

- Static $`\to`$ Dynamic (e.g., <span class="sans-serif">Rust</span> $`\to`$ <span class="sans-serif">Python</span>)

- Can SKIP check (source guarantees type)

**Critical**: Polarity **FLIPS** at function argument position! (Contravariance in domain)

### Polarity Determination

<div class="fstarcode">

(\* Determine initial polarity based on language pair \*) val initial_polarity : lang_id -\> lang_id -\> guard_polarity let initial_polarity source target = let source_strict = type_strictness source in let target_strict = type_strictness target in if target_strict \> source_strict then Positive else Negative

(\* Flip polarity (used at function arguments) \*) val flip : guard_polarity -\> guard_polarity let flip Positive = Negative let flip Negative = Positive

</div>

### Guard Generation

<div class="fstarcode">

(\* Generate guard for type at given polarity \*) val generate_guard : ir_type -\> guard_polarity -\> guard let rec generate_guard ty pol = match ty, pol with (\* Base types \*) \| TInt, Positive -\> GCheckNumber \| TInt, Negative -\> GNoCheck \| TBool, Positive -\> GCheckBool \| TBool, Negative -\> GNoCheck \| TString, Positive -\> GCheckString \| TString, Negative -\> GNoCheck

(\* Nullable / Option types \*) \| TOption inner, Positive -\> GCompose GCheckNotNull (generate_guard inner Positive) \| TOption inner, Negative -\> GNoCheck

(\* Function types - FLIP polarity for argument! \*) \| TArrow arg_ty ret_ty, pol -\> GFunctionWrap (generate_guard arg_ty (flip pol)) (\* CONTRAVARIANT \*) (generate_guard ret_ty pol) (\* COVARIANT \*)

(\* Product types \*) \| TProd t1 t2, pol -\> GCompose (generate_guard t1 pol) (generate_guard t2 pol)

(\* Reference types need type check \*) \| TRef inner, Positive -\> GCheckType (TRef inner) \| TRef inner, Negative -\> GNoCheck

(\* Polymorphic types \*) \| TForall \_ \_, Positive -\> GDynamic (\* Must check dynamically \*) \| TForall \_ \_, Negative -\> GNoCheck

(\* Any/Dynamic type \*) \| TAny, \_ -\> GDynamic

</div>

### Function Wrapping Example

**Example**: <span class="sans-serif">Python</span> function $`\to`$ <span class="sans-serif">Rust</span> callback

- <span class="sans-serif">Python</span> function type: `(Any) -> Any`

- <span class="sans-serif">Rust</span> callback type: `(i32) -> String`

At boundary (<span class="sans-serif">Python</span> $`\to`$ <span class="sans-serif">Rust</span>), polarity = Positive

Generate guard for `(i32) -> String` at Positive:

- Argument `i32`: `flip(Positive)` = Negative $`\to`$ `GNoCheck` (<span class="sans-serif">Rust</span> provides `i32`, <span class="sans-serif">Python</span> receives)

- Return `String`: Positive $`\to`$ `GCheckString` (<span class="sans-serif">Python</span> returns, must verify is string)

**Wrapped Function**:

    fn wrapped(arg: i32) -> String {
      let py_result = python_fn(arg);  // No check on arg
      GCheckString(py_result)           // Check return
    }

### Integration with Boundary Analysis

<div class="fstarcode">

(\* Complete boundary crossing with guards \*) val cross_with_guards : source:lang_id -\> target:lang_id -\> ty:ir_type -\> value:value -\> brrr_result value

let cross_with_guards source target ty v = let pol = initial_polarity source target in let guard = generate_guard ty pol in match apply_guard guard v with \| GPass v’ -\> Ok v’ \| GFail expected actual -\> Error (BoundaryTypeError at_boundary = (source, target); expected_type = expected; actual_value = actual; )

(\* Static analysis: verify guards sufficient \*) val verify_boundary_safety : cpg -\> boundary_node -\> list boundary_issue

let verify_boundary_safety cpg bnd = let guard = generate_guard bnd.ty (initial_polarity bnd.source bnd.target) in let incoming_types = analyze_incoming_types cpg bnd in List.filter_map (fun ty -\> if guard_covers ty guard then None else Some (IssueGuardInsufficient bnd ty guard) ) incoming_types

</div>

### Guards with Type Refinement Propositions

**Source**: **\[TobinHochstadt08\]** + **\[Matthews07\]**

<div class="pillarbox">

**Key Insight**: Guards are not just runtime checks — they also establish **TYPE KNOWLEDGE** for subsequent code.

**When a guard CHECK PASSES**:

- We know the value HAS the checked type

- This is a VISIBLE PREDICATE **\[TobinHochstadt08\]**

- Subsequent code can use this type information

**When a guard CHECK FAILS**:

- Exception/error raised — unreachable code

- But in analysis, we can refine with NEGATIVE proposition

**Polarity Determines Which Branch Gets Which Refinement**:

- **Positive guard** (checking incoming value): Pass branch: value HAS type $`\tau`$; Fail branch: unreachable

- **Negative guard** (checking outgoing value): Pass branch: value HAS type $`\tau`$ (trivially); Fail branch: BUG in source

</div>

<div class="fstarcode">

(\* Guard result includes type proposition for refinement \*) type guard_with_refinement = guard : guard; positive_prop : type_prop; (\* Proposition when guard passes \*) negative_prop : type_prop; (\* Proposition when guard fails \*) target_var : option string; (\* Variable being checked, if known \*)

(\* Generate guard with associated type propositions \*) val generate_guard_with_prop : ir_type -\> guard_polarity -\> option string -\> guard_with_refinement

let generate_guard_with_prop ty pol var_opt = let guard = generate_guard ty pol in let make_prop = match var_opt with \| Some v -\> (fun t -\> PropHasType v t) \| None -\> (fun \_ -\> PropTrue) in match pol with \| Positive -\> (\* Checking incoming value - pass means has type \*) guard; positive_prop = make_prop ty; negative_prop = PropFalse; (\* Unreachable - error raised \*) target_var = var_opt \| Negative -\> (\* Checking outgoing value - should always pass \*) guard; positive_prop = PropTrue; (\* Source guarantees type \*) negative_prop = PropFalse; (\* Bug if reached \*) target_var = var_opt

(\* Apply guard and get refined type environment \*) type guard_application_result = \| GuardPassed of value : ir_value; env_refinement : prop_type_env -\> prop_type_env; \| GuardFailed of expected : ir_type; actual : ir_value;

</div>

### Guard Composition Optimization

**Optimization**: Cancel redundant guards at boundary chains.

If value crosses: $`A \to B \to C`$, then Guard($`A \to B`$) followed by Guard($`B \to C`$) can often simplify to: Guard($`A \to C`$).

**Rules**:

- Positive $`\circ`$ Positive = Positive (still need check)

- Negative $`\circ`$ Negative = Positive (double negative = positive)

- Positive $`\circ`$ Negative = Negative

- Negative $`\circ`$ Positive = Negative

**Optimization**:

- `GNoCheck` $`\circ`$ G = G

- G $`\circ`$ `GNoCheck` = G

- `GCheckT` $`\circ`$ `GCheckT` = `GCheckT` (idempotent)

## FFI Contracts

**Source**: VeriFFI **\[Wang25\]** — “A Verified Foreign Function Interface between Coq and C”

<div class="pillarbox">

FFI boundaries require **EXPLICIT contracts** specifying:

- Input type representation requirements

- Output type representation guarantees

- Memory ownership transfer

- Side effect bounds

**Without Contracts**: FFI calls are opaque black boxes.

**With Contracts**: FFI calls can be verified for type safety.

**Key Insight**: Contracts bridge the semantic gap between languages by specifying both **WHAT** the function does and **HOW** values are represented.

**Cross-References**:

- Section 7.5: Representation predicates used in contracts

- Section 9.1.2: Contracts formalize boundary risk mitigation

- Section 9.3: Guards generated from contract type requirements

- Section 12.7: Realizability theorems justify contract soundness

</div>

### Contract Structure

FFI contracts formalize the requirements for safely calling across language boundaries. The following F\* code defines the `ffi_contract` type which captures:

- **Type mappings** between source and foreign types

- **Preconditions** that the caller must establish (representation predicates, ownership)

- **Postconditions** that the callee guarantees (memory effects, error handling)

- **Ownership transfer** semantics for resource management across boundaries

The `type_mapping` type describes how values are transformed when crossing the boundary: directly (`TMDirect`), via conversion (`TMConverted`), or as opaque references (`TMOpaque`).

<div class="fstarcode">

module BrrrMachine.FFIContract

(\* Type Mappings Across Boundaries \*) type type_mapping = \| TMDirect (\* Same representation in both languages. Example: i32 (Rust) \<-\> int (C) \*) \| TMBoxed (\* Heap-allocated wrapper around the value. Example: Python int \<-\> boxed arbitrary-precision integer \*) \| TMConverted of convert_fn : ir_value -\> foreign_value; unconvert_fn : foreign_value -\> option ir_value; (\* Explicit conversion required. Example: Rust String (UTF-8) \<-\> Java String (UTF-16) \*) \| TMOpaque (\* No access across boundary - treat as opaque handle. Example: Python object \<-\> void\* in C \*)

(\* FFI Contract Type \*) type ffi_contract = foreign_function : string; source_language : language_id; target_language : language_id; param_types : list (ir_type \* foreign_type \* type_mapping); return_type : (ir_type \* foreign_type \* type_mapping); precondition : ffi_precondition; postcondition : ffi_postcondition; ownership_transfer : ownership_spec; effect_bounds : effect_row;

</div>

<div class="fstarcode">

type rep_requirement = \| RepRequired : t:Type -\> rep_predicate t -\> rep_requirement \| RepNonNull : rep_requirement \| RepInitialized : rep_requirement \| RepAligned : alignment:nat -\> rep_requirement \| RepSized : min_size:nat -\> rep_requirement

type ffi_precondition = rep_requirements : list (var_id \* rep_requirement); ownership_requirements : list (var_id \* access_permission); value_constraints : list (var_id \* ir_expr); thread_requirements : option thread_safety_requirement;

type thread_safety_requirement = \| TSRSingleThreaded (\* Must be called from single thread \*) \| TSRMainThread (\* Must be called from main thread \*) \| TSRWithLock of lock_id (\* Must hold specified lock \*) \| TSRAnyThread (\* Safe to call from any thread \*)

</div>

<div class="fstarcode">

type memory_effect_spec = \| MEUnchanged (\* Memory not modified \*) \| MEModifiedOnly of set address (\* Only specified addresses modified \*) \| MEMayAllocate (\* May allocate new memory \*) \| MEMayFree of set address (\* May free specified addresses \*) \| MEArbitrary (\* No guarantees (unsafe) \*)

type ownership_transfer = \| OTRetained (\* Ownership unchanged \*) \| OTTransferred (\* Ownership transferred to callee \*) \| OTBorrowed of lifetime_bound (\* Temporarily borrowed \*) \| OTShared (\* Shared access granted \*)

type ffi_postcondition = return_rep : rep_requirement; memory_effects : memory_effect_spec; ownership_effects : list (var_id \* ownership_transfer); error_conditions : list (ir_expr \* error_behavior); may_trigger_gc : bool;

type error_behavior = \| EBException of ir_type (\* Throws exception of type \*) \| EBReturnCode of ir_value (\* Returns error code \*) \| EBSetErrno (\* Sets errno \*) \| EBAbort (\* Aborts process \*) \| EBUndefined (\* Undefined behavior \*)

</div>

### Contract Generation

<div class="fstarcode">

(\* For common patterns, we can generate contracts automatically from type signatures and calling conventions. \*)

val generate_ffi_contract : source_type : ir_type -\> target_type : foreign_type -\> calling_convention : calling_conv -\> ffi_contract

val infer_type_mapping : ir_type -\> foreign_type -\> type_mapping let infer_type_mapping src tgt = match src, tgt with \| TInt I32 true, ForeignInt32 -\> TMDirect \| TInt I64 true, ForeignInt64 -\> TMDirect \| TFloat F32, ForeignFloat -\> TMDirect \| TFloat F64, ForeignDouble -\> TMDirect \| TBool, ForeignBool -\> TMDirect \| TPtr inner_src, ForeignPtr inner_tgt -\> let inner_mapping = infer_type_mapping inner_src inner_tgt in if inner_mapping = TMDirect then TMDirect else TMOpaque \| TString, ForeignCString -\> TMConverted convert_fn = string_to_cstring; unconvert_fn = cstring_to_string; \| TStruct \_ \_, \_ -\> TMOpaque \| TVariant \_ \_, \_ -\> TMOpaque \| \_, \_ -\> TMOpaque

type calling_conv = \| CCDefault (\* Default C calling convention \*) \| CCStdcall (\* Windows stdcall \*) \| CCFastcall (\* Fast register-based calling \*) \| CCRust (\* Rust ABI \*) \| CCOwned (\* Ownership transferred \*) \| CCBorrowed (\* Immutable borrow \*) \| CCMutBorrowed (\* Mutable borrow \*)

</div>

<div class="fstarcode">

type ffi_annotation = \| AnnNonnull of var_id (\* Parameter cannot be null \*) \| AnnNullable of var_id (\* Parameter may be null \*) \| AnnOwned of var_id (\* Ownership transferred \*) \| AnnBorrowed of var_id (\* Borrowed reference \*) \| AnnOut of var_id (\* Output parameter \*) \| AnnInOut of var_id (\* Input/output parameter \*) \| AnnArraySize of var_id \* nat (\* Array with fixed size \*) \| AnnBufferSize of var_id \* var_id (\* Buffer sized by another param \*) \| AnnMayAllocate (\* May allocate memory \*) \| AnnMayFree (\* May free memory \*) \| AnnPure (\* No side effects \*) \| AnnThreadSafe (\* Safe for concurrent calls \*)

val refine_contract_with_annotations : ffi_contract -\> list ffi_annotation -\> ffi_contract

</div>

### Contract Verification

<div class="fstarcode">

type ffi_violation = \| RepViolation : param:var_id -\> expected:rep_requirement -\> actual:abs_value -\> ffi_violation \| OwnershipViolation : param:var_id -\> required:access_permission -\> actual:access_permission -\> ffi_violation \| TypeMismatch : expected:foreign_type -\> actual:ir_type -\> ffi_violation \| NullViolation : param:var_id -\> ffi_violation \| UninitializedViolation : param:var_id -\> ffi_violation \| ThreadViolation : required:thread_safety_requirement -\> actual:thread_context -\> ffi_violation \| MissingContract : function_name:string -\> ffi_violation \| ConversionFailure : param:var_id -\> from_type:ir_type -\> to_type:foreign_type -\> ffi_violation

</div>

<div class="fstarcode">

val verify_ffi_call : call_site : node_id -\> contract : ffi_contract -\> abstract_state : abs_state -\> result (abs_state, list ffi_violation)

val check_rep_requirement : abs_state -\> abs_value -\> rep_requirement -\> option string

let check_rep_requirement state val req = match req with \| RepNonNull -\> if may_be_null state val then Some "may be null" else None \| RepInitialized -\> if may_be_uninitialized state val then Some "may be uninitialized" else None \| RepAligned align -\> if not (is_aligned state val align) then Some "misaligned" else None \| RepSized min_size -\> if not (has_min_size state val min_size) then Some "too small" else None \| RepRequired t rp -\> if not (may_satisfy_rep graph addr rp) then Some "rep predicate fails" else None

val permission_satisfies : access_permission -\> access_permission -\> bool

</div>

### Reified Type Descriptors

<div class="fstarcode">

(\* Type descriptors that exist at runtime, enabling: - Dynamic type checking at FFI boundaries - Automatic marshalling code generation - Reflection-based contract verification \*)

module BrrrMachine.ReifiedTypes

type type_descriptor = \| TDPrimitive : prim:primitive_type -\> size:nat -\> align:nat -\> type_descriptor \| TDPointer : pointee:type_descriptor -\> type_descriptor \| TDStruct : name:string -\> fields:list field_descriptor -\> type_descriptor \| TDArray : element:type_descriptor -\> length:nat -\> type_descriptor \| TDUnion : variants:list (string \* type_descriptor) -\> type_descriptor \| TDFunction : params:list type_descriptor -\> ret:type_descriptor -\> type_descriptor \| TDOpaque : name:string -\> type_descriptor

type primitive_type = \| PTVoid \| PTBool \| PTInt8 \| PTInt16 \| PTInt32 \| PTInt64 \| PTUInt8 \| PTUInt16 \| PTUInt32 \| PTUInt64 \| PTFloat32 \| PTFloat64 \| PTChar \| PTWChar

type field_descriptor = field_name : string; field_type : type_descriptor; field_offset : nat; field_padding : nat;

val reify_type : ir_type -\> type_descriptor val size_of : type_descriptor -\> nat val alignment_of : type_descriptor -\> nat val validate_against_descriptor : raw_bytes -\> type_descriptor -\> bool

</div>

### Integration with Boundary Analysis

<div class="pillarbox">

FFI contracts connect to the broader boundary analysis framework:

**Section 9.1.2 (Boundary Risk Analysis)**: When `BoundaryFFI` edges are detected, verify against FFI contracts. Contracts specify: representation predicate requirements, ownership transfer semantics, memory effect bounds.

**Section 9.3 (Guard Generation)**: Guards are generated from contract type requirements. Contract specifies WHAT to check; guards specify HOW.

**Part XI (IR Specification)**: IR types can be reified into runtime type descriptors (Section 9.4.4) for FFI boundary verification.

**Verification Flow**:

1.  Detect FFI call site

2.  Look up or generate contract

3.  Verify preconditions via abstract state

4.  Generate guards for runtime checks

5.  Apply postcondition to compute post-state

6.  Report violations

</div>

<div class="fstarcode">

val verify_all_ffi_calls : cpg -\> contracts:map string ffi_contract -\> list (node_id \* ffi_violation)

let verify_all_ffi_calls cpg contracts = let ffi_calls = find_ffi_call_sites cpg in List.concat_map (fun call_site -\> let func_name = get_called_function cpg call_site in match Map.find func_name contracts with \| Some contract -\> let state = get_abstract_state_at cpg call_site in (match verify_ffi_call call_site contract state with \| Ok \_ -\> \[\] \| Error violations -\> List.map (fun v -\> (call_site, v)) violations) \| None -\> \[(call_site, MissingContract func_name)\] ) ffi_calls

</div>

### Multilingual Type Inference for FFI

**Source**: **\[Furr08\]** — “Checking Type Safety of Foreign Function Calls”

<div class="pillarbox">

**Key Insight**: <span class="sans-serif">C</span> glue code has a RICHER view of types than source language. <span class="sans-serif">C</span> can observe physical representations (boxed vs unboxed, tags, offsets).

**Approach**:

1.  Extract type info from high-level language (OCaml, Java)

2.  Infer REPRESENTATIONAL TYPES for <span class="sans-serif">C</span> glue code

3.  Compare inferred types against declared types

4.  Report inconsistencies as potential FFI bugs

**Representational Types**: Model <span class="sans-serif">C</span>’s low-level view of high-level data.

- OCaml: $`(n, \pi)`$ where $`n`$ = nullary constructor count, $`\pi`$ = sum of products

- Java: `jt jobject` where `jt` embeds Java type info into <span class="sans-serif">C</span>’s `jobject`

**Flow-Sensitivity (O-Saffire for OCaml)**: Types of form $`ct\{B, I, T\}`$ where:

- $`B`$ = boxedness (boxed $`|`$ unboxed $`|`$ unknown)

- $`I`$ = offset into structured block

- $`T`$ = tag value or integer value

**Polymorphism (J-Saffire for JNI)**: Unification-based analysis with polymorphic signatures. Essential for analyzing wrapper functions and the 200+ JNI API functions. JNI uses string singletons to track class names and field descriptors across <span class="sans-serif">C</span> code.

**GC Safety**: Track which functions may invoke garbage collector via effects. Ensure pointers to GC’d heap are registered before GC-triggering calls.

**Bugs Found** (experimentally validated):

- O-Saffire: 24 errors + 22 suspicious patterns in 11 OCaml benchmarks

- J-Saffire: 156 errors + 124 suspicious patterns in 12 JNI benchmarks

</div>

The following F\* code defines **representational types** that model how C glue code views high-level language data. The `rep_type` algebraic data type captures sum types as (nullary_count, products) pairs, where nullary_count is the number of nullary constructors and products describes the fields of non-nullary constructors. The `flow_type` extends this with flow-sensitive information: `boxedness` (is it a pointer or an immediate value?), `offset` (position within a structured block), and `tag` (known discriminator value). The `gc_effect` type tracks whether a function may trigger garbage collection, which is critical for ensuring that pointers to the managed heap are properly registered before GC-triggering calls.

<div class="fstarcode">

module BrrrMachine.FFITypeSafety

type rep_type = \| RepInt (\* Unbounded integer - value unknown \*) \| RepConstructor of nat (\* Known nullary constructor tag \*) \| RepSum of nullary_count : nat; (\* Number of nullary constructors \*) products : list rep_product; (\* Nonnullary constructor representations \*) \| RepFunc of rep_type -\> gc_effect -\> rep_type \| RepVar of string (\* Type variable for inference \*)

and rep_product = list rep_type (\* Fields of a structured block \*)

and gc_effect = \| GCNone (\* Cannot trigger GC \*) \| GCMay (\* May trigger GC \*) \| GCVar of string (\* Effect variable for inference \*)

(\* Flow-sensitive type extensions \*) type boxedness = \| BBoxed (\* Known to be pointer to heap block \*) \| BUnboxed (\* Known to be unboxed (integer/tag) \*) \| BUnknown (\* Boxedness unknown \*) \| BBottom (\* Unreachable code \*)

type flow_type = base_type : rep_type; (\* The representational type \*) boxedness : boxedness; (\* Is it boxed or unboxed? \*) offset : option nat; (\* Offset into structured block \*) tag : option nat; (\* Known tag value \*)

</div>

<div class="fstarcode">

type ffi_type_violation = \| ViolationTypeMismatch of location : node_id; expected : rep_type; inferred : rep_type; \| ViolationUnregisteredGCPointer of location : node_id; pointer_var : string; gc_triggering_call : string; \| ViolationInvalidOffset of location : node_id; offset : nat; block_size : nat; \| ViolationWrongTag of location : node_id; tested_tag : nat; max_valid_tag : nat;

(\* Translate source type to representational type \*) val translate_source_type : ir_type -\> rep_type

(\* Analyze C glue code \*) val analyze_ffi_code : ir_func -\> source_types : map string ir_type -\> ffi_inference_result

</div>

## Occurrence Typing Analysis

**Source**: **\[TobinHochstadt08\]** (Typed Scheme)

<div class="pillarbox">

This section provides the full occurrence typing analysis, integrating:

- Type predicate detection (Section 9.5.1)

- Conditional refinement propagation (Section 9.5.2)

- Union type narrowing (Section 9.5.3)

- Language-specific handling (Section 9.5.4)

**Integration with CPG**:

- Type propositions flow along CFG edges

- DDG edges track type dependencies

- Guards (Section 9.3) produce type propositions

**Complements**:

- Gradual typing (Section 9.1.2) at module boundaries

- Taint analysis (Section 4.2) for security

- Nullability analysis (Section 2.1.7) for null safety

</div>

### Type Predicate Detection

Occurrence typing refines variable types based on runtime type tests. Different languages have different idioms for type testing: Python uses `isinstance()` and `is None`, TypeScript uses `typeof` and `instanceof`, Go uses type assertions and type switches. The following F\* code defines a unified representation `detected_type_test` that captures the essential information from any type test: the tested expression, the type being tested for, and both positive and negative type propositions. The `positive_prop` is established when the test succeeds (true branch), and `negative_prop` when it fails (false branch).

<div class="fstarcode">

module BrrrMachine.OccurrenceTyping

(\* Unified type test representation \*) type detected_type_test = tested_expr : ir_expr; (\* The expression being tested \*) tested_var : option string; (\* Variable if direct var test \*) test_type : ir_type; (\* Type being tested for \*) positive_prop : type_prop; (\* Proposition if test is true \*) negative_prop : type_prop; (\* Proposition if test is false \*) language : language_id; (\* Source language \*) node_id : node_id; (\* Location in CPG \*)

(\* Python: isinstance(x, T), type(x) is T \*) val detect_python_type_test : ir_expr -\> node_id -\> option detected_type_test let detect_python_type_test expr node = match expr with (\* isinstance(x, T) \*) \| ECall (EVar "isinstance") \[EVar var; EType ty\] -\> Some tested_expr = EVar var; tested_var = Some var; test_type = ty; positive_prop = PropHasType var ty; negative_prop = PropNotType var ty; language = Python; node_id = node; (\* x is None \*) \| EBinOp OpIs (EVar var) ENone -\> Some tested_expr = EVar var; tested_var = Some var; test_type = TNone; positive_prop = PropHasType var TNone; negative_prop = PropNotType var TNone; language = Python; node_id = node; \| \_ -\> None

</div>

<div class="fstarcode">

val detect_typescript_type_test : ir_expr -\> node_id -\> option detected_type_test let detect_typescript_type_test expr node = match expr with (\* typeof x === "string" etc. \*) \| EBinOp (OpEq \| OpStrictEq) (ECall (EVar "typeof") \[EVar var\]) (EString ty_name) -\> let ty = typescript_typeof_to_type ty_name in Some tested_expr = EVar var; tested_var = Some var; test_type = ty; positive_prop = PropHasType var ty; negative_prop = PropNotType var ty; language = TypeScript; node_id = node; (\* x instanceof T \*) \| EBinOp OpInstanceof (EVar var) (EType ty) -\> Some tested_expr = EVar var; tested_var = Some var; test_type = ty; positive_prop = PropHasType var ty; negative_prop = PropNotType var ty; language = TypeScript; node_id = node; (\* "prop" in obj - discriminated union check \*) \| EBinOp OpIn (EString prop) (EVar var) -\> Some tested_expr = EVar var; tested_var = Some var; test_type = TWithProperty prop; positive_prop = PropHasType var (TWithProperty prop); negative_prop = PropNotType var (TWithProperty prop); language = TypeScript; node_id = node; \| \_ -\> None

(\* TypeScript typeof string to type mapping \*) let typescript_typeof_to_type (s : string) : ir_type = match s with \| "string" -\> TString \| "number" -\> TNumber \| "boolean" -\> TBool \| "undefined" -\> TUndefined \| "object" -\> TObject \| "function" -\> TFunction \| "symbol" -\> TSymbol \| "bigint" -\> TBigInt \| \_ -\> TAny

</div>

### Conditional Refinement Propagation

<div class="fstarcode">

type branch_refinement = condition_node : node_id; true_branch : node_id; false_branch : node_id; true_refinement : prop_type_env -\> prop_type_env; false_refinement : prop_type_env -\> prop_type_env;

(\* Analyze a conditional and extract branch refinements \*) val analyze_conditional : cpg -\> node_id -\> language_id -\> option branch_refinement

(\* Truthiness test: if x: ... (Python), if (x) ... (JS/TS) \*) val analyze_truthiness_test : cpg -\> node_id -\> language_id -\> option branch_refinement

(\* Falsy types by language \*) let get_falsy_types (lang : language_id) : list ir_type = match lang with \| Python -\> \[TNone; TBool (\* False \*); TInt (\* 0 \*); TString (\* "" \*)\] \| JavaScript \| TypeScript -\> \[TNull; TUndefined; TBool; TNumber (\* 0, NaN \*); TString (\* "" \*)\] \| \_ -\> \[\]

</div>

<div class="fstarcode">

type occurrence_analysis_state = env_at_node : map node_id prop_type_env; worklist : set node_id;

val run_occurrence_analysis : cpg -\> language_id -\> occurrence_analysis_state

let run_occurrence_analysis cpg lang = let entry = get_entry_node cpg in let initial_env = create_initial_env cpg in let state = ref env_at_node = Map.singleton entry initial_env; worklist = Set.singleton entry; in (\* Worklist algorithm \*) while not (Set.is_empty !state.worklist) do let node = Set.choose !state.worklist in state := !state with worklist = Set.remove node !state.worklist ; let current_env = Map.find_default initial_env node !state.env_at_node in if is_conditional_node cpg node then begin match analyze_conditional cpg node lang with \| Some br -\> let true_env = br.true_refinement current_env in let false_env = br.false_refinement current_env in propagate_env state br.true_branch true_env; propagate_env state br.false_branch false_env \| None -\> propagate_to_successors cpg state node current_env end else begin let new_env = occurrence_transfer current_env (get_stmt cpg node) in propagate_to_successors cpg state node new_env end done; !state

</div>

### Union Type Narrowing

When occurrence typing refines a union type, it can either **narrow** the union (keeping only types that overlap with the test type) or **remove** specific types from the union. The following F\* code implements these operations. The `narrow_union` function filters a union to types that overlap with the tested type—if the result is `TBottom`, the branch is unreachable (useful for exhaustiveness checking). The `remove_from_union` function removes types that are subtypes of the tested type, used in the false branch of type tests. TypeScript’s discriminated unions receive special handling via the `discriminated_union` type.

<div class="fstarcode">

(\* Narrow a union type by restricting to a subtype \*) val narrow_union : ir_type -\> ir_type -\> ir_type let rec narrow_union original test_type = match original with \| TUnion types -\> let narrowed = List.filter (fun t -\> types_overlap t test_type ) types in (match narrowed with \| \[\] -\> TBottom (\* Contradiction - unreachable \*) \| \[t\] -\> t (\* Single type remaining \*) \| ts -\> TUnion ts) \| \_ -\> if types_overlap original test_type then original else TBottom

(\* Narrow a union type by removing a subtype \*) val remove_from_union : ir_type -\> ir_type -\> ir_type let rec remove_from_union original remove_type = match original with \| TUnion types -\> let remaining = List.filter (fun t -\> not (subtype t remove_type) ) types in (match remaining with \| \[\] -\> TBottom \| \[t\] -\> t \| ts -\> TUnion ts) \| \_ -\> if subtype original remove_type then TBottom else original

(\* Check if two types have any overlap \*) val types_overlap : ir_type -\> ir_type -\> bool let rec types_overlap t1 t2 = match t1, t2 with \| TBottom, \_ \| \_, TBottom -\> false \| TAny, \_ \| \_, TAny -\> true \| TUnion ts1, \_ -\> List.exists (fun t -\> types_overlap t t2) ts1 \| \_, TUnion ts2 -\> List.exists (fun t -\> types_overlap t1 t) ts2 \| \_, \_ -\> subtype t1 t2 \|\| subtype t2 t1

</div>

<div class="fstarcode">

(\* TypeScript discriminated union: union of objects with common literal property \*) type discriminated_union = discriminant : string; (\* Property name used to discriminate \*) variants : list (ir_value \* ir_type); (\* (literal value, variant type) \*)

(\* Detect discriminated union pattern \*) val detect_discriminated_union : ir_type -\> option discriminated_union

(\* Narrow discriminated union based on discriminant check \*) val narrow_discriminated_union : discriminated_union -\> string -\> ir_value -\> ir_type

(\* Example: type Shape = \| kind: "circle", radius: number \| kind: "square", side: number

After: if (shape.kind === "circle") Narrowed to: kind: "circle", radius: number \*)

</div>

### Language-Specific Type Refinement

<div class="fstarcode">

(\* Python: hasattr() check \*) val refine_hasattr : string -\> string -\> prop_type_env -\> prop_type_env let refine_hasattr var attr env = refine_positive env (PropHasType var (TWithAttribute attr))

(\* Python: callable() check \*) val refine_callable : string -\> prop_type_env -\> prop_type_env let refine_callable var env = refine_positive env (PropHasType var TCallable)

(\* Python: Literal type narrowing from match statement \*) val refine_match_case : string -\> ir_pattern -\> prop_type_env -\> prop_type_env

</div>

<div class="fstarcode">

(\* TypeScript: Array.isArray() \*) val refine_is_array : string -\> prop_type_env -\> prop_type_env let refine_is_array var env = refine_positive env (PropHasType var (TArray TAny))

(\* TypeScript: type predicate functions \*) type type_predicate_fn = fn_name : string; param_index : nat; asserted_type : ir_type;

(\* Detect user-defined type predicates: function isString(x): x is string \*) val detect_type_predicate : ir_func -\> option type_predicate_fn

(\* Apply user-defined type predicate \*) val refine_type_predicate : type_predicate_fn -\> list ir_expr -\> prop_type_env -\> prop_type_env

</div>

<div class="fstarcode">

(\* Go: type switch \*) type go_type_switch = switched_var : string; cases : list (ir_type \* node_id); (\* type, case body \*) default_case : option node_id;

val refine_type_switch_case : go_type_switch -\> ir_type -\> prop_type_env -\> prop_type_env

(\* Go: nil check for interfaces \*) val refine_nil_check : string -\> bool -\> prop_type_env -\> prop_type_env let refine_nil_check var is_nil env = if is_nil then refine_positive env (PropHasType var TNil) else refine_positive env (PropNotType var TNil)

</div>

### Integration with CPG Analysis

<div class="fstarcode">

(\* CPG node annotation for occurrence typing \*) type occurrence_annotation = refined_types : map string ir_type; (\* var -\> refined type at this point \*) active_props : list type_prop; (\* Active propositions \*)

(\* Annotate CPG with occurrence typing results \*) val annotate_cpg_with_occurrences : cpg -\> occurrence_analysis_state -\> unit

(\* Query refined type at a specific node \*) val get_refined_type : cpg -\> node_id -\> string -\> option ir_type let get_refined_type cpg node var = match get_node_annotation cpg node "occurrence" with \| Some ann -\> Map.find_opt var ann.refined_types \| None -\> None

(\* Use refined types in taint analysis \*) val refine_taint_with_occurrences : cpg -\> taint_state -\> node_id -\> taint_state

(\* Use refined types for null analysis \*) val refine_null_with_occurrences : cpg -\> null_state -\> node_id -\> null_state

</div>

**Cross-Reference Summary**:

- Section 2.1.7b: Occurrence type domain definition

- Section 9.1.2: Integration with gradual typing

- Section 9.3.5b: Guards produce type propositions

- Section 12.3.5: F\* soundness theorem

- Section 4.2: Integration with taint analysis

- Section 2.1.7: Integration with nullability domain

# Incrementality and Scalability

<div class="pillarbox">

**10.1 INCREMENTALITY:** Avoid re-analyzing unchanged code

- Adapton-style demand-driven computation

- Dependency tracking for minimal re-computation

**10.2 TIME BUDGETS:** Graceful degradation under resource pressure

- Per-function/file/total budgets

- Adaptive precision levels based on available time

**Together:** Scale from laptop to datacenter while maintaining useful results.

</div>

## Demand-Driven Incremental Computation

**Papers:** **\[Hammer14\]** (Adapton), **\[Wagner98\]**

Real-world analysis must be incremental: when a file changes, we shouldn’t re-analyze the entire codebase.

### The Adapton Model

<div class="definition">

**Definition 1.10** (Adapton’s Key Insight **\[Hammer14\]**). Track *dependencies* between computations. When input changes, only recompute *affected* outputs.

</div>

#### Performance Characteristics

**Performance Gains:** $`7\times`$ to $`2000\times`$ speedups over naive recomputation for typical edit-reanalyze workflows. Traditional eager IC often incurs $`4\times`$–$`500\times`$ *slowdowns* due to unnecessary recomputation.

#### The Demanded Computation Graph (DCG)

<div class="definition">

**Definition 1.11** (Demanded Computation Graph). The DCG consists of:

- **Nodes:** Thunks (suspended computations) and refs (mutable cells)

- **Edges:** Dependencies (which thunks read which refs/thunks)

</div>

#### Adapton Primitives

- $`\texttt{thunk}(e)`$ — Create suspended computation

- $`\texttt{force}(t)`$ — Evaluate thunk, cache result, record dependencies

- $`\texttt{ref}(v)`$ — Create mutable reference cell

- $`\texttt{get}(r)`$ — Read ref (records dependency)

- $`\texttt{set}(r,v)`$ — Write ref (triggers dirtying phase)

#### Inner/Outer Layer Separation

<div class="pillarbox">

**OUTER LAYER (Driver):**

- Allocates and mutates ref cells (set operations)

- Demands results by forcing thunks

- NOT incrementalized (runs once per change)

**INNER LAYER (Analysis computations):**

- May only READ refs (get) and force other thunks

- Cannot allocate new refs or mutate existing ones

- This restriction enables incremental reuse

</div>

#### Two-Phase Algorithm

##### Phase 1: Dirtying (on input change)

When $`\texttt{set}(r, \textit{new\_value})`$ is called:

1.  Mark $`r`$ as dirty

2.  Walk *backwards* through DCG edges

3.  Mark all transitive dependents as dirty

4.  Stop at thunks that don’t depend on $`r`$

##### Phase 2: Propagation (on demand)

When $`\texttt{force}(t)`$ is called on dirty thunk:

1.  Re-execute $`t`$’s computation

2.  If result **unchanged** from cached value: Mark $`t`$ clean, *don’t* propagate to dependents

3.  If result **changed**: Update cache, dependents stay dirty until forced

<div class="theorem">

**Theorem 1.12** (Key Insight). *Phase 2 is *lazy* — only runs when results demanded. This enables “demand-driven” incremental computation.*

</div>

#### Sharing, Swapping, Switching

Why DCG beats total-order IC:

- **SHARING:** Same subcomputation reused in different contexts

- **SWAPPING:** Reorder computations without invalidating memos

- **SWITCHING:** Toggle between computations, restore previous results

Traditional IC (Acar’s self-adjusting computation) uses *total order*, which prevents these reuse patterns. DCG uses *partial order*.

#### Adapton Example

    CPG(file1) ---+---> Analysis1 ---+---> Report
    CPG(file2) ---+                  |
    CPG(file3) -------> Analysis2 ---+

    If file1 changes:
      - DIRTYING: CPG(file1), Analysis1, Report marked dirty
      - CPG(file2), CPG(file3), Analysis2 stay clean

    On Report demand:
      - force(Report) -> force(Analysis1) -> force(CPG(file1))
      - CPG(file1) recomputes, Analysis1 recomputes
      - Analysis2 NOT recomputed (never forced, not dirty)

### Wagner 1998: Optimal Incremental Parsing

<div class="theorem">

**Theorem 1.13** (Wagner’s Incremental Parsing **\[Wagner98\]**). *Optimal algorithm for LR parsing with incremental edits.*

***Complexity:** $`\mathcal{O}{t + s \lg N}`$ where:*

- *$`t`$ = new terminal symbols introduced*

- *$`s`$ = modification sites (edit points)*

- *$`N`$ = total tree nodes*

*This is *asymptotically optimal* for incremental LR parsing.*

</div>

#### Key Technique: Sentential-Form Parsing

Traditional incremental parsing stores *parse states* in tree nodes. Wagner’s approach: **zero** per-node storage overhead beyond tree structure! Instead: Compute parse states *on demand* via breakdown procedures.

#### Balanced Sequences (Critical Insight)

**Problem:** Statement lists like “`stmt; stmt; stmt; ...`” are *linear*. With linear structure, incremental parsing degenerates to $`\mathcal{O}{N}`$.

**Solution:** Transform grammar to allow *balanced* tree representation.
``` math
\begin{aligned}
\text{Before:} \quad & L^* \to L \; L^* \mid \varepsilon & \text{(right-recursive, linear)} \\
\text{After:} \quad & L^* \to B \mid \varepsilon; \quad B \to L \mid B \; B & \text{(nondeterministic, balanced)}
\end{aligned}
```

**Runtime:** Parser chooses balanced structure during reduce.

**Result:** Logarithmic access to any statement in a list.

#### Relation to Tree-sitter

Tree-sitter uses a *different* incremental algorithm (error-recovery-based). Wagner’s algorithm is more theoretically optimal but requires grammar transformation. Tree-sitter is more practical for arbitrary grammars.

**Brrr-machine:** Use tree-sitter for parsing, apply balanced sequence concepts to CPG representation for incremental analysis efficiency.

### Incremental Analysis Strategy

The following F\* formalization captures the core Adapton abstractions. The key insight is the `thunk_state` type which tracks whether a computation is *clean* (cached value is valid), *dirty* (needs recomputation), or *unevaluated* (never computed). The `deps` field in `ThunkClean` records which other thunks this computation depends on, enabling transitive invalidation.

<div class="fstarcode">

(\* ================================================== INCREMENTAL ANALYSIS Source: Adapton (Hammer 2014), simplified ================================================== \*) module BrrrMachine.Incremental

(\* ————————————————– THUNKS — Cached computations with dependencies ————————————————– \*) type thunk_id = nat

type thunk_state (a : Type) = \| ThunkClean : value:a -\> deps:set thunk_id -\> thunk_state a \| ThunkDirty : thunk_state a \| ThunkUnevaluated : thunk_state a

type thunk (a : Type) = id : thunk_id; compute : unit -\> a; mutable state : thunk_state a;

(\* Global thunk registry \*) let thunk_registry : ref (map thunk_id (exists a. thunk a)) = ref Map.empty

(\* ————————————————– CORE OPERATIONS ————————————————– \*) (\* Create a new thunk \*) val create_thunk : \#a:Type -\> (unit -\> a) -\> thunk a let create_thunk \#a compute = let id = fresh_thunk_id () in let t = id; compute; state = ThunkUnevaluated in thunk_registry := Map.add id (\| a, t \|) !thunk_registry; t

(\* Force a thunk (evaluate if needed) \*) val force : \#a:Type -\> thunk a -\> a let force \#a t = match t.state with \| ThunkClean v \_ -\> v \| ThunkDirty \| ThunkUnevaluated -\> (\* Track dependencies during evaluation \*) push_dependency_context t.id; let result = t.compute () in let deps = pop_dependency_context () in t.state \<- ThunkClean result deps; (\* Register reverse dependencies \*) Set.iter (fun dep_id -\> add_dependent dep_id t.id ) deps; result

(\* Mark a thunk as dirty \*) val mark_dirty : thunk_id -\> unit let rec mark_dirty id = match Map.find id !thunk_registry with \| Some (\| \_, t \|) -\> begin match t.state with \| ThunkClean \_ \_ -\> t.state \<- ThunkDirty; (\* Propagate to dependents \*) let dependents = get_dependents id in Set.iter mark_dirty dependents \| \_ -\> () end \| None -\> ()

</div>

The core operations `create_thunk`, `force`, and `mark_dirty` implement the two-phase algorithm:

- `force` performs lazy evaluation: if the thunk is clean, return the cached value; otherwise recompute, cache, and record dependencies.

- `mark_dirty` performs backward propagation: when a dependency changes, mark all dependent thunks as dirty recursively.

The following code shows how to build an incremental CPG where each file has its own thunk, and the merged CPG thunk depends on all file thunks:

<div class="fstarcode">

(\* ————————————————– INCREMENTAL CPG ————————————————– \*) type incremental_cpg = (\* Per-file CPG thunks \*) file_cpgs : map string (thunk cpg); (\* Merged CPG thunk \*) full_cpg : thunk cpg; (\* Analysis result thunks \*) analyses : map string (thunk analysis_result);

val create_incremental_cpg : list string -\> incremental_cpg let create_incremental_cpg files = (\* Create per-file CPG thunks \*) let file_cpgs = List.fold_left (fun m file -\> let thunk = create_thunk (fun () -\> parse_and_build_cpg file) in Map.add file thunk m ) Map.empty files in (\* Create merged CPG thunk \*) let full_cpg = create_thunk (fun () -\> let cpgs = Map.map force file_cpgs in merge_cpgs (Map.values cpgs) ) in file_cpgs; full_cpg; analyses = Map.empty

(\* Update when file changes \*) val file_changed : incremental_cpg -\> string -\> unit let file_changed icpg file = match Map.find file icpg.file_cpgs with \| Some thunk -\> mark_dirty thunk.id \| None -\> () (\* New file, need to add \*)

(\* Get current CPG (recomputes if dirty) \*) val get_cpg : incremental_cpg -\> cpg let get_cpg icpg = force icpg.full_cpg

</div>

The `incremental_cpg` type encapsulates a two-level thunk hierarchy: file-level CPG thunks and the merged full CPG thunk. When `file_changed` is called, only the affected file’s thunk is marked dirty, and the full CPG thunk (which depends on it) becomes dirty too. On the next `get_cpg` call, only the changed file is re-parsed.

Analysis results can also be incrementalized by wrapping them in thunks that depend on the CPG:

<div class="fstarcode">

(\* ————————————————– INCREMENTAL ANALYSIS ————————————————– \*) val add_analysis : incremental_cpg -\> name:string -\> (cpg -\> analysis_result) -\> incremental_cpg let add_analysis icpg name analyze = let analysis_thunk = create_thunk (fun () -\> let cpg = force icpg.full_cpg in analyze cpg ) in icpg with analyses = Map.add name analysis_thunk icpg.analyses

val get_analysis : incremental_cpg -\> string -\> option analysis_result let get_analysis icpg name = match Map.find name icpg.analyses with \| Some thunk -\> Some (force thunk) \| None -\> None

</div>

#### Cross-References: Incrementality Integration Points

<div class="pillarbox">

**Section 6.5 (Memory Model):** Incremental data race detection: When code changes, only re-check happens-before relations affected by the change. Use DCG to track which `candidate_execution` components depend on changed CPG nodes.

**Section 8.1 (Taint Analysis):** Incremental taint propagation: When taint sources/sinks change, only recompute IFDS paths through affected graph regions. Use Wagner’s balanced sequences for efficient path updates. Use DRedL (Section <a href="#sec:dredl" data-reference-type="ref" data-reference="sec:dredl">1.4</a>) for lattice-based incremental IFDS.

**Section 5.4–5.5 (Shape Analysis):** Incremental shape analysis: Cache symbolic heaps per function. When function body changes, invalidate only its footprint summary. Frame rule (Section 7.4) ensures callers reuse unchanged summaries. Use compositional bi-abduction (Section <a href="#sec:infer-deployment" data-reference-type="ref" data-reference="sec:infer-deployment">1.5</a>) for summary reuse.

**Section 12.12 (Bi-Abduction):** Compositional analysis enables near-linear scaling **\[Distefano19\]**. Function summaries cached and invalidated incrementally. Expected speedup: $`65\times`$–$`243\times`$ for typical edit-reanalyze workflows.

**Part XI (IR):** IR node identity preservation enables incremental analysis. When source changes, map old IR nodes to new ones for cache reuse.

</div>

### Incremental Lattice-Based Analysis (DRedL)

**Paper:** **\[Szabo18\]** — “Incrementalizing Lattice-Based Program Analyses in Datalog”

#### The Challenge

Adapton handles general incremental computation, but program analysis has *special structure*: lattice-based fixpoints. Standard incremental Datalog (DRed algorithm) only supports the powerset lattice. Many practical analyses (interval, points-to with strong updates, string analysis) require *custom lattices* with aggregation.

#### DRedL Key Insight: Increasing Replacements Can Skip Deletion

When a lattice value changes from $`\textit{old\_val}`$ to $`\textit{new\_val}`$:

- If $`\textit{new\_val} \geq \textit{old\_val}`$ (in lattice order): **Increasing Replacement**

  - The change is *monotonic*

  - Skip expensive delete-rederive cycle

  - Propagate new value directly

#### Change Classification

**MONOTONIC:**

- All insertions

- Deletions that are part of increasing replacements (where $`\textit{new\_value} \geq \textit{old\_value}`$)

**ANTI-MONOTONIC:**

- All other deletions

- Require full delete-rederive treatment

#### Three-Phase Algorithm

Phase 1: ANTI-MONOTONIC (Delete)  
- Process all deletions NOT in increasing replacements

- May over-delete due to cyclic dependencies

- Iterate to fixpoint within each SCC

Phase 2: RE-DERIVATION  
- For deleted tuples with positive support count: re-insert

- For deleted aggregates: recompute from remaining aggregands

- Fixes over-deletion from Phase 1

Phase 3: MONOTONIC (Insert)  
- Process insertions and increasing replacements

- Standard semi-naive evaluation

#### Support Tracking

**For non-aggregating relations: SUPPORT COUNTS**

- Count how many derivations produce each tuple

- Tuple deleted when count reaches 0

**For aggregating relations: SUPPORT MULTISETS**

- Track which aggregands contribute to each lattice value

- Recompute aggregate from remaining aggregands on change

#### Performance (from IncA evaluation)

- Strong-update points-to: 2.5ms median update time

- String analysis: 3.8ms median update time

- Speedup: $`65\times`$–$`243\times`$ vs from-scratch recomputation

- Target: sub-second updates for IDE integration

#### F\* Formalization of DRedL

The following F\* code formalizes DRedL’s lattice requirements. The `lattice_requirements` record captures the three key assumptions from Szabo et al.: (A1) monotonic aggregation, (A2) finite lattice height for termination, and (A3) functional aggregation. The `finite_height` field uses F\*’s `squash` type to carry a termination proof.

The `lattice_change` type distinguishes the four kinds of changes: insertions, deletions, increasing replacements (monotonic), and decreasing changes (anti-monotonic). The key insight is that `LatticeIncreasingReplace` carries a proof that $`\mathit{new\_val} \geq \mathit{old\_val}`$, enabling the algorithm to skip deletion.

<div class="fstarcode">

(\* ================================================== INCREMENTAL LATTICE-BASED ANALYSIS (DRedL) Source: Szabo et al. 2018 ================================================== \*) module BrrrMachine.Incremental.DRedL

(\* ————————————————– LATTICE REQUIREMENTS (DRedL Assumptions A1-A3) ————————————————– \*) type lattice_requirements (a : Type) = leq : a -\> a -\> bool; (\* Partial order \*) join : a -\> a -\> a; (\* Least upper bound \*) bot : a; (\* Bottom element \*) finite_height : squash (has_finite_height leq); (\* Termination guarantee \*)

(\* Monotonic aggregation: S1 subset S2 ==\> aggr(S1) \<= aggr(S2) \*) type monotonic_aggregation (#a:Type) (lat : lattice_requirements a) (aggr : multiset a -\> a) = squash (forall s1 s2. Multiset.subset s1 s2 ==\> lat.leq (aggr s1) (aggr s2))

(\* ————————————————– CHANGE CLASSIFICATION ————————————————– \*) type lattice_change (a : Type) = \| LatticeInsert : key:grouping_key -\> new_val:a -\> lattice_change a \| LatticeDeletion : key:grouping_key -\> old_val:a -\> lattice_change a \| LatticeIncreasingReplace : key:grouping_key -\> old_val:a -\> new_val:a -\> pf:squash (lat.leq old_val new_val) -\> lattice_change a \| LatticeDecreasingChange : key:grouping_key -\> old_val:a -\> new_val:a -\> lattice_change a

(\* Split changeset into monotonic/anti-monotonic \*) val split_changes : \#a:Type -\> lat:lattice_requirements a -\> changes:list (lattice_change a) -\> (list (lattice_change a) \* list (lattice_change a)) let split_changes \#a lat changes = List.partition (fun c -\> match c with \| LatticeInsert \_ \_ -\> true \| LatticeIncreasingReplace \_ \_ \_ \_ -\> true (\* KEY: treat as monotonic! \*) \| \_ -\> false ) changes

</div>

The `split_changes` function is the core of DRedL’s change classification. Note the critical line: `LatticeIncreasingReplace` is treated as *monotonic*, not anti-monotonic. This is the key optimization that avoids expensive delete-rederive cycles for increasing updates.

The support tracking data structures maintain provenance information for each derived tuple:

<div class="fstarcode">

(\* ————————————————– SUPPORT TRACKING ————————————————– \*) type support_store = counts : map (relation_name \* tuple) nat; multisets : map (relation_name \* grouping_key) (multiset lattice_value);

val add_aggregand : support_store -\> relation_name -\> grouping_key -\> lattice_value -\> support_store val remove_aggregand : support_store -\> relation_name -\> grouping_key -\> lattice_value -\> support_store val recompute_aggregate : \#lat:lattice_requirements lattice_value -\> support_store -\> relation_name -\> grouping_key -\> option lattice_value

</div>

The `support_store` tracks two kinds of information: (1) `counts` maps each (relation, tuple) pair to the number of distinct derivations that produce it, and (2) `multisets` tracks which aggregands contribute to each lattice value. When a tuple’s support count drops to zero, it must be deleted. When aggregands change, the aggregate is recomputed from the remaining multiset.

The main algorithm processes changes through SCCs in topological order, executing the three phases for each component:

<div class="fstarcode">

(\* ————————————————– MAIN ALGORITHM ————————————————– \*) val maintain_incrementally : prog:datalog_program -\> state:relation_store -\> support:support_store -\> changes:changeset -\> (relation_store \* support_store) let maintain_incrementally prog state support changes = (\* Process SCCs in topological order \*) List.fold_left (fun (st, sup) scc -\> maintain_component prog st sup scc changes ) (state, support) prog.sccs

val maintain_component : prog:datalog_program -\> state:relation_store -\> support:support_store -\> component:set relation_name -\> changes:changeset -\> (relation_store \* support_store) let maintain_component prog state support component changes = let (mon, anti) = split_changes changes in (\* Phase 1: Anti-monotonic to fixpoint \*) let (state1, support1, deleted) = anti_monotonic_fixpoint prog state support component anti in (\* Phase 2: Re-derive over-deleted \*) let (state2, support2) = rederive prog state1 support1 deleted in (\* Phase 3: Monotonic \*) monotonic_phase prog state2 support2 component mon

</div>

The `maintain_component` function shows the three-phase structure clearly: first anti-monotonic changes (deletions) are processed to fixpoint, then over-deleted tuples are re-derived, and finally monotonic changes (insertions and increasing replacements) are propagated. The correctness theorem states that incremental maintenance produces exactly the same result as from-scratch recomputation:

<div class="fstarcode">

(\* ————————————————– CORRECTNESS THEOREM ————————————————– \*) val dredl_correctness : prog:datalog_program -\> state:relation_store -\> support:support_store -\> changes:changeset -\> Lemma ( let (inc_state, \_) = maintain_incrementally prog state support changes in let scratch_state = compute_fixpoint_from_scratch prog (apply_changes state changes) in inc_state == scratch_state ) \[SMTPat (maintain_incrementally prog state support changes)\]

</div>

#### Integration with Adapton (Hierarchical Incrementality)

    File-level changes:      Analysis-level changes:
        |                          |
        v                          v
    +---------+                +---------+
    | Adapton |--------------->|  DRedL  |
    |  (DCG)  | dirty CPG      |(Datalog)|
    +---------+                +---------+
        |                          |
        v                          v
      Coarse-grained            Fine-grained
      (file thunks)             (lattice fixpoints)

- Use Adapton for **file-level** incrementality (CPG per file)

- Use DRedL for **analysis-level** incrementality (within fixpoint)

- **Result:** Sub-second updates even for large codebases

### Diff-Based Industrial Deployment (Infer)

**Paper:** **\[Distefano19\]** — “Scaling Static Analyses at Facebook”

<div class="theorem">

**Theorem 1.14** (The Key Insight). *Deployment context matters as much as analysis precision.*

</div>

#### Traditional (Batch) Deployment

1.  Run analysis

2.  Generate bug list

3.  Present to engineers

**Result:** Engineers overwhelmed, bugs ignored, 0% fix rate (“bug bankruptcy”)

#### Diff-Time Deployment (Infer’s Model)

1.  Engineer submits code change

2.  Analysis runs *only* on diff

3.  Results posted as code review comments

**Result:** 70% fix rate observed at Facebook scale

#### Why Diff-Time Works

1.  **CONTEXT:** Developer is already reviewing the code

2.  **OWNERSHIP:** The bug is in code they just wrote

3.  **MANAGEABLE:** Only a few reports, not thousands

4.  **TIMELY:** Fix now, not in some future cleanup sprint

5.  **INCREMENTAL:** Only recompute for changed code

#### Bug Bankruptcy Anti-Pattern

When bug list exceeds $`\sim`$<!-- -->50–100 items:

- Engineers stop triaging

- Fix rate approaches 0%

- Trust in tool decreases

- Technical debt accumulates

**Prevention:**

- Cap reports per diff (max 5–10)

- Prioritize by severity AND actionability

- Never let bug lists grow unbounded

#### Compositional Analysis for Scale

Infer uses bi-abduction (Section 12.12) for compositionality:

- Each procedure analyzed *independently*

- Summaries capture (precondition, postcondition) pairs

- Interprocedural analysis via summary composition

- Result: Near-linear scaling with codebase size

<div class="definition">

**Definition 1.15** (Bi-Abduction Judgment).
``` math
p * \;?\!M \vdash q * \;?\!F
```
where:

- $`p`$ = current state

- $`M`$ = anti-frame (missing precondition requirement)

- $`F`$ = frame (unchanged state passed through)

</div>

#### Scale Achieved at Facebook

- **Codebase size:** 10–100 million LOC

- **Analysis time per diff:** $`< 1`$ minute

- **Memory per analysis:** $`< 4`$GB (enables parallelization)

- **Bugs found:** 100,000+ fixed before production

- **Fix rate:** 70% diff-time vs $`\sim`$<!-- -->5% batch

#### Zoncolan (Taint Analysis for Security)

- **Codebase:** 100M LOC Hack/PHP

- Taint analysis with custom sources/sinks

- Compositional summary-based propagation

- Outperforms all other security detection methods

#### F\* Formalization of Deployment Strategy

The following F\* code formalizes deployment modes and their expected outcomes. The `deployment_mode` type captures four distinct scenarios: diff-time (code review integration), batch (periodic runs), IDE-integrated (real-time feedback), and security engineer (deep audits). Each mode has different time/memory budgets and fix rate expectations.

The `expected_fix_rate` function encodes the empirical observation from Facebook: diff-time deployment to all engineers achieves 70% fix rate, while batch deployment drops to 5% due to the “bug bankruptcy” effect.

<div class="fstarcode">

(\* ================================================== DIFF-BASED INDUSTRIAL DEPLOYMENT Source: Distefano et al. 2019 (Infer at Facebook) ================================================== \*) module BrrrMachine.Deployment

(\* ————————————————– DEPLOYMENT MODES ————————————————– \*) type deployment_mode = \| DiffTime : deployment_mode (\* Analysis runs on code diff, reports as code review comments \*) \| BatchOffline : deployment_mode (\* Analysis runs periodically, generates bug lists \*) \| IDE_Integrated : deployment_mode (\* Real-time analysis in development environment \*) \| SecurityEngineer : deployment_mode (\* Deep analysis for security team review \*)

type target_audience = \| AllEngineers : target_audience \| SecurityTeam : target_audience \| PlatformSpecific : platform:string -\> target_audience

(\* Expected fix rate based on deployment context \*) val expected_fix_rate : deployment_mode -\> target_audience -\> float let expected_fix_rate mode audience = match mode, audience with \| DiffTime, AllEngineers -\> 0.70 (\* 70 \| DiffTime, SecurityTeam -\> 0.85 \| BatchOffline, AllEngineers -\> 0.05 (\* Bug bankruptcy effect \*) \| BatchOffline, SecurityTeam -\> 0.40 \| IDE_Integrated, AllEngineers -\> 0.80 (\* Immediate context \*) \| SecurityEngineer, SecurityTeam -\> 0.90 \| \_, \_ -\> 0.30

</div>

The `analysis_budget` type captures the resource constraints for each deployment context. The key insight is that different contexts have dramatically different budgets: diff-time analysis must complete in under a minute with limited memory, while security audits can run for hours with generous resources. The `max_report_count` field prevents “bug bankruptcy” by capping how many issues are reported:

<div class="fstarcode">

(\* ————————————————– ANALYSIS BUDGET PER CONTEXT ————————————————– \*) type analysis_budget = time_limit_per_diff : nat; (\* milliseconds \*) memory_limit : nat; (\* megabytes \*) max_paths_explored : nat; max_report_count : nat; (\* Cap to avoid overwhelming \*)

val budget_for_context : deployment_mode -\> analysis_budget let budget_for_context mode = match mode with \| DiffTime -\> time_limit_per_diff = 60000; (\* 1 minute \*) memory_limit = 4096; (\* 4GB \*) max_paths_explored = 1000; max_report_count = 5; (\* Don’t overwhelm reviewer \*) \| BatchOffline -\> time_limit_per_diff = 3600000; (\* 1 hour \*) memory_limit = 32768; (\* 32GB \*) max_paths_explored = 100000; max_report_count = 1000; \| IDE_Integrated -\> time_limit_per_diff = 5000; (\* 5 seconds for interactivity \*) memory_limit = 512; max_paths_explored = 100; max_report_count = 3; \| SecurityEngineer -\> time_limit_per_diff = 7200000; (\* 2 hours \*) memory_limit = 65536; max_paths_explored = 1000000; max_report_count = 10000;

</div>

The following code shows how to combine incremental analysis with summary caching. The `invalidation_set` function computes the transitive closure of affected functions when code changes: directly modified functions plus all their callers (since callee summaries may have changed). The `incremental_analyze` function reuses cached summaries for unchanged code and only recomputes what is necessary:

<div class="fstarcode">

(\* ————————————————– INCREMENTAL ANALYSIS WITH SUMMARY CACHING ————————————————– \*) type change_delta = added_functions : set func_id; modified_functions : set func_id; deleted_functions : set func_id; modified_callsites : set (func_id \* node_id);

type cached_summaries = map func_id procedure_summary

(\* Determine what needs recomputation \*) val invalidation_set : change_delta -\> call_graph -\> cached_summaries -\> set func_id let invalidation_set delta cg cache = let directly_changed = Set.union delta.added_functions (Set.union delta.modified_functions delta.deleted_functions) in (\* Transitively find affected callers \*) let rec find_affected current visited = if Set.is_empty current then visited else let callers = Set.fold (fun f acc -\> Set.union acc (get_callers cg f) ) current Set.empty in let new_affected = Set.diff callers visited in find_affected new_affected (Set.union visited new_affected) in find_affected directly_changed directly_changed

(\* Incremental analysis - only recompute what’s needed \*) val incremental_analyze : cpg:cpg -\> delta:change_delta -\> cache:cached_summaries -\> budget:analysis_budget -\> (cached_summaries \* list bug_report) let incremental_analyze cpg delta cache budget = let to_recompute = invalidation_set delta (extract_callgraph cpg) cache in (\* Reuse unchanged summaries \*) let valid_cache = Map.filter (fun f \_ -\> not (Set.mem f to_recompute)) cache in (\* Recompute affected functions \*) let new_summaries = analyze_functions_with_cache cpg to_recompute valid_cache in let updated_cache = Map.union valid_cache new_summaries in (\* Only report bugs from CHANGED code (diff-time principle) \*) let new_bugs = extract_bugs_in_changed_code new_summaries delta in (\* Apply report cap \*) let capped_bugs = take budget.max_report_count (sort_by_actionability new_bugs) in (updated_cache, capped_bugs)

</div>

The critical insight from Infer’s deployment is that report *quality* determines fix rate. The `report_quality` record captures the key dimensions: precise location, execution trace, explanation, and suggested fix. The `actionability_score` function weights these factors based on empirical observations—reports involving changed code and short traces are most likely to be fixed:

<div class="fstarcode">

(\* ————————————————– REPORT QUALITY AND ACTIONABILITY ————————————————– \*) type report_quality = has_precise_location : bool; has_trace : bool; has_explanation : bool; has_suggested_fix : bool; trace_length : nat; involves_changed_code : bool;

val actionability_score : bug_report -\> report_quality -\> float let actionability_score report quality = let base = 0.3 in let score = base +. (if quality.has_precise_location then 0.15 else 0.0) +. (if quality.has_trace then 0.20 else 0.0) +. (if quality.has_explanation then 0.10 else 0.0) +. (if quality.has_suggested_fix then 0.15 else 0.0) +. (if quality.involves_changed_code then 0.20 else 0.0) -. (float_of_int quality.trace_length \*. 0.01) in min 1.0 (max 0.0 score)

</div>

#### Industrial Deployment Checklist

- Diff-time integration with CI/CD pipeline

- Summary caching with invalidation tracking

- Report capping (max 5–10 per diff)

- Actionability scoring and prioritization

- Execution trace in reports

- Suggested fixes where possible

- Time budget enforcement ($`< 1`$ minute per diff)

- Memory budget enforcement ($`< 4`$GB)

- Graceful degradation on budget exhaustion

- Metrics: fix rate, time-to-fix, false positive rate

#### Target Metrics

- **Fix rate:** 70%+ for diff-time deployment

- **Analysis time:** $`< 60`$ seconds per diff

- **Memory:** $`< 4`$GB per analysis worker

- **Report cap:** 5 per diff (engineering), 1000 per batch (security)

## Time Budgets and Graceful Degradation

IFDS is $`\mathcal{O}{ED^3}`$, but $`D`$ (domain size) can explode. Pointer analysis on large codebases can run for hours. Real tools need time limits and graceful degradation.

**Papers:** **\[Tan22\]** (Qilin), **\[Sridharan05\]** (Demand-driven)

### Degradation Strategy

#### Time Budgets

- Per-function budget: 100ms default

- Per-file budget: 10s default

- Total budget: configurable

#### Degradation Levels

Level 0:  
Full precision (Andersen + IFDS + path-sensitive)

Level 1:  
Reduced precision (Steensgaard + IFDS)

Level 2:  
Fast approximation (flow-insensitive)

Level 3:  
Syntactic only (pattern matching)

#### Adaptive Analysis

The following pseudo-F\* code illustrates the basic strategy for adaptive analysis: attempt full-precision analysis first, but fall back to faster approximations when the time budget is exhausted. This enables graceful degradation rather than complete failure on complex code:

<div class="fstarcode">

val analyze_adaptive : cpg -\> time_budget:duration -\> findings let analyze_adaptive cpg budget = let start = now () in try (\* Start with full precision \*) analyze_full cpg with Timeout -\> (\* Degrade and continue \*) let remaining = budget - (now () - start) in analyze_approximate cpg remaining

</div>

#### Incremental Results

- Yield findings as they’re discovered

- Don’t wait for complete analysis

- Mark findings with “analysis complete” vs “partial”

### Implementation TODOs

1.  **Adaptive precision selection** per function based on complexity

2.  **Progress reporting** for long-running analyses

3.  **Checkpointing** for resumable analysis

4.  **Resource monitoring** (memory, not just time)

### Industrial Lessons

**Source:** **\[Distefano19\]**

#### Precision vs Performance Trade-off

Facebook’s Infer made a deliberate choice:

> “We prioritize soundness less and precision more, accepting that some issues would be missed in exchange for fewer false positives.”

This is NOT abandoning soundness — it’s a *layered* approach:

1.  **Core bi-abduction algorithm:** SOUND (formal guarantees)

2.  **Industrial deployment:** UNDER-APPROXIMATE (bounded exploration)

    - Bounded symbolic execution paths

    - Timeouts on complex procedures

    - Heuristic prioritization

**Result:** Sound for analyzed paths, incomplete overall. Reported bugs are *real* bugs. Some bugs are *missed*.

#### Analysis Profile Configurations

<div id="tab:analysis-profiles">

| **Profile**                    | **Precision** | **Max Time** | **Max Reports** |
|:-------------------------------|:-------------:|:------------:|:---------------:|
| EngineeringProfile (diff-time) |     0.90      |     60s      |        5        |
| SecurityProfile (batch)        |     0.50      |    3600s     |      1000       |
| ComplianceProfile (audit)      |     0.99      |    7200s     |      10000      |

Analysis Profile Configurations

</div>

**Goals:**

- **EngineeringProfile:** High fix rate (few false positives)

- **SecurityProfile:** Find all vulnerabilities (accept more FPs for coverage)

- **ComplianceProfile:** Full audit record (must be confident)

#### Bug Bankruptcy Prevention

When bug list exceeds $`\sim`$<!-- -->50–100 items:

- Engineers stop triaging (overwhelmed)

- Fix rate approaches 0%

- Trust in tool decreases

- Technical debt accelerates

**Prevention strategies:**

- Cap reports per diff (max 5–10)

- Prioritize by severity AND actionability

- Triage queue with assignment

- Regular “bankruptcy” cleanup (declare amnesty, start fresh)

- Focus on NEW bugs in changed code

#### Actionability Requirements

Every bug report must answer:

1.  **What is wrong?** (Clear error type)

2.  **Where is it?** (Precise location)

3.  **Why is it wrong?** (Execution trace)

4.  **How to fix it?** (Actionable guidance)

<div class="pillarbox">

Reports without traces have significantly lower fix rates.

</div>

# The Brrr-Machine IR

## Unified Intermediate Representation

All source languages are translated to a common IR. This enables:

- Single analysis implementation for all languages

- Consistent effect tracking

- Cross-language dataflow

### IR Design Principles

<div class="pillarbox">

**PRINCIPLE 1: EXPLICIT EFFECTS**

- Every side effect is represented explicitly.

- Pure expressions separated from effectful statements.

- No hidden control flow.

**PRINCIPLE 2: EXPLICIT MEMORY**

- All memory operations through uniform interface.

- Memory mode (GC/RC/owned/manual) is a parameter.

**PRINCIPLE 3: SSA-LIKE STRUCTURE**

- Each variable assigned once (in its scope).

- Phi nodes at control flow merge points.

- Enables efficient dataflow analysis.

**PRINCIPLE 4: TYPE-PRESERVING**

- Type information preserved when available.

- Abstract types for dynamic languages.

- Gradual types for mixed scenarios.

**CRITICAL FOR RUST** **\[Rupta24\]**: Do NOT lower Rust to LLVM IR for analysis—type info is LOST! LLVM IR loses: ownership, borrowing, lifetimes, trait bounds. Use MIR-level representation for Rust to preserve:

- Borrow regions and lifetimes

- Move vs copy semantics

- Trait object type info

See Section <a href="#sec:stack-filtering" data-reference-type="ref" data-reference="sec:stack-filtering">[sec:stack-filtering]</a> for stack filtering which requires this info.

</div>

<div class="pillarbox">

**Source**: **\[Siek06\]** – “Gradual Typing for Functional Languages”

**KEY OPTIMIZATION**: Runtime overhead only for dynamic types.

For gradual typing at language boundaries (Python$`\leftrightarrow`$Rust, JS$`\leftrightarrow`$Go):

**KNOWN TYPES (statically typed):**

- Use UNBOXED representation—direct machine values

- `int` $`\to`$ native int64, `bool` $`\to`$ native bool, `struct` $`\to`$ native layout

- No runtime type checks, no boxing overhead

- Same performance as fully static code

**DYNAMIC TYPES (? / $`\mathsf{Any}`$):**

- Use BOXED representation with type tag

- Runtime type checks at cast boundaries

- `{ tag: TypeTag, value: *void }` layout

- Overhead only where types are actually unknown

**CANONICAL FORMS** (Siek Lemma 5): Values of ground types (int, bool) have predictable representation:

- If $`v : \ensuremath{\mathsf{Int}}`$ (not ?) then $`v`$ is unboxed integer

- If $`v : \text{?}`$ then $`v`$ is boxed with runtime tag

This enables efficient code generation at FFI boundaries.

**APPLICATION TO IR:**

- $`\mathsf{Any}`$ in `ir_type` signals “needs boxing”

- Known types ($`\mathsf{Int}`$, $`\mathsf{Bool}`$, `TStruct`) signal “unboxed OK”

- Code generator chooses representation based on type precision

Cross-reference: Section <a href="#sec:gradual-typing-boundaries" data-reference-type="ref" data-reference="sec:gradual-typing-boundaries">[sec:gradual-typing-boundaries]</a> for gradual typing at boundaries. Cross-reference: **\[Bierhoff07\]** review for typestate + gradual integration.

</div>

### Complete IR Specification

<div class="fstarcode">

(\* ================================================== BRRR-MACHINE INTERMEDIATE REPRESENTATION ================================================== \*) module BrrrMachine.IR

(\* ————————————————– TYPES ————————————————– \*) type ir_type = (\* Primitives \*) \| TUnit : ir_type \| TBool : ir_type \| TInt : width:int_width -\> signed:bool -\> ir_type \| TFloat : width:float_width -\> ir_type \| TString : ir_type \| TChar : ir_type (\* References \*) \| TRef : pointee:ir_type -\> ir_type \| TPtr : pointee:ir_type -\> ir_type (\* Raw, potentially null \*) \| TOption : inner:ir_type -\> ir_type (\* Aggregates \*) \| TArray : element:ir_type -\> size:option nat -\> ir_type \| TStruct : name:string -\> fields:list (string \* ir_type) -\> ir_type \| TTuple : elements:list ir_type -\> ir_type \| TVariant : name:string -\> cases:list (string \* ir_type) -\> ir_type (\* Functions \*) \| TFunc : params:list ir_type -\> ret:ir_type -\> effects:effect_row -\> ir_type \| TClosure : captures:list ir_type -\> func:ir_type -\> ir_type (\* Special \*) \| TAny : ir_type (\* For dynamic languages \*) \| TNever : ir_type (\* Bottom type, never returns \*) \| TTypeVar : name:string -\> ir_type (\* Generic parameter \*)

(\* NOTE: In actual F\*, int_width and float_width are declared separately as noeq types before ir_type. The ’and’ syntax shown here is for illustration of the mutual dependency in the conceptual type system. \*) and int_width = \| I8 \| I16 \| I32 \| I64 \| I128 \| ISize \| IBigInt and float_width = \| F32 \| F64

</div>

The `ir_type` definition demonstrates several key design choices:

- `TInt` carries explicit width (`I8`–`I128`, `ISize`, `IBigInt`) and signedness, preserving platform-specific semantics from C, Rust, and Python

- `TRef` vs `TPtr` distinguishes safe references (guaranteed non-null, valid memory) from raw pointers (may be null, may dangle)

- `TFunc` includes an `effect_row` parameter, enabling the effect system from Part VI to track side effects at the type level

- `TAny` supports dynamic languages (Python, JavaScript) using gradual typing (Siek 2006)

<div class="fstarcode">

(\* ————————————————– TYPE DESCRIPTORS FOR FFI Source: VeriFFI (Wang et al. 2025)

IR types can be reified into runtime type descriptors for FFI boundary verification. See Section 9.4.4 for the complete type_descriptor type and the reify_type function that converts ir_type to type_descriptor.

This enables: - Dynamic type checking at FFI boundaries - Automatic marshalling code generation - Reflection-based contract verification

Cross-reference: Section 7.5 (Representation Predicates) defines how type descriptors relate to actual memory layout via rep_predicate. ————————————————– \*)

</div>

The IR value type represents runtime values in a language-neutral way. Key features include language-specific null representations (`VNull` for C/Go, `VUndefined` for JavaScript, `VNone` for Python) and explicit width annotations for numeric types.

<div class="fstarcode">

(\* ————————————————– VALUES — Pure data

NOTE: F\* does not have a built-in ’float’ type. In actual implementation, use FStar.Float64.t or an abstract floating-point type. The ’float’ here is illustrative pseudo-code representing IEEE 754 floating-point values. ————————————————– \*) assume type fp_value : Type (\* Abstract floating-point value \*)

type ir_value = \| VUnit : ir_value \| VBool : b:bool -\> ir_value \| VInt : i:int -\> width:int_width -\> ir_value \| VFloat : f:fp_value -\> width:float_width -\> ir_value \| VString : s:string -\> ir_value \| VNull : ir_value (\* C/Go null pointer \*) \| VUndefined : ir_value (\* JavaScript undefined \*) \| VNone : ir_value (\* Python None singleton \*)

</div>

IR expressions represent pure computations without side effects. The separation of pure expressions from effectful statements (defined below) is fundamental to the IR’s design, enabling effect-free analysis of expression subgraphs.

<div class="fstarcode">

(\* ————————————————– EXPRESSIONS — Pure computations

Key design: expressions are PURE - they have no side effects. All memory access, I/O, and concurrency operations are in the statement layer. This separation enables compositional reasoning about expression semantics. ————————————————– \*) type var_id = string type func_id = string

type ir_expr = (\* Atoms \*) \| EVal : v:ir_value -\> ir_expr \| EVar : v:var_id -\> ir_expr \| EGlobal : g:var_id -\> ir_expr (\* Arithmetic \*) \| EBinOp : op:bin_op -\> e1:ir_expr -\> e2:ir_expr -\> ir_expr \| EUnOp : op:un_op -\> e:ir_expr -\> ir_expr (\* Comparisons \*) \| ECmp : op:cmp_op -\> e1:ir_expr -\> e2:ir_expr -\> ir_expr (\* Type operations \*) \| ECast : e:ir_expr -\> target:ir_type -\> ir_expr \| ETypeOf : e:ir_expr -\> ir_expr \| EInstanceOf : e:ir_expr -\> t:ir_type -\> ir_expr (\* Aggregate access (pure — reads from value, not memory) \*) \| EField : e:ir_expr -\> field:string -\> ir_expr \| EIndex : arr:ir_expr -\> idx:ir_expr -\> ir_expr \| ETupleProj : e:ir_expr -\> index:nat -\> ir_expr (\* Aggregate construction \*) \| EStruct : name:string -\> fields:list (string \* ir_expr) -\> ir_expr \| EArray : elements:list ir_expr -\> ir_expr \| ETuple : elements:list ir_expr -\> ir_expr \| EVariant : name:string -\> tag:string -\> payload:ir_expr -\> ir_expr (\* Functions \*) \| ELambda : params:list (var_id \* ir_type) -\> body:ir_stmt -\> captures:list var_id -\> ir_expr \| EFuncRef : f:func_id -\> ir_expr (\* Option/null handling \*) \| ESome : e:ir_expr -\> ir_expr \| ENone : t:ir_type -\> ir_expr \| EIsSome : e:ir_expr -\> ir_expr \| EUnwrap : e:ir_expr -\> ir_expr (\* Panics if None \*)

and bin_op = \| OpAdd \| OpSub \| OpMul \| OpDiv \| OpMod \| OpPow \| OpBitAnd \| OpBitOr \| OpBitXor \| OpShl \| OpShr \| OpAnd \| OpOr

and un_op = \| OpNeg \| OpNot \| OpBitNot

and cmp_op = \| OpEq \| OpNe \| OpLt \| OpLe \| OpGt \| OpGe \| OpRefEq (\* Reference equality \*)

</div>

<div class="fstarcode">

(\* ————————————————– STATEMENTS — Effectful operations ————————————————– \*) type ir_stmt = (\* Sequencing \*) \| SSeq : s1:ir_stmt -\> s2:ir_stmt -\> ir_stmt \| SNop : ir_stmt (\* Variable binding \*) \| SLet : var:var_id -\> typ:ir_type -\> init:ir_expr -\> body:ir_stmt -\> ir_stmt \| SLetMut : var:var_id -\> typ:ir_type -\> init:ir_expr -\> body:ir_stmt -\> ir_stmt \| SAssign : var:var_id -\> value:ir_expr -\> ir_stmt (\* Memory operations \*) \| SAlloc : dst:var_id -\> typ:ir_type -\> ir_stmt \| SAllocArray : dst:var_id -\> elem_type:ir_type -\> size:ir_expr -\> ir_stmt \| SFree : ptr:ir_expr -\> ir_stmt \| SRead : dst:var_id -\> ptr:ir_expr -\> ir_stmt \| SWrite : ptr:ir_expr -\> value:ir_expr -\> ir_stmt (\* Aggregate memory operations \*) \| SFieldRead : dst:var_id -\> obj:ir_expr -\> field:string -\> ir_stmt \| SFieldWrite : obj:ir_expr -\> field:string -\> value:ir_expr -\> ir_stmt \| SIndexRead : dst:var_id -\> arr:ir_expr -\> idx:ir_expr -\> ir_stmt \| SIndexWrite : arr:ir_expr -\> idx:ir_expr -\> value:ir_expr -\> ir_stmt (\* Control flow \*) \| SIf : cond:ir_expr -\> then\_:ir_stmt -\> else\_:ir_stmt -\> ir_stmt \| SMatch : scrutinee:ir_expr -\> cases:list match_case -\> ir_stmt \| SWhile : cond:ir_expr -\> body:ir_stmt -\> ir_stmt \| SFor : var:var_id -\> init:ir_expr -\> cond:ir_expr -\> update:ir_stmt -\> body:ir_stmt -\> ir_stmt \| SForEach : var:var_id -\> iter:ir_expr -\> body:ir_stmt -\> ir_stmt \| SBreak : ir_stmt \| SContinue : ir_stmt \| SReturn : value:option ir_expr -\> ir_stmt (\* Function calls \*) \| SCall : dst:option var_id -\> func:ir_expr -\> args:list ir_expr -\> ir_stmt \| STailCall : func:ir_expr -\> args:list ir_expr -\> ir_stmt (\* Exception handling \*) \| SThrow : exn:ir_expr -\> ir_stmt \| STry : body:ir_stmt -\> catches:list catch_clause -\> finally:option ir_stmt -\> ir_stmt

</div>

<div class="fstarcode">

(\* Concurrency \*) \| SSpawn : dst:var_id -\> func:ir_expr -\> args:list ir_expr -\> ir_stmt \| SJoin : handle:ir_expr -\> ir_stmt \| SSend : chan:ir_expr -\> value:ir_expr -\> ir_stmt \| SRecv : dst:var_id -\> chan:ir_expr -\> ir_stmt \| SLock : mutex:ir_expr -\> ir_stmt \| SUnlock : mutex:ir_expr -\> ir_stmt \| SAtomic : body:ir_stmt -\> ir_stmt (\* Channel operations — Honda 1998/2008 session type primitives \*) \| SChanCreate : dst:var_id -\> elem_type:ir_type -\> buffer_size:nat -\> ir_stmt (\* Create channel: dst = make(chan elem_type, buffer_size) \*) \| SChanClose : chan:ir_expr -\> ir_stmt (\* Close channel: close(chan) \*) \| SSelect : cases:list select_case -\> default:option ir_stmt -\> ir_stmt (\* Select statement: select case ch \<- v: ...; case x := \<-ch: ...; default: ... Models both Go’s select and Rust’s tokio::select! \*) \| SSelectSend : chan:ir_expr -\> value:ir_expr -\> body:ir_stmt -\> ir_stmt (\* Select case for send: case chan \<- value: body \*) \| SSelectRecv : dst:var_id -\> chan:ir_expr -\> body:ir_stmt -\> ir_stmt (\* Select case for receive: case dst := \<-chan: body \*) \| SChanDelegate : chan:ir_expr -\> delegated:ir_expr -\> ir_stmt (\* Session delegation: transfer session capability through channel \*) \| SChanBranch : chan:ir_expr -\> branches:list (string \* ir_stmt) -\> ir_stmt (\* Offer labeled branches on channel (session type branching) \*) \| SChanSelect : chan:ir_expr -\> label:string -\> body:ir_stmt -\> ir_stmt (\* Select labeled branch on channel (session type selection) \*)

</div>

<div class="fstarcode">

(\* Resource management \*) \| SAcquire : dst:var_id -\> resource:ir_expr -\> ir_stmt \| SRelease : resource:ir_expr -\> ir_stmt \| SDefer : action:ir_stmt -\> ir_stmt \| SWith : resource:ir_expr -\> var:var_id -\> body:ir_stmt -\> ir_stmt (\* SSA \*) \| SPhi : dst:var_id -\> sources:list (var_id \* block_id) -\> ir_stmt (\* Annotations \*) \| SAssert : cond:ir_expr -\> msg:string -\> ir_stmt \| SAssume : cond:ir_expr -\> ir_stmt \| SAnnotate : annotation:ir_annotation -\> body:ir_stmt -\> ir_stmt

and match_case = pattern : ir_pattern; guard : option ir_expr; body : ir_stmt;

(\* Select case for channel select statements (Go select / Rust tokio::select!) \*) and select_case = \| SelectSend : chan:ir_expr -\> value:ir_expr -\> body:ir_stmt -\> select_case (\* case chan \<- value: body \*) \| SelectRecv : dst:option var_id -\> chan:ir_expr -\> body:ir_stmt -\> select_case (\* case dst := \<-chan: body (dst=None for case \<-chan:) \*) \| SelectDefault : body:ir_stmt -\> select_case (\* default: body \*)

and ir_pattern = \| PatWildcard : ir_pattern \| PatVar : var:var_id -\> ir_pattern \| PatLiteral : v:ir_value -\> ir_pattern \| PatVariant : tag:string -\> payload:ir_pattern -\> ir_pattern \| PatTuple : elements:list ir_pattern -\> ir_pattern \| PatStruct : fields:list (string \* ir_pattern) -\> ir_pattern

and catch_clause = exn_type : ir_type; exn_var : var_id; handler : ir_stmt;

and ir_annotation = \| AnnPure (\* This code is pure \*) \| AnnInline (\* Inline at call sites \*) \| AnnNoReturn (\* This code never returns normally \*) \| AnnUnsafe (\* This code has unchecked assumptions \*) \| AnnBoundary : source:string -\> target:string -\> ir_annotation \| AnnTainted (\* Value is tainted \*) \| AnnSanitized (\* Value has been sanitized \*)

and block_id = nat

</div>

### SSA Form Considerations for Channels

<div class="pillarbox">

Channel variables have special SSA semantics due to their LINEAR nature:

1.  Channel creation assigns the channel variable exactly once

2.  Channel operations (send/recv/close) do NOT reassign the channel variable

3.  Channel closing TERMINATES the channel’s SSA lifetime

4.  Select introduces phi-like merge points for channel state

Unlike regular SSA where variables can be assigned multiple values (with phi nodes merging), channel endpoints follow session type discipline:

- Each operation CONSUMES the current session type prefix

- Produces a NEW session type (the continuation)

- The channel VARIABLE stays the same, but its TYPE evolves

This is LINEAR SSA: assignment happens once, but type state changes.

</div>

<div class="fstarcode">

(\* Channel SSA form validation \*) val validate_channel_ssa : ir_func -\> list ssa_violation let validate_channel_ssa func = let violations = ref \[\] in let channel_defs : map var_id node_id = Map.empty in let channel_closed : set var_id = Set.empty in

let rec check_stmt stmt = match stmt with \| SChanCreate dst \_ \_ -\> if Map.mem dst channel_defs then violations := SSAViolation_ChannelRedefined dst :: !violations else channel_defs := Map.add dst (current_node ()) channel_defs

\| SSend ch \_ \| SRecv \_ ch -\> let ch_var = extract_var ch in if not (Map.mem ch_var channel_defs) then violations := SSAViolation_UseBeforeDef ch_var :: !violations; if Set.mem ch_var channel_closed then violations := SSAViolation_UseAfterClose ch_var :: !violations

\| SChanClose ch -\> let ch_var = extract_var ch in if Set.mem ch_var channel_closed then violations := SSAViolation_DoubleClose ch_var :: !violations; channel_closed := Set.add ch_var channel_closed

\| SSelect cases default -\> (\* Select creates a merge point for channel state. After select, channel state depends on which case was taken. This is analogous to phi for channel type state. \*) let merge_states = List.map (fun case -\> match case with \| SelectSend ch \_ \_ -\> (extract_var ch, get_type_state ch) \| SelectRecv \_ ch \_ -\> (extract_var ch, get_type_state ch) \| \_ -\> (fresh_var (), LTEnd) ) cases in (\* Validate all cases lead to consistent merge state \*) validate_select_merge merge_states

\| SSeq s1 s2 -\> check_stmt s1; check_stmt s2 \| SIf \_ s1 s2 -\> check_stmt s1; check_stmt s2 \| SWhile \_ body -\> check_stmt body \| \_ -\> () in check_stmt func.body; !violations

type ssa_violation = \| SSAViolation_ChannelRedefined : var_id -\> ssa_violation \| SSAViolation_UseBeforeDef : var_id -\> ssa_violation \| SSAViolation_UseAfterClose : var_id -\> ssa_violation \| SSAViolation_DoubleClose : var_id -\> ssa_violation \| SSAViolation_SelectMergeMismatch : list var_id -\> ssa_violation

</div>

<div class="fstarcode">

(\* Channel type state phi node for select statements \*) type chan_phi = dst_chan : var_id; sources : list (block_id \* local_session_type); (\* After select, channel has type that is the "meet" of branch types \*) merged_type : local_session_type;

(\* Compute merged session type after select (conservative: use most permissive) \*) val merge_session_types : list local_session_type -\> local_session_type let rec merge_session_types types = match types with \| \[\] -\> LTEnd \| \[t\] -\> t \| t1 :: t2 :: rest -\> let merged = match t1, t2 with \| LTEnd, \_ -\> LTEnd \| \_, LTEnd -\> LTEnd \| LTSend p1 c1, LTSend p2 c2 when p1 = p2 -\> LTSend p1 (merge_session_types \[c1; c2\]) \| LTRecv p1 c1, LTRecv p2 c2 when p1 = p2 -\> LTRecv p1 (merge_session_types \[c1; c2\]) \| LTSelect bs1, LTSelect bs2 -\> (\* Merge: intersection of offered labels \*) let common = List.filter (fun (l, \_) -\> List.exists (fun (l’, \_) -\> l = l’) bs2) bs1 in LTSelect common \| LTBranch bs1, LTBranch bs2 -\> (\* Merge: union of accepted labels \*) LTBranch (bs1 @ bs2) \| \_, \_ -\> LTEnd (\* Incompatible types merge to end \*) in merge_session_types (merged :: rest)

</div>

### Functions and Programs

<div class="fstarcode">

(\* ————————————————– FUNCTIONS AND PROGRAMS ————————————————– \*) type ir_func = id : func_id; name : string; params : list (var_id \* ir_type); return_type : ir_type; body : ir_stmt; effect_sig : effect_signature; is_public : bool; source_lang : string;

type ir_program = functions : list ir_func; globals : list (var_id \* ir_type \* option ir_expr); entry : option func_id; language : language_config;

</div>

## Semantic IR Principles for Interoperability

**Source**: **\[Patterson22\]** – “Semantic Soundness for Language Interoperability”

<div class="pillarbox">

**KEY INSIGHT**: Language interoperability is soundly achieved by COMPILATION to a common target language, with convertibility relations defining glue.

**PROBLEM STATEMENT**: How can separately-compiled languages safely interoperate while PRESERVING source-level semantic properties (type safety, memory safety)?

**SOLUTION FRAMEWORK**:

1.  **REALIZABILITY MODELS**: $`\mathcal{V}\llbracket\tau\rrbracket`$ = target terms that behave as source $`\tau`$

2.  **CONVERTIBILITY**: $`\tau_A \sim \tau_B`$ iff types are interconvertible

3.  **GLUE CODE**: Target-level conversion functions between representations

4.  **SEMANTIC TYPE SOUNDNESS**: Source invariants preserved through compilation

**CORE DEFINITIONS** (Patterson Section 3):

**REALIZABILITY INTERPRETATION** (Definition 3.1):
``` math
\mathcal{V}\llbracket\tau\rrbracket = \{\text{target\_terms} \mid \text{they satisfy the source type's behavioral spec}\}
```
Example: $`\mathcal{V}\llbracket\ensuremath{\mathsf{Int}}\rrbracket = \{0, 1, -1, 2, \ldots\}`$ target integers  
$`\mathcal{V}\llbracket\ensuremath{\mathsf{Int}}\to \ensuremath{\mathsf{Int}}\rrbracket = \{\text{target closures that map ints to ints}\}`$

**CONVERTIBILITY RELATION** (Definition 3.3): $`\tau_A \sim \tau_B`$ means there exists target-level glue code that:

- Converts $`\mathcal{V}\llbracket\tau_A\rrbracket`$ to $`\mathcal{V}\llbracket\tau_B\rrbracket`$ (AtoB direction)

- Converts $`\mathcal{V}\llbracket\tau_B\rrbracket`$ to $`\mathcal{V}\llbracket\tau_A\rrbracket`$ (BtoA direction)

- Preserves semantic properties (no UB introduced)

**SEMANTIC TYPE SOUNDNESS** (Theorem 3.5): If source program is well-typed, then:

- Compiled target program preserves source invariants

- Cross-language calls via glue code maintain type safety

- Memory safety violations in source are detected (not introduced)

**CASE STUDIES** (Patterson Section 5):

1.  Shared memory: $`\text{ptr}_A \sim \text{ptr}_B`$ via pointer coercion

2.  Affine/unrestricted: $`\text{affine} \sim \text{linear}`$ via drop tracking

3.  GC/manual: $`\text{gc\_ptr} \sim \text{raw\_ptr}`$ via root registration

**APPLICATION TO BRRR-MACHINE IR**: The IR serves as the “target language” for interoperability. Source languages compile to IR preserving their semantic properties. Cross-language boundaries are EXPLICIT IR nodes with convertibility specs.

</div>

### Realizability Models

Realizability models formalize the semantic meaning of types across language boundaries. Following Patterson & Ahmed (2022), we define $`\mathcal{V}\llbracket\tau\rrbracket`$ as the set of IR values that “behave as” source type $`\tau`$. This provides the theoretical foundation for:

- **Type-safe interoperability**: Values crossing language boundaries satisfy target type semantics

- **Glue code correctness**: Conversion functions preserve behavioral specifications

- **Sound compilation**: Source-level type invariants are preserved through compilation to IR

The `realizability_model` record captures three essential components: type interpretation as a predicate on values, soundness evidence linking interpretation to type safety, and language-specific compatibility rules.

<div class="fstarcode">

(\* ================================================== SEMANTIC IR FOR LANGUAGE INTEROPERABILITY Source: Patterson & Ahmed 2022, Section 3

Formalize IR as a semantic target language where source properties are preserved through compilation via realizability models. ================================================== \*)

module BrrrMachine.SemanticIR

(\* ————————————————– REALIZABILITY MODELS V\[tau\] defines the set of IR values that "behave as" source type tau ————————————————– \*)

(\* Semantic interpretation of types as sets of IR values \*) type realizability_model (source_lang : language_id) =

(\* Interpret a source type as a predicate on IR values \*) interpret : ir_type -\> (ir_value -\> bool);

(\* Evidence that interpretation is sound \*) soundness : forall tau v. interpret tau v ==\> ir_value_safe_at_type v tau;

(\* Language-specific type compatibility \*) compatible : ir_type -\> ir_type -\> bool;

</div>

<div class="fstarcode">

(\* Build realizability model for each supported language \*) val rust_realizability : realizability_model let rust_realizability =

interpret = (fun tau v -\> match tau, v with (\* Rust integers are exact-width \*) \| TInt I32 true, VInt i I32 -\> Int.fits_i32 i \| TInt I64 true, VInt i I64 -\> Int.fits_i64 i

(\* Rust references are non-null and point to valid memory \*) \| TRef pointee, VRef ptr -\> ptr \<\> null && valid_memory ptr (size_of pointee)

(\* Rust Options use discriminant tag \*) \| TOption inner, VVariant "Option" "Some" v -\> interpret inner v \| TOption \_, VVariant "Option" "None" VUnit -\> true

\| \_, \_ -\> false ); soundness = (fun tau v -\> assume_rust_type_soundness tau v); compatible = rust_type_compatible;

</div>

The Rust realizability model above demonstrates how language-specific semantics are captured: `TInt I32 true` (signed 32-bit integer) realizes only values that fit in the i32 range; `TRef` requires non-null pointers to valid memory; `TOption` maps to Rust’s `Option<T>` with discriminant tags. Each language defines its own model reflecting its type semantics.

<div class="fstarcode">

(\* Python’s dynamic typing means most values realize TAny \*) val python_realizability : realizability_model let python_realizability =

interpret = (fun tau v -\> match tau, v with (\* Python int is arbitrary precision \*) \| TInt IBigInt \_, VInt \_ \_ -\> true

(\* Python None is singleton \*) \| TOption \_, VNone -\> true \| TOption inner, v -\> interpret inner v

(\* Python objects are always heap-allocated \*) \| TAny, \_ -\> true

\| \_, \_ -\> false ); soundness = (fun tau v -\> assume_python_gc_safety tau v); compatible = python_type_compatible;

</div>

### Convertibility Relations

Convertibility relations define when types from different languages can be safely interconverted. The relation $`\tau_A \sim \tau_B`$ holds when there exist bidirectional conversion functions (“glue code”) that preserve semantic properties. This is the theoretical foundation for FFI type checking and automatic marshalling.

The `convertibility` record captures:

- Source and target types with their respective languages

- Bidirectional conversion expressions (`convert_AtoB`, `convert_BtoA`)

- Soundness proofs that conversions preserve realizability

<div class="fstarcode">

(\* ————————————————– CONVERTIBILITY RELATIONS tau_A   tau_B iff there exists sound glue code between them ————————————————– \*)

type convertibility =

source_type : ir_type; target_type : ir_type; source_lang : language_id; target_lang : language_id;

(\* Glue code for conversion \*) convert_AtoB : ir_expr; (\* Convert source to target \*) convert_BtoA : ir_expr; (\* Convert target to source \*)

(\* Soundness evidence \*) sound_AtoB : forall v. realizes source_lang source_type v ==\> realizes target_lang target_type (eval convert_AtoB v);

sound_BtoA : forall v. realizes target_lang target_type v ==\> realizes source_lang source_type (eval convert_BtoA v);

</div>

<div class="fstarcode">

(\* Establish convertibility between language types \*) val establish_convertibility : source_lang : language_id -\> target_lang : language_id -\> source_type : ir_type -\> target_type : ir_type -\> option convertibility

let establish_convertibility src_lang tgt_lang src_ty tgt_ty = match (src_lang, tgt_lang, src_ty, tgt_ty) with

(\* Rust\<-\>C: integers are directly compatible \*) \| (Rust, C, TInt w1 s1, TInt w2 s2) when w1 = w2 && s1 = s2 -\> Some source_type = src_ty; target_type = tgt_ty; source_lang = Rust; target_lang = C; convert_AtoB = ELambda \[("x", src_ty)\] (EVar "x") \[\]; (\* Identity \*) convert_BtoA = ELambda \[("x", tgt_ty)\] (EVar "x") \[\]; sound_AtoB = int_identity_sound w1 s1; sound_BtoA = int_identity_sound w1 s1;

(\* Rust\<-\>C: pointers require null check on Rust side \*) \| (Rust, C, TRef pointee, TPtr pointee’) when pointee = pointee’ -\> Some source_type = src_ty; target_type = tgt_ty; source_lang = Rust; target_lang = C; (\* Rust ref to C ptr: direct cast (always non-null) \*) convert_AtoB = ELambda \[("x", src_ty)\] (ECast (EVar "x") tgt_ty) \[\]; (\* C ptr to Rust ref: must check non-null \*) convert_BtoA = ELambda \[("x", tgt_ty)\] (EIf (ECmp OpNe (EVar "x") ENull) (ECast (EVar "x") src_ty) (ECall (EVar "panic") \[EString "null ptr"\])) \[\]; sound_AtoB = rust_ref_to_c_ptr_sound pointee; sound_BtoA = c_ptr_to_rust_ref_sound pointee;

(\* Python\<-\>Rust: requires boxing/unboxing \*) \| (Python, Rust, TAny, TInt I64 true) -\> Some source_type = TAny; target_type = TInt I64 true; source_lang = Python; target_lang = Rust; (\* Python any to Rust i64: runtime type check + extract \*) convert_AtoB = ELambda \[("x", TAny)\] (ECall (EVar "PyLong_AsLongLong") \[EVar "x"\]) \[\]; (\* Rust i64 to Python any: box into PyObject \*) convert_BtoA = ELambda \[("x", TInt I64 true)\] (ECall (EVar "PyLong_FromLongLong") \[EVar "x"\]) \[\]; sound_AtoB = python_int_extract_sound; sound_BtoA = python_int_box_sound;

(\* GC\<-\>Manual memory: Patterson Case Study 3 \*) \| (lang_gc, lang_manual, TRef pointee, TPtr pointee’) when is_gc_language lang_gc && is_manual_language lang_manual && pointee = pointee’ -\> Some source_type = src_ty; target_type = tgt_ty; source_lang = lang_gc; target_lang = lang_manual; (\* GC ref to manual ptr: register as GC root \*) convert_AtoB = ELambda \[("x", src_ty)\] (ESeq (ECall (EVar "gc_register_root") \[EAddr (EVar "x")\]) (ECast (EVar "x") tgt_ty)) \[\]; (\* Manual ptr to GC ref: verify not dangling, adopt into GC \*) convert_BtoA = ELambda \[("x", tgt_ty)\] (ECall (EVar "gc_adopt_external") \[EVar "x"\]) \[\]; sound_AtoB = gc_to_manual_sound lang_gc pointee; sound_BtoA = manual_to_gc_sound lang_manual pointee;

\| \_ -\> None (\* No automatic convertibility \*)

</div>

### Semantic Type Soundness for Cross-Language Calls

<div class="fstarcode">

(\* ————————————————– SEMANTIC TYPE SOUNDNESS FOR CROSS-LANGUAGE CALLS ————————————————– \*)

(\* Cross-language call with convertibility specification \*) type cross_lang_call = caller_lang : language_id; callee_lang : language_id; callee_func : func_id; arg_conversions : list convertibility; ret_conversion : convertibility; call_site : node_id;

</div>

<div class="fstarcode">

(\* Verify semantic soundness of cross-language call \*) val verify_cross_lang_soundness : call : cross_lang_call -\> caller_state : abstract_state -\> callee_contract : ffi_contract -\> result (abstract_state, list semantic_violation)

let verify_cross_lang_soundness call caller_state contract = (\* Phase 1: Verify all argument conversions are sound \*) let arg_violations = List.filter_map (fun conv -\> if not conv.sound_AtoB then Some (UnsoundArgConversion conv.source_type conv.target_type) else None ) call.arg_conversions in

(\* Phase 2: Verify preconditions hold after conversion \*) let converted_args = List.map2 (fun arg conv -\> apply_conversion conv.convert_AtoB arg ) (get_args caller_state) call.arg_conversions in

let precond_violations = verify_ffi_preconditions converted_args contract.precondition in

(\* Phase 3: Apply callee effects and convert return value \*) let callee_post_state = apply_ffi_effects caller_state contract.effect_bounds in

let return_conversion_sound = call.ret_conversion.sound_BtoA in

let final_state = if return_conversion_sound then apply_conversion call.ret_conversion.convert_BtoA callee_post_state else callee_post_state (\* Return type error \*) in

(\* Phase 4: Collect all violations \*) let all_violations = arg_violations @ precond_violations @ (if return_conversion_sound then \[\] else \[UnsoundReturnConversion\]) in

if List.is_empty all_violations then Ok final_state else Error (final_state, all_violations)

type semantic_violation = \| UnsoundArgConversion of ir_type \* ir_type \| UnsoundReturnConversion \| RealizabilityViolation of ir_value \* ir_type \| GlueCodeFailure of string

</div>

### IR Annotations for Semantic Properties

<div class="fstarcode">

(\* ————————————————– IR ANNOTATIONS FOR SEMANTIC PROPERTIES Mark IR nodes with semantic preservation requirements ————————————————– \*)

(\* Extend IR annotations for semantic IR \*) type semantic_annotation = \| AnnRealizesType of ir_type \* language_id (\* Value realizes source type in language \*) \| AnnConvertible of convertibility (\* Conversion is semantically sound \*) \| AnnPreservesInvariant of string (\* Code preserves named source invariant \*) \| AnnBoundaryGlue of cross_lang_call (\* This is glue code for cross-language call \*)

</div>

### Integration with Boundary Analysis

<div class="pillarbox">

Semantic IR connects to boundary detection and FFI contracts through the following integration flow:

**1. Boundary Detection** (Section <a href="#sec:boundary-detection" data-reference-type="ref" data-reference="sec:boundary-detection">[sec:boundary-detection]</a>): When `BoundaryFFI` edge is found, create `cross_lang_call` record.

**2. Convertibility Establishment** (Section <a href="#sec:ffi-contract-synthesis" data-reference-type="ref" data-reference="sec:ffi-contract-synthesis">[sec:ffi-contract-synthesis]</a>): For each arg/return type pair, `establish_convertibility`. If `None`, report as type mismatch.

**3. Glue Code Generation** (Section <a href="#sec:boundary-guards" data-reference-type="ref" data-reference="sec:boundary-guards">[sec:boundary-guards]</a>): Generate guards from convertibility’s `convert_AtoB`/`convert_BtoA`. Guards implement the runtime checks for conversion.

**4. Semantic Verification**: `verify_cross_lang_soundness` ensures:

- All conversions are sound (soundness evidence exists)

- Preconditions hold post-conversion

- Return value correctly converted back

**5. FFI Contract Synthesis** (Section <a href="#sec:furr-type-inference" data-reference-type="ref" data-reference="sec:furr-type-inference">[sec:furr-type-inference]</a>): Furr’s type inference infers representational types. Patterson’s convertibility validates inferred specs. Combined: inferred types + semantic soundness proofs.

**CROSS-REFERENCES**:

- Section <a href="#sec:representation-predicates" data-reference-type="ref" data-reference="sec:representation-predicates">[sec:representation-predicates]</a> (Representation Predicates): `rep_predicate` connects to realizability interpretation $`\mathcal{V}\llbracket\tau\rrbracket`$

- Section <a href="#sec:boundary-risk" data-reference-type="ref" data-reference="sec:boundary-risk">1.2</a> (Boundary Risk): convertibility failure = high risk

- Sections <a href="#sec:ffi-contracts-preconditions" data-reference-type="ref" data-reference="sec:ffi-contracts-preconditions">[sec:ffi-contracts-preconditions]</a>–<a href="#sec:ffi-contracts-type-errors" data-reference-type="ref" data-reference="sec:ffi-contracts-type-errors">[sec:ffi-contracts-type-errors]</a> (FFI Contracts): contracts specify what convertibility must establish

- Section <a href="#sec:furr-type-inference" data-reference-type="ref" data-reference="sec:furr-type-inference">[sec:furr-type-inference]</a> (Furr 2008): representational types align with Patterson’s target-level type interpretation

</div>

# Complete F\* Formalization

## Module Structure

<div class="fstarcode">

(\* ======================================================================= BRRR-MACHINE F\* MODULE STRUCTURE ======================================================================= \*) (\* BrrrMachine +– Core \| +– AbstractDomain – Lattices, Galois connections \| +– Domains – Concrete domains (intervals, taint, etc.) \| +– Effects – Effect types and rows \| +– Ownership – Resource algebras, ownership states \| +– Representation \| +– IR – Intermediate representation \| +– CPG – Code Property Graph types \| +– CPG.Builder – CPG construction \| +– CPG.Traversal – Graph traversal primitives \| +– Analysis \| +– IFDS – IFDS algorithm \| +– PointerAnalysis – Andersen, Steensgaard \| +– TaintAnalysis – Source-sink analysis \| +– NullAnalysis – Nullability tracking \| +– ResourceAnalysis – Lifecycle tracking \| +– OwnershipAnalysis – Rust-style ownership \| +– RaceDetection – Data race detection \| +– Security \| +– Taint – Taint sources, sinks, sanitizers \| +– Vulnerabilities – Vulnerability types \| +– SARIF – Output format \| +– Boundary \| +– Languages – Language configurations \| +– Boundaries – Boundary detection and analysis \| +– Risks – Risk calculation \| +– Incremental \| +– Thunks – Cached computations \| +– IncrementalCPG – Incremental graph \| +– Theorems +– Soundness – Soundness proofs +– Termination – Termination proofs +– Correctness – Correctness lemmas \*)

</div>

## Key Soundness Theorems

<div class="pillarbox">

- **\[Cousot77\]** — Abstract Interpretation Soundness

- **\[Reps95\]** — IFDS Algorithm Soundness and Completeness

- **\[HerlihyWing90\]** — Linearizability and Locality Theorem

- **\[SabelfeldMyers03\]** — Information Flow Soundness

- **\[Zilberstein23\]** — Outcome Logic Theorems

- **\[Bruni23\]** — Local Completeness

</div>

<div class="fstarcode">

module BrrrMachine.Theorems.Soundness

(\* ————————————————– ABSTRACT INTERPRETATION SOUNDNESS Source: Cousot 1977 ————————————————– \*) (\* If abstract analysis says "safe", concrete execution is safe \*) val abstract_interpretation_sound : \#c:Type -\> \#a:Type -\> gc:galois_connection c a -\> transfer_concrete:(c -\> c) -\> transfer_abstract:(a -\> a) -\> (\* Transfer function is sound \*) (forall x. gc.alpha (transfer_concrete x) ‘leq‘ transfer_abstract (gc.alpha x)) -\> (\* Fixpoint is sound \*) Lemma (gc.alpha (lfp transfer_concrete) ‘leq‘ compute_fixpoint transfer_abstract)

</div>

<div class="theorem">

**Theorem 2.1** (IFDS Soundness — **\[Reps95\]**). *The IFDS solution is sound: all reported facts actually hold.*

<div class="fstarcode">

*val ifds_sound : \#d:Type -\> problem:ifds_problem d -\> node:node_id -\> fact:d -\> (\* If fact is in the solution \*) (node, fact) ‘mem‘ solve problem -\> (\* Then there exists a concrete execution where fact holds \*) Lemma (exists exec. reaches exec node / fact_holds exec node fact)*

</div>

</div>

<div class="theorem">

**Theorem 2.2** (IFDS Completeness for Distributive Problems). *For distributive dataflow problems, IFDS is complete.*

<div class="fstarcode">

*val ifds_complete : \#d:Type -\> problem:ifds_problem d -\> (\* Problem is distributive \*) (forall e s1 s2. problem.transfer e (s1 ‘union‘ s2) == problem.transfer e s1 ‘union‘ problem.transfer e s2) -\> node:node_id -\> fact:d -\> (\* If fact actually holds in some execution \*) (exists exec. reaches exec node / fact_holds exec node fact) -\> (\* Then it’s in the solution \*) Lemma ((node, fact) ‘mem‘ solve problem)*

</div>

</div>

<div class="fstarcode">

(\* No false negatives: all actual taint flows are detected \*) val taint_analysis_sound : cpg:cpg -\> sources:(node_id -\> option taint_source) -\> sinks:(node_id -\> option taint_sink) -\> sanitizers:(node_id -\> option sanitizer) -\> result:taint_analysis_result -\> (\* Result is from our analysis \*) result == run_taint_analysis cpg sources sinks sanitizers -\> (\* For any concrete execution \*) forall (exec : concrete_execution). (\* If tainted data reaches a sink unsanitized \*) (exists src sink. is_source src / is_sink sink / reaches_unsanitized exec src sink) -\> (\* Then we reported it \*) Lemma (exists flow. flow ‘mem‘ result.flows / flow.source_location == src / flow.sink_location == sink)

</div>

<div class="theorem">

**Theorem 2.3** (Locality Theorem — **\[HerlihyWing90\]**). *Linearizability is compositional.*

<div class="fstarcode">

*val locality_theorem : h:history -\> specs:(string -\> sequential_spec) -\> Lemma (linearizable h (combined_spec specs) \<==\> (forall obj. linearizable (subhistory h obj) (specs obj)))*

</div>

</div>

<div class="fstarcode">

(\* If implicit flow analysis reports no leaks, noninterference holds \*) val implicit_analysis_sound : cpg -\> labeling:(string -\> security_level) -\> (run_implicit_analysis cpg labeling = \[\]) -\> Lemma (termination_insensitive_noninterference (semantics cpg) labeling)

(\* Falsification completeness: if spec is violated, we can prove it \*) val falsification_complete : pre:outcome_assertion -\> prog:program -\> post:outcome_assertion -\> Lemma (requires (not (valid_ol_triple pre prog post))) (ensures (valid_ol_triple pre prog (OAConj (OANot post) OATop)))

(\* If locally complete, abstract analysis is EXACT for that input \*) val local_completeness_precision : \#a:Type -\> dom:abstract_domain a -\> f:(concrete -\> concrete) -\> f_sharp:(a -\> a) -\> c:concrete -\> is_locally_complete dom f c -\> (forall x. dom.lat.alpha (f x) ‘leq‘ f_sharp (dom.lat.alpha x)) -\> Lemma (dom.lat.alpha (f c) = f_sharp (dom.lat.alpha c))

</div>

## Provable Bug Classification (Manifest/Latent)

<div class="pillarbox">

**Source**: **\[Le22\]** (ISL) via **\[Vanegue25\]** — “Non-Termination Proving”

**OLD (HEURISTIC)**: confidence = 0.0 to 1.0 float  
*Problem*: No formal semantics. What does “0.7 confidence” mean?

**NEW (PROVABLE)**: Manifest vs Latent classification

- **Manifest**: Bug triggers IN ALL CALLING CONTEXTS

- **Latent**: Bug triggers ONLY IN SPECIFIC CONTEXTS

**KEY THEOREM (True Positives Property)**:  
Manifest bugs are GUARANTEED to be real bugs (no false positives).  
This is a THEOREM, not a heuristic!

</div>

<div class="definition">

**Definition 3.1** (ISL Triple — **\[Le22\]**). An Incorrectness Separation Logic Triple $`[p]\, C\, [q; \mathit{exit}]`$ consists of:

- $`p`$ = PRESUMPTION (existentially quantified precondition)

- $`C`$ = CODE

- $`q`$ = RESULT (postcondition)

- $`\mathit{exit}`$ = EXIT CONDITION ($`\mathsf{Ok}`$ or $`\mathsf{Er}`$ for error)

**Semantics**: If $`C`$ starts in state satisfying $`p`$ and terminates with exit condition, then result satisfies $`q`$.

</div>

<div class="fstarcode">

module BrrrMachine.ManifestLatent

type exit_condition = Ok \| Er (\* Normal vs Error termination \*)

type isl_triple = presumption : assertion; (\* p - starting states (existential) \*) code : cpg_slice; (\* C - analyzed code \*) result : assertion; (\* q - ending states \*) exit_cond : exit_condition; (\* Ok or Er \*)

(\* ————————————————– MANIFEST vs LATENT CLASSIFICATION (Le 2022, Definition 3.3) A bug is MANIFEST if it satisfies ALL of these conditions: 1. Exit condition is Er (error) 2. Presumption is emp / true (empty heap, no constraints) 3. Result assertion is satisfiable 4. All heap locations in result are existentially quantified 5. Pure constraints in result are universally satisfiable

MANIFEST = Bug triggers regardless of calling context LATENT = Bug requires specific calling context to trigger ————————————————– \*) type bug_classification = \| Manifest : proof:manifest_proof -\> bug_classification \| Latent : required_context:assertion -\> bug_classification \| RelaxedManifest : violations:list manifest_violation -\> bug_classification

type manifest_proof = triple : isl_triple; empty_pre : squash (triple.presumption ‘equiv‘ (emp ‘conj‘ true_pure)); result_sat : squash (satisfiable triple.result); locs_existential : squash (locs (spatial triple.result) ‘subset‘ existential_vars triple.result); pure_universal : squash (forall_instantiations_sat (pure triple.result));

</div>

<div class="theorem">

**Theorem 3.2** (True Positives Property — **\[Le22\]**, Theorem 3.4). *If a bug is classified as MANIFEST, then either:*

1.  *The code is dead (unreachable), OR*

2.  *There EXISTS a concrete input that triggers the bug*

*This is the fundamental soundness theorem for bug detection. It guarantees NO FALSE POSITIVES for manifest bugs.*

<div class="fstarcode">

*val true_positives_property : cpg:cpg -\> triple:isl_triple -\> proof:manifest_proof -\> Lemma (is_dead_code cpg triple.code (exists input. triggers_bug cpg input triple))*

</div>

</div>

<div class="theorem">

**Theorem 3.3** (Falsification Completeness — **\[Le22\]**, Theorem 3.5). *If a bug is LATENT with required context $`C`$, then ANY calling context satisfying $`C`$ will trigger the bug.*

<div class="fstarcode">

*val falsification_completeness : cpg:cpg -\> triple:isl_triple -\> required_ctx:assertion -\> Latent? (classify_bug triple) -\> classify_bug triple = Latent required_ctx -\> Lemma (forall ctx_state. ctx_state ‘satisfies‘ required_ctx ==\> triggers_bug cpg ctx_state triple)*

</div>

</div>

<div class="definition">

**Definition 3.4** (Relaxed Manifest Criterion — **\[Le22\]** Section 4). Strict manifest requires $`\mathsf{emp}`$ precondition (empty heap). RELAXED allows non-emp precondition IF:

- All heap cells in precondition are POSITIVE (allocated, not deallocated)

- No negative cells (freed memory) in precondition

A heap cell is “positive” if it represents allocated memory ($`x \mapsto v`$), not deallocated memory or an absence constraint.

</div>

<div class="fstarcode">

(\* ————————————————– ISL CONSEQUENCE RULE (Le 2022) NOTE: Entailment direction is REVERSED from Hoare logic!

Hoare: p’ \|= p, p C q, q \|= q’ ==\> p’ C q’ ISL: p \|= p’, \[p\] C \[q\], q \|= q’ ==\> \[p’\] C \[q’\]  WEAKENED precondition entails ORIGINAL (opposite direction!)

REASON: ISL is under-approximate. Weaker precondition means FEWER starting states, which is sound for "bug exists" claims. ————————————————– \*) val isl_consequence : triple:isl_triple -\> p’:assertionp’ \|= triple.presumption -\> (\* p’ entails p \*) q’:assertiontriple.result \|= q’ -\> (\* q entails q’ \*) isl_triple

</div>

### Occurrence Typing Soundness Theorems

<div class="pillarbox">

The key theorem: Type refinements are SOUND with respect to runtime values.

If occurrence typing says variable $`x`$ has type $`\tau`$ at program point $`p`$, then at runtime, the value of $`x`$ is indeed a member of $`\tau`$.

**CRITICAL INSIGHT**: Soundness requires VISIBLE predicates to be accurate. A visible predicate can only be established by an ACTUAL runtime check.

</div>

<div class="fstarcode">

module BrrrMachine.Theorems.OccurrenceTyping

(\* A visible predicate is a proposition known to hold at the current point \*) type visible_predicate = \| VPHasType : var:string -\> ty:ir_type -\> visible_predicate \| VPNotType : var:string -\> ty:ir_type -\> visible_predicate \| VPAnd : visible_predicate -\> visible_predicate -\> visible_predicate \| VPOr : visible_predicate -\> visible_predicate -\> visible_predicate \| VPTrue : visible_predicate \| VPFalse : visible_predicate

(\* Restrict operation: narrow sigma to tau \*) val restrict : ir_type -\> ir_type -\> ir_type let rec restrict sigma tau = match sigma with \| TUnion types -\> let filtered = List.filter (fun t -\> types_overlap t tau) types in type_union_of_list filtered \| TTop -\> tau \| \_ -\> if subtype sigma tau then sigma else if subtype tau sigma then tau else TBottom (\* No overlap \*)

(\* THEOREM: restrict is sound \*) val restrict_sound : sigma:ir_type -\> tau:ir_type -\> v:runtime_value -\> Lemma (requires value_has_type v sigma && value_has_type v tau) (ensures value_has_type v (restrict sigma tau))

</div>

## Under-Approximation Theorems

<div class="pillarbox">

**VERIFICATION SOUNDNESS (over-approx)**: $`\alpha(\text{concrete}) \subseteq \text{abstract}`$  
“If analysis says safe, truly safe” — may have FALSE POSITIVES

**DETECTION SOUNDNESS (under-approx)**: $`\text{abstract} \subseteq \text{concrete}`$  
“If analysis says bug, truly a bug” — may have FALSE NEGATIVES

The synthesis REQUIRES BOTH for complete analysis.

</div>

<div class="fstarcode">

module BrrrMachine.Theorems.UnderApproximation

(\* ————————————————– UNDER-APPROXIMATE TRIPLE SEMANTICS \[p\] C \[q\] means: every state in q is REACHABLE from some state in p. This is the DUAL of Hoare’s p C q (over-approximation). ————————————————– \*) type under_triple (a : Type) = presumption : a; (\* Starting states \*) code : cpg; (\* Program \*) result_ok : a; (\* Normal termination states \*) result_err : a; (\* Error termination states \*)

(\* Semantic validity: result UNDER-approximates reachable states \*) val valid_under_approx : \#a:Type -\> \| abstract_domain a \| -\> under_triple a -\> bool let valid_under_approx \#a \#d t = forall (s : concrete_state). s ‘in_concretization‘ t.result_ok ==\> (exists (s0 : concrete_state). s0 ‘in_concretization‘ t.presumption / can_reach t.code s0 s)

</div>

<div class="theorem">

**Theorem 4.1** (Reversed Consequence Rule). ***CRITICAL DIFFERENCE** from Hoare logic:*

- *Hoare: stronger precondition, weaker postcondition*

- *Incorrectness: WEAKER precondition, STRONGER postcondition*

<div class="fstarcode">

*val under_approx_consequence : \#a:Type -\> \| d : abstract_domain a \| -\> t:under_triple a -\> p’:a -\> q’:a -\> (\* WEAKER precondition: p ==\> p’ (note direction!) \*) d.leq t.presumption p’ == true -\> (\* STRONGER postcondition: q’ ==\> q (note direction!) \*) d.leq q’ t.result_ok == true -\> valid_under_approx t -\> Lemma (valid_under_approx t with presumption = p’; result_ok = q’ )*

</div>

</div>

<div class="theorem">

**Theorem 4.2** (Disjunction Rule: Path Dropping is Sound). *
``` math
\frac{[p_1]\, C\, [q_1] \quad [p_2]\, C\, [q_2]}{[p_1 \vee p_2]\, C\, [q_1 \vee q_2]}
```
*

***CRITICAL**: Can FORGET paths in under-approximation! This is UNSOUND in over-approximation but SOUND here. Justifies partial coverage in bug finding.*

<div class="fstarcode">

*val under_approx_disjunction : \#a:Type -\> \| d : abstract_domain a \| -\> t1 t2 : under_triple a -\> valid_under_approx t1 -\> valid_under_approx t2 -\> t1.code == t2.code -\> (\* Same program \*) Lemma (valid_under_approx presumption = d.join t1.presumption t2.presumption; code = t1.code; result_ok = d.join t1.result_ok t2.result_ok; result_err = d.join t1.result_err t2.result_err; )*

</div>

</div>

## Manifest Error Theorems

<div class="pillarbox">

Some bugs are MANIFEST (context-independent) while others are LATENT (context-dependent). Outcome Logic provides the formal framework for distinguishing these.

</div>

## Institution Theory Foundations

<div class="pillarbox">

Institutions provide the MATHEMATICAL FOUNDATION for reasoning across multiple logics and languages.

An institution $`\mathcal{I} = (\mathbf{Sign}, \mathbf{Sen}, \mathbf{Mod}, \models)`$ consists of:

- $`\mathbf{Sign}`$: Category of signatures

- $`\mathbf{Sen}`$: $`\mathbf{Sign} \to \mathbf{Set}`$, sentence functor

- $`\mathbf{Mod}`$: $`\mathbf{Sign}^{op} \to \mathbf{Cat}`$, model functor (contravariant)

- $`\models`$: Satisfaction relation

**THE SATISFACTION CONDITION (Goguen’s Key Axiom)**:  
“Truth is invariant under change of notation.”

For any signature morphism $`\sigma : \Sigma \to \Sigma'`$, model $`M'`$ in $`\mathbf{Mod}(\Sigma')`$, sentence $`\phi`$ in $`\mathbf{Sen}(\Sigma)`$:
``` math
M' \models_{\Sigma'} \mathbf{Sen}(\sigma)(\phi) \iff \mathbf{Mod}(\sigma)(M') \models_{\Sigma} \phi
```

</div>

<div class="fstarcode">

module BrrrMachine.Institutions

(\* Signature category \*) type signature type sig_morphism = source : signature; target : signature

(\* Sentence functor \*) type sentence (s : signature) val translate_sentence : sig_morphism -\> sentence s.source -\> sentence s.target

(\* Model functor (contravariant) \*) type model (s : signature) val translate_model : sig_morphism -\> model s.target -\> model s.source

(\* Satisfaction relation \*) val satisfies : \#s:signature -\> model s -\> sentence s -\> bool

(\* THE SATISFACTION CONDITION - FUNDAMENTAL THEOREM \*) val satisfaction_condition : sigma : sig_morphism -\> m’ : model sigma.target -\> phi : sentence sigma.source -\> Lemma (satisfies m’ (translate_sentence sigma phi) \<==\> satisfies (translate_model sigma m’) phi)

</div>

<div class="theorem">

**Theorem 6.1** (Theory Colimits — **\[GoguenBurstall92\]**, Theorem 11). *If the category of signatures has colimits, then the category of theories has colimits.*

***Application**: Multi-language analysis combines language theories via pushouts:
``` math
\text{Python theory} \leftarrow \text{Common theory} \rightarrow \text{Rust theory}
```
*

</div>

## C11 Memory Model Theorems

<div class="pillarbox">

“Mathematizing C++ Concurrency”

This section provides the complete formal verification of the C11 memory model theorems, building on Section 6.5’s definitions.

</div>

<div id="thm:drf-sc" class="theorem">

**Theorem 7.1** (DRF-SC: Data Race Freedom implies Sequential Consistency). ***THE FUNDAMENTAL THEOREM OF THE C11 MEMORY MODEL***

*If a program has no data races under SC semantics, then all its behaviors are sequentially consistent.*

*This is the contract between programmer and implementation:*

- *Programmer ensures: no data races (via proper synchronization)*

- *Implementation guarantees: SC semantics (simple reasoning)*

***Formal Statement**:
``` math
\mathsf{DRF}(P) \Rightarrow \forall \mathit{exec}.\, \mathsf{consistent}(\mathit{exec}) \Rightarrow \exists \mathit{sc\_exec}.\, \mathsf{SC}(\mathit{sc\_exec}) \land \mathsf{observable}(\mathit{exec}) = \mathsf{observable}(\mathit{sc\_exec})
```
*

<div class="fstarcode">

*val data_race_free_implies_sc : program:program_type -\> (\* Precondition: Program has no data races when executed under SC \*) Lemma (requires is_drf_program_formal program) (\* Postcondition: ALL consistent executions are SC-equivalent \*) (ensures forall (exec : candidate_execution). is_execution_of program exec && is_consistent exec ==\> exists (sc_exec : candidate_execution). is_execution_of program sc_exec && is_sc_execution sc_exec && same_observable_behavior exec sc_exec)*

</div>

</div>

<div class="pillarbox">

The C11 axiomatic model (**\[Batty11\]**) permits “out-of-thin-air” values in programs using relaxed atomics. Coherence axioms are NECESSARY but NOT SUFFICIENT to prevent thin-air values.

**Safe Subset**: Programs using ONLY release/acquire synchronization (no relaxed atomics) are safe. See Theorem <a href="#thm:release-acquire-drf" data-reference-type="ref" data-reference="thm:release-acquire-drf">7.2</a>.

**Recommended**: For sound reasoning about programs with relaxed atomics, use Promising Semantics 2.0 (**\[Lee20\]**).

</div>

<div id="thm:release-acquire-drf" class="theorem">

**Theorem 7.2** (Release-Acquire Programs are DRF). *Programs using only release-acquire synchronization (no relaxed atomics) are automatically DRF and thus SC.*

<div class="fstarcode">

*val only_release_acquire_is_drf : program:program_type -\> Lemma (requires uses_only_release_acquire program) (ensures is_drf_program_formal program)*

</div>

</div>

## Effect Absence Theorems

<div class="pillarbox">

“Koka: Programming with Row-polymorphic Effect Types”

Key semantic insight: if an effect is NOT present in the inferred effect row, the program CANNOT exhibit that behavior. This gives us mathematically proven bug detection capabilities.

</div>

<div class="theorem">

**Theorem 8.1** (Exception Soundness — **\[Leijen14\]**, Theorem 2). *If the inferred effect row does NOT contain the exception effect ($`\mathsf{exn}`$), then evaluation can NEVER produce an unhandled exception.*

***Formal Statement**:
``` math
\forall e : \mathit{Expr}, \tau : \mathit{Type}, \epsilon : \mathit{Effect}.\quad (\Gamma \vdash e : \tau \mid \epsilon) \land \mathsf{exn} \notin \epsilon \Rightarrow \neg(e \to^* \mathsf{throw}\, c)
```
*

<div class="fstarcode">

*val exception_soundness : gamma:typing_context -\> e:expr -\> tau:type\_ -\> eff:effect_row -\> (\* Precondition: e is well-typed with effect eff, and exn not in eff \*) Lemma (requires type_judgment gamma e tau eff && effect_definitely_absent eff LExn) (\* Postcondition: evaluation cannot produce unhandled exception \*) (ensures forall (outcome : eval_outcome). evaluates_to e outcome ==\> not (OutThrow? outcome) && not (OutHeapThrow? outcome))*

</div>

</div>

<div class="theorem">

**Theorem 8.2** (Termination Guarantee — **\[Leijen14\]**, Theorem 4). *If the inferred effect row does NOT contain the divergence effect ($`\mathsf{div}`$), then evaluation is GUARANTEED to terminate.*

<div class="fstarcode">

*val termination_guarantee : gamma:typing_context -\> e:expr -\> tau:type\_ -\> eff:effect_row -\> Lemma (requires type_judgment gamma e tau eff && effect_definitely_absent eff LDiv) (ensures exists (outcome : eval_outcome). evaluates_to e outcome && not (OutDiverge? outcome))*

</div>

</div>

<div class="theorem">

**Theorem 8.3** (State Isolation — **\[Leijen14\]**, Theorem 3). *If the inferred effect row does NOT contain a state effect ($`\mathsf{st}\langle h \rangle`$), then evaluation CANNOT produce heap-bound answers for that heap.*

</div>

## Session Type Theorems

<div class="pillarbox">

This section formalizes the theoretical foundations of multiparty session types for analyzing communication protocols in concurrent and distributed systems.

</div>

<div class="fstarcode">

(\* Global type syntax - describes conversation from global viewpoint \*) type global_type = \| GMsg : sender:participant -\> receiver:participant -\> channel:nat -\> payload:sort -\> continuation:global_type -\> global_type \| GBranch : sender:participant -\> receiver:participant -\> channel:nat -\> branches:list (label \* global_type) -\> global_type \| GPar : left:global_type -\> right:global_type -\> global_type \| GRec : var:type_var -\> body:global_type -\> global_type \| GVar : var:type_var -\> global_type \| GEnd : global_type

(\* Local type syntax - describes behavior from single participant’s viewpoint \*) type local_type = \| LSend : channel:nat -\> payload:sort -\> continuation:local_type -\> local_type \| LRecv : channel:nat -\> payload:sort -\> continuation:local_type -\> local_type \| LSelect : channel:nat -\> branches:list (label \* local_type) -\> local_type \| LBranch : channel:nat -\> branches:list (label \* local_type) -\> local_type \| LRec : var:type_var -\> body:local_type -\> local_type \| LVar : var:type_var -\> local_type \| LEnd : local_type

</div>

<div class="theorem">

**Theorem 9.1** (Communication Safety — **\[Honda08\]**, Theorem 5.5). *For well-typed, coherent processes, communication is always safe: linearity is preserved and delivered values match receiving prefixes.*

</div>

<div class="theorem">

**Theorem 9.2** (Session Fidelity — **\[Honda08\]**, Corollary 5.6). *Process execution follows global specification: if $`\Delta(s) = \mathsf{encode}(G)`$ and the process reduces at $`s_k`$, then there exists $`G'`$ such that $`G`$ reduces at $`k`$ to $`G'`$ and $`\mathsf{encode}(G') = \Delta'(s)`$.*

</div>

<div class="theorem">

**Theorem 9.3** (Progress — **\[Honda08\]**, Theorem 5.12). *For simple, well-linked, well-typed processes: every reachable state is either terminated or can take a step.*

</div>

## Sparse Value-Flow Analysis Theorems

<div class="pillarbox">

ISSTA 2012: “Detecting Memory Leaks Statically with Full-Sparse Value-Flow Analysis”  
CC 2016: “SVF: Interprocedural Static Value-Flow Analysis in LLVM”

</div>

<div class="theorem">

**Theorem 10.1** (Memory SSA Soundness). *If $`\mathsf{pts}`$ is a sound may-alias analysis, then Memory SSA construction captures all concrete def-use relationships for address-taken variables.*

</div>

<div class="theorem">

**Theorem 10.2** (SVFG Construction Preserves Def-Use — **\[Sui12\]**, Theorem 1). *For every def-use pair $`(d, u)`$ in the concrete semantics, there exists a path in the SVFG from the node representing $`d`$ to the node representing $`u`$.*

</div>

<div class="theorem">

**Theorem 10.3** (Leak Detection Correctness — **\[Sui12\]**, Theorem 3). *If $`\mathsf{detect\_leak}`$ returns $`\mathsf{DefinitelyLeaks}`$, then every concrete execution that allocates at that source will leak that allocation.*

</div>

## Selective Context Sensitivity Theorems (ZIPPER)

<div class="pillarbox">

“A Principled Approach to Selective Context Sensitivity for Pointer Analysis”

Key insight: Only approximately 38% of methods benefit from context sensitivity. The remaining 62% receive CS treatment that wastes analysis time.

ZIPPER identifies precision-critical methods via three observable value-flow patterns: Direct, Wrapped, and Unwrapped flows.

</div>

<div class="theorem">

**Theorem 11.1** (PFG Captures Flows). *If a flow pattern exists from In method $`M_1`$ to Out method $`M_2`$ of class $`C`$, then there exists a path in $`\mathsf{PFG}_C`$ from a parameter of $`M_1`$ to a return of $`M_2`$.*

</div>

<div class="theorem">

**Theorem 11.2** (Precision Preservation (Empirical)). *ZIPPER-guided selective CS preserves 98.8% of full 2obj precision.*

***Source**: **\[Li20\]**, Table 1*

</div>

## Chopped Symbolic Execution Theorems

<div class="pillarbox">

“Chopped Symbolic Execution” (ICSE 2018)

CSE provides directed symbolic execution by skipping irrelevant code and recovering on-demand when dependencies are discovered.

</div>

<div class="theorem">

**Theorem 12.1** (CSE Soundness). *All bugs found by CSE are real bugs (reachable in concrete execution).*

***Exception**: Non-termination in skipped functions is not detected.*

<div class="fstarcode">

*val cse_soundness : prog:ir_program -\> skip_funcs:skip_set -\> target:program_point -\> bug:bug_witness -\> found:boolfound = cse_finds_bug prog skip_funcs target bug -\> terminates:boolterminates = all_skips_terminate prog skip_funcs bug.input_model -\> Lemma (found && terminates ==\> reachable_concrete prog target bug)*

</div>

</div>

<div class="theorem">

**Theorem 12.2** (Relative Completeness). *For bugs NOT depending on skip internals, CSE will find them (if full SE would find them).*

</div>

## Python Function Type Graph Theorems

<div class="pillarbox">

“JARVIS: Scalable and Precise Application-Centered Call Graph Construction for Python”

FTG provides flow-sensitive type inference for Python with strong updates and C3 MRO support.

</div>

<div class="theorem">

**Theorem 13.1** (FTG Type Safety). *If FTG infers type $`T`$ for expression $`e`$, then at runtime $`e`$ will have a type that is a subtype of $`T`$.*

</div>

<div class="theorem">

**Theorem 13.2** (Call Graph Soundness). *All actual calls during execution are captured in the computed call graph. No false negatives for call edges.*

</div>

## Data Structure Analysis (DSA) Theorems

<div class="pillarbox">

“Making Context-sensitive Points-to Analysis with Heap Cloning Practical For The Real World”

</div>

<div class="theorem">

**Theorem 14.1** (Completeness Preservation). *If a node has the Complete flag, it will never be merged with another node in subsequent analysis phases.*

</div>

<div class="theorem">

**Theorem 14.2** (DSA Soundness). *If two pointers $`p`$ and $`q`$ may alias in concrete execution, then they point to the same DS node (or at least one is incomplete).*

</div>

<div class="theorem">

**Theorem 14.3** (Heap Cloning Distinguishes Instances). *If two pointers point to objects allocated at different call contexts to the same allocation function, they are in different DS nodes.*

</div>

## Robust Declassification Theorems

<div class="pillarbox">

“Security Policies for Downgrading”

Security under declassification requires tracking BOTH confidentiality AND integrity. Robustness is fundamentally an INTEGRITY property: low-integrity (attacker-controlled) code cannot influence what information is declassified.

</div>

<div class="definition">

**Definition 15.1** (4-Point Security Lattice). The minimal structure needed for robust declassification:

<div class="center">

</div>

**Ordering**: $`(c_1, i_1) \sqsubseteq (c_2, i_2)`$ iff $`c_1 \sqsubseteq_C c_2 \land i_2 \sqsubseteq_I i_1`$

Note: Integrity is INVERTED in the product lattice.

</div>

<div class="definition">

**Definition 15.2** (Semantic Security Under Declassification). A program is semantically secure under a declassification policy iff for all low-equivalent initial memories, the visible outputs are equivalent:
``` math
M_1 \sim_L M_2 \Rightarrow \mathsf{visible}(P(M_1)) \sim_{\mathit{vis}} \mathsf{visible}(P(M_2))
```

</div>

<div class="theorem">

**Theorem 15.3** (Robustness Implies Security — **\[ChongMyers04\]**, Theorem 3). *If the static robustness check passes for all declassification points, then semantic security holds.*

***Robustness Condition**: For a declassification at program point $`p`$ with expression $`e`$:*

1.  *$`\mathsf{PC\_integrity}(p) = \mathsf{HIGH}`$ (no low-integrity code on path to $`p`$)*

2.  *For all $`v \in \mathsf{deps}(e)`$: $`\mathsf{integrity}(v) = \mathsf{HIGH}`$ (no low-integrity data)*

</div>

### Speculative Non-Interference Theorems

<div class="pillarbox">

Speculative Non-Interference ensures that speculative execution does not reveal more than non-speculative execution.

</div>

<div class="definition">

**Definition 15.4** (SNI). A program satisfies Speculative Non-Interference iff:
``` math
\forall m_1, m_2.\quad m_1 \sim_L m_2 \Rightarrow (\mathsf{trace}_{spec}(m_1) = \mathsf{trace}_{spec}(m_2) \Rightarrow \mathsf{trace}_{std}(m_1) = \mathsf{trace}_{std}(m_2))
```

</div>

<div class="theorem">

**Theorem 15.5** (Worst-Case Predictor Soundness). *SNI with worst-case predictor implies SNI with any concrete predictor.*

</div>

### Constant-Time Product Program Theorems

<div class="pillarbox">

Constant-time security is a 2-safety hyperproperty that can be reduced to assertion checking via product program construction.

</div>

<div class="theorem">

**Theorem 15.6** (Product Reduction — **\[Almeida16\]**). *A program is constant-time secure iff its product program is assertion-safe:
``` math
\mathsf{is\_ct\_secure}(\mathit{prog}, \mathit{model}, \mathit{pub}) \iff \mathsf{is\_assertion\_safe}(\mathsf{construct\_product}(\mathit{prog}, \mathit{model}))
```
*

*This is the main soundness theorem, verified in Coq by the CT-Verif authors.*

</div>

## Capability Algebra Theorems

<div class="pillarbox">

“Typed Memory Management”

Ownership states track LIFECYCLE (acquired $`\to`$ in-use $`\to`$ released). Multiplicities track ALIASING (unique vs potentially aliased). Both are needed for sound memory management.

</div>

<div class="definition">

**Definition 16.1** (Multiplicity Types).

- $`\mathsf{MUnique}`$: Provably no aliases exist. CAN free safely.

- $`\mathsf{MDup}`$: May have aliases. Can only ACCESS, not free.

This is STRONGER than “owned” — owned but aliased CANNOT free.

</div>

<div class="theorem">

**Theorem 16.2** (Free Requires Unique). *Only unique capabilities can safely free memory. This prevents use-after-free from aliased pointers.*

<div class="fstarcode">

*val can_free : capability -\> region -\> bool let can_free c r = match Map.find r c with \| Some MUnique -\> true \| \_ -\> false*

*val free_unique_safe : heap:heap -\> cap:capability -\> r:region -\> can_free cap r == true -\> (forall ptr. ptr ‘points_to‘ r ==\> ptr ‘in‘ cap_owned_ptrs cap r) -\> Lemma (free heap r ‘results_in‘ safe_heap)*

</div>

</div>

<div class="theorem">

**Theorem 16.3** (Complete Collection — **\[Crary99\]**, Section 4.2). *Well-typed terminating programs return ALL memory to the system. This proves LEAK-FREEDOM for the type system.*

<div class="fstarcode">

*val complete_collection : prog:ir_program -\> well_typed prog -\> Lemma ( diverges prog (\* Either it runs forever... \*) (exists result. terminates prog result / final_heap prog == empty_heap) (\* ...or heap is empty \*) )*

</div>

</div>

## Realizability Theorems

<div class="pillarbox">

“Semantic Soundness for Interoperability”

Matthews-Findler syntactic boundaries are insufficient. For SEMANTIC soundness, interpret source types as sets of TARGET values.

</div>

<div class="theorem">

**Theorem 17.1** (Shared Memory Identity — **\[Patterson22\]**, Section 4.2). *Sharing $`\mathsf{ref}\, \tau_1`$ with $`\mathsf{ref}\, \tau_2`$ (no copy) requires $`V[\tau_1] = V[\tau_2]`$.*

***Examples**:*

- *Rust `&mut i32` $`\leftrightarrow`$ C `int*` (same target representation)*

- *Python `list` $`\leftrightarrow`$ C `array` (different representations, must copy)*

</div>

## Verified Interpreter Pattern

<div class="pillarbox">

“Mechanising and Verifying the WebAssembly Specification”

The verified interpreter pattern separates RELATIONAL specification from EXECUTABLE implementation, then proves equivalence. This provides a verified executable that is provably correct with respect to the declarative semantics.

**Key Insight**: Declarative specifications are easy to reason about but cannot execute. Executable implementations can run but are complex. By proving equivalence, we get the best of both worlds.

</div>

The following F\* code demonstrates the core pattern: a relational reduction specification alongside an executable step function, with soundness and completeness theorems proving their equivalence.

<div class="fstarcode">

module BrrrMachine.Theorems.VerifiedInterpreter

(\* ————————————————– SEPARATION OF SPECIFICATION AND IMPLEMENTATION Relational spec: declarative, easy to reason about Executable impl: can actually run, may be complex Prove they’re equivalent -\> verified executable. ————————————————– \*)

(\* Relational specification (declarative) \*) val reduce : cpg -\> state -\> state -\> bool (\* "reduce cpg s1 s2" means s1 reduces to s2 in one step \*)

(\* Executable implementation (can run) \*) val step : cpg -\> state -\> option state (\* "step cpg s" computes next state if possible \*)

(\* SOUNDNESS: step produces valid reductions \*) val step_sound : cpg:cpg -\> s:state -\> s’:state -\> step cpg s = Some s’ -\> Lemma (reduce cpg s s’)

(\* COMPLETENESS: all reductions can be computed \*) val step_complete : cpg:cpg -\> s:state -\> s’:state -\> reduce cpg s s’ -\> Lemma (step cpg s = Some s’)

(\* Combined: step is a decision procedure for reduce \*) val step_decides_reduce : cpg:cpg -\> s:state -\> s’:state -\> Lemma (step cpg s = Some s’ \<==\> reduce cpg s s’)

</div>

The control result type below handles WebAssembly-style control flow without complex evaluation context encoding. This simplifies the interpreter while maintaining semantic precision.

<div class="fstarcode">

(\* ————————————————– CONTROL RESULT TYPE WebAssembly-style control flow handling. Avoids complex evaluation context encoding. ————————————————– \*) type control_result = \| Normal : state:state -\> control_result \| Break : depth:nat -\> values:list value -\> control_result \| Return : values:list value -\> control_result \| Trap : reason:string -\> control_result

(\* Evaluation with control flow \*) val eval_with_control : cpg -\> state -\> control_result let rec eval_with_control cpg state = match step cpg state with \| None -\> Normal state (\* Stuck = terminated \*) \| Some state’ -\> match check_control state’ with \| Some (Break d vs) -\> Break d vs \| Some (Return vs) -\> Return vs \| Some (Trap r) -\> Trap r \| None -\> eval_with_control cpg state’

</div>

<div class="theorem">

**Theorem 18.1** (Verified Type Checker — **\[Watt18\]**, Section 4). *The same pattern applies to type checking: a relational typing judgment paired with an executable type checker, with proofs of soundness and completeness.*

<div class="fstarcode">

*(\* Relational typing judgment \*) val has_type : context -\> expr -\> ir_type -\> bool*

*(\* Executable type checker \*) val check_type : context -\> expr -\> option ir_type*

*(\* Type checker soundness \*) val type_check_sound : ctx:context -\> e:expr -\> tau:ir_type -\> check_type ctx e = Some tau -\> Lemma (has_type ctx e tau)*

*(\* Type checker completeness \*) val type_check_complete : ctx:context -\> e:expr -\> tau:ir_type -\> has_type ctx e tau -\> Lemma (check_type ctx e = Some tau)*

</div>

</div>

<div class="pillarbox">

By abstracting over arithmetic implementation details (e.g., IEEE 754 NaN behavior), proofs work for ALL conforming implementations. This is essential for cross-platform analysis correctness.

</div>

<div class="fstarcode">

(\* Abstract arithmetic interface \*) class arithmetic_impl (a : Type) = add : a -\> a -\> a; mul : a -\> a -\> a; div : a -\> a -\> option a; (\* Partial for division by zero \*) (\* Axioms implementations must satisfy \*) add_comm : squash (forall x y. add x y == add y x); add_assoc : squash (forall x y z. add (add x y) z == add x (add y z)); (\* Non-determinism specification for IEEE 754 \*) nan_behavior : a -\> bool;

(\* Analysis sound for ALL conforming implementations \*) val analysis_sound_for_all_arith : \#a:Type -\> \| arithmetic_impl a \| -\> cpg:cpg -\> result:analysis_result -\> result == run_analysis cpg -\> Lemma (forall (impl : arithmetic_impl a). sound_wrt_impl cpg result impl)

</div>

## System Dependence Graph Theorems

<div class="pillarbox">

“Interprocedural Slicing Using Dependence Graphs”

Two-phase slicing prevents spurious interprocedural paths. Weiser’s transitive closure is WRONG (context-insensitive).

</div>

<div class="theorem">

**Theorem 19.1** (Two-Phase Context Sensitivity). *The two-phase algorithm produces context-sensitive slices: only nodes on REALIZABLE paths are included.*

<div class="fstarcode">

*val interprocedural_slice : sdg -\> criterion:node_id -\> set node_id let interprocedural_slice sdg crit = let phase1_exclude = \[DefOrder; ParamOut\] in let phase1_marked = reach_backwards sdg crit phase1_exclude in let phase2_exclude = \[DefOrder; Call; ParamIn\] in let phase2_marked = reach_backwards sdg phase1_marked phase2_exclude in Set.union phase1_marked phase2_marked*

*val two_phase_context_sensitive : sdg:sdg -\> crit:node_id -\> slice:set node_idslice = interprocedural_slice sdg crit -\> Lemma (forall n. Set.mem n slice ==\> exists path. is_realizable_path sdg path / head path = n / last path = crit)*

</div>

</div>

## Probabilistic Analysis Theorems

<div class="pillarbox">

“Probabilistic Abstract Interpretation”

Probabilistic programs are measurable functions from scenario space to deterministic semantics. Classical abstract interpretation is the special case where the scenario space is a singleton. This insight enables:

- Branch probability estimation without profiling

- Confidence scoring for bug reports

- Graceful degradation from full probabilistic to non-deterministic analysis

</div>

The following F\* code formalizes scenario spaces as probability spaces over which deterministic semantics are indexed. This generalizes classical AI where $`\Omega = \{*\}`$ (singleton).

<div class="fstarcode">

module BrrrMachine.Theorems.Probabilistic

(\* ————————————————– SCENARIO SPACE ABSTRACTION S_p\[\[P\]\] : Omega -\> D Where: - Omega = probability space (scenarios) - D = deterministic semantics - mu = probability measure on Omega ————————————————– \*) type scenario_space (omega : Type) = space : omega; measure : omega -\> real; (\* Measure axioms \*) measure_positive : squash (forall s. measure s \>= 0.0); measure_total : squash (integral measure = 1.0);

type prob_semantics (omega : Type) (d : Type) = scenarios : scenario_space omega; sem : omega -\> d; (\* Interpretation per scenario \*)

(\* ————————————————– CLASSICAL AI AS SPECIAL CASE When Omega = \* singleton, probabilistic AI collapses to classical AI. ————————————————– \*) type singleton_scenario = unit

val classical_is_singleton : \#a:Type -\> dom:abstract_domain a -\> prob:prob_semantics singleton_scenario a -\> prob.scenarios.space == () -\> Lemma (prob_domain_of prob = dom)

</div>

Law abstraction projects to probability distributions, enabling precision-vs-cost tradeoffs. The law ordering reflects information content: more precise analyses assign higher probability to precise properties.

<div class="fstarcode">

(\* ————————————————– LAW ABSTRACTION Abstract to probability distributions (laws). Law ordering reflects precision. ————————————————– \*) type law (a : Type) = (a -\> bool) -\> real (\* Pr(property holds) \*)

val law_leq : \#a:Type -\> law a -\> law a -\> bool let law_leq l1 l2 = (\* l1 more precise than l2 if assigns higher prob to precise properties \*) forall (q : a -\> bool). l1 (downset q) \>= l2 (downset q)

(\* Law abstraction preserves soundness \*) val law_abstraction_sound : \#a:Type -\> \| d : abstract_domain a \| -\> concrete_law : law a -\> abstract_law : law a -\> law_leq concrete_law abstract_law -\> Lemma (forall prop. concrete_law prop \>= abstract_law prop)

(\* ————————————————– BRANCH PROBABILITY ESTIMATION Compute branch probability from abstract domain constraints. No profiling needed! ————————————————– \*) val estimate_branch_prob : cpg:cpg -\> cond:node_id -\> input:abstract_state -\> real (\* Probability condition is true \*) let estimate_branch_prob cpg cond input = let cond_expr = get_condition cpg cond in let true_states = restrict input cond_expr in let false_states = restrict input (negate cond_expr) in (\* Estimate based on abstract domain volume \*) let true_vol = volume true_states in let false_vol = volume false_states in true_vol /. (true_vol +. false_vol)

</div>

<div class="theorem">

**Theorem 20.1** (Graceful Degradation). *Analysis precision can be traded for performance by merging scenarios. Soundness is preserved: degraded analysis may have less information but no false negatives for detected bugs.*

<div class="fstarcode">

*type precision_level = \| FullProbabilistic (\* All scenarios distinct \*) \| PartialProbabilistic (\* Some scenarios merged \*) \| NonDeterministic (\* Omega = singleton, no probability \*)*

*val analyze_with_precision : cpg:cpg -\> level:precision_level -\> budget:duration -\> analysis_result*

*(\* Degradation preserves soundness \*) val degradation_sound : cpg:cpg -\> full:analysis_result -\> degraded:analysis_result -\> full == analyze_with_precision cpg FullProbabilistic infinity -\> degraded == analyze_with_precision cpg NonDeterministic infinity -\> (\* Degraded may have less info but no false negatives \*) Lemma (forall bug. bug ‘mem‘ full.bugs ==\> bug ‘mem‘ degraded.bugs)*

</div>

</div>

## Unified Constraint Domain Theorems

<div class="pillarbox">

“Dependent Types in Practical Programming”

All abstract domains (interval, taint, nullability, resource, ownership) share a common structure: they are instantiations of Dependent ML with different constraint systems $`C`$. This insight unifies:

- **Implementation**: One parametric domain module, many instantiations

- **Composition**: Cross-domain reasoning via constraint conjunction

- **Decidability**: Constraint satisfiability gives analysis termination

- **Theory**: Single soundness proof covers all domains

</div>

The DML(C) pattern indexes types by constraint domain values. The following signature captures all our abstract domains uniformly.

<div class="fstarcode">

module BrrrMachine.Theorems.ConstraintDomains

(\* ————————————————– UNIFIED CONSTRAINT DOMAIN SIGNATURE Every abstract domain is an instance of this interface. ————————————————– \*) class constraint_domain (c : Type) = (\* Index sorts and terms \*) index_sort : Type; index_term : Type; (\* Constraint language \*) constraint\_ : Type; (\* Core operations \*) satisfiable : constraint\_ -\> bool; implies : constraint\_ -\> constraint\_ -\> bool; (\* Abstract interpretation operations \*) meet : constraint\_ -\> constraint\_ -\> constraint\_; join : constraint\_ -\> constraint\_ -\> constraint\_; widen : constraint\_ -\> constraint\_ -\> constraint\_; (\* Axioms \*) implies_refl : squash (forall c. implies c c); implies_trans : squash (forall a b c. implies a b && implies b c ==\> implies a c); meet_glb : squash (forall a b. implies (meet a b) a && implies (meet a b) b); join_lub : squash (forall a b. implies a (join a b) && implies b (join a b));

</div>

Each of our analysis domains becomes an instance of this unified interface:

<div class="fstarcode">

(\* Interval domain: C = linear integer arithmetic \*) instance interval_constraint_domain : constraint_domain interval = index_sort = IntSort; index_term = LinExpr; (\* a\*x + b\*y + ... + c \*) constraint\_ = LinConstraint; satisfiable = fourier_motzkin_sat; implies = fun c1 c2 -\> not (satisfiable (And \[c1; Not c2\])); meet = fun c1 c2 -\> And \[c1; c2\]; join = fun c1 c2 -\> convex_hull c1 c2; widen = interval_widen;

(\* Nullability domain: C = boolean constraints \*) instance null_constraint_domain : constraint_domain nullability = index_sort = BoolSort; index_term = BoolExpr; constraint\_ = BoolConstraint; satisfiable = sat_solve; implies = fun c1 c2 -\> not (satisfiable (And \[c1; Not c2\])); meet = fun c1 c2 -\> And \[c1; c2\]; join = fun c1 c2 -\> Or \[c1; c2\]; widen = id; (\* Boolean domain is finite \*)

(\* Taint domain: C = security lattice constraints \*) instance taint_constraint_domain : constraint_domain taint = index_sort = SecurityLevel; index_term = LevelExpr; constraint\_ = FlowConstraint; (\* l1 \<= l2 \*) satisfiable = lattice_sat; implies = fun c1 c2 -\> lattice_implies c1 c2; meet = fun c1 c2 -\> And \[c1; c2\]; join = fun c1 c2 -\> lub_constraint c1 c2; widen = id; (\* Security lattice is finite \*)

</div>

<div class="theorem">

**Theorem 21.1** (Array Bounds Verification). *Using the unified constraint framework, array bounds can be statically verified via constraint solving, eliminating runtime checks when provable.*

<div class="fstarcode">

*type bounds_check_result = \| StaticallyVerified : proof:constraint_proof -\> bounds_check_result \| NeedsRuntimeCheck : bounds_check_result \| AlwaysOutOfBounds : witness:index_term -\> bounds_check_result*

*val check_array_bounds : ctx:index_constraint -\> (\* What we know \*) arr:refined_array -\> idx:index_term -\> bounds_check_result let check_array_bounds ctx arr idx = let bounds_ok = ICAnd (ICPred "\>=" \[idx; IConst 0\]) (ICPred "\<" \[idx; IVar arr.length_var\]) in if implies ctx bounds_ok then StaticallyVerified (construct_proof ctx bounds_ok) else if satisfiable (ICAnd ctx (ICNot bounds_ok)) then NeedsRuntimeCheck else AlwaysOutOfBounds (find_counterexample ctx bounds_ok)*

*(\* Bounds check elimination theorem \*) val bounds_check_elimination : ctx:index_constraint -\> arr:refined_array -\> idx:index_term -\> check_array_bounds ctx arr idx = StaticallyVerified \_ -\> Lemma (safe_to_access arr idx)*

</div>

</div>

<div class="theorem">

**Theorem 21.2** (Conservative Extension — **\[XiPfenning99\]**). *DML(C) is conservative over ML: existing code works unchanged. Index erasure preserves typing.*

<div class="fstarcode">

*(\* Index erasure \*) val erase_indices : ir_refined_type -\> ir_type*

*(\* Conservative extension: typing preserved under erasure \*) val conservative_extension : ctx:context -\> e:expr -\> rt:ir_refined_type -\> has_refined_type ctx e rt -\> Lemma (has_type (erase_ctx ctx) (erase_expr e) (erase_indices rt))*

</div>

</div>

## Bi-Abduction Theorems

<div class="pillarbox">

“Compositional Shape Analysis by means of Bi-Abduction”

Given current state $`p`$ and target $`q`$, bi-abduction discovers BOTH the missing resource $`M`$ (anti-frame) AND the leftover resource $`F`$ (frame).

Judgment: $`p * ?M \vdash q * ?F`$

</div>

<div class="theorem">

**Theorem 22.1** (Compositionality — **\[Distefano19\]**). *Analysis can be done function-by-function. Summaries capture precondition/postcondition contracts.*

<div class="fstarcode">

*val compositional_soundness : cpg:cpg -\> func:func_id -\> table:map func_id procedure_summary -\> summary:procedure_summary -\> analyze_procedure_compositionally cpg func table = Some summary ==\> (\* For any calling context satisfying the precondition... \*) (forall ctx. ctx ‘entails‘ summary.precondition ==\> (\* ...execution produces state satisfying postcondition \*) exists result. executes_to cpg func ctx result / result ‘entails‘ summary.postcondition)*

</div>

</div>

## Divergence (Non-Termination) Theorems

<div class="pillarbox">

“Non-Termination Proving: 100 Million LoC and Beyond”

To PROVE divergence, use UNDER-approximation. Two main detection strategies:

1.  Repeating abstract states (loops)

2.  Same-state recursive calls (recursion)

</div>

<div class="theorem">

**Theorem 23.1** (Repeating State Soundness). *A loop diverges if execution returns to the same abstract state. This is SOUND for proving divergence (under-approximation).*

<div class="fstarcode">

*val repeating_state_sound : cpg:cpg -\> w:divergence_witnessInfiniteLoop? w -\> Lemma (exists input. satisfies input w.path_condition / diverges (execute cpg input))*

</div>

</div>

## Widening and Narrowing Theorems

<div class="pillarbox">

“Comparing the Galois Connection and Widening/Narrowing Approaches”

Widening/narrowing is STRICTLY MORE POWERFUL than finite Galois connections. No finite domain can achieve equivalent results for all programs.

**Critical insight**: Discovered invariants may NOT appear in program text! Example: McCarthy’s F91 $`\to`$ analysis discovers $`[91, \mathsf{maxint}-10]`$.

</div>

<div class="theorem">

**Theorem 24.1** (Widening Termination). *For any ascending chain with widening, the widened chain stabilizes.*

<div class="fstarcode">

*val widening_terminates : \#l:lattice -\> widen:widening \#l -\> transfer:(l.carrier -\> l.carrier) -\> Lemma (exists n. let rec iter k x = if k = 0 then x else iter (k-1) (widen x (transfer x)) in let result = iter n l.bottom in l.order (transfer result) result) (\* Post-fixpoint \*)*

</div>

</div>

<div class="theorem">

**Theorem 24.2** (Narrowing Improves Precision).

<div class="fstarcode">

*val narrowing_improves : \#l:lattice -\> narrow:narrowing \#l -\> transfer:(l.carrier -\> l.carrier) -\> post_widen:l.carrier -\> l.order (transfer post_widen) post_widen -\> (\* post_widen is a post-fixpoint \*) Lemma (let refined = narrow post_widen (transfer post_widen) in l.order (transfer refined) refined / (\* Still post-fixpoint \*) l.order refined post_widen) (\* But smaller \*)*

</div>

</div>

## Symbolic Execution Theorems

<div class="pillarbox">

“Symbolic Execution and Program Testing”

Key concepts:

1.  Path condition (pc): Boolean expression over symbolic inputs

2.  Symbolic values: Expressions over input symbols

3.  Forking: When condition cannot be resolved, explore both branches

4.  Commutativity: $`\mathsf{instantiate}(\mathsf{exec\_symbolic}(P)) = \mathsf{exec\_concrete}(P)`$

</div>

<div class="theorem">

**Theorem 25.1** (Commutativity — **\[King76\]**, Section 5). *The fundamental correctness criterion for symbolic execution.*

<div class="fstarcode">

*val symbolic_execution_sound : cpg:cpg -\> func:ir_func -\> tree:exec_tree -\> v:valuation -\> (\* For any valuation satisfying some leaf’s path condition... \*) (exists leaf. leaf ‘mem‘ tree.leaves / pc_satisfied (get_node tree leaf).state.pc v) ==\> (\* ...the symbolic result instantiated equals concrete result \*) Lemma ( let leaf = find_satisfied_leaf tree v in let sym_env = (get_node tree leaf).state.env in let concrete_result = execute_concrete cpg func v in Map.for_all (fun var sym_val -\> instantiate sym_val v = Map.find var concrete_result ) sym_env)*

</div>

</div>

## Frame Rule and Local Reasoning

<div class="pillarbox">

“Separation Logic: A Logic for Shared Mutable Data Structures”

The Frame Rule enables COMPOSITIONAL analysis by allowing us to reason about a command using only its “footprint” (accessed memory).

</div>

<div class="theorem">

**Theorem 26.1** (Frame Rule — **\[Reynolds02\]**, Theorem 1). *
``` math
\frac{\{p\}\, c\, \{q\}}{\{p * r\}\, c\, \{q * r\}}
```
*

***Side condition**: $`r`$ does not mention modified variables or freed locations.*

<div class="fstarcode">

*val frame_rule : p:sl_assertion -\> c:ir_stmt -\> q:sl_assertion -\> r:sl_assertion -\> hoare_valid p c q -\> disjoint (footprint c).writes (free_vars r) -\> disjoint (footprint c).frees (locs_in r) -\> Lemma (hoare_valid (SLSep p r) c (SLSep q r))*

</div>

</div>

<div class="theorem">

**Theorem 26.2** (Parallel Rule — **\[Reynolds02\]**, Section 11.3). *
``` math
\frac{\{p_1\}\, c_1\, \{q_1\} \quad \{p_2\}\, c_2\, \{q_2\}}{\{p_1 * p_2\}\, c_1 \parallel c_2\, \{q_1 * q_2\}}
```
*

***Side condition**: Disjoint footprints.*

***Corollary**: Disjoint parallel threads do not race.*

</div>

## Effect Handler Correctness Theorems

<div class="pillarbox">

“Handlers of Algebraic Effects”

Key insight: Handlers are HOMOMORPHISMS from free algebraic models. A handler is CORRECT if it forms a valid model of the effect theory.

**CRITICAL LIMITATIONS**:

- Continuations (call/cc) are NOT algebraic effects

- Parallel composition CANNOT be expressed with unary handlers

</div>

<div class="fstarcode">

module BrrrMachine.EffectHandlers

type operation = name : string; param_type : Type; arity : nat; result_types : list Type;

type effect_signature = operations : list operation; equations : list effect_equation;

type handler (e:effect_signature) (m:Type) = handle_return : value -\> m; handle_op : op:operationop ‘mem‘ e.operations -\> op.param_type -\> (op.result_types -\> m) -\> m;

</div>

<div class="theorem">

**Theorem 27.1** (Handler Correctness — **\[PlotkinPretnar09\]**). *A handler is CORRECT if it preserves the effect theory’s equations.*

<div class="fstarcode">

*val handler_correct : \#e:effect_signature -\> \#m:Type -\> h:handler e m -\> (\* Handler must preserve all equations \*) (forall eq. eq ‘mem‘ e.equations ==\> interpret_with_handler h eq.lhs == interpret_with_handler h eq.rhs)*

</div>

</div>

## Probabilistic Semantics Theorems

<div class="pillarbox">

“Semantics of Probabilistic Programs”

Key insight: Programs are LINEAR OPERATORS on MEASURES. Dual semantics:

1.  Operational: $`f_S : X^{n+\omega} \to X^{n+\omega}`$ (input with random tape $`\to`$ output)

2.  Denotational: $`S : B \to B`$ (input distribution $`\to`$ output distribution)

</div>

<div class="theorem">

**Theorem 28.1** (Discrete Sufficiency — **\[Kozen81\]**, Theorem 6.1). *If two programs agree on all point masses, they agree on all distributions.*

<div class="fstarcode">

*val discrete_sufficiency : s1:(state -\> linear_operator state) -\> s2:(state -\> linear_operator state) -\> (\* If they agree on all point masses... \*) (forall x. s1 (point_mass x) == s2 (point_mass x)) ==\> (\* ...they agree on all distributions \*) Lemma (forall mu. s1 mu == s2 mu)*

</div>

***Application**: Testing on discrete inputs is SUFFICIENT for correctness validation.*

</div>

## Set Constraint Resolution Theorems

<div class="pillarbox">

“Introduction to Set Constraint-Based Program Analysis”

Key insight: Dataflow, type inference, and closure analysis are ALL instances of set constraint solving. Constraint form:
``` math
E_1 \subseteq E_2 \quad\text{where}\quad E ::= \alpha \mid \emptyset \mid E_1 \cup E_2 \mid E_1 \cap E_2 \mid \neg E \mid c(E_1,\ldots,E_n) \mid c^{-i}(E) \mid Y \Rightarrow X
```

</div>

<div class="theorem">

**Theorem 29.1** (Inductive Form Existence — **\[Aiken99\]**, Section 5). *Any satisfiable constraint system can be transformed to inductive form with finite representation.*

<div class="fstarcode">

*val inductive_form_theorem : cs:constraint_system -\> Lemma (satisfiable cs \<==\> exists (sys:inductive_system). forall sol. satisfies_all sol cs \<==\> exists beta. sol = extract_solution sys beta)*

</div>

</div>

<div class="theorem">

**Theorem 29.2** (Function Subtyping (Contravariance)). *
``` math
(\tau_1 \to \tau_2) \subseteq (\sigma_1 \to \sigma_2) \iff \sigma_1 \subseteq \tau_1 \land \tau_2 \subseteq \sigma_2
```
*

*Note: Domain is CONTRAVARIANT, codomain is covariant.*

</div>

## Datalog Compilation and Analysis Composition

<div class="pillarbox">

“Souffle: On Synthesis of Program Analyzers” and “From Datalog to Flix”

**Key Insight**: COMPILE Datalog rules to specialized code, don’t interpret them. Compilation achieves 50x+ speedup over interpretation (bddbddb, muZ).

**Benchmark** (OpenJDK7: 1.4M variables, 350K objects, 160K methods):  
Souffle (compiled): 35 seconds \| bddbddb: 30 minutes \| SQLite: 6h20m \| muZ: DNF

</div>

The Souffle compilation pipeline uses staged specialization via Futamura projections. Each stage eliminates interpretation overhead while preserving semantics.

<div class="fstarcode">

module BrrrMachine.DatalogCompile

(\* ————————————————– RELATIONAL ALGEBRA MACHINE (RAM) OPERATIONS Source: Jordan, Scholz, Subotic 2016 RAM is the intermediate representation between Datalog rules and generated code. Enables optimization before code generation. ————————————————– \*) type ram_expr = \| RAMScan : relation:string -\> ram_expr \| RAMFilter : expr:ram_expr -\> column:nat -\> value:string -\> ram_expr \| RAMProject : expr:ram_expr -\> columns:list nat -\> ram_expr \| RAMJoin : left:ram_expr -\> right:ram_expr -\> left_col:nat -\> right_col:nat -\> ram_expr \| RAMUnion : left:ram_expr -\> right:ram_expr -\> ram_expr \| RAMDiff : left:ram_expr -\> right:ram_expr -\> ram_expr

type ram_stmt = \| RAMInsert : target:string -\> source:ram_expr -\> ram_stmt \| RAMLoop : delta:string -\> body:list ram_stmt -\> ram_stmt (\* Semi-naive \*) \| RAMSeq : stmts:list ram_stmt -\> ram_stmt

</div>

<div class="theorem">

**Theorem 30.1** (Optimal Index Selection via Dilworth’s Theorem — **\[Jordan16\]**). *Given $`N`$ index requirements from queries, find MINIMAL set of indices such that each requirement is subsumed by at least one selected index.*

***Insight**: Index subsumption forms a partial order. A lexicographic index $`[a, b, c]`$ subsumes $`[a, b]`$ and $`[a]`$. Dilworth’s theorem: the width of a partial order equals the minimum number of chains needed to cover all elements.*

<div class="fstarcode">

*(\* Index specification \*) type index_spec = relation : string; key_columns : list nat; (\* Lexicographic order \*) *

*(\* Index subsumption: \[a,b,c\] subsumes \[a,b\] and \[a\] \*) let subsumes (super sub : index_spec) : bool = super.relation = sub.relation && List.length sub.key_columns \<= List.length super.key_columns && List.for_all2 (=) sub.key_columns (List.take (List.length sub.key_columns) super.key_columns)*

*(\* Dilworth-based minimal index selection \*) val compute_minimal_indices : required:list index_spec -\> minimal:list index_spec (\* Every required index is subsumed by some minimal index \*) forall r. List.mem r required ==\> exists m. List.mem m minimal / subsumes m r *

</div>

</div>

<div class="pillarbox">

Standard Datalog operates on relations (finite sets). Flix extends Datalog with lattice predicates, enabling elegant expression of IFDS, IDE, and value analyses.

**Key Difference**:

- Standard Datalog: Multiple tuples with same key = set (keep all)

- Flix Extension: Same key = JOIN lattice values (collapse to single)

This enables constant propagation, interval analysis, and IDE (micro-function lattices).

</div>

The following F\* code shows how IDE is expressed as Flix with a specific lattice: the micro-function space of environment transformers.

<div class="fstarcode">

module BrrrMachine.IDE

(\* ————————————————– IDE IS FLIX WITH MICRO-FUNCTION LATTICE Source: Madsen, Yee, Lhotak 2016

IFDS path edge: PathEdge(d1, n, d2) – fact d2 holds at n IDE path edge: PathEdge(d1, n, d2, f) – with environment xformer J micro-function lattice element

IDE key insight: transformers COMPOSE along paths PathEdge(d1, m, d3, compose(f, g)) :- PathEdge(d1, n, d2, f), CFG(n, m), EdgeFunction(n, d2, m, d3, g). ————————————————– \*)

(\* Micro-function: environment transformer \*) type micro_fn (v : Type) = v -\> v

(\* Micro-functions form a lattice under pointwise ordering \*) instance micro_fn_lattice (v : Type) \| complete_lattice v \| : complete_lattice (micro_fn v) = bot = (fun \_ -\> bot); top = (fun \_ -\> top); join = (fun f g -\> fun x -\> join (f x) (g x)); meet = (fun f g -\> fun x -\> meet (f x) (g x)); leq = (fun f g -\> forall x. leq (f x) (g x));

(\* Micro-function composition for IDE path edges \*) val compose_micro : \#v:Type -\> \| complete_lattice v \| -\> micro_fn v -\> micro_fn v -\> micro_fn v let compose_micro f g = fun x -\> f (g x)

(\* Distributivity requirement for IFDS/IDE soundness \*) val distributive_transfer : \#d:Type -\> \| complete_lattice d \| -\> f:micro_fn d -\> Lemma (requires forall x y. f (join x y) == join (f x) (f y)) (ensures distributive f)

</div>

<div class="theorem">

**Theorem 30.2** (Semantic Preservation — **\[Jordan16\]**). *The staged translation preserves Datalog semantics:
``` math
\mathsf{result} = \llbracket \mathsf{Int} \rrbracket(\mathsf{Source}, \mathsf{Input}) = \llbracket \mathsf{Compiled} \rrbracket(\mathsf{Input})
```
*

<div class="fstarcode">

*(\* Compilation correctness theorem \*) val compilation_correct : prog:datalog_program -\> Lemma (eval_compiled (compile_to_ram prog) == datalog_semantics prog)*

*(\* IFDS can be expressed as Datalog for compilation \*) val ifds_to_datalog : \#d:Type -\> ifds_problem d -\> datalog_program*

*(\* Compiled IFDS computes same result as tabulation algorithm \*) val compiled_ifds_correct : \#d:Type -\> prob:ifds_problem d -\> Lemma (eval_compiled (compile_ifds prob) == ifds_tabulation prob)*

</div>

</div>

## Local-to-Global Consistency Theorems

<div class="pillarbox">

“Abstract Interpretation: A Unified Lattice Model”

Key insight: If abstract interpretation is locally consistent with concrete semantics, then global analysis results are sound. This enables MODULAR soundness proofs.

</div>

<div class="theorem">

**Theorem 31.1** (Fundamental Theorem of Abstract Interpretation — **\[Cousot77\]**, Theorem 4.1). *Local consistency implies global consistency.*

<div class="fstarcode">

*val local_global_consistency : \#c:Type -\> \#a:Type -\> gc:galois_connection c a -\> f_c:(c -\> c) -\> f_a:(a -\> a) -\> locally_consistent gc f_c f_a -\> monotone f_c -\> monotone f_a -\> Lemma (forall n. gc.alpha (iterate f_c n gc.concrete_lattice.bot) ‘leq_a‘ iterate f_a n gc.abstract_lattice.bot)*

*(\* Corollary: Fixpoints are consistent \*) val fixpoint_consistency : \#c \#a gc f_c f_a -\> locally_consistent gc f_c f_a -\> monotone f_c -\> monotone f_a -\> Lemma (gc.alpha (lfp f_c) ‘leq_a‘ lfp f_a)*

</div>

</div>

## Boundary Guard Soundness Theorems

<div class="pillarbox">

“Operational Semantics for Multi-Language Programs”

Boundaries between languages require GUARDS to ensure type safety. Guards have POLARITY that flips at function types.

</div>

<div class="definition">

**Definition 32.1** (Guard Polarity).

- **Positive**: Value entering STRICTER language (dynamic $`\to`$ static). MUST check.

- **Negative**: Value from STRICTER language (static $`\to`$ dynamic). Can skip.

</div>

<div class="theorem">

**Theorem 32.2** (Guard Soundness — **\[MatthewsFindler07\]**, Theorem 1). *Guards ensure type soundness at boundaries.*

<div class="fstarcode">

*val guard_soundness : source:language_config -\> target:language_config -\> e:expr -\> ty:ir_type -\> pol:guard_polaritypol = polarity source target -\> guard:guardguard = generate_guard ty pol -\> well_typed source e ty -\> Lemma (reduces_safely target (apply_guard guard (eval source e)))*

</div>

</div>

<div class="theorem">

**Theorem 32.3** (Lump Cancellation). *Boundary pairs cancel: $`\hat{\tau}\, \mathsf{MS}(\mathsf{SM}\, \hat{\tau}\, v) \to v`$*

<div class="fstarcode">

*val cancellation_sound : e:boundary_expr -\> Lemma (eval_boundary e = eval_boundary (cancel_boundaries e))*

</div>

</div>

## Vault Adoption and Focus Theorems

<div class="pillarbox">

“Vault, Fugue: Typestate for Object-Oriented Languages”

Adoption permanently transfers unique ownership to shared-frozen state. Focus temporarily upgrades shared permission to unique within a scope.

</div>

<div class="theorem">

**Theorem 33.1** (Adoption Preserves Typestate). *Adoption is irreversible and freezes typestate.*

<div class="fstarcode">

*val adoption_preserves_typestate : perm:access_permission_v2APUnique_v2? perm -\> state_at_adoption:state_node -\> Lemma (let (APUnique_v2 r n g) = perm in let adopted = APPure_v2 r n g (Some n) in n = state_at_adoption ==\> forever_frozen adopted n)*

*val adoption_irreversible : original:access_permission_v2APUnique_v2? original -\> Lemma (let adopted = perform_adoption original in APPure_v2? adopted / not (exists op. apply_transition adopted op = Some (APUnique_v2? \_)))*

</div>

</div>

<div class="theorem">

**Theorem 33.2** (Focus Scope Soundness). *Focus temporarily upgrades shared to unique, with proper scope management.*

<div class="fstarcode">

*val focus_scope_sound : original:access_permission_v2APPure_v2? original -\> lock_acquired:bool -\> body:(access_permission_v2 -\> access_permission_v2) -\> (FocusSuccess? (enter_focus original lock_acquired)) ==\> Lemma (let FocusSuccess token unique = enter_focus original lock_acquired in let result = body unique in let restored = exit_focus token result in APPure_v2? restored / get_state restored = get_state result)*

</div>

</div>

## Stack Filtering Theorems (Rupta)

<div class="pillarbox">

“Rupta: Efficient and Precise Pointer Analysis for Rust”

**NOTE**: This is a NOVEL technique not found in prior literature.

Stack objects allocated in function $`f`$ are ONLY alive when $`f`$’s frame is on the call stack. Filtering out dead stack objects dramatically improves both PRECISION and SPEED.

</div>

<div class="theorem">

**Theorem 34.1** (Stack Filtering Soundness). *If we filter out a stack location from a points-to set, then that location cannot be pointed to in any concrete execution at that program point.*

<div class="fstarcode">

*val stack_filtering_sound : cg:call_graph -\> pts:pts_solution -\> var:var_id -\> ctx:context -\> filtered:set abstract_locfiltered = filter_dead_stack cg pts var ctx -\> Lemma (forall loc. loc ‘in‘ filtered ==\> loc ‘in‘ concrete_pts var ctx)*

</div>

</div>

<div id="tab:part12-crossref">

| **Theorem** | **Source** | **Related Sections** |
|:---|:---|:---|
| Abstract Interpretation Soundness | **\[Cousot77\]** | Section 2.1 |
| IFDS Soundness/Completeness | **\[Reps95\]** | Section 4.1 |
| True Positives Property | **\[Le22\]** | Section 12.3 |
| Occurrence Typing Soundness | **\[TobinHochstadt08\]** | Section 2.1.7b |
| DRF-SC Theorem | **\[Batty11\]** | Section 6.5 |
| Effect Absence Theorems | **\[Leijen14\]** | Section 6.1.3 |
| Session Type Safety | **\[Honda08\]** | Section 7.3 |
| SVF Soundness | **\[Sui12\]** | Section 5.6 |
| ZIPPER Precision | **\[Li20\]** | Section 5.3.2 |
| CSE Soundness | **\[Trabish18\]** | Section 4.4.6 |
| FTG Soundness | **\[Huang23\]** | Section 5.3.3 |
| DSA Soundness | **\[Lattner07\]** | Section 5.2.5 |
| Robustness Theorem | **\[ChongMyers04\]** | Section 8.1.4.3 |
| SNI Soundness | **\[Guarnieri20\]** | Section 8.1.4.6 |
| CT-Verif Reduction | **\[Almeida16\]** | Section 8.1.4.7 |
| Capability Algebra | **\[CraryWalkerMorrisett99\]** | Section 7.1 |
| Realizability | **\[Patterson22\]** | Section 9.1 |
| SDG Two-Phase | **\[HorwitzRepsBinkley90\]** | Section 3.3 |
| Bi-Abduction | **\[Calcagno09\]** | Section 7.4 |
| Divergence Analysis | **\[Vanegue25\]** | Section 4.4 |
| Widening/Narrowing | **\[Cousot92\]** | Section 2.3 |
| Symbolic Execution | **\[King76\]** | Section 4.4 |
| Frame Rule | **\[Reynolds02\]** | Section 7.4 |
| Stack Filtering | **\[Li24\]** | Section 5.3.4 |

Part XII Theorem Cross-References

</div>

<div id="tab:part12-sections">

| **Section** | **Topic** | **Key Theorems** |
|:---|:---|:---|
| 12.1 | Module Structure | F\* module hierarchy |
| 12.2 | Soundness Theorems | AI, IFDS, Taint, Boundary, Ownership |
| 12.3 | Manifest/Latent | True Positives Property, Falsification |
| 12.4 | Under-Approximation | Reversed Consequence, Disjunction |
| 12.5 | Manifest Errors | Outcome Logic characterization |
| 12.6 | Capability Algebra | Free Requires Unique, Complete Collection |
| 12.7 | Realizability | Shared Memory Identity |
| 12.8 | Verified Interpreter | Specification-Implementation equivalence |
| 12.9 | SDG Theorems | Two-Phase slicing |
| 12.10 | Probabilistic | Scenario space, Law abstraction |
| 12.11 | Constraint Domains | DML(C) unification |
| 12.12 | Bi-Abduction | Compositional analysis |
| 12.13 | Divergence | Repeating states, Recursive divergence |
| 12.14 | Widening/Narrowing | Termination, Precision improvement |
| 12.15 | Symbolic Execution | Commutativity theorem |
| 12.16 | Effect Handlers | Algebraic effect correctness |
| 12.17 | Probabilistic Semantics | Linear operators on measures |
| 12.18 | Set Constraints | Resolution complexity |
| 12.19 | Local-Global Consistency | Fundamental AI theorem |
| 12.20 | Boundary Guards | Guard soundness |
| 12.21 | Frame Rule | Separation logic compositionality |
| 12.22 | Stack Filtering | Rupta precision theorem |
| 12.25 | Institution Theory | Satisfaction condition |
| 12.26 | C11 Memory Model | DRF-SC theorem |
| 12.27 | Effect Absence | Exception, Termination, State isolation |
| 12.28 | Session Types | Communication safety, Fidelity, Progress |
| 12.29 | SVF Theorems | Memory SSA, SVFG, Leak detection |
| 12.30 | ZIPPER | PFG captures flows, Precision preservation |
| 12.31 | Chopped SE | Soundness, Relative completeness |
| 12.32 | Python FTG | Type safety, Call graph soundness |
| 12.33 | DSA Theorems | Completeness, Soundness, Heap cloning |
| 12.34 | Robust Declassification | Robustness implies security, SNI, CT-Verif |

Part XII Section Summary

</div>

# Implementation Roadmap and Engineering Specification

## Executive Summary

This part provides a comprehensive implementation roadmap for the brrr-machine framework, translating the theoretical foundations established in Parts I–XII into concrete engineering deliverables. The implementation follows a phased approach with explicit dependencies, risk mitigations, and quality gates.

<div class="pillarbox">

Primary Languages  
Rust (implementation), F\* (verification)

Target Duration  
6 phases, modular and parallelizable

Key Constraint  
Interleaved CG/PTS for OOP languages (Section <a href="#sec:qilin-interleaving" data-reference-type="ref" data-reference="sec:qilin-interleaving">[sec:qilin-interleaving]</a>)

Verification Target  
Mechanized proofs for core soundness theorems

</div>

## Phase Architecture and Critical Dependencies

<div class="artifactbox">

**ARCHITECTURAL CONSTRAINT: INTERLEAVED CALL GRAPH + POINTER ANALYSIS**

For object-oriented languages with virtual dispatch (Java, Python, JS, C++), call graph construction and points-to analysis exhibit **MUTUAL DEPENDENCY**:

- Call graph resolution requires points-to information for receivers

- Points-to propagation requires call graph edges for interprocedural flow

**SOLUTION:** Qilin-style on-the-fly algorithm (Section <a href="#sec:qilin-interleaving" data-reference-type="ref" data-reference="sec:qilin-interleaving">[sec:qilin-interleaving]</a>) computes both simultaneously via worklist iteration until mutual fixpoint.

For procedural languages (C, Fortran), sequential phasing is acceptable.

</div>

### Implementation Phase Overview

The six phases of implementation proceed as follows, with dependencies indicated by vertical arrows:

<div class="center">

<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Phase 1: Foundation</strong></th>
<th style="text-align: left;"><strong>Phase 2: CPG + Pointer Analysis</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><ul>
<li><p>Abstract domain typeclass hierarchy</p></li>
<li><p>Complete lattice with verified laws</p></li>
<li><p>Galois connection interface</p></li>
<li><p>IR type specification</p></li>
<li><p>Tree-sitter parser integration</p></li>
<li><p>AST + CFG construction</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>On-the-fly CG + PTS (Qilin)</p></li>
<li><p>Virtual call resolution</p></li>
<li><p>PDG construction (post-CG)</p></li>
<li><p>Effect edge computation</p></li>
<li><p>Complete CPG with all edge types</p></li>
</ul></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Phase 3: Core Dataflow</strong></td>
<td style="text-align: left;"><strong>Phase 4: Precision &amp; Ownership</strong></td>
</tr>
<tr>
<td style="text-align: left;"><ul>
<li><p>IFDS tabulation algorithm</p></li>
<li><p>Reaching definitions analysis</p></li>
<li><p>Live variable analysis</p></li>
<li><p>Taint propagation (source<span class="math inline">→</span>sink)</p></li>
<li><p>Nullability analysis</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p><span class="math inline"><em>k</em></span>-CFA context sensitivity</p></li>
<li><p>Thin slicing (TAJ-style)</p></li>
<li><p>Ownership state machine</p></li>
<li><p>Resource lifecycle tracking</p></li>
<li><p>Bi-abduction for specs</p></li>
</ul></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Phase 5: Multi-Lang + Security + Concurrency</strong></td>
<td style="text-align: left;"><strong>Phase 6: Production Hardening</strong></td>
</tr>
<tr>
<td style="text-align: left;"><ul>
<li><p>Cross-language boundary analysis</p></li>
<li><p>Matthews-Findler boundary semantics</p></li>
<li><p>DLM information flow control</p></li>
<li><p>Implicit flow tracking (PC labels)</p></li>
<li><p>Data race detection (happens-before)</p></li>
<li><p>Linearizability verification</p></li>
<li><p>Outcome Logic bug classification</p></li>
<li><p>Concolic witness generation</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>Incremental re-analysis (DRedL)</p></li>
<li><p>Adaptive precision budgets</p></li>
<li><p>SARIF output generation</p></li>
<li><p>IDE/CI integration</p></li>
<li><p>Runtime debugger hooks</p></li>
<li><p>Performance optimization</p></li>
</ul></td>
</tr>
</tbody>
</table>

</div>

## Phase 1: Foundation

### Objective

Establish the core infrastructure upon which all subsequent analysis phases depend. This phase produces verified abstract domain implementations, the intermediate representation type system, and basic program graph construction.

### Deliverables and Acceptance Criteria

#### Milestone 1.1: Abstract Domain Infrastructure

<div class="artifactbox">

**Deliverables:**

- `PartialOrder` typeclass with reflexivity, antisymmetry, transitivity proofs

- `CompleteLattice` typeclass with join/meet/top/bottom verified laws

- `GaloisConnection` type with soundness lemma ($`\gamma \circ \alpha \sqsupseteq \mathrm{id}`$)

- `AbstractDomain` typeclass with widening operator

- Chaotic iteration algorithm with Bourdoncle WTO

**Acceptance Criteria:**

- All algebraic laws verified in F\* (no admits)

- Interval domain passes reference test suite

- Fixpoint computation terminates on all test cases

</div>

#### Milestone 1.2: Concrete Abstract Domains

<div class="artifactbox">

**Deliverables:**

- Interval domain with widening thresholds $`\{-\infty, -1, 0, 1, +\infty\}`$

- Taint lattice: $`\bot \sqsubset \mathsf{Untainted} \sqsubset \mathsf{Unknown} \sqsubset \mathsf{Tainted} \sqsubset \top`$

- Nullability lattice: $`\bot \sqsubset \{\mathsf{NonNull}, \mathsf{Null}\} \sqsubset \top`$

- Ownership state machine: $`\mathsf{Uninit} \rightarrow \mathsf{Owned} \rightarrow \{\mathsf{Moved}, \mathsf{Borrowed}\} \rightarrow \mathsf{Freed}`$

- Effect row with row polymorphism support

**Acceptance Criteria:**

- Each domain implements `AbstractDomain` typeclass

- Galois connection soundness verified per domain

- Transfer functions monotone (verified)

</div>

#### Milestone 1.3: Code Property Graph Infrastructure

<div class="artifactbox">

**Deliverables:**

- Node type hierarchy (Statement, Expression, Declaration, …)

- Edge type hierarchy (AST, CFG, PDG_Data, PDG_Control, Call, Effect)

- CPG data structure with $`O(1)`$ node lookup, $`O(\mathrm{degree})`$ edge traversal

- Traversal primitives: `successors`, `predecessors`, `reachable`, `filtered`

- Pattern matching DSL for node/edge queries

**Acceptance Criteria:**

- CPG construction from reference programs produces expected structure

- Traversal primitives pass property-based tests

- Memory usage $`< 10\times`$ source file size

</div>

#### Milestone 1.4: IR and Parser Integration

<div class="artifactbox">

**Deliverables:**

- Complete Brrr-IR type specification (Section <a href="#sec:brrr-ir" data-reference-type="ref" data-reference="sec:brrr-ir">[sec:brrr-ir]</a>)

- Tree-sitter grammar bindings (Python, Rust, Go as initial targets)

- AST $`\rightarrow`$ IR lowering transformations per language

- CFG construction with proper exception/return edge handling

- Basic CPG construction (AST nodes + CFG edges)

**Acceptance Criteria:**

- Round-trip: parse $`\rightarrow`$ IR $`\rightarrow`$ pretty-print preserves semantics

- CFG dominance frontier computation matches reference implementation

- All control flow constructs correctly modeled (loops, exceptions, etc.)

</div>

### Risk Analysis and Mitigations

| **Risk** | **Likelihood** | **Impact** | **Mitigation** |
|:---|:--:|:--:|:---|
| F\* proof complexity exceeds estimates | Medium | High | Start with key lemmas; defer non-critical proofs |
| Tree-sitter grammar edge cases | High | Medium | Comprehensive test suite; fallback to partial parse |
| IR design inadequate for later phases | Medium | High | Design review with Phase 3–5 requirements |
| Performance bottleneck in CPG construction | Low | Medium | Profile early; use arena allocation |

## Phase 2: CPG + Pointer Analysis

### Objective

Construct complete Code Property Graphs with resolved call edges via interleaved call graph and points-to analysis. This phase addresses the mutual dependency between virtual dispatch resolution and pointer analysis through on-the-fly computation.

### Deliverables and Acceptance Criteria

#### Milestone 2.1: On-the-fly Call Graph + Points-to (Qilin)

<div class="artifactbox">

**Deliverables:**

- Worklist-based interleaved CG/PTS algorithm

- Points-to set representation (BDD or explicit set based on size)

- Virtual call resolution for each target language

- Heap abstraction via allocation-site or recency abstraction

- Context-insensitive baseline with selective refinement hooks

**Acceptance Criteria:**

- Virtual call resolution precision $`\geq`$ CHA baseline

- Terminates on 100KLOC codebases within configurable timeout

- Points-to set queries complete in $`O(1)`$ amortized

</div>

#### Milestone 2.2: Language-Specific Dispatch Resolution

<div class="artifactbox">

**Deliverables:**

- Java: vtable + interface dispatch resolution

- Python: MRO (C3 linearization) + `__getattr__` handling

- JavaScript: prototype chain traversal + Proxy handling

- C++: virtual table + multiple inheritance disambiguation

- Rust: trait object dispatch + monomorphization tracking

**Acceptance Criteria:**

- Dispatch resolution matches language specification semantics

- Dynamic dispatch edge marked with uncertainty when imprecise

</div>

#### Milestone 2.3: PDG Construction

<div class="artifactbox">

**Deliverables:**

- Data dependence edges (def-use chains)

- Control dependence edges (via dominance frontier)

- Interprocedural summary edges for call sites

- Effect edges linking operations to their side effects

**Acceptance Criteria:**

- PDG slicing produces minimal relevant code for test queries

- Control dependence correctly handles structured exception handling

</div>

## Phase 3: Core Dataflow

### Objective

Implement the IFDS/IDE algorithmic framework for interprocedural dataflow analysis, along with foundational client analyses (reaching definitions, taint tracking, nullability).

### Deliverables and Acceptance Criteria

#### Milestone 3.1: IFDS Tabulation Algorithm

<div class="artifactbox">

**Deliverables:**

- Exploded supergraph construction from CPG + call graph

- Path edge computation via tabulation

- Summary edge caching for interprocedural reuse

- Demand-driven variant for interactive queries

**Acceptance Criteria:**

- Complexity $`O(E \cdot D^3)`$ verified on benchmark suite

- Summary edge reuse achieves $`\geq 2\times`$ speedup on repeated queries

</div>

#### Milestone 3.2: Taint Analysis

<div class="artifactbox">

**Deliverables:**

- Source/sink/sanitizer specification language

- Taint propagation rules (assignment, call, return, field access)

- Context-sensitive taint tracking via IFDS

- Path reconstruction for vulnerability reports

**Acceptance Criteria:**

- Detects OWASP Top 10 injection vulnerabilities on test suite

- False positive rate $`< 30\%`$ on labeled benchmark

</div>

#### Milestone 3.3: Nullability Analysis

<div class="artifactbox">

**Deliverables:**

- Null state lattice: $`\mathsf{NonNull} \mid \mathsf{Null} \mid \mathsf{MaybeNull} \mid \mathsf{Unknown}`$

- Null check recognition (`if x != null`, `x?.method`, etc.)

- Nullability inference for unannotated code

- Integration with type system annotations (`@Nullable`, `Option<T>`)

**Acceptance Criteria:**

- Correctly handles language-specific null semantics

- No false positives after explicit null checks

</div>

## Phase 4: Precision and Ownership

### Objective

Enhance analysis precision through context sensitivity, thin slicing, and ownership/resource lifecycle tracking. This phase bridges the gap between basic dataflow and production-quality bug detection.

### Deliverables and Acceptance Criteria

#### Milestone 4.1: Context-Sensitive Analysis

<div class="artifactbox">

**Deliverables:**

- $`k`$-CFA call-string context sensitivity (configurable $`k`$)

- Object sensitivity for OOP languages (1-object, 2-object)

- ZIPPER-guided selective context sensitivity

- Context abstraction for recursive call chains

**Acceptance Criteria:**

- Precision improvement $`\geq 20\%`$ over context-insensitive baseline

- Performance overhead $`< 3\times`$ for $`k \leq 2`$

</div>

#### Milestone 4.2: Thin Slicing (TAJ-Style)

<div class="artifactbox">

**Deliverables:**

- Relevant dependency identification (producer statements only)

- Seed-based backward slicing from sinks

- Slice prioritization by security relevance

**Acceptance Criteria:**

- Slice size reduction $`\geq 50\%`$ vs traditional slicing

- No loss of true positives from slice reduction

</div>

#### Milestone 4.3: Ownership and Resource Tracking

<div class="artifactbox">

**Deliverables:**

- Ownership state machine per abstract location

- Borrow tracking (shared/mutable, lifetime scope)

- Resource lifecycle analysis (file handles, connections, locks)

- Use-after-free, double-free, leak detection

**Acceptance Criteria:**

- Detects Rust-style ownership violations in non-Rust languages

- Resource leak detection with $`< 20\%`$ false positive rate

</div>

## Phase 5: Multi-Language, Security, and Concurrency

### Objective

Extend the analysis framework to handle cross-language boundaries, advanced security properties (information flow, implicit flows), and concurrent program verification (data races, linearizability).

### Deliverables and Acceptance Criteria

#### Milestone 5.1: Cross-Language Boundary Analysis

<div class="artifactbox">

**Deliverables:**

- Matthews-Findler boundary term representation

- Property preservation analysis at FFI boundaries

- Type compatibility verification (ABI layout matching)

- Risk scoring for cross-language calls

**Acceptance Criteria:**

- Detects type/layout mismatches at FFI boundaries

- Correctly propagates taint across language boundaries

</div>

#### Milestone 5.2: Advanced Information Flow Control

<div class="artifactbox">

**Deliverables:**

- DLM (Decentralized Label Model) security labels

- PC (Program Counter) label tracking for implicit flows

- Declassification policy specification and verification

- 4-point security lattice (confidentiality $`\times`$ integrity)

**Acceptance Criteria:**

- Detects implicit flows through control dependencies

- Supports principal-based access control policies

</div>

#### Milestone 5.3: Data Race Detection

<div class="artifactbox">

**Deliverables:**

- Happens-before relation construction (fork/join, sync primitives)

- Lock set analysis for mutex-protected accesses

- Atomic operation modeling

- Race condition reporting with witness traces

**Acceptance Criteria:**

- Detects races on standard concurrency benchmarks

- Correctly handles language-specific synchronization primitives

</div>

#### Milestone 5.4: Outcome Logic Bug Classification

<div class="artifactbox">

**Deliverables:**

- Manifest vs Latent bug classification **\[Le22\]**

- ISL triple representation for under-approximation

- Concolic witness generation for manifest bugs

- Context extraction for latent bugs

**Acceptance Criteria:**

- Manifest bugs have $`0\%`$ false positive rate (by construction)

- Witness generation succeeds for $`\geq 80\%`$ of manifest classifications

</div>

## Phase 6: Production Hardening

### Objective

Prepare the framework for production deployment with incremental analysis, adaptive precision, standard output formats, and integration hooks for IDEs and CI/CD pipelines.

### Deliverables and Acceptance Criteria

#### Milestone 6.1: Incremental Analysis (DRedL)

<div class="artifactbox">

**Deliverables:**

- Dependency tracking at function/file/statement granularity

- Dirty-marking propagation on code changes

- DRedL-style lattice-based incremental Datalog evaluation

- Summary invalidation and re-computation

**Acceptance Criteria:**

- Re-analysis time $`< 10\%`$ of full analysis for single-file changes

- Incremental results equivalent to full re-analysis

</div>

#### Milestone 6.2: Adaptive Precision and Time Budgets

<div class="artifactbox">

**Deliverables:**

- Per-function/file/project time budget configuration

- Graceful degradation levels (Full $`\rightarrow`$ Standard $`\rightarrow`$ Fast $`\rightarrow`$ Syntactic)

- Precision escalation for high-uncertainty findings

- Complexity estimation for budget allocation

**Acceptance Criteria:**

- Analysis completes within configured time budget

- Degradation preserves high-confidence findings

</div>

#### Milestone 6.3: Output and Integration

<div class="artifactbox">

**Deliverables:**

- SARIF 2.1 output for IDE/CI integration

- LSP (Language Server Protocol) integration

- CLI with configurable verbosity and filtering

- Structured output for LLM consumption

**Acceptance Criteria:**

- SARIF output validates against schema

- IDE integration provides real-time feedback on file save

</div>

#### Milestone 6.4: Runtime Debugger Integration

<div class="artifactbox">

**Deliverables:**

- Trace collection hooks for Python/Node/Rust/Go

- Static$`\leftrightarrow`$dynamic reconciliation engine

- Confidence boost/reduction based on runtime evidence

- Coverage-guided analysis prioritization

**Acceptance Criteria:**

- Runtime confirmation elevates finding confidence to $`\geq 0.95`$

- Runtime contradiction marks finding as false positive

</div>

## Dependency Graph

The following diagram illustrates the dependencies between major components. Note that the interleaved CG + Pointer Analysis (Qilin-style) receives input from both the parser infrastructure and the static AST/CFG construction.

<div class="center">

</div>

## Complete System Architecture

<div class="contributionbox">

A universal multi-language program analyzer with adaptive precision, parallelizable design, and best-effort F1 optimization.

</div>

### Layer 0: Input Sources

The system accepts multiple input types:

<div class="center">

| **Source Code**    | **Build Config** | **Test Suites** | **Runtime Traces** |
|:-------------------|:-----------------|:----------------|:-------------------|
| Python, Rust, Go,  | `Cargo.toml`     | pytest,         | Debugger hooks     |
| C, JS, TS, Java, … | `tsconfig`       | go test,        |                    |
|                    | `compile_cmds`   | `#[test]`, …    |                    |

</div>

### Layer 1: Parsing and IR

- **Tree-sitter parsers** (per language)

- **Brrr-Machine IR**: Unified SSA-like representation with:

  - Language-agnostic operations

  - Explicit effects (Part <a href="#part:effects" data-reference-type="ref" data-reference="part:effects">[part:effects]</a>)

  - Type/ownership annotations

### Layer 2: CPG Construction (Interleaved)

<div class="pillarbox">

For OOP languages, CG and PTS are computed **TOGETHER** (Qilin-style).

</div>

The interleaved worklist algorithm operates as follows:

<div class="fstarcode">

while worklist not empty: point \<- pop(worklist) if is_virtual_call(point): for type T in PTS(receiver): (\* MUTUAL \*) resolved \<- resolve_method(T, method) (\* DEPENDENCY \*) add_cg_edge(point, resolved) update_pts(point) if pts_changed: add_dependents(worklist)

</div>

**Algorithm Explanation:** This pseudocode captures the core mutual dependency between call graph construction and points-to analysis. When encountering a virtual call site, the algorithm queries the current points-to set for the receiver to determine potential target methods. Each resolved method creates a new call graph edge, which in turn may introduce new points-to constraints. The algorithm iterates until reaching a fixpoint where neither the call graph nor points-to sets change. The mutual dependency (marked in comments) is the fundamental reason why these analyses cannot be performed sequentially for object-oriented languages with dynamic dispatch.

The output is a **Complete Code Property Graph** containing:

- AST nodes

- CFG edges

- CG edges (call graph)

- PDG edges (program dependence graph)

- Effect edges

- Points-To Solution (PTS)

### Layer 3: Abstract Domains

<div class="center">

| **Taint** | **Nullability** | **Ownership** | **Effects** |
|:---|:---|:---|:---|
| $`\bot_T`$ | $`\bot_N`$ | $`\mathsf{Uninit}`$ | $`\{\mathsf{Read}`$, |
| $`\uparrow`$ | $`\uparrow \quad \uparrow`$ | $`\downarrow`$ | $`\mathsf{Write}`$, |
| $`\mathsf{Untainted}`$ | $`\mathsf{NonNull} \; \mathsf{Null}`$ | $`\mathsf{Owned}`$ | $`\mathsf{Alloc}`$, |
| $`\uparrow \quad \uparrow`$ | $`\uparrow \quad \uparrow`$ | $`\downarrow \quad \downarrow`$ | $`\mathsf{Free}`$, |
| $`\mathsf{Maybe}`$ (!) | $`\top_N`$ | $`\mathsf{Moved} \; \mathsf{Borrowed}`$ | $`\mathsf{IO}`$, |
| $`\uparrow`$ |  | $`\downarrow`$ | $`\mathsf{Throw}, \ldots\}`$ |
| $`\mathsf{Tainted}`$ |  | $`\mathsf{Freed}`$ |  |

</div>

**Note:** The $`\mathsf{Maybe}`$ element in the taint domain breaks Galois insertion, meaning this domain proves *incorrectness* only.

Additional domains include:

- **Security Labels**: DLM multi-principal + PC label for implicit flow

- **Access Permissions** (Bierhoff): unique/full/share/imm/pure

- **Shape (TVLA)**: 3-valued logic for heap shapes

### Layer 4: Analysis Algorithms

<div class="center">

<table>
<thead>
<tr>
<th style="text-align: left;"><strong>IFDS</strong></th>
<th style="text-align: left;"><strong>CFL-Reachability</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="math inline"><em>O</em>(<em>E</em> ⋅ <em>D</em><sup>3</sup>)</span> interprocedural</td>
<td style="text-align: left;">Context sensitivity via matched parentheses (Dyck)</td>
</tr>
<tr>
<td style="text-align: left;"><ul>
<li><p>Exploded supergraph</p></li>
<li><p>Path edges</p></li>
<li><p>Summary edges</p></li>
<li><p>Tabulation algorithm</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>Demand-driven variant</p></li>
<li><p>On-the-fly computation</p></li>
</ul></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Thin Slicing</strong></td>
<td style="text-align: left;"><strong>Fixpoint Iteration</strong></td>
</tr>
<tr>
<td style="text-align: left;">Relevant dependencies only (not all reaching defs)</td>
<td style="text-align: left;">With widening/narrowing for infinite domains</td>
</tr>
</tbody>
</table>

</div>

### Layer 5: Specific Analyses

<div class="center">

| **Taint Analysis** | **Nullability Analysis** | **Ownership Tracking** | **Resource Leaks** |
|:---|:---|:---|:---|
| source $`\rightarrow`$ sink with sanitizers | may-be-null dereference | use-after-move/free | unclosed handles |
| **Implicit Flow** | **Boundary Risk** | **Data Races** |  |
| PC label tracking **\[Sabelfeld\]** | FFI/IPC type/memory mismatch | concurrent access detection |  |

</div>

### Layer 5.5: ML-Augmented Pattern Detection (Optional)

<div class="pillarbox">

Sources: **\[Pradel18\]** (DeepBugs), **\[Si18\]** (Code2Inv)

ML provides *STATISTICAL EVIDENCE*, not soundness guarantees. Use for prioritization and pattern detection, not proofs.

</div>

ML-based detectors include:

- **Swapped Arguments Detector**: e.g., `setTimeout(delay, fn)`

- **Wrong Operator Detector**: e.g., `x == y` vs `x != y`

- **Wrong Operand Detector**: e.g., `height + width` bug

- **Name Anomaly Detector**: unusual naming patterns

These feed into a **Semantic Embedding Service** (word2vec/code2vec).

**Name-Based Security Role Classification:**

- “sanitize”, “escape” $`\rightarrow`$ `LikelySanitizer`

- “input”, “request” $`\rightarrow`$ `LikelySource`

- “query”, “exec” $`\rightarrow`$ `LikelySink`

This augments taint configuration with discovered sanitizers/sources/sinks.

### Layer 6: Verification and Confidence

#### Confidence Inputs

All of the following feed into confidence computation:

<div class="center">

<div class="adjustbox">

max width=

|  |  |  |  |  |
|:--:|:--:|:--:|:--:|:--:|
| **Static** | **Local Completeness** | **Test/Code** | **Runtime** | **ML** |
| **Results** | **(LCL)** | **Discrepancy** | **Traces** | **Confidence** |
| violations | $`\alpha(f(c)) = f^\#(\alpha(c))`$? | test vs code | ground truth | DeepBugs |

</div>

</div>

<div class="pillarbox">

- Static + ML agree: **BOOST** confidence (ensemble effect)

- Static only: Use static confidence

- ML only: Report with ML confidence (no soundness)

- ML provides prioritization, static provides soundness

</div>

#### Outcome Logic Classification

For each violation:

<div class="center">

| **Manifest Error** **\[Le22\]** | **Latent Error** |
|:---|:---|
| ISL proof has presumption $`(\mathsf{emp} \land \mathsf{true})`$ — no caller constraints needed | Requires specific precondition from caller |
| Bug happens regardless of calling context | Bug happens only in specific contexts |
| $`\rightarrow`$ HIGH confidence | $`\rightarrow`$ Report with context |

</div>

**Falsification (OL Theorem 5.1):** If spec violated, we can prove it.

#### Final Confidence Levels

| **Classification** | **Description**                 | **Action**     |
|:-------------------|:--------------------------------|:---------------|
| `ConfirmedBug`     | Manifest + concolic witness     | Report: HIGH   |
| `HighConfBug`      | Manifest, no witness yet        | Report: HIGH   |
| `TrueAlarm`        | Violation + locally complete    | Report: MEDIUM |
| `ConditionalBug`   | Latent with plausible context   | Report: MEDIUM |
| `PossibleFP`       | Violation but LC failed         | Investigate    |
| `ConfirmedSafe`    | No violation + locally complete | Skip           |
| `LikelySafe`       | No violation, LC unknown        | Skip           |

### Layer 7: Output

<div class="center">

| **SARIF** (tooling) | **CLI** (human) | **LLM** (consumable) |
|:---|:---|:---|
| Standard format for IDE/CI | Prioritized findings with traces | Structured for agent consumption |

</div>

### Cross-Cutting Concerns

<div class="center">

<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Incrementality</strong> (Part <a href="#part:incremental" data-reference-type="ref" data-reference="part:incremental">[part:incremental]</a>)</th>
<th style="text-align: left;"><strong>Multi-Language</strong> (Part <a href="#part:multi-language" data-reference-type="ref" data-reference="part:multi-language"></a>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><ul>
<li><p>Adapton-style dirty marking</p></li>
<li><p>File/function/statement level</p></li>
<li><p>Dependency tracking</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>Matthews boundary semantics</p></li>
<li><p>Risk analysis at FFI/IPC</p></li>
<li><p>Type consistency checking</p></li>
</ul></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Time Budgets</strong> (D.9)</td>
<td style="text-align: left;"><strong>Parallelization</strong></td>
</tr>
<tr>
<td style="text-align: left;"><ul>
<li><p>Per-function/file/total</p></li>
<li><p>Graceful degradation levels</p></li>
<li><p>Adaptive precision</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>Laptop <span class="math inline">→</span> datacenter scalable</p></li>
<li><p>Independent analyses parallel</p></li>
<li><p>Worklist can be parallelized</p></li>
</ul></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Configurability</strong></td>
<td style="text-align: left;"><strong>Uncertainty Propagation</strong></td>
</tr>
<tr>
<td style="text-align: left;"><ul>
<li><p>Everything is a spectrum</p></li>
<li><p>User can tune all parameters</p></li>
<li><p>Fork-friendly extensibility</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>Each block outputs confidence</p></li>
<li><p>Propagates through pipeline</p></li>
<li><p>Guides adaptive precision</p></li>
</ul></td>
</tr>
<tr>
<td style="text-align: left;"><strong>ML Integration</strong> (Layer 5.5)</td>
<td style="text-align: left;"><strong>Hybrid Testing</strong> (Section <a href="#sec:hybrid-testing" data-reference-type="ref" data-reference="sec:hybrid-testing">[sec:hybrid-testing]</a>)</td>
</tr>
<tr>
<td style="text-align: left;"><ul>
<li><p>DeepBugs name-based detection</p></li>
<li><p>Semantic identifier embeddings</p></li>
<li><p>Augments static with ML conf</p></li>
<li><p>Section <a href="#sec:ml-augmentation" data-reference-type="ref" data-reference="sec:ml-augmentation">[sec:ml-augmentation]</a> for details</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>QSYM: concolic + fuzzer cooperation</p></li>
<li><p>Validation-based soundness</p></li>
<li><p>10–100<span class="math inline">×</span> faster than pure concolic</p></li>
<li><p>Native execution + selective instr</p></li>
</ul></td>
</tr>
</tbody>
</table>

</div>

## Mutually Exclusive Analysis Paths

<div class="pillarbox">

Multiple implementations producing compatible output.

The brrr-machine design allows **SWAPPABLE** components at key decision points. Each path trades precision for speed. Output types remain compatible.

</div>

### Pointer Analysis (Layer 2)

**Note:** Choice is LANGUAGE-DEPENDENT (Part <a href="#part:pointer-analysis" data-reference-type="ref" data-reference="part:pointer-analysis"></a>).

|  |  |  |  |  |
|:---|:---|:---|:---|:---|
| **Option A** | **Option B** | **Option C** | **Option D** | **Option E** |
| **Andersen** | **Steensgaard** | **Rupta** | **DSA** | **Qilin+ZIPPER** |
| $`O(n^3)`$ subset-based | $`O(n \cdot \alpha)`$ unification | 1-callsite + stack filter | $`O(n \cdot \alpha)`$ + heap cloning | On-the-fly CG + PTS |
| Flow-sensitive possible | Flow-insensitive | For Rust: FASTER + precise | Large C/C++ ($`>`$<!-- -->100K LOC) | OOP with virtual dispatch |
| Higher precision ($`<`$<!-- -->50K) | Fastest, least precise | Section <a href="#sec:rupta" data-reference-type="ref" data-reference="sec:rupta">[sec:rupta]</a> | Section <a href="#sec:dsa" data-reference-type="ref" data-reference="sec:dsa">2.2</a> | Section <a href="#sec:qilin" data-reference-type="ref" data-reference="sec:qilin">[sec:qilin]</a> |

**Language Guide:**

- C/C++ $`<`$<!-- -->50K: Andersen

- C/C++ $`>`$<!-- -->100K: DSA

- Rust: Rupta

- OOP: Qilin

**Unified Output:** $`\mathtt{pts} : \mathtt{var\_id} \rightarrow \mathsf{set}\ \mathtt{abstract\_loc}`$

### Dataflow Algorithm (Layer 4)

**Note:** IFDS requires DISTRIBUTIVE functions.

| **Option A: IFDS** | **Option B: Symbolic Exec** | **Option C: Set Constraints** |
|:---|:---|:---|
| $`O(ED^3)`$ path-insensitive | Path-sensitive | Section <a href="#sec:set-constraints" data-reference-type="ref" data-reference="sec:set-constraints">[sec:set-constraints]</a> **\[Aiken\]** |
| DISTRIBUTIVE only! | Input-specific | Handles NON-distributive |
| Taint, reaching defs | Precise witnesses | Pointer, type inference |

**Unified Output:** $`\mathtt{finding} : \{ \mathtt{location}, \mathtt{kind}, \mathtt{confidence} \}`$

### Heap Abstraction (Layer 3)

| **Option A: TVLA/Shape** | **Option B: Allocation-Site** | **Unified Output** |
|:---|:---|:---|
| 3-valued logic | Simple abstraction |  |
| Precise shapes | Fast, imprecise | $`\mathtt{heap\_shape} : \mathtt{abstract\_heap}`$ |
| Very expensive | Good enough for most |  |

### Precision Mode (Cross-cutting)

| **Option A: Full Precision** | **Option B: Degraded/Fast** | **Unified Output** |
|:---|:---|:---|
| Andersen + IFDS + shapes | Steensgaard + patterns |  |
| High confidence | Lower confidence | Same output types, |
| Slow, thorough | Fast, approximate | different confidence |

### Bug Verification (Layer 6)

| **Option A: OL Proofs** | **Option B: Concolic Exec** | **Unified Output** |
|:---|:---|:---|
| Symbolic bug classif. | Concrete witnesses |  |
| Manifest/Latent | Actual inputs | $`\mathtt{bug\_classification}`$ |
| Sound (when succeeds) | Complete (when finds) | \+ optional witness |

### Adaptive Selection Strategy

The brrr-machine framework uses a configurable analysis pipeline where different algorithms can be selected based on language characteristics, codebase size, and time constraints. The following F\*-style type definitions formalize the configuration space:

<div class="fstarcode">

(\* Pointer algorithm selection based on language and size \*) type PointerAlgorithm = \| Andersen (\* C/C++ \< 50K LOC, most precise \*) \| Steensgaard (\* Quick pre-analysis, least precise \*) \| DSA (\* C/C++ \> 100K LOC, heap cloning \*) \| Qilin (\* OOP with virtual dispatch \*) \| Rupta (\* Rust with stack filtering \*)

type DataflowAlgorithm = \| IFDS \| Symbolic \| Hybrid type HeapAbstraction = \| TVLA \| AllocationSite \| Recency type PrecisionMode = \| Full \| Standard \| Fast \| Syntactic type VerificationMode = \| OLOnly \| ConcolicOnly \| Both

(\* Main configuration record combining all choices \*) type analysis_config = pointer_analysis : PointerAlgorithm; dataflow_algo : DataflowAlgorithm; heap_abstraction : HeapAbstraction; precision_mode : PrecisionMode; verification : VerificationMode;

</div>

**Type Signature Explanation:** The `analysis_config` record bundles five orthogonal configuration choices. The `PointerAlgorithm` type captures the trade-off between precision (Andersen) and scalability (Steensgaard, DSA), with language-specific options (Qilin for OOP, Rupta for Rust). The `VerificationMode` determines whether bugs are classified using Outcome Logic proofs, concolic witness generation, or both approaches in combination.

The following function demonstrates how the framework automatically selects an appropriate configuration based on the input code property graph and available time budget:

<div class="fstarcode">

(\* Type signature: takes a CPG and time budget, returns configuration \*) (\* This enables adaptive precision based on code characteristics \*) val select_config : cpg -\> time_budget -\> analysis_config let select_config cpg budget = let complexity = estimate_complexity cpg in let loc_count = count_loc cpg in let lang = primary_language cpg in let has_concurrency = uses_threads cpg in match budget, complexity, lang with \| \_, \_, Rust -\> (\* Rust: always use Rupta for stack filtering \*) pointer_analysis = Rupta; dataflow_algo = IFDS; heap_abstraction = AllocationSite; precision_mode = Standard; verification = Both \| Unlimited, \_, C \| Unlimited, \_, Cpp when loc_count \< 50000 -\> (\* Small C/C++: Andersen for max precision \*) pointer_analysis = Andersen; dataflow_algo = IFDS; heap_abstraction = TVLA; precision_mode = Full; verification = Both \| \_, \_, C \| \_, \_, Cpp when loc_count \> 100000 -\> (\* Large C/C++: DSA for scalability with heap cloning \*) pointer_analysis = DSA; dataflow_algo = IFDS; heap_abstraction = AllocationSite; precision_mode = Standard; verification = OLOnly \| \_, \_, Java \| \_, \_, Python -\> (\* OOP: Qilin for virtual dispatch \*) pointer_analysis = Qilin; dataflow_algo = IFDS; heap_abstraction = AllocationSite; precision_mode = Standard; verification = Both \| Limited t, High, \_ when t \< minutes 5 -\> pointer_analysis = Steensgaard; dataflow_algo = IFDS; heap_abstraction = AllocationSite; precision_mode = Fast; verification = OLOnly \| \_ -\> pointer_analysis = Andersen; dataflow_algo = Hybrid; heap_abstraction = AllocationSite; precision_mode = Standard; verification = Both

</div>

**Selection Logic:** The function uses pattern matching on language type, codebase size, and time budget to select the most appropriate algorithm combination. Key heuristics include: (1) Rust always uses Rupta for stack filtering benefits; (2) small C/C++ codebases ($`<`$<!-- -->50K LOC) use Andersen for maximum precision; (3) large C/C++ codebases ($`>`$<!-- -->100K LOC) use DSA for scalability; (4) OOP languages (Java, Python) use Qilin for on-the-fly virtual dispatch resolution; (5) tight time budgets trigger graceful degradation to faster, less precise algorithms.

### Output Compatibility Guarantee

All paths produce compatible output types. This enables:

1.  **Progressive refinement** — Start fast, refine suspicious findings

2.  **Parallel execution** — Run multiple paths, merge results

3.  **User choice** — Configure based on needs (CI vs IDE vs audit)

4.  **Fallback** — Degrade gracefully when precise analysis times out

The unified finding type ensures that different analysis paths produce compatible outputs that can be merged and compared:

<div class="fstarcode">

(\* Core finding type: all analysis paths produce this structure \*) (\* Key fields: location, kind identify the finding; confidence enables ranking \*) type unified_finding = location : node_id; (\* Where in the CPG \*) kind : vulnerability_type; (\* SQL injection, XSS, null deref, etc. \*) confidence : confidence_level; (\* Certainty from 0.0 to 1.0 \*) path_used : analysis_path; (\* Which algorithm produced this \*) witness : option concrete_trace; (\* Concrete input triggering the bug \*) can_refine : bool; (\* Is higher-precision analysis available? \*)

(\* Merge findings from multiple analysis paths \*) (\* Groups by (location, kind), keeps highest-confidence finding per group \*) val merge_findings : list unified_finding -\> list unified_finding let merge_findings findings = let grouped = group_by (fun f -\> (f.location, f.kind)) findings in List.map (fun fs -\> let best = max_by (fun f -\> confidence_to_nat f.confidence) fs in best with can_refine = List.length fs \> 1 ) grouped

</div>

**Merging Strategy:** When multiple analysis paths report findings at the same location with the same vulnerability kind, the `merge_findings` function retains the finding with the highest confidence score. The `can_refine` flag is set to `true` when multiple paths contributed findings, indicating that the location was flagged by independent analyses—increasing overall confidence in the result. This enables the ensemble effect described in Layer 6 (Verification and Confidence).

# Channel Analysis

**Foundation**: **\[Honda08\]** “Multiparty Asynchronous Session Types”, **\[Honda98\]** “Language Primitives and Type Discipline for Structured Communication”

This part provides the theoretical foundations for analyzing communication channels in concurrent and distributed systems. Channel analysis extends the core session type theory (Section <a href="#sec:session-types" data-reference-type="ref" data-reference="sec:session-types">[sec:session-types]</a>) with detailed syntax, semantics, and causality analysis for multiparty protocols.

## Binary Session Types

**Paper**: **\[Honda98\]** “Language Primitives and Type Discipline for Structured Communication-Based Programming”

Binary session types provide the foundational theory for structured communication between exactly two parties. This precedes and motivates the multiparty extensions in subsequent sections.

### The Session Concept

<div class="pillarbox">

A **session** is a chain of dyadic (two-party) reciprocal interactions serving as a unit of abstraction for describing communication behavior.

**Motivation**: Traditional primitives (RPC, method invocation, rendez-vous) express only one-time interactions. Complex protocols require sequences of related communications, but prior languages had no construct to structure them.

**Analogy to Structured Programming**:

- Imperative (pre-1970s): assignment + goto $`\rightarrow`$ spaghetti code

- Imperative (post-1970s): assignment + if/while $`\rightarrow`$ structured programs

- Concurrent (pre-1998): send/receive + parallel $`\rightarrow`$ unstructured interaction

- Concurrent (post-1998): session primitives + types $`\rightarrow`$ structured communication

**Session Structure**:

- Sessions designated by **channels** (distinct from names/ports)

- Channel $`k`$ is private to session, generated fresh at session initiation

- All interactions in session occur through its channel

- Session combines: value passing, label branching, delegation

</div>

### Binary Session Type Syntax

<div class="definition">

**Definition 1.1** (Binary Session Type Grammar (Honda 1998, Definition 5.1)). **Sorts** ($`S`$):
``` math
S ::= \mathsf{nat} \mid \mathsf{bool} \mid \langle\alpha, \bar{\alpha}\rangle
```

**Types** ($`\alpha`$, $`\beta`$):
``` math
\begin{aligned}
\alpha ::=\ & \downarrow[\tilde{S}];\, \alpha & \text{(Input: receive values of sorts $\tilde{S}$)} \\
      \mid\ & \downarrow[\alpha];\, \beta & \text{(Channel input: delegation receive)} \\
      \mid\ & \&\{l_1: \alpha_1, \ldots, l_n: \alpha_n\} & \text{(Branching: offer $n$ labeled options)} \\
      \mid\ & \mathbf{1} & \text{(Termination/inaction)} \\
      \mid\ & \bot & \text{(No further connection possible)} \\
      \mid\ & \uparrow[\tilde{S}];\, \alpha & \text{(Output: send values of sorts $\tilde{S}$)} \\
      \mid\ & \uparrow[\alpha];\, \beta & \text{(Channel output: delegation send)} \\
      \mid\ & \oplus\{l_1: \alpha_1, \ldots, l_n: \alpha_n\} & \text{(Selection: choose one of $n$ labels)} \\
      \mid\ & t & \text{(Type variable)} \\
      \mid\ & \mu t.\, \alpha & \text{(Recursive type)}
\end{aligned}
```

</div>

**Type Interpretation**:

- $`\downarrow[S];\, \alpha`$: First receive value of sort $`S`$, then behave as $`\alpha`$

- $`\uparrow[S];\, \alpha`$: First send value of sort $`S`$, then behave as $`\alpha`$

- $`\&\{l_i: \alpha_i\}`$: **External choice** — wait for partner to select label

- $`\oplus\{l_i: \alpha_i\}`$: **Internal choice** — select label to send to partner

- $`\mu t.\, \alpha`$: Recursive behavior (loops, unbounded interactions)

- $`\mathbf{1}`$: Session complete, no more actions

- $`\bot`$: Channel consumed/hidden, no reconnection

**Example Types**:

- **Remote Procedure Call**:

  - Caller: $`\uparrow[\mathsf{int}];\, \downarrow[\mathsf{int}]`$ (send argument, receive result)

  - Callee: $`\downarrow[\mathsf{int}];\, \uparrow[\mathsf{int}]`$ (receive argument, send result)

- **Cell Object**:

  - Cell: $`\&\{\mathsf{read}: \uparrow[\mathsf{int}],\, \mathsf{write}: \downarrow[\mathsf{int}]\}`$

  - Client: $`\oplus\{\mathsf{read}: \downarrow[\mathsf{int}],\, \mathsf{write}: \uparrow[\mathsf{int}]\}`$

- **ATM Protocol (unbounded)**:
  ``` math
  \mathsf{ATM}: \downarrow[\mathsf{nat}];\, \mu t.\, \&\left\{\begin{array}{l}
        \mathsf{deposit}: \downarrow[\mathsf{nat}];\, t,\\
        \mathsf{withdraw}: \downarrow[\mathsf{nat}];\, \oplus\{\mathsf{success}: \uparrow[\mathsf{nat}];\, t,\, \mathsf{failure}: t\},\\
        \mathsf{balance}: \uparrow[\mathsf{nat}];\, t,\\
        \mathsf{quit}: \mathbf{1}
      \end{array}\right\}
  ```

### The Duality Principle

<div class="definition">

**Definition 1.2** (Duality (Co-Type)). For type $`\alpha`$ where $`\bot`$ does not occur, the co-type $`\bar{\alpha}`$ is defined:
``` math
\begin{aligned}
\overline{\uparrow[S];\, \alpha} &= \downarrow[S];\, \bar{\alpha} & \text{(send dual is receive)} \\
\overline{\downarrow[S];\, \alpha} &= \uparrow[S];\, \bar{\alpha} & \text{(receive dual is send)} \\
\overline{\oplus\{l_i: \alpha_i\}} &= \&\{l_i: \bar{\alpha_i}\} & \text{(selection dual is branching)} \\
\overline{\&\{l_i: \alpha_i\}} &= \oplus\{l_i: \bar{\alpha_i}\} & \text{(branching dual is selection)} \\
\overline{\uparrow[\alpha];\, \beta} &= \downarrow[\alpha];\, \bar{\beta} & \text{(delegation duals)} \\
\overline{\downarrow[\alpha];\, \beta} &= \uparrow[\alpha];\, \bar{\beta} \\
\overline{\mu t.\, \alpha} &= \mu t.\, \bar{\alpha} & \text{(recursion preserves duality)} \\
\bar{t} &= t & \text{(variable unchanged)} \\
\bar{\mathbf{1}} &= \mathbf{1} & \text{(termination self-dual)}
\end{aligned}
```

</div>

**Fundamental Property**: $`\bar{\bar{\alpha}} = \alpha`$ (involution: dualizing twice returns original)

**Significance**: If two processes communicate on channel $`k`$ with types $`\alpha`$ and $`\bar{\alpha}`$, their interaction patterns are **compatible** — no type errors occur.

**Examples**:

- Type: $`\uparrow[\mathsf{int}];\, \downarrow[\mathsf{bool}]`$, Dual: $`\downarrow[\mathsf{int}];\, \uparrow[\mathsf{bool}]`$

- Type: $`\oplus\{\mathsf{ok}: \uparrow[\mathsf{int}],\, \mathsf{err}: \mathbf{1}\}`$, Dual: $`\&\{\mathsf{ok}: \downarrow[\mathsf{int}],\, \mathsf{err}: \mathbf{1}\}`$

### Type Algebra for Composition

<div class="definition">

**Definition 1.3** (Typing Context Composition (Honda 1998, Definition 5.2)). The type algebra governs how typings combine under parallel composition.

**Compatibility** ($`\Delta_1 \diamond \Delta_2`$): Two typings are compatible if common channels have **dual** types:
``` math
\Delta_1 \diamond \Delta_2 \iff \forall k \in \mathrm{dom}(\Delta_1) \cap \mathrm{dom}(\Delta_2).\; \Delta_1(k) = \overline{\Delta_2(k)}
```

**Composition** ($`\Delta_1 \circ \Delta_2`$): When $`\Delta_1 \diamond \Delta_2`$, the composition is:
``` math
(\Delta_1 \circ \Delta_2)(k) = \begin{cases}
\bot & \text{if } k \in \mathrm{dom}(\Delta_1) \cap \mathrm{dom}(\Delta_2) \\
\Delta_i(k) & \text{if } k \in \mathrm{dom}(\Delta_i) \setminus \mathrm{dom}(\Delta_{\text{other}}) \\
\text{undefined} & \text{otherwise}
\end{cases}
```

</div>

**Why $`\bot`$ After Composition**: When two processes with dual types compose on channel $`k`$, no **third** process can join that channel — it’s fully consumed. This prevents interference and ensures session isolation.

**Typing Rules** (key rules):

<div class="mathpar">

; P Q ’

; k\![\];  P , k: ; 

; k?()    P , k: ; 

; k {l_1: P_1 l_n: P_n} , k: &{l_1: \_1, …, l_n: \_n}

; k l_j;  P , k: {l_1: \_1, …, l_n: \_n}

</div>

### Session Initiation and Delegation

**Session Initiation**:
``` math
\begin{aligned}
\mathbf{request}\; a(k)\; \mathbf{in}\; P &\quad \text{Request new session via name $a$, bind channel $k$} \\
\mathbf{accept}\; a(k)\; \mathbf{in}\; P &\quad \text{Accept session request via $a$, bind channel $k$}
\end{aligned}
```

**Operational Semantics**:
``` math
[\text{Link}]\quad (\mathbf{accept}\; a(k)\; \mathbf{in}\; P_1) \mid (\mathbf{request}\; a(k)\; \mathbf{in}\; P_2) \rightarrow (\nu k)(P_1 \mid P_2)
```
Fresh channel $`k`$ is generated, private to the session. $`P_1`$ and $`P_2`$ proceed with complementary views of the protocol.

**Delegation** (Channel Passing):
``` math
\begin{aligned}
\mathbf{throw}\; k[k'];\, P &\quad \text{Send channel $k'$ through channel $k$} \\
\mathbf{catch}\; k(k')\; \mathbf{in}\; P &\quad \text{Receive channel through $k$, bind as $k'$}
\end{aligned}
```

**Purpose**: Delegation allows **dynamic redistribution** of session participation. An ongoing session can be handed off to another process.

**Typing Rules**:

<div class="mathpar">

; k\[k’\];  P , k: ;  , k’:

; k(k’) P , k: ; 

</div>

**Example** (FTP Server with Delegation): The following code illustrates how delegation enables dynamic session redistribution. The FTP server accepts client connections on a public name, then immediately delegates the session to a worker thread, freeing the main loop to continue accepting new connections. This pattern is fundamental to scalable server design.

<div class="fstarcode">

(\* FTP server that delegates sessions to worker threads \*) Ftpd(pid, b) = accept pid(s) in (\* Accept client connection \*) request b(k) in (\* Get worker thread \*) throw k\[s\]; (\* Delegate session to worker \*) Ftpd\[pid, b\] (\* Continue accepting \*)

(\* The client sees a single session; delegation is transparent \*)

</div>

**Key Type Signature**: The `throw k[s]` operation has type $`k : \uparrow[T];\, \beta`$ where $`T`$ is the session type being delegated. The worker receives the session with type $`k : \downarrow[T];\, \beta'`$ and continues the protocol from where it was delegated.

### Safety Theorems

<div class="theorem">

**Theorem 1.4** (Safety (Honda 1998, Theorem 5.4)).

1.  ***Invariance under Structural Equivalence**: $`\Theta; \Gamma \vdash P \triangleright \Delta`$ and $`P \equiv Q`$ implies $`\Theta; \Gamma \vdash Q \triangleright \Delta`$*

2.  ***Subject Reduction**: $`\Theta; \Gamma \vdash P \triangleright \Delta`$ and $`P \rightarrow^* Q`$ implies $`\Theta; \Gamma \vdash Q \triangleright \Delta`$*

3.  ***Error Freedom**: A typable program **never** reduces to an error state.*

</div>

<div class="definition">

**Definition 1.5** (Error States). $`P`$ is an error if $`P \equiv \mathbf{def}\; D\; \mathbf{in}\; (\nu \tilde{u})(P' \mid R)`$ where $`P'`$ contains, for some channel $`k`$, either:

- Two $`k`$-processes that don’t form a $`k`$-redex (mismatched operations)

- Three or more $`k`$-processes (race condition)

</div>

**What This Guarantees**:

- No message type mismatches (send int, expect bool)

- No unhandled selections (select label not in branch)

- No races on channels (exactly two parties per channel)

- Sessions complete or progress (no stuck states except inaction)

**Connection to Multiparty**: Binary session types ensure 2-party safety. Multiparty session types **\[Honda08\]** generalize to $`n`$ parties via global types and projection, with similar safety guarantees.

### Relationship to Go Channels and Rust mpsc

<div class="pillarbox">

**Go Channels**: Go channels are **untyped at the session level** — only element type is tracked. Go provides **no static guarantee** of protocol compliance.

<div class="center">

| **Go Pattern**         | **Session Type Equivalent**  |
|:-----------------------|:-----------------------------|
| `ch := make(chan int)` | Create session channel       |
| `ch <- 42`             | $`\uparrow[\mathsf{int}]`$   |
| `x := <-ch`            | $`\downarrow[\mathsf{int}]`$ |
| `close(ch)`            | End session                  |

</div>

Session types **add**: sequence constraints, branching/selection typing.

**Rust mpsc**: Rust’s `std::sync::mpsc` provides typed channels but not session typing. The `session_types` crate adds Honda-style session types to Rust.

<div class="center">

| **Rust Pattern**           | **Session Type Equivalent**        |
|:---------------------------|:-----------------------------------|
| `let (tx, rx) = channel()` | Create session with dual endpoints |
| `tx.send(42)`              | $`\uparrow[\mathsf{int}]`$         |
| `let x = rx.recv()`        | $`\downarrow[\mathsf{int}]`$       |
| `drop(tx), drop(rx)`       | Session end (via affine types)     |

</div>

Session types **add**: protocol state machine, compile-time progress checks.

</div>

**Static Analysis Application**: For Go/Rust channel analysis, we can:

1.  **Infer** session types from usage patterns

2.  **Check** that inferred types are dual at connection points

3.  **Detect** protocol violations (missing receives, wrong order)

4.  **Verify** deadlock freedom via causality analysis (Section <a href="#sec:causality-analysis" data-reference-type="ref" data-reference="sec:causality-analysis">[sec:causality-analysis]</a>)

## Global Type Syntax and Semantics

Global types describe conversation protocols from a bird’s-eye view, specifying the complete interaction structure among all participants.

### Global Type Grammar

<div class="definition">

**Definition 2.1** (Global Type Grammar).
``` math
\begin{aligned}
G ::=\ & p \rightarrow q : k\langle U\rangle.G & \text{(Message passing: $p$ sends $U$ to $q$ via channel $k$)} \\
  \mid\ & p \rightarrow q : k\{l_j : G_j\}_{j \in J} & \text{(Labeled choice: $p$ selects label $l_j$, $q$ branches)} \\
  \mid\ & G_1 \mid G_2 & \text{(Parallel composition)} \\
  \mid\ & \mu t.\, G & \text{(Recursive type)} \\
  \mid\ & t & \text{(Type variable)} \\
  \mid\ & \mathbf{end} & \text{(Termination)}
\end{aligned}
```

**Payload Types** ($`U`$):
``` math
\begin{aligned}
U ::=\ & \tilde{S} & \text{(Sequence of base sorts)} \\
  \mid\ & T@p & \text{(Located local type for delegation)}
\end{aligned}
```

**Base Sorts** ($`S`$):
``` math
\begin{aligned}
S ::=\ & \mathsf{bool} \mid \mathsf{nat} \mid \mathsf{int} \mid \mathsf{string} \mid \cdots \\
  \mid\ & \langle\alpha, \bar{\alpha}\rangle & \text{(Session sort: pair of dual types)} \\
  \mid\ & G & \text{(Global type as sort for nested sessions)}
\end{aligned}
```

</div>

### Prefix Ordering

The prefix ordering ($`\prec`$) defines the causal structure of global types. A prefix $`a`$ precedes prefix $`b`$ (written $`a \prec b`$) when $`a`$ must complete before $`b`$ can execute.

<div class="definition">

**Definition 2.2** (Prefix Ordering Rules). **Sequential Composition**:

<div class="mathpar">

</div>

**Parallel Independence**:

<div class="mathpar">

</div>

**Branch Ordering**:

<div class="mathpar">

</div>

**Recursion Unfolding**:

<div class="mathpar">

</div>

</div>

### Equi-Recursive Type Handling

Global types use an equi-recursive interpretation where $`\mu t.\, G \equiv G[\mu t.\, G/t]`$. This enables infinite unfolding for ongoing protocols.

<div class="definition">

**Definition 2.3** (Equi-Recursive Equivalence).

- **Unfolding**: $`\mu t.\, G \equiv G[\mu t.\, G/t]`$

- **Contractivity**: In $`\mu t.\, G`$, the variable $`t`$ must be guarded by at least one message prefix (ensures productive recursion)

- **Regular Trees**: Global types denote regular infinite trees (finitely representable infinite structures)

</div>

**Example** — Streaming Protocol:
``` math
\mu t.\, \mathsf{Producer} \rightarrow \mathsf{Consumer} : \mathsf{data}\langle\mathsf{int}\rangle.\,
\mathsf{Consumer} \rightarrow \mathsf{Producer} : \mathsf{ack}\langle\mathsf{bool}\rangle.\, t
```
Unfolds to infinite alternation of data/ack exchanges.

## Causality Analysis

Causality analysis determines which prefixes must complete before others can execute. This is fundamental for ensuring progress and detecting deadlocks.

### Input-Input Dependency

<div class="definition">

**Definition 3.1** (Input-Input Dependency ($`\prec^{II}`$)). $`a \prec^{II} b`$ holds when:

- Both $`a`$ and $`b`$ are input prefixes at the same participant $`p`$

- $`a`$ must be executed before $`b`$ according to the global type structure

- They share the same session (belong to same global type $`G`$)

Formally:

<div class="mathpar">

</div>

</div>

**Example**: $`G = A \rightarrow B : s_1\langle\mathsf{int}\rangle.\, A \rightarrow B : s_2\langle\mathsf{int}\rangle.\, \mathbf{end}`$

At participant $`B`$: $`s_1? \prec^{II}_B s_2?`$ (B must receive on $`s_1`$ before $`s_2`$)

**Chain Property**: If $`k_1 \prec^{II} k_2 \prec^{II} \cdots \prec^{II} k_n`$, then $`B`$ executes inputs in order $`k_1, k_2, \ldots, k_n`$.

### Input-Output Dependency

<div class="definition">

**Definition 3.2** (Input-Output Dependency ($`\prec^{IO}`$)). $`a \prec^{IO} b`$ holds when:

- $`a`$ is an input prefix and $`b`$ is an output prefix at the same participant $`p`$

- The output depends on data received by the input

- $`a`$ must complete before $`b`$ can execute

Formally:

<div class="mathpar">

</div>

</div>

**Example**:
``` math
G = \mathsf{Client} \rightarrow \mathsf{Server} : \mathsf{req}\langle\mathsf{Query}\rangle.\,
\mathsf{Server} \rightarrow \mathsf{Client} : \mathsf{resp}\langle\mathsf{Result}\rangle.\, \mathbf{end}
```
At Server: $`\mathsf{req}? \prec^{IO}_{\mathsf{Server}} \mathsf{resp}!`$ (Server must receive query before sending response)

**Importance**: Captures request-response patterns, essential for detecting causal violations, ensures data availability before output.

### Output-Output Dependency

<div class="definition">

**Definition 3.3** (Output-Output Dependency ($`\prec^{OO}`$)). $`a \prec^{OO} b`$ holds when:

- Both $`a`$ and $`b`$ are output prefixes at the same participant $`p`$

- **Synchronous communication**: $`a`$ must complete (including ack) before $`b`$

- **Asynchronous communication**: ordering imposed by channel or explicit sequencing

**Synchronous Case**:

<div class="mathpar">

</div>

**Asynchronous Case**: $`\prec^{OO}`$ is **only** induced by:

1.  Same channel (FIFO ordering): $`s_1!`$ before $`s_2!`$ on same $`s`$

2.  Explicit control flow in process

</div>

**Note**: This is a **key difference** between sync and async session types. Sync has more causality (OO-edges between any sequential outputs). Async has less causality (more parallelism possible).

**Examples**:

- **Sync**: $`G = A \rightarrow B : s\langle\mathsf{int}\rangle.\, A \rightarrow C : t\langle\mathsf{int}\rangle.\, \mathbf{end}`$ — At $`A`$ (sync): $`s! \prec^{OO}_A t!`$

- **Async**: Same $`G`$, but at $`A`$ (async): $`s!`$ and $`t!`$ can proceed in parallel

### Dependency Chain Analysis

<div class="definition">

**Definition 3.4** (Dependency Chains). **Input Dependency Chain**: A sequence $`k_1, k_2, \ldots, k_n`$ forms an input chain at $`p`$ if:
``` math
\forall i \in [1, n-1].\; k_i \prec^{II}_p k_{i+1}
```
Property: Participant $`p`$ must execute inputs in exactly this order.

**Output Dependency Chain**: A sequence $`k_1, k_2, \ldots, k_n`$ forms an output chain at $`p`$ if:
``` math
\forall i \in [1, n-1].\; (k_i \prec^{OO}_p k_{i+1}) \lor (k_i \prec^{IO}_p k_{i+1})
```
Property: Participant $`p`$’s outputs are ordered by this chain.

</div>

**Mixed Dependency Graph**: Combine all three relations into directed graph:

- Nodes: all prefixes in $`G`$

- Edges: $`\prec^{II}`$, $`\prec^{IO}`$, $`\prec^{OO}`$ edges

**Acyclicity**: The dependency graph **must** be acyclic for deadlock freedom.

**Deadlock Detection**: Cycle in dependency graph $`\Rightarrow`$ potential deadlock.

**Example deadlock**:
``` math
\begin{aligned}
A&: s_1?;\, s_2! \quad \text{(input before output)} \\
B&: s_2?;\, s_1! \quad \text{(input before output)}
\end{aligned}
```
Creates cycle: $`s_1? \xrightarrow{IO} s_2! \rightarrow s_2? \xrightarrow{IO} s_1! \rightarrow s_1?`$

## Linearity and Coherence

### Linear Global Types

<div class="definition">

**Definition 4.1** (Linearity (Definition 3.5 from Honda 2008)). A global type $`G`$ is **linear** if every prefix in $`G`$:

1.  Is not suppressed by $`\prec^{II}`$ (no conflicting input ordering)

2.  Is not suppressed by $`\prec^{IO}`$ (no conflicting input-output ordering)

3.  Is not suppressed by $`\prec^{OO}`$ (no conflicting output ordering)

**Suppression by $`\prec^{II}`$**: Prefix $`k`$ is suppressed if $`\exists k'.\, k' \prec^{II} k \land k' \prec^{II} k`$ (same predecessor leads to different paths). Intuition: Two inputs at same participant cannot both be “first”.

**Suppression by $`\prec^{IO}`$**: Prefix $`k`$ (output) is suppressed if $`\exists k'.\, k' \prec^{IO} k \land \neg\mathrm{enabled}(k')`$ (required input not yet available).

**Suppression by $`\prec^{OO}`$**: Prefix $`k`$ is suppressed if $`\exists k'.\, k' \prec^{OO} k \land \neg\mathrm{completed}(k')`$ (required prior output not done).

</div>

The linearity check algorithm verifies that a global type satisfies the suppression-freedom conditions required for well-formed protocols. The algorithm traverses the global type structure, checking at each prefix whether any of the three dependency relations ($`\prec^{II}`$, $`\prec^{IO}`$, $`\prec^{OO}`$) would suppress execution.

<div class="fstarcode">

let rec linear (g : global_type) : bool = match g with \| GMsg p q k u g’ -\> not (suppressed_II g k) && not (suppressed_IO g k) && not (suppressed_OO g k) && linear g’

\| GBranch p q k branches -\> not (suppressed_II g k) && not (suppressed_IO g k) && not (suppressed_OO g k) && List.for_all (fun (\_, gj) -\> linear gj) branches

\| GPar g1 g2 -\> linear g1 && linear g2 && no_conflicts g1 g2

\| GRec \_ g’ -\> linear g’ \| GVar \_ \| GEnd -\> true

</div>

**Complexity**: The algorithm runs in $`O(|G|^2)`$ time, where $`|G|`$ is the size of the global type. The quadratic factor arises from checking suppression conditions for each prefix pair. For practical protocols (typically $`|G| < 100`$ prefixes), this is fast.

### Coherence Condition

<div class="definition">

**Definition 4.2** (Coherence (Definition 4.2 from Honda 2008)). A family of local types $`\{T_p@p\}_{p \in P}`$ is **coherent** if:

1.  There exists a linear global type $`G`$

2.  For each participant $`p \in P`$: $`T_p = G \upharpoonright p`$ (projection of $`G`$ onto $`p`$)

Formally:
``` math
\mathrm{coherent}(\{T_p@p\}_{p \in P}) \iff \exists G.\, \mathrm{linear}(G) \land \forall p \in P.\, T_p = \mathrm{project}(G, p)
```

</div>

**Coherence Ensures**:

1.  All participants agree on the protocol structure

2.  Sends match receives (no lost messages)

3.  Selections match branches (no unhandled cases)

4.  Delegations are properly received

**Coherence Examples**:

- **Coherent**: $`\{k\oplus\{\mathsf{ok}:\mathbf{end}, \mathsf{quit}:\mathbf{end}\}@A,\, k\&\{\mathsf{ok}:\mathbf{end}, \mathsf{quit}:\mathbf{end}\}@B\}`$ (A’s selection matches B’s branches)

- **Coherent**: $`\{k!\langle\mathsf{int}\rangle@A,\, k?\langle\mathsf{int}\rangle@B\}`$ (A’s send matches B’s receive)

- **Not Coherent** (from Honda 2008 Figure 5): $`\{s!@A,\, s?;\,s?@B,\, s!@C\}`$ — No linear global type generates these projections

### Projection Algorithm

<div class="definition">

**Definition 4.3** (Projection Algorithm (Definition 4.1 from Honda 2008)). The projection $`G \upharpoonright p`$ extracts participant $`p`$’s local view:

</div>

The projection algorithm transforms a global type into a local type for a specific participant. This is the core operation that enables decentralized implementation of multiparty protocols: each participant only needs to know their local view, not the full global type.

<div class="fstarcode">

let rec project (g : global_type) (p : participant) : local_type option = match g with \| GMsg q r k u g’ -\> if p = q then Some (LSend k u (project g’ p)) else if p = r then Some (LRecv k u (project g’ p)) else project g’ p (\* p not involved \*)

\| GBranch q r k branches -\> if p = q then Some (LSelect k (List.map (fun (l, gj) -\> (l, project gj p)) branches)) else if p = r then Some (LBranch k (List.map (fun (l, gj) -\> (l, project gj p)) branches)) else (\* p not involved: all branches must project identically \*) let projs = List.map (fun (\_, gj) -\> project gj p) branches in if all_equal projs then List.hd projs else None (\* UNDEFINED \*)

\| GPar g1 g2 -\> merge (project g1 p) (project g2 p)

\| GRec v g’ -\> Some (LRec v (project g’ p)) \| GVar v -\> Some (LVar v) \| GEnd -\> Some LEnd

</div>

**Projection Properties**:

- **Totality for Linear Types**: If $`G`$ is linear and $`p \in \mathrm{participants}(G)`$, then $`\mathrm{project}(G, p)`$ is defined.

- **Projection Preserves Typing**: If $`\Gamma \vdash P \triangleright \Delta`$ and $`\Delta(s) = \llbracket G \rrbracket`$ then $`\mathrm{project}(G, p)`$ types $`P`$’s behavior at $`s`$ for $`p`$.

- **Merge Operation**: For parallel composition, merge combines independent local behaviors.

## Local Type Syntax

### Local Type Grammar

<div class="definition">

**Definition 5.1** (Local Type Grammar).
``` math
\begin{aligned}
T ::=\ & k!\langle U\rangle;\, T & \text{(Output: send payload $U$ via channel $k$)} \\
  \mid\ & k?\langle U\rangle;\, T & \text{(Input: receive payload $U$ via channel $k$)} \\
  \mid\ & k\oplus\{l_j : T_j\}_{j \in J} & \text{(Selection: choose one label to send)} \\
  \mid\ & k\&\{l_j : T_j\}_{j \in J} & \text{(Branching: offer labels, receive choice)} \\
  \mid\ & k!\langle T'@p\rangle;\, T & \text{(Delegation: send session capability)} \\
  \mid\ & k?\langle T'@p\rangle;\, T & \text{(Session receive: get delegated capability)} \\
  \mid\ & \mu t.\, T & \text{(Recursive type)} \\
  \mid\ & t & \text{(Type variable)} \\
  \mid\ & \mathbf{end} & \text{(Termination)}
\end{aligned}
```
where $`k \in \mathbb{N}`$ (channel number), $`U ::= \tilde{S} \mid T@p`$ (payload), $`l \in \mathrm{Labels}`$, $`p \in \mathrm{Participants}`$.

</div>

### Duality (Co-Type) Relation

<div class="definition">

**Definition 5.2** (Duality for Local Types). For binary sessions, each type has a dual representing the partner’s view:
``` math
\begin{aligned}
\overline{k!\langle U\rangle;\, T} &= k?\langle U\rangle;\, \bar{T} & \text{(send dual is receive)} \\
\overline{k?\langle U\rangle;\, T} &= k!\langle U\rangle;\, \bar{T} & \text{(receive dual is send)} \\
\overline{k\oplus\{l_j : T_j\}} &= k\&\{l_j : \bar{T_j}\} & \text{(selection dual is branch)} \\
\overline{k\&\{l_j : T_j\}} &= k\oplus\{l_j : \bar{T_j}\} & \text{(branch dual is selection)} \\
\overline{k!\langle T'@p\rangle;\, T} &= k?\langle T'@p\rangle;\, \bar{T} & \text{(delegation dual)} \\
\overline{k?\langle T'@p\rangle;\, T} &= k!\langle T'@p\rangle;\, \bar{T} & \text{(session receive dual)} \\
\overline{\mu t.\, T} &= \mu t.\, \bar{T} & \text{(unfold before dualizing)} \\
\bar{t} &= t & \text{(variable unchanged)} \\
\overline{\mathbf{end}} &= \mathbf{end} & \text{(termination self-dual)}
\end{aligned}
```

</div>

**Duality Properties**:

- **Involution**: $`\bar{\bar{T}} = T`$ (dualizing twice returns original)

- **Compatibility**: If two processes have types $`T`$ and $`\bar{T}`$ at a shared channel, their interactions will be compatible (no type errors)

- **Session Safety**: Well-typed binary session with dual endpoints never fails

### Type Isomorphism Rules

<div class="definition">

**Definition 5.3** (Type Isomorphism ($`\approx`$)). **Permutation of Independent Outputs**:
``` math
k_1!\langle U_1\rangle;\, k_2!\langle U_2\rangle;\, T \approx k_2!\langle U_2\rangle;\, k_1!\langle U_1\rangle;\, T \quad \text{(when } k_1 \neq k_2\text{)}
```
Rationale: Outputs on different channels can be reordered since they are causally independent in async communication.

**Permutation of Independent Inputs**:
``` math
k_1?\langle U_1\rangle;\, k_2?\langle U_2\rangle;\, T \approx k_2?\langle U_2\rangle;\, k_1?\langle U_1\rangle;\, T \quad \text{(when } k_1 \neq k_2\text{)}
```
Rationale: Inputs on different channels can be reordered when no data dependency exists.

**Associativity**: $`(T_1;\, T_2);\, T_3 \approx T_1;\, (T_2;\, T_3)`$

**Identity**: $`\mathbf{end};\, T \approx T \approx T;\, \mathbf{end}`$

</div>

**Isomorphism Preserves Typing**: If $`T_1 \approx T_2`$ and $`\Gamma \vdash P \triangleright \Delta, s:T_1`$ then $`\Gamma \vdash P \triangleright \Delta, s:T_2`$.

### Subtyping Relation

<div class="definition">

**Definition 5.4** (Session Subtyping (Gay & Hole 2005)). The subtyping relation $`\leq_{\mathrm{sub}}`$ allows safe substitution of more specific types.

**Output** (Covariant in payload):

<div class="mathpar">

</div>

**Input** (Contravariant in payload):

<div class="mathpar">

</div>

**Selection** (Covariant in labels — fewer choices OK):

<div class="mathpar">

</div>

**Branching** (Contravariant in labels — more cases OK):

<div class="mathpar">

</div>

</div>

**Subtyping Examples**:

- $`k\oplus\{\mathsf{ok} : T_1\} \leq_{\mathrm{sub}} k\oplus\{\mathsf{ok} : T_1, \mathsf{quit} : T_2\}`$ (Can select fewer options than offered)

- $`k\&\{\mathsf{ok} : T_1, \mathsf{quit} : T_2\} \leq_{\mathrm{sub}} k\&\{\mathsf{ok} : T_1\}`$ (Can handle more cases than required)

## Runtime Typing

Runtime typing extends the static type system to handle message queues, which appear during execution of asynchronous communication.

### Type Contexts for Message Queues

<div class="definition">

**Definition 6.1** (Type Context Grammar).
``` math
\begin{aligned}
T[\cdot] ::=\ & [\cdot] & \text{(Hole where local type goes)} \\
  \mid\ & k!\langle U\rangle;\, T[\cdot] & \text{(Output prefix context)} \\
  \mid\ & k\oplus l : T[\cdot] & \text{(Selection prefix context, singleton)}
\end{aligned}
```

**Context Operations**:

- **Hole Filling**: $`T[T']`$ plugs local type $`T'`$ into context $`T[\cdot]`$

- **Context Composition**: $`T_1[\cdot] \circ T_2[\cdot] = T_1[T_2[\cdot]]`$ (sequential composition)

</div>

**Queue Typing Intuition**: A message queue contains outputs that have been sent but not yet received. The type context “remembers” these pending outputs.

**Example**: Queue: $`s : \langle 3\rangle`$ (value 3 waiting), Context: $`1!\langle\mathsf{nat}\rangle;\, [\cdot]`$, Combined: $`1!\langle\mathsf{nat}\rangle;\, T`$ (rollback: pretend output hasn’t happened).

This “rollback” technique lets us type runtime states using static types.

### Extended Typing Judgment

<div class="definition">

**Definition 6.2** (Runtime Typing Judgment).
``` math
\Gamma \vdash P \triangleright_{\tilde{s}} \Delta
```
The subscript $`\tilde{s}`$ tracks which session channels have associated queues.

**Meaning**: Under environment $`\Gamma`$, process $`P`$ with queues for channels $`\tilde{s}`$ has typing $`\Delta`$.

**Key Difference from Static Typing**:

- Static: $`\Gamma \vdash P \triangleright \Delta`$ (no queues)

- Runtime: $`\Gamma \vdash P \triangleright_{\tilde{s}} \Delta`$ (queues for $`\tilde{s}`$)

**Coherence of $`\tilde{s}`$**: For well-typed runtime states:

- Each channel in $`\tilde{s}`$ has exactly one queue

- Queue contents match the type context

- No dangling or duplicated queues

</div>

### Queue Typing Rules

<div class="mathpar">

(s_k : ) \_s_k : {\[\]@p}\_p

(s_k : ) \_s_k , :(T\[k!; \[\]\]@q) R

(s_k : ) \_s_k , :(T\[k!T’@p’; \[\]\]@q) R, :T’@p’

(s_k : l) \_s_k , :(T\[kl:\[\]\]@q) R

\_\_1 \_2 P Q \_\_1 \_2 ’

\_ ()P

</div>

### Runtime Typing Properties

<div class="proposition">

**Proposition 6.3** (Static/Runtime Equivalence (Proposition 5.1)). *For program phrases (no queues):
``` math
\Gamma \vdash P \triangleright \Delta \iff \Gamma \vdash P \triangleright_\emptyset \Delta \quad \text{(without [Subs])}
```
*

</div>

<div class="proposition">

**Proposition 6.4** (Queue Uniqueness (Proposition 5.2)). *If $`\Gamma \vdash P \triangleright_{s_1 \ldots s_m} \Delta`$ then:*

1.  *$`P`$ has exactly one queue at each $`s_i`$ ($`1 \leq i \leq m`$)*

2.  *No other queues occur free in $`P`$*

3.  *No queue is under any prefix*

</div>

**Rollback Principle**: Runtime typing “rolls back” queue contents into types.

- Runtime state: Process $`P \mid \mathrm{Queue}(s, [v_1, v_2])`$

- After rollback: Type includes $`k!\langle S_1\rangle;\, k!\langle S_2\rangle;\, [\text{actual local type}]`$

This makes subject reduction work: the queue’s effect is captured in types.

**Type Safety for Runtime**: Well-typed runtime states (including queues) reduce to well-typed states. Combined with communication safety, this ensures no message type mismatches, no lost messages, and no unhandled selections.

## Integration with Static Analysis

### Channel Analysis in CPG

<div class="pillarbox">

Add to Code Property Graph:

1.  **Session nodes**: represent session channels

2.  **Protocol edges**: connect session nodes per global type

3.  **Causality edges**: encode $`\prec^{II}`$, $`\prec^{IO}`$, $`\prec^{OO}`$ relations

4.  **Projection nodes**: local type for each participant

**CPG Node Types**:

- $`\mathsf{SessionNode}(\mathit{id}, \mathit{global\_type})`$

- $`\mathsf{ParticipantNode}(\mathit{id}, \mathit{participant}, \mathit{local\_type})`$

- $`\mathsf{PrefixNode}(\mathit{id}, \mathit{kind}, \mathit{channel}, \mathit{payload})`$

**CPG Edge Types**:

- $`\mathsf{SessionOf}(\mathit{prefix\_node}, \mathit{session\_node})`$

- $`\mathsf{ParticipantOf}(\mathit{participant\_node}, \mathit{session\_node})`$

- $`\mathsf{Causality}(\mathit{prefix\_node}, \mathit{prefix\_node}, \mathit{kind})`$ — II, IO, or OO

- $`\mathsf{Next}(\mathit{prefix\_node}, \mathit{prefix\_node})`$ — sequential composition

</div>

**Analysis Queries**:

**Deadlock Detection**:

    MATCH path = (p1:PrefixNode)-[:Causality*]->(p1)
    RETURN path  // cycle indicates potential deadlock

**Protocol Violation**:

    MATCH (p:ParticipantNode)-[:LocalType]->(t:Type)
    MATCH (proc:Process)-[:Implements]->(p)
    WHERE NOT conforms(proc, t)
    RETURN proc, t  // process doesn't follow local type

**Orphan Message**:

    MATCH (send:PrefixNode {kind: 'output'})-[:SessionOf]->(s:Session)
    WHERE NOT exists((recv:PrefixNode {kind: 'input'})-[:Matches]->(send))
    RETURN send  // output with no matching input

### Cross-Reference with Synthesis Sections

<div class="contributionbox">

**Section 5.3 (CPG Construction)**:

- Session nodes added as new vertex type

- Causality edges extend data dependency edges

- Protocol structure becomes queryable via graph traversal

**Section 6.4 (IFDS/IDE)**:

- Session type checking as IFDS problem

- Facts: (node, type_state) pairs

- Edges: type transitions at prefix nodes

**Section 8.1 (Taint Analysis)**:

- Session channels as additional taint propagation paths

- Delegation transfers taint along with capability

- Protocol-aware sanitization (type-directed)

**Part IX (Multi-Language)**:

- Global types describe cross-language protocols

- FFI boundaries require session type consistency

- Projection per language gives local implementation spec

**Part XI (Uncertainty)**:

- Incomplete session specifications $`\rightarrow`$ uncertainty

- Missing participants $`\rightarrow`$ coherence unknown

- Dynamic session creation $`\rightarrow`$ parametric analysis

</div>

### Language-Specific Channel Mappings

This section defines how channel operations in concrete languages map to the unified IR channel primitives (Part <a href="#part:ir-specification" data-reference-type="ref" data-reference="part:ir-specification">[part:ir-specification]</a>).

#### Go Channel Mappings

<div class="center">

| **Go Syntax** | **IR Statement** |
|:---|:---|
| `ch := make(chan T)` | $`\mathsf{SChanCreate}(\mathit{ch}, T, 0)`$ — Unbuffered (synchronous) |
| `ch := make(chan T, n)` | $`\mathsf{SChanCreate}(\mathit{ch}, T, n)`$ — Buffered with capacity $`n`$ |
| `ch <- v` | $`\mathsf{SSend}(\mathit{ch}, v)`$ — Send (may block) |
| `v := <-ch` | $`\mathsf{SRecv}(v, \mathit{ch})`$ — Receive (may block) |
| `v, ok := <-ch` | $`\mathsf{SRecvOk}(v, \mathit{ok}, \mathit{ch})`$ — With closed-channel check |
| `close(ch)` | $`\mathsf{SChanClose}(\mathit{ch})`$ — Close channel |
| `select {...}` | $`\mathsf{SSelect}([\ldots])`$ — Non-deterministic choice |

</div>

**Go Type Mappings**:

<div class="center">

| **Go Type** | **IR Type**                                      |
|:------------|:-------------------------------------------------|
| `chan T`    | $`\mathsf{TChannel}(T, \mathsf{Bidirectional})`$ |
| `chan<- T`  | $`\mathsf{TChannel}(T, \mathsf{SendOnly})`$      |
| `<-chan T`  | $`\mathsf{TChannel}(T, \mathsf{RecvOnly})`$      |

</div>

#### Rust Channel Mappings

<div class="center">

| **Rust Syntax (std::sync::mpsc)** | **IR Statement** |
|:---|:---|
| `let (tx, rx) = mpsc::channel()` | $`\mathsf{SChanCreate}(\mathit{ch}, T, \mathsf{UNBOUNDED})`$ |
| `let (tx, rx) = mpsc::sync_channel(n)` | $`\mathsf{SChanCreate}(\mathit{ch}, T, n)`$ |
| `tx.send(v).unwrap()` | $`\mathsf{SSend}(\mathit{tx}, v)`$ |
| `let v = rx.recv().unwrap()` | $`\mathsf{SRecv}(v, \mathit{rx})`$ |
| `let v = rx.try_recv()` | $`\mathsf{SRecvTry}(v, \mathit{rx})`$ |
| `drop(tx)` | $`\mathsf{SChanClose}(\mathit{tx})`$ — Implicit close |

</div>

**Rust Type Mappings**:

<div class="center">

| **Rust Type**              | **IR Type**                                 |
|:---------------------------|:--------------------------------------------|
| `Sender<T>`                | $`\mathsf{TChannel}(T, \mathsf{SendOnly})`$ |
| `Receiver<T>`              | $`\mathsf{TChannel}(T, \mathsf{RecvOnly})`$ |
| `(Sender<T>, Receiver<T>)` | $`\mathsf{TChannelPair}(T)`$                |

</div>

**Ownership Considerations**:

- $`\mathsf{Sender}\langle T\rangle`$: Can be `Clone`’d (multiple producers)

- $`\mathsf{Receiver}\langle T\rangle`$: Cannot be `Clone`’d (single consumer)

- Channel ownership: Sender owns send capability, Receiver owns recv capability

- Linearity: Each endpoint used linearly (no aliasing of recv end)

#### Python Channel Mappings

<div class="center">

| **Python Syntax (asyncio.Queue)** | **IR Statement** |
|:---|:---|
| `q = asyncio.Queue()` | $`\mathsf{SChanCreate}(q, \mathsf{Any}, \mathsf{UNBOUNDED})`$ |
| `q = asyncio.Queue(maxsize=n)` | $`\mathsf{SChanCreate}(q, \mathsf{Any}, n)`$ |
| `await q.put(v)` | $`\mathsf{SSend}(q, v)`$ with $`\mathsf{EAsync}`$ |
| `v = await q.get()` | $`\mathsf{SRecv}(v, q)`$ with $`\mathsf{EAsync}`$ |
| `await q.put(None)` | $`\mathsf{SSend}(q, \mathsf{None})`$ — Convention for “close” |

</div>

**Taint Considerations**:

- Pickled data through `multiprocessing.Queue`: taint propagates with serialization

- `asyncio.Queue`: taint propagates within same process

- Cross-process channels: require taint boundary analysis

#### TypeScript/JavaScript Channel Mappings

Note: JS/TS don’t have built-in channels, but common patterns include async iterators/generators, RxJS Observables, and Web Workers with `postMessage`.

<div class="center">

| **TypeScript Syntax (RxJS)** | **IR Statement** |
|:---|:---|
| `const subject = new Subject<T>()` | $`\mathsf{SChanCreate}(\mathit{ch}, T, 0)`$ — Unbuffered |
| `const subject = new ReplaySubject(n)` | $`\mathsf{SChanCreate}(\mathit{ch}, T, n)`$ — Buffered |
| `subject.next(v)` | $`\mathsf{SSend}(\mathit{ch}, v)`$ |
| `subject.subscribe(v => body)` | $`\mathsf{SRecv}(v, \mathit{ch})`$ in async loop |
| `subject.complete()` | $`\mathsf{SChanClose}(\mathit{ch})`$ |

</div>

#### Session Type Annotations

For languages supporting annotations/attributes, session types can be specified:

**Go** (via comments or build tags):

    // @session: !string; ?int; &{ok: !string, quit: end}
    func BuyerProtocol(ch chan interface{}) { ... }

**Rust** (via attributes):

    #[session_type("!String; ?i32; +{ok: !String, quit: end}")]
    async fn buyer_protocol(tx: Sender<...>, rx: Receiver<...>) { ... }

**TypeScript** (via decorators or JSDoc):

    /** @session !string; ?number; &{ok: !string, quit: end} */
    async function buyerProtocol(ch: Channel<...>): Promise<void> { ... }

**Python** (via type hints or decorators):

    @session_type("!str; ?int; &{ok: !str, quit: end}")
    async def buyer_protocol(ch: Channel) -> None: ...

These annotations enable: (1) Static protocol conformance checking, (2) Runtime protocol monitoring, (3) Documentation generation, (4) Cross-language protocol compatibility verification.

### Channel Analysis in Existing Analyses

The following code extends the IFDS dataflow framework (Section <a href="#sec:ifds-foundations" data-reference-type="ref" data-reference="sec:ifds-foundations">[sec:ifds-foundations]</a>) to handle channel operations. Each channel operation creates or transforms dataflow facts that propagate through the interprocedural analysis. This enables tracking tainted data as it flows through channels, monitoring session type state at each program point, and verifying ownership invariants.

<div class="fstarcode">

(\* Channel operations create dataflow facts that propagate through IFDS. This enables tracking of: - Tainted data through channels - Session type state at each program point - Channel aliasing and ownership \*)

(\* IFDS fact domain extended for channels \*) type channel_ifds_fact = \| ChanTainted : chan_id:nat -\> source:taint_source -\> channel_ifds_fact (\* Channel carries tainted data from source \*) \| ChanTypeState : chan_id:nat -\> state:local_session_type -\> channel_ifds_fact (\* Channel is at session type state \*) \| ChanOwned : chan_id:nat -\> owner:var_id -\> channel_ifds_fact (\* Channel endpoint owned by variable \*) \| ChanClosed : chan_id:nat -\> channel_ifds_fact (\* Channel has been closed \*) \| ChanDelegated : chan_id:nat -\> to_chan:nat -\> channel_ifds_fact (\* Channel capability was delegated through to_chan \*)

(\* Transfer function for channel operations \*) val channel_transfer : ir_stmt -\> set channel_ifds_fact -\> set channel_ifds_fact let channel_transfer stmt facts = match stmt with \| SChanCreate dst elem_ty buf_sz -\> (\* New channel: add ownership fact \*) let chan_id = fresh_chan_id () in Set.add (ChanOwned chan_id dst) facts

\| SSend ch v -\> (\* Send: propagate taint from value to channel \*) let chan_id = resolve_chan ch in let v_taint = get_taint v facts in Set.union facts (Set.map (fun t -\> ChanTainted chan_id t) v_taint)

\| SRecv dst ch -\> (\* Receive: propagate taint from channel to destination \*) let chan_id = resolve_chan ch in let chan_taint = Set.filter (fun f -\> match f with ChanTainted cid \_ -\> cid = chan_id \| \_ -\> false) facts in let dst_taint = Set.map (fun (ChanTainted \_ src) -\> VarTainted dst src) chan_taint in Set.union facts dst_taint

\| SChanClose ch -\> (\* Close: mark channel as closed \*) let chan_id = resolve_chan ch in Set.add (ChanClosed chan_id) facts

\| SChanDelegate ch delegated -\> (\* Delegate: transfer ownership, track delegation \*) let chan_id = resolve_chan ch in let del_id = resolve_chan delegated in Set.add (ChanDelegated del_id chan_id) facts

\| \_ -\> facts (\* Non-channel statements don’t affect channel facts \*)

</div>

**Key Type Signatures**: The `channel_ifds_fact` type captures the different kinds of facts that can be tracked for channels. `ChanTainted` records that a channel carries tainted data, `ChanTypeState` tracks the current session type state, and `ChanOwned`/`ChanClosed`/`ChanDelegated` track ownership transitions.

The following code extends taint analysis (Section <a href="#sec:taint-analysis" data-reference-type="ref" data-reference="sec:taint-analysis">[sec:taint-analysis]</a>) to handle channel-based communication. When tainted data is sent through a channel, the channel itself becomes tainted, and any values received from that channel inherit the taint. This models information flow through concurrent communication paths.

<div class="fstarcode">

(\* Tainted data sent through a channel taints all received values. This models information flow through concurrent communication. \*)

(\* Taint flow rules for channels \*) type channel_taint_rule = \| SendTaintsChannel : tainted_value -\> channel -\> channel_taint_rule (\* Sending tainted value taints the channel \*) \| RecvInheritsTaint : channel -\> recv_variable -\> channel_taint_rule (\* Receiving from tainted channel taints the variable \*) \| DelegationTransfersTaint : delegated_chan -\> via_chan -\> channel_taint_rule (\* Delegation transfers taint along with capability \*) \| ClosedChannelNoTaint : channel -\> channel_taint_rule (\* Closed channels don’t propagate new taint \*)

val apply_channel_taint : taint_state -\> ir_stmt -\> taint_state let apply_channel_taint state stmt = match stmt with \| SSend ch v when is_tainted state v -\> (\* Taint propagates: value -\> channel \*) add_channel_taint state ch (get_taint_sources state v)

\| SRecv dst ch when is_channel_tainted state ch -\> (\* Taint propagates: channel -\> variable \*) add_var_taint state dst (get_channel_taint_sources state ch)

\| SChanDelegate ch delegated when is_channel_tainted state delegated -\> (\* Taint propagates with delegation \*) add_channel_taint state ch (get_channel_taint_sources state delegated)

\| \_ -\> state

</div>

**Security Implication**: The rule `DelegationTransfersTaint` is particularly important—when a session capability is delegated, any taint associated with that session transfers with it. This prevents information laundering through delegation.

The following code extends ownership analysis (Section <a href="#sec:ownership-analysis" data-reference-type="ref" data-reference="sec:ownership-analysis">[sec:ownership-analysis]</a>) to handle channel endpoints. Channel endpoints in languages like Rust have explicit ownership semantics: sender endpoints can be cloned (shared ownership via reference counting), while receiver endpoints are unique (linear). This distinction is critical for detecting use-after-close bugs and ensuring proper resource cleanup.

<div class="fstarcode">

(\* Channel endpoints have ownership semantics: - Sender endpoints: can be cloned (shared ownership) - Receiver endpoints: unique ownership (linear) - Closing: requires ownership of endpoint \*)

(\* Ownership states for channel endpoints (extends Section 7.1.1 Camera) \*) type channel_ownership = \| OwnedSender : refcount:nat -\> channel_ownership (\* Sender can be cloned; tracks reference count \*) \| OwnedReceiver : channel_ownership (\* Receiver is unique (linear) \*) \| Borrowed : from:var_id -\> channel_ownership (\* Temporarily borrowed \*) \| Moved : channel_ownership (\* Ownership has been transferred \*)

(\* Channel ownership checking \*) val check_channel_ownership : ownership_context -\> ir_stmt -\> option ownership_error let check_channel_ownership ctx stmt = match stmt with \| SSend ch \_ -\> (\* Requires ownership or borrow of sender \*) if not (has_send_capability ctx ch) then Some (UseAfterMove ch) else None

\| SRecv \_ ch -\> (\* Requires ownership or borrow of receiver \*) if not (has_recv_capability ctx ch) then Some (UseAfterMove ch) else None

\| SChanClose ch -\> (\* Requires ownership (not just borrow) \*) if not (owns_endpoint ctx ch) then Some (InvalidClose ch) else None

\| SChanDelegate \_ delegated -\> (\* Moves ownership of delegated channel \*) if not (owns_endpoint ctx delegated) then Some (CannotDelegateUnowned delegated) else None

\| \_ -\> None

(\* Integration with Iris cameras (Section 7.1.1) \*) val channel_camera : channel_ownership -\> camera_element let channel_camera own = match own with \| OwnedSender n -\> Frac (1.0 /. float_of_int n) (\* Fractional for senders \*) \| OwnedReceiver -\> Exclusive () (\* Exclusive for receiver \*) \| Borrowed \_ -\> Frac 0.5 (\* Temporary borrow \*) \| Moved -\> Invalid (\* No ownership \*)

</div>

## Theoretical Reconciliation: Session Types and Outcome Logic

This chapter resolves fundamental theoretical tensions between classical session type theory **\[Honda98\]**, **\[Honda08\]** and the under-approximation framework of Outcome Logic. These tensions arise because session types provide **sound** guarantees (may reject valid programs) while Outcome Logic targets bug finding via **under-approximation**. We provide formal resolutions that enable both frameworks to coexist productively.

### Tension 1: Over-Approximation vs Under-Approximation

<div class="pillarbox">

**Session Types (Honda 1998, 2008)**:

- **Sound** type system: well-typed programs **never** violate protocols

- **Over-approximation**: may **reject** valid programs (false negatives for bugs)

- Guarantees: communication safety, session fidelity, progress

**Outcome Logic (Zilberstein 2023)**:

- **Under-approximation**: may **miss** bugs (false negatives for safety)

- Focus: **finding** bugs, not proving absence

- Guarantees: found bugs are **real** (0% false positives for manifest)

**Resolution**: Classify channel bugs by **detectability**.

</div>

<div class="definition">

**Definition 8.1** (Channel Bug Classification). **Manifest Channel Bugs** (0% False Positive Rate): Bugs detectable with **empty presumption** ($`\mathsf{emp}`$), independent of calling context. These correspond to Le 2022’s manifest errors.

1.  **Type Mismatch** — Send value of wrong type:
    ``` math
    [\mathsf{emp}]\; \mathsf{send}(k, 42)\; [k \text{ expects string};\, \mathsf{Er}]
    ```

2.  **Protocol Violation** — Wrong operation at current state:
    ``` math
    [\mathsf{emp}]\; \mathsf{send}(k, v)\; [\mathsf{session\_state}(k) = \mathsf{receiving};\, \mathsf{Er}]
    ```

3.  **Definite Deadlock** — All participants blocked forever:
    ``` math
    [\mathsf{emp}]\; P_1 \mid P_2 \mid \ldots \mid P_n\; [\forall i.\, \mathsf{blocked}(P_i);\, \mathsf{Er}]
    ```

4.  **Channel Linearity Violation** — Use after close/move:
    ``` math
    [\mathsf{emp}]\; \mathsf{close}(k);\, \mathsf{send}(k, v)\; [\mathsf{closed}(k);\, \mathsf{Er}]
    ```

5.  **Session Type Structural Mismatch**:
    ``` math
    [\mathsf{emp}]\; P \text{ at } G \upharpoonright p\; [G \upharpoonright p \text{ undefined or } P \text{ violates } G \upharpoonright p;\, \mathsf{Er}]
    ```

**Latent Channel Bugs** (Require Context/Schedule): Bugs requiring specific **precondition** or schedule to manifest. These are under-approximate findings with confidence $`< 100\%`$.

1.  **Potential Deadlock** under specific interleavings:
    ``` math
    [\mathsf{schedule} = \sigma]\; P_1 \mid P_2\; [\mathsf{blocked};\, \mathsf{Er}]
    ```

2.  **Race on Channel** under specific timing:
    ``` math
    [\mathsf{arrival\_order} = (m_1, m_2)]\; \mathsf{recv}(k)\; [\text{wrong message received};\, \mathsf{Er}]
    ```

3.  **Conditional Protocol Violation**:
    ``` math
    [x > 0]\; \mathsf{branch\_on}(x);\, \mathsf{send}(k, v)\; [\text{protocol error};\, \mathsf{Er}]
    ```

</div>

<div class="theorem">

**Theorem 8.2** (Session Type Violations Are Manifest (Theorem 14.7.1)). *If $`G`$ is a coherent global type and process $`P`$ has $`\Gamma \vdash P \triangleright \Delta`$ where $`\Delta(s) = \llbracket G \rrbracket`$ and $`P`$ violates a session type rule, then the violation is a **manifest bug** under Outcome Logic classification.*

</div>

<div class="proof">

*Proof Sketch.*

1.  Session type rules (Figure 7, Honda 2008) are **local** to each process

2.  Type errors (wrong sort, wrong prefix, wrong branch) are detectable from $`P`$’s code alone, without caller information

3.  Therefore ISL presumption is $`\mathsf{emp} \land \mathsf{true}`$ (Le 2022 Theorem 3.4)

4.  By True Positives Property: manifest error implies real bug

 ◻

</div>

**Consequence**: Session type violations detected statically have 0% false positive rate. This justifies **high** confidence for session type findings in Layer 6.

### Tension 2: Static vs Dynamic Participants

<div class="pillarbox">

**Honda 2008**: Participant count $`n`$ is **static** (compile-time)

- Global type $`G`$ has fixed set $`\mathrm{pid}(G) = \{1, 2, \ldots, n\}`$

- rule synchronizes **exactly** $`n`$ participants

- Projection $`G \upharpoonright p`$ is defined for each $`p \in \mathrm{pid}(G)`$

**Go/Rust/Real Programs**: Goroutines/threads created **dynamically**

- `for i := 0; i < n; i++ { go worker(ch) }`

- Number of workers determined at **runtime**

- Channel may be shared by unbounded number of participants

</div>

**Resolution Options**:

**Option 1: Bounded Model Checking** — Unroll dynamic participant creation to depth $`k`$ (typically $`k = 3`$–$`5`$).

**Option 2: Symbolic Participants** — Abstract over participant count using universal quantification:
``` math
\forall n \geq 1.\; G(n) = \mu t.\, \mathsf{Worker}_i \rightarrow \mathsf{Coordinator} : \mathit{ch}\langle\mathsf{result}\rangle.\, t \quad (i \in [1,n])
```

**Option 3: Annotation Requirement** — User specifies participant bounds via annotations.

**Option 4: Pattern Recognition (Recommended Default)** — Detect common patterns and apply known-safe protocols.

<div class="center">

| **Pattern** | **Global Type Template** |
|:---|:---|
| Fan-Out (1-to-N) | $`\mu t.\, \forall i \in [1,n].\, \mathsf{Master} \rightarrow \mathsf{Worker}_i : k\langle\mathsf{task}\rangle.\, t`$ |
| Fan-In (N-to-1) | $`\mu t.\, \exists i \in [1,n].\, \mathsf{Worker}_i \rightarrow \mathsf{Master} : k\langle\mathsf{result}\rangle.\, t`$ |
| Producer-Consumer | $`\mu t.\, \mathsf{Producer} \rightarrow \mathsf{Consumer} : k\langle\mathsf{item}\rangle.\, t`$ |
| Scatter-Gather | Fan-Out **then** Fan-In composition |
| Pipeline | $`\mathsf{Stage}_1 \rightarrow \mathsf{Stage}_2 \rightarrow \cdots \rightarrow \mathsf{Stage}_n`$ |
| Request-Response | $`\mathsf{Client} \rightarrow \mathsf{Server} : \mathsf{req}\langle Q\rangle.\, \mathsf{Server} \rightarrow \mathsf{Client} : \mathsf{resp}\langle R\rangle`$ |

</div>

**Implementation Strategy**:

1.  First attempt pattern recognition (fast, high confidence)

2.  If pattern found: instantiate known-safe global type

3.  If no pattern: fall back to bounded model checking with $`k = 3`$

4.  Allow user annotations to override/refine bounds

The following code implements automatic pattern detection from channel usage in the Code Property Graph. The algorithm counts senders and receivers to classify common communication patterns, then generates appropriate global types.

<div class="fstarcode">

(\* Communication patterns recognized from channel usage \*) type channel_pattern = \| FanOut : master:participant -\> channel_pattern \| FanIn : collector:participant -\> channel_pattern \| ProducerConsumer : channel_pattern \| RequestResponse : channel_pattern \| Pipeline : stages:list participant -\> channel_pattern \| UnrecognizedPattern : channel_pattern

(\* Detect pattern from sender/receiver counts \*) val detect_channel_pattern : cpg -\> chan_id -\> channel_pattern let detect_channel_pattern cpg ch = let senders = get_senders cpg ch in let receivers = get_receivers cpg ch in match (List.length senders, List.length receivers) with \| (1, n) when n \> 1 -\> FanOut (List.hd senders) \| (n, 1) when n \> 1 -\> FanIn (List.hd receivers) \| (1, 1) -\> if is_bidirectional cpg ch then RequestResponse else ProducerConsumer \| \_ -\> UnrecognizedPattern

(\* Generate global type from detected pattern \*) val pattern_to_global_type : channel_pattern -\> nat -\> global_type let pattern_to_global_type pattern n = match pattern with \| FanOut master -\> GRec "t" (fold_participants n (fun i acc -\> GMsg master (Worker i) 1 SortAny acc) (GVar "t")) \| FanIn collector -\> GRec "t" (GBranch collector (List.map (fun i -\> (label i, GMsg (Worker i) collector 1 SortAny GEnd)) (range 1 n))) \| ProducerConsumer -\> GRec "t" (GMsg Producer Consumer 1 SortAny (GVar "t")) \| RequestResponse -\> GMsg Client Server 1 SortAny (GMsg Server Client 1 SortAny GEnd) \| \_ -\> GEnd (\* Fallback: use bounded checking \*)

</div>

**Connection to Engineering**: This pattern-based approach enables practical analysis of dynamically-spawned goroutines and threads without requiring explicit annotations. The recognized patterns cover the majority of real-world concurrent communication idioms.

### Tension 3: Order Preservation

<div class="pillarbox">

**Honda 2008 Assumption**: TCP-like FIFO per channel

- Messages on **same** channel preserve sending order

- $`\prec^{OO}`$ (Output-Output) dependency exploits this

- $`s!m_1;\, s!m_2`$ guarantees $`m_1`$ arrives before $`m_2`$

**Reality**: Cross-channel messages have **no** ordering guarantee

- $`s!m_1;\, t!m_2`$ does **not** guarantee $`m_1`$ arrives before $`m_2`$

- Even same sender: different channels = no ordering

- Two Buyer Protocol **requires** separate channels for this reason

</div>

**Resolution**: Use Honda 2008’s **modular approach** (Section 6.1).

**Ordering Rules**:

1.  **Same-Channel Ordering** ($`\prec^{OO}`$ preserved): For prefixes $`n_1, n_2`$ with same sender $`p`$ and same channel $`k`$, if $`n_1 \prec n_2`$ then $`n_1 \prec^{OO} n_2`$.

2.  **Cross-Channel Independence**: For prefixes $`n_1, n_2`$ with different channels $`k_1 \neq k_2`$, $`\neg(n_1 \prec^{OO} n_2) \land \neg(n_2 \prec^{OO} n_1)`$.

3.  **Explicit Synchronization**: If ordering between $`k_1`$ and $`k_2`$ is required, add sync edge:
    ``` math
    G = p \rightarrow q : k_1\langle U_1\rangle.\, q \rightarrow r : \mathsf{sync}\langle\mathsf{ack}\rangle.\, p \rightarrow r : k_2\langle U_2\rangle.\, \ldots
    ```

**Analysis Implication**: When analyzing code, do **not** assume cross-channel ordering. Flag potential races where code assumes ordering without sync.

The following code implements a safety check that verifies cross-channel ordering assumptions are properly synchronized:

<div class="fstarcode">

(\* Verify that cross-channel message pairs are properly synchronized \*) val cross_channel_ordering_safe : global_type -\> bool let cross_channel_ordering_safe g = let prefixes = collect_prefixes g in let cross_channel_pairs = List.filter (fun (n1, n2) -\> channel_of n1 \<\> channel_of n2 && sender_of n1 = sender_of n2 && n1 ‘prefix_ordered‘ n2 ) (pairs prefixes) in (\* For each cross-channel pair, verify no ordering assumption \*) List.for_all (fun (n1, n2) -\> not (requires_ordering g n1 n2) \|\| has_sync_path g n1 n2 ) cross_channel_pairs

</div>

**Two Buyer Protocol Revisited**: The Two Buyer Protocol from **\[Honda08\]** uses **four** channels ($`b_1`$, $`b_2`$, $`b_2'`$, $`s`$) precisely because:

- Buyer$`_2`$ receives from Seller on $`b_2`$ (quote)

- Buyer$`_2`$ receives from Buyer$`_1`$ on $`b_2'`$ (contribution)

- Without separate channels, messages could arrive out of order

- This is **not** a workaround but **correct** protocol design

### Tension 4: Select Non-Determinism

<div class="pillarbox">

**Honda 2008**: Deterministic branching (**receiver** chooses)

- $`G = p \rightarrow q : k\{l_1 : G_1, l_2 : G_2\}`$

- Sender $`p`$ **selects** which label to send

- Receiver $`q`$ **branches** based on received label

- Selection is **internal choice** at sender

**Go select**: Non-deterministic (**scheduler** chooses)

    select {
      case msg := <-ch1: handle1(msg)
      case msg := <-ch2: handle2(msg)
    }

- Which case fires depends on message **arrival order**

- This is **external choice** resolved by environment

</div>

**Resolution**: Model select as **external choice on multiple channels**.

**Key Insight**: Go select is **not** the same as session type selection. It is better modeled as external choice where the environment (message arrival timing) determines which branch executes.

<div class="fstarcode">

type select_branch = channel : chan_id; direction : SendOrRecv; body : process; local_type : local_type;

type select_stmt = list select_branch

(\* Type check select statement \*) val check_select : gamma -\> select_stmt -\> delta -\> option delta let check_select g branches d = (\* All branches must be individually well-typed \*) let branch_results = List.map (fun br -\> match br.direction with \| Recv -\> let expected_type = LRecv br.channel (infer_sort br.body) br.local_type in check_process g br.body (update d br.channel expected_type) \| Send -\> let expected_type = LSend br.channel (infer_sort br.body) br.local_type in check_process g br.body (update d br.channel expected_type) ) branches in (\* If all succeed, combine results (external choice) \*) if List.for_all is_some branch_results then Some (merge_branch_deltas (List.map Option.get branch_results)) else None

</div>

<div class="theorem">

**Theorem 8.3** (Select Safety (Theorem 14.7.2)). *If process $`P`$ uses select over channels $`\{k_1, \ldots, k_n\}`$ where:*

1.  *Each channel $`k_i`$ has valid local type $`T_i`$ at $`P`$*

2.  *The select body $`B_i`$ for each case follows $`T_i`$*

3.  *All branches $`B_i`$ are type-safe continuations*

*Then $`P`$ is safe regardless of which branch the scheduler selects.*

</div>

The following code models how select statements can be encoded in global types as parallel composition (external choice):

<div class="fstarcode">

(\* Model select as external choice in global type \*) val encode_select : select_stmt -\> participant -\> global_type let encode_select branches self = let branch_globals = List.map (fun br -\> match br.direction with \| Recv -\> GMsg (get_sender br.channel) self br.channel (sort_of br) GEnd \| Send -\> GMsg self (get_receiver br.channel) br.channel (sort_of br) GEnd ) branches in GPar branch_globals (\* Parallel composition = external choice \*)

</div>

**Key Insight**: The `GPar` constructor represents parallel composition where exactly one branch executes (first message arrival wins). This differs from session type selection where the sender actively chooses.

**Bug Detection**: For `select { case x := <-ch1: ... case ch2 <- y: ... }`, if `ch1` and `ch2` connected to **same** remote, potential race. Analysis: warn if both branches communicate with same participant.

### Tension 5: Global Type Origin

<div class="pillarbox">

**Honda 2008 Assumption**: $`G`$ is **given** by programmer

- Development methodology: design $`G`$ first, implement to $`G`$

- Type checking: verify process $`P`$ conforms to $`G \upharpoonright p`$

- $`G`$ serves as contract/specification

**Reality**: Most code has **implicit** protocols

- Legacy code: no explicit protocol specification

- Quick prototypes: evolve protocol during development

- Third-party code: protocols undocumented

</div>

**Resolution**: Three-Tier Protocol Extraction.

**Tier 1: Explicit Annotation** (Highest Confidence, 100%) — User provides global type via annotation. Verification: Check code conforms to annotation.

**Tier 2: Pattern Inference** (Medium Confidence, 80–90%) — Detect common communication idioms automatically.

<div class="center">

| **Code Pattern** | **Inferred Global Type** |
|:---|:---|
| `ch <- x; y := <-ch` | $`p \rightarrow q : k\langle T\rangle.\, q \rightarrow p : k'\langle T'\rangle`$ |
| (send then recv = req-resp) |  |
| `for { ch <- data }` | $`\mu t.\, p \rightarrow q : k\langle T\rangle.\, t`$ |
| (loop send = streaming) |  |
| `switch msg.Type { case A: ... }` | $`p \rightarrow q : k\{A: G_A, B: G_B, \ldots\}`$ |
| (type switch = branching) |  |

</div>

**Tier 3: Local Type Extraction + Coherence Check** (Lower Confidence, 60–80%) — Extract local types from each participant, then check coherence.

**Algorithm**:

1.  For each goroutine/thread $`p`$, extract local type $`T_p`$ by:

    - Traversing channel operations in CFG order

    - Building type from send $`\rightarrow`$ $`!U`$, recv $`\rightarrow`$ $`?U`$, select $`\rightarrow`$ $`\&/\oplus`$

2.  Check if $`\{T_p@p\}`$ family is **coherent**:

    - Attempt to construct $`G`$ such that $`G \upharpoonright p = T_p`$ for all $`p`$

    - If construction fails, report potential protocol error

The following code implements the three-tier resolution strategy, returning both the inferred global type and a confidence level indicating which tier was used:

<div class="fstarcode">

(\* Source of global type with confidence \*) type global_type_source = \| ExplicitAnnotation : text:string -\> global_type_source \| PatternInferred : pattern:channel_pattern -\> global_type_source \| LocalExtracted : local_types:list (participant \* local_type) -\> global_type_source

(\* Resolve global type using three-tier strategy \*) val resolve_global_type : cpg -\> chan_id -\> (global_type \* float \* global_type_source) let resolve_global_type cpg ch = (\* Tier 1: Check for explicit annotation \*) match find_session_annotation cpg ch with \| Some annot -\> let g = parse_global_type annot in (g, 1.0, ExplicitAnnotation annot) \| None -\> (\* Tier 2: Attempt pattern inference \*) let pattern = detect_channel_pattern cpg ch in match pattern with \| UnrecognizedPattern -\> (\* Tier 3: Extract local types and check coherence \*) let participants = get_channel_participants cpg ch in let local_types = List.map (fun p -\> (p, extract_local_type cpg ch p)) participants in (match construct_global_from_locals local_types with \| Some g -\> (g, 0.7, LocalExtracted local_types) \| None -\> (GEnd, 0.0, LocalExtracted local_types)) (\* Incoherent! \*) \| \_ -\> let n = count_participants cpg ch in let g = pattern_to_global_type pattern n in (g, 0.85, PatternInferred pattern)

(\* Extract local type by traversing CFG \*) val extract_local_type : cpg -\> chan_id -\> participant -\> local_type let extract_local_type cpg ch p = let cfg = get_cfg cpg p in let chan_ops = filter_channel_ops cfg ch in fold_cfg_order chan_ops (fun acc op -\> match op with \| SSend \_ v -\> LSend ch (type_of v) acc \| SRecv dst \_ -\> LRecv ch (type_of dst) acc \| SSelect \_ cases -\> LSelect ch (List.map (fun (l, \_) -\> (l, LEnd)) cases) \| \_ -\> acc ) LEnd

</div>

**Connection to Engineering**: The confidence values (1.0, 0.85, 0.7, 0.0) directly inform the finding severity in SARIF output. Explicit annotations give confirmed bugs, pattern inference gives high-confidence warnings, and local extraction gives medium-confidence warnings.

### Integration with Outcome Logic Bug Classification

<div class="fstarcode">

(\* OL Triples for Channel Operations:

\<chan_created(k,T)\> send(k,v) \<v:T / msg_in_flight(k,v)\> After send, message v of type T is in the channel’s queue.

\<msg_in_flight(k,v)\> recv(k) \<received(k,v)\> After recv, if message was in flight, it’s now received.

\< closed(k)\> close(k) \<closed(k)\> After close on open channel, channel is closed.

\<closed(k)\> send(k,v) \<error: send_on_closed\> Send on closed channel is an error.

\<session_state(k) = T\> send(k,v) \<session_state(k) = advance(T,!v)\> Session type advances after send. \*)

type channel_postcondition = \| ChanOk : state:channel_state -\> channel_postcondition \| ChanError : error:channel_error -\> channel_postcondition

type channel_state = created : bool; closed : bool; msg_queue : list value; session_state : option local_type;

type channel_error = \| SendOnClosed : chan:nat -\> channel_error \| RecvOnClosed : chan:nat -\> channel_error \| TypeMismatch : expected:sort -\> actual:sort -\> channel_error \| ProtocolViolation : expected:local_type -\> actual:chan_op -\> channel_error \| Deadlock : blocked:list participant -\> channel_error

(\* Classification: manifest vs latent for channel bugs \*) val classify_channel_bug : channel_error -\> isl_presumption -\> bug_classification let classify_channel_bug err presumption = match err with \| SendOnClosed \_ \| RecvOnClosed \_ \| TypeMismatch \_ \_ -\> (\* These are manifest: detectable without caller context \*) if presumption = EmpPresumption then Manifest kind = channel_error_to_kind err; confidence = 1.0 else Latent kind = channel_error_to_kind err; required_context = presumption

\| ProtocolViolation \_ \_ -\> (\* Manifest if local type violation, latent if depends on message order \*) if is_structural_violation err then Manifest kind = ProtocolError; confidence = 1.0 else Latent kind = ProtocolError; required_context = MessageOrderContext

\| Deadlock blocked -\> (\* Manifest if ALL paths lead to deadlock, latent if schedule-dependent \*) if all_paths_deadlock blocked then Manifest kind = DefiniteDeadlock; confidence = 1.0 else Latent kind = PotentialDeadlock; required_context = ScheduleContext

</div>

### Integration with Security Analysis

Channels are conduits for both **data** and **control**. Security analysis must track taint propagation through channels, detect information flow violations, and identify channels as potential covert channels. Session types can serve as information flow policies, declaring the security level of data flowing through each channel.

The following code defines security labels for channels and implements taint propagation rules:

<div class="fstarcode">

(\* Security labels for channels \*) type security_level = High \| Low \| Unknown

type channel_security = chan_id : nat; data_level : security_level; (\* Security of data on channel \*) timing_level : security_level; (\* Security of timing information \*) participants : list (participant \* security_level); (\* Cleared participants \*)

(\* Taint propagation through channels \*) val propagate_channel_taint : taint_state -\> ir_stmt -\> taint_state let propagate_channel_taint state stmt = match stmt with \| SSend ch v -\> if is_tainted state v then mark_channel_tainted state ch (get_taint_source state v) else state

\| SRecv dst ch -\> if is_channel_tainted state ch then mark_var_tainted state dst (get_channel_taint_source state ch) else state

\| SChanDelegate ch delegated -\> if is_channel_tainted state delegated then mark_channel_tainted state ch (get_channel_taint_source state delegated) else state

\| \_ -\> state

(\* Information flow check: does channel violate policy? \*) type channel_ifc_violation = \| HighToLowLeak : high_chan:nat -\> low_receiver:participant -\> channel_ifc_violation \| UnclearedReceiver : chan:nat -\> required_level:security_level -\> actual_level:security_level -\> channel_ifc_violation \| TimingLeak : chan:nat -\> channel_ifc_violation

</div>

### Integration with Multi-Language Analysis

<div class="pillarbox">

Channels can cross language boundaries (FFI, IPC, RPC):

- Go channel to Rust mpsc via CGO

- Python multiprocessing.Queue to C extension

- TypeScript WebSocket to Go server

Session types must be **compatible** (not necessarily identical) at boundaries.

</div>

**Cross-Language Session Type Compatibility**: Two session types $`T_1`$ (language $`L_1`$) and $`T_2`$ (language $`L_2`$) are **compatible** at a boundary if there exists a subtyping relation:
``` math
T_1 \leq_{\mathrm{boundary}} T_2 \quad \text{or} \quad T_2 \leq_{\mathrm{boundary}} T_1
```

**Subtyping Across Boundaries** (extends Gay & Hole 2005):

- **Output is covariant**: Can send more specific type

- **Input is contravariant**: Can receive more general type

- **Selection is covariant in labels**: Can select fewer labels

- **Branching is contravariant in labels**: Can handle more labels

The following code implements boundary subtyping checking between session types from different languages:

<div class="fstarcode">

(\* Cross-language type mapping \*) type cross_lang_type_map = source_lang : language; target_lang : language; type_mappings : list (sort \* sort);

(\* Boundary subtyping: T1 \<=\_boundary T2 \*) val boundary_subtype : cross_lang_type_map -\> local_type -\> local_type -\> bool let rec boundary_subtype map t1 t2 = match (t1, t2) with \| (LSend k1 u1 cont1, LSend k2 u2 cont2) when k1 = k2 -\> sort_compat map u1 u2 && (\* Covariant \*) boundary_subtype map cont1 cont2

\| (LRecv k1 u1 cont1, LRecv k2 u2 cont2) when k1 = k2 -\> sort_compat map u2 u1 && (\* Contravariant! \*) boundary_subtype map cont1 cont2

\| (LSelect k1 branches1, LSelect k2 branches2) when k1 = k2 -\> (\* Covariant: t1 can select fewer labels \*) List.for_all (fun (l, t1_l) -\> List.mem_assoc l branches2 && boundary_subtype map t1_l (List.assoc l branches2) ) branches1

\| (LBranch k1 branches1, LBranch k2 branches2) when k1 = k2 -\> (\* Contravariant: t1 can handle more labels \*) List.for_all (fun (l, t2_l) -\> List.mem_assoc l branches1 && boundary_subtype map (List.assoc l branches1) t2_l ) branches2

\| (LEnd, LEnd) -\> true \| \_ -\> false

(\* Boundary guard for runtime checking when static verification fails \*) type boundary_guard = \| TypeGuard : expected:sort -\> boundary_guard \| ProtocolGuard : expected:local_type -\> boundary_guard

val needs_boundary_guard : lang_session_type -\> lang_session_type -\> bool let needs_boundary_guard t1 t2 = let map = get_type_map (lang_of t1) (lang_of t2) in not (boundary_subtype map (session_type_of t1) (session_type_of t2))

</div>

**Connection to Part IX**: This boundary subtyping integrates with the multi-language semantic framework (Part <a href="#part:multi-language" data-reference-type="ref" data-reference="part:multi-language"></a>). When static verification of boundary compatibility fails, runtime guards are inserted to check type and protocol compliance dynamically.

### Decidability Results

<div class="contributionbox">

**Decidable (Polynomial)**:

- Linearity checking: $`O(|G|^2)`$

- Coherence checking: $`O(n \times |G|^2)`$ where $`n`$ = participants

- Type inference for processes: $`O(|P| \times |G|)`$

- Projection $`G \upharpoonright p`$: $`O(|G|)`$

- Subtyping $`T_1 \leq T_2`$: $`O(|T_1| \times |T_2|)`$ for finite types

**Decidable (Exponential)**:

- Subtyping with recursive types: EXPTIME **\[GayHole05\]**

- Global type synthesis from locals: 2-EXPTIME in worst case

**Undecidable (in general)**:

- Deadlock detection: PSPACE-complete, undecidable for infinite state

- Liveness properties: undecidable for general processes

**Practical Approach**:

- Use bounded model checking for undecidable properties

- Exploit session type structure for efficiency

- Under-approximate when sound analysis times out

</div>

<div class="theorem">

**Theorem 8.4** (Linearity Decidability (Honda 2008 Prop 3.6)). *Linearity of a global type $`G`$ is decidable in $`O(|G|^2)`$ time.*

</div>

<div class="proof">

*Proof Sketch.*

1.  Unfold $`G`$ exactly once (linear in $`|G|`$)

2.  Check suppression conditions for each prefix pair

3.  At most $`|G|^2`$ prefix pairs to check

 ◻

</div>

<div class="theorem">

**Theorem 8.5** (Coherence Decidability (Honda 2008 Thm 4.3)). *Coherence of $`G`$ is decidable in $`O(n \times |G|^2)`$ time.*

</div>

<div class="proof">

*Proof Sketch.*

1.  Check linearity: $`O(|G|^2)`$

2.  For each participant $`p`$ ($`n`$ total):

    - Compute projection $`G \upharpoonright p`$: $`O(|G|)`$

    - Check projection is defined: $`O(|G|)`$

3.  Total: $`O(|G|^2) + n \times O(|G|) = O(n \times |G|^2)`$

 ◻

</div>

<div class="theorem">

**Theorem 8.6** (Type Inference Decidability (Honda 2008 Thm 4.6)). *Given annotated program phrase $`P`$ and $`\Gamma`$, deciding if $`\exists\Delta.\, \Gamma \vdash P \triangleright \Delta`$ is decidable, and such $`\Delta`$ can be computed.*

</div>

<div class="proof">

*Proof Sketch.*

1.  Type rules (Figure 7, Honda 2008) are syntax-directed

2.  Each rule determines unique typing for subterms

3.  Traverse $`P`$ once, applying rules bottom-up

 ◻

</div>

**Deadlock Decidability**:

- For **single session** with finite recursive types: Progress is guaranteed by typing (Theorem 5.12, Honda 2008). Therefore: no deadlock analysis needed — types ensure progress.

- For **multiple interleaved sessions**: Progress requires “simple” and “well-linked” conditions. Checking these is decidable but may require exploring execution tree. PSPACE-complete in general.

The following code provides a practical algorithm that combines these decidability results with the tier-based resolution strategy:

<div class="fstarcode">

(\* Combined channel analysis using decidability results \*) type channel_analysis_result = global_type : global_type; is_linear : bool; is_coherent : bool; session_typed : option bool; deadlock_free : option bool;

val analyze_channel_properties : cpg -\> chan_id -\> channel_analysis_result let analyze_channel_properties cpg ch = let (g, confidence, source) = resolve_global_type cpg ch in global_type = g; is_linear = check_linearity g; (\* O(\|G\|^2), always run \*) is_coherent = check_coherence g; (\* O(n\*\|G\|^2), always run \*) session_typed = if confidence \> 0.7 then Some (check_process_conformance cpg g) (\* O(\|P\|\*\|G\|), run if confident \*) else None; deadlock_free = if is_simple cpg && is_well_linked cpg then Some true (\* Type system guarantees \*) else bounded_deadlock_check cpg 10; (\* Bounded check, depth 10 \*)

</div>

**Connection to Engineering**: This algorithm runs linearity and coherence checks unconditionally (fast, polynomial), but gates session type conformance on confidence level. Deadlock checking uses the type-theoretic guarantee when applicable, falling back to bounded model checking otherwise.

### F\* Theorem: Manifest Bug Classification

This section presents the main theoretical result connecting session type theory with Outcome Logic: session type violations are **manifest bugs** with 0% false positive rate. This justifies high confidence for session type findings in the analysis output.

The theorem relies on two key observations: (1) session type rules are **local** to each process—they examine only the process structure, not the calling context; and (2) protocol violations are therefore detectable with an empty ISL presumption ($`\mathsf{emp}`$), which by Le 2022’s True Positives Property implies the bug is manifest.

<div class="fstarcode">

(\* This theorem establishes that session type violations are MANIFEST bugs under Outcome Logic, meaning they have 0

(\* Prerequisites \*) assume type process : Type assume type global_type : Type assume type gamma : Type (\* Sorting context \*) assume type delta : Type (\* Typing context \*) assume type participant : Type

(\* Typing judgment: Gamma \|- P \|\> Delta \*) assume val typing_judgment : gamma -\> process -\> delta -\> Type

(\* Coherent global type \*) assume val coherent : global_type -\> bool

(\* Encode global type as family of local types \*) assume val encode_global : global_type -\> delta

(\* Protocol violation predicate \*) assume val protocol_violation : process -\> global_type -\> bool

(\* Manifest bug predicate (from Section 12.3) \*) assume val manifest_bug : process -\> Type

(\* True Positives Property (Le 2022 Theorem 3.4) \*) assume val true_positives_property : t:isl_triple -\> Lemma (requires t.presumption == EmpPresumption / satisfiable t.postcondition) (ensures manifest_error t.command)

(\* MAIN THEOREM: Channel Bugs Are Manifest \*) (\* \* Theorem: Session Type Violations Are Manifest Bugs \* \* If: \* 1. G is a coherent global type \* 2. Process P is well-typed under Gamma with Delta \* 3. Delta at session s encodes G \* 4. P violates the protocol specified by G \* \* Then: \* The violation is a MANIFEST bug (0 \*) val channel_manifest_bugs : p:process -\> gt:global_type -\> g:gamma -\> d:delta -\> s:nat -\> Lemma (requires coherent gt / typing_judgment g p d / delta_at_session d s == encode_global gt / protocol_violation p gt) (ensures manifest_bug p)

let channel_manifest_bugs p gt g d s = (\* Step 1: Session type rules are LOCAL to process p \*) assert (local_checkable (typing_judgment g p d));

(\* Step 2: Protocol violation is detectable from P alone \*) let violation_location = find_violation_site p gt in assert (violation_in_process violation_location p);

(\* Step 3: Construct ISL triple with emp presumption \*) let isl = presumption = EmpPresumption; command = p; postcondition = protocol_error_formula violation_location; in

(\* Step 4: Postcondition is satisfiable (violation exists) \*) assert (satisfiable isl.postcondition);

(\* Step 5: Apply True Positives Property (Le 2022 Theorem 3.4) \*) true_positives_property isl;

(\* Step 6: Conclude: manifest bug \*) ()

</div>

**Corollaries**:

1.  **Type mismatch is manifest**: If expected $`\neq`$ actual sort, it’s a manifest bug.

2.  **Definite deadlock is manifest**: If all paths lead to deadlock with no progress possible, it’s a manifest bug.

3.  **Linearity violation is manifest**: Use-after-close or use-after-move is a manifest bug.

**Integration with Layer 6 Confidence Levels**: The following code maps channel bug classifications to confidence levels for SARIF output. Manifest bugs (type mismatch, definite deadlock, linearity violation) are reported as confirmed bugs with “error” severity. Latent bugs (potential deadlock, race conditions) are reported as conditional warnings requiring further investigation.

<div class="fstarcode">

val session_finding_confidence : channel_bug_class -\> confidence_level let session_finding_confidence bug = match bug with \| ChannelManifest kind -\> (\* Manifest bugs have HIGH confidence — 0 match kind with \| ChanTypeMismatch \_ \_ \_ -\> ConfirmedBug (\* Static type error \*) \| ChanProtocolViolation \_ \_ -\> HighConfBug (\* Protocol state error \*) \| ChanDefiniteDeadlock \_ -\> ConfirmedBug (\* All paths deadlock \*) \| ChanLinearityViolation \_ \_ -\> ConfirmedBug (\* Ownership error \*) \| ChanStructuralMismatch \_ \_ -\> HighConfBug (\* Structure mismatch \*)

\| ChannelLatent kind -\> (\* Latent bugs have LOWER confidence — require context \*) match kind with \| ChanPotentialDeadlock \_ -\> ConditionalBug (\* Schedule-dependent \*) \| ChanRaceCondition \_ -\> TrueAlarm (\* Timing-dependent \*) \| ChanConditionalViolation \_ -\> ConditionalBug (\* Path-dependent \*)

(\* Final classification for SARIF output \*) type sarif_channel_finding = rule_id : string; level : string; (\* "error" \| "warning" \| "note" \*) message : string; location : source_location; confidence : float; (\* 0.0 to 1.0 \*)

val to_sarif : channel_bug_class -\> source_location -\> sarif_channel_finding let to_sarif bug loc = let conf = session_finding_confidence bug in rule_id = channel_bug_rule_id bug; level = if conf = ConfirmedBug \|\| conf = HighConfBug then "error" else "warning"; message = channel_bug_message bug; location = loc; confidence = confidence_to_float conf;

</div>

**Connection to Engineering**: This mapping directly determines how channel analysis findings appear in IDE integrations and CI/CD pipelines. Confirmed bugs (0.95–1.0 confidence) break builds; high-confidence warnings (0.8–0.95) require review; conditional findings (0.5–0.8) are informational.

#### Part XIV Summary

This part established the theoretical and practical foundations for channel analysis in the synthesis framework:

- **Binary Session Types** (Section <a href="#sec:binary-session-types" data-reference-type="ref" data-reference="sec:binary-session-types">[sec:binary-session-types]</a>): Formalized Honda 1998’s type system for structured communication, including delegation.

- **Global Types and Projection** (Sections <a href="#sec:global-types" data-reference-type="ref" data-reference="sec:global-types">[sec:global-types]</a>–<a href="#sec:projection-algorithm" data-reference-type="ref" data-reference="sec:projection-algorithm">4.3</a>): Defined multiparty session types following Honda 2008, with causality analysis and coherence conditions.

- **Theoretical Reconciliation** (Sections <a href="#sec:tension-participants" data-reference-type="ref" data-reference="sec:tension-participants">8.2</a>–<a href="#sec:tension-origin" data-reference-type="ref" data-reference="sec:tension-origin">8.5</a>): Resolved fundamental tensions between session type theory and practical engineering (dynamic participants, message ordering, select non-determinism, implicit protocols).

- **Integration with Existing Analyses** (Sections <a href="#sec:channel-existing-analyses" data-reference-type="ref" data-reference="sec:channel-existing-analyses">7.4</a>–<a href="#sec:channel-multilang" data-reference-type="ref" data-reference="sec:channel-multilang">8.8</a>): Extended IFDS, taint analysis, ownership tracking, and multi-language boundaries for channel operations.

- **Manifest Bug Theorem** (Section <a href="#sec:manifest-theorem" data-reference-type="ref" data-reference="sec:manifest-theorem">8.10</a>): Proved that session type violations are manifest bugs under Outcome Logic, justifying high confidence for channel analysis findings.

The key engineering insight is that session types provide a **compositional** approach to verifying concurrent programs: local type checking at each participant guarantees global protocol compliance, enabling scalable analysis of complex distributed systems.

## Paper Priority Matrix

This appendix provides a prioritized reference of foundational papers organized by contribution category, with priority scores indicating their importance to the brrr-machine implementation.

### Priority Matrix

| **Paper** | **Priority** | **Category** | **Key Contribution** |
|:---|:--:|:---|:---|
| **Paper** | **Priority** | **Category** | **Key Contribution** |
| *Continued on next page* |  |  |  |
| Cousot 1977 | 10 | Foundation | Abstract interpretation |
| Yamaguchi 2014 | 10 | Representation | Code Property Graph |
| Reps 1995 | 9 | Algorithm | IFDS |
| Reps 1997 | 9 | Algorithm | CFL-reachability |
| Andersen 1994 | 9 | Pointer | Inclusion-based |
| Steensgaard 1996 | 8 | Pointer | Unification-based |
| **Lattner 2007 (DSA)** | **9** | **Pointer** | **Unification + context-sensitivity + heap cloning for C/C++ \> 100K LOC** |
| Moggi 1991 | 9 | Effects | Monads |
| Plotkin 2003/2009 | 9 | Effects | Algebraic effects |
| Leijen 2014/2017 | 9 | Effects | Row polymorphism |
| Girard 1987 | 9 | Types | Linear logic |
| Reynolds 2002 | 9 | Types | Separation logic |
| Jung 2018 (Iris) | 9 | Types | Iris framework, higher-order ghost state, cameras, view shifts, step-indexed semantics |
| **Muller 2016 (Viper)** | **9** | **Verification** | **IVL for permission-based reasoning, magic wands, quantified permissions** |
| **Mulder 2022 (Diaframe)** | **9** | **Verification** | **Automated Iris proofs, bi-abduction with postponed existentials** |
| Denning 1977 | 9 | Security | Information flow |
| Livshits 2005 | 9 | Security | Taint analysis |
| Tripp 2009 | 9 | Security | Thin slicing |
| Matthews 2007 | 9 | Multi-lang | Boundary semantics |
| Hammer 2014 | 9 | Incremental | Adapton |
| Ferrante 1987 | 9 | Representation | PDG |
| Horwitz 1990 | 9 | Representation | SDG |
| Weiser 1984 | 8 | Representation | Slicing |
| Sridharan 2005 | 8 | Pointer | Demand-driven |
| Smaragdakis 2011 | 8 | Pointer | Datalog |
| Calcagno 2009 | 9 | Pointer | Bi-abduction |
| Aiken 1999 | 9 | Types | Set constraints |
| Cousot 1992 | 9 | Foundation | Widening |
| Goguen 1992 | 8 | Multi-lang | Institutions |
| Wagner 1998 | 8 | Incremental | Parsing |
| **Zilberstein 2023** | **10** | **Foundation** | **Outcome Logic (replaces IL)** |
| **Kang 2017** | **10** | **Memory Model** | **Promising Semantics (fixes C11 thin-air bug)** |
| **Lee 2020** | **10** | **Memory Model** | **Promising 2.0 (capped memory, global opts, fixes ARMv8)** |
| **Podkopaev 2019** | **10** | **Memory Model** | **IMM intermediate model (O(n+m) compilation proofs, 33K Coq)** |
| **Leroy 2009 (CompCert)** | **10** | **Verification** | **Verified C compiler, semantics preservation proofs, simulation relations** |
| O’Hearn 2020 | 8 | Foundation | Incorrectness Logic (historical) |
| Patterson 2022 | 9 | Multi-lang | Realizability models |
| **King 1976** | **9** | **Algorithm** | **Symbolic execution, path conditions** |
| **Kozen 1981** | **8** | **Foundation** | **Probabilistic semantics, Theorem 6.1** |
| **Cousot 2012** | **8** | **Foundation** | **Probabilistic AI, three abstraction axes** |
| **Bruni 2023 (LCL)** | **9** | **Foundation** | **Local Completeness Logic, unified over/under approx** |
| **Le 2022 (Pulse-X)** | **10** | **Foundation** | **ISL, Manifest/Latent classification, True Positives Property** |
| **Vanegue 2025** | **9** | **Algorithm** | **Non-termination proving, Pulse-infinity** |
| Crary 1999 | 9 | Types | Capability multiplicities |
| Xi 1999 | 8 | Types | Dependent ML / constraint domains |
| Watt 2018 | 8 | Verification | Verified interpreter pattern |
| **Disselkoen 2019 (MS-Wasm)** | **8** | **Memory Safety** | **Progressive memory safety for WebAssembly** |
| **Perrone & Romano 2024** | **7** | **Security Survey** | **Comprehensive WebAssembly security review (121 papers)** |
| **Rupta/Li 2024** | **9** | **Pointer** | **Stack filtering, Rust MIR analysis, on-the-fly CG** |
| **Sagiv 2002 (TVLA)** | **9** | **Foundation** | **Three-valued logic, shape analysis, focus/coerce** |
| **Distefano 2006** | **8** | **Shape Analysis** | **Symbolic heaps, junk predicate, canonicalization** |
| **Pnueli 1977** | **8** | **Temporal Logic** | **G, F, $`\leadsto`$, U operators, P1/P2 liveness principles** |
| **Batty 2011** | **10** | **Memory Model** | **C11 semantics, coherence axioms, DRF-SC** |
| **VeriFFI 2025** | **8** | **FFI** | **Representation predicates, GC-isomorphism** |
| **Furr & Foster 2008** | **8** | **FFI** | **Multilingual type inference, representational types** |
| **Patterson & Ahmed 2022** | **9** | **Multi-lang/IR** | **Semantic IR via realizability models, convertibility relations** |
| **Strom & Yemini 1986** | **9** | **Typestate** | **Original typestate concept, state machine semantics** |
| **Boyapati 2003** | **9** | **Ownership** | **Owner-as-dominator discipline, ownership types** |
| **Bierhoff 2007** | **9** | **Ownership** | **5 access permissions (unique/full/share/immutable/pure)** |
| **Siek 2006** | **8** | **Types** | **Gradual typing, non-transitive consistency relation** |
| **Garcia 2016 (AGT)** | **9** | **Types** | **Abstracting Gradual Typing: gradual types as abstract interpretations** |
| **Honda 1998** | **9** | **Session Types** | **Binary session types, duality, linear channel usage** |
| **Trabish 2018** | **9** | **Symbolic Exec** | **Chopped SE, skip regions, lazy recovery** |
| **Honda 2008** | **9** | **Session Types** | **Multiparty asynchronous session types, global types, projection** |
| **Sui & Xue 2016 (SVF)** | **9** | **Value-Flow** | **Sparse value-flow analysis, Memory SSA, SVFG construction** |
| **Sui, Ye & Xue 2012** | **9** | **Value-Flow** | **Full-sparse value-flow, source-sink reachability** |
| **Chow et al. 1996** | **8** | **Memory SSA** | **Memory SSA foundation, mu/chi annotations** |
| **Li et al. 2020 (ZIPPER)** | **9** | **Pointer** | **Selective context sensitivity, precision flow graph** |
| **Huang et al. 2023 (JARVIS)** | **9** | **Call Graph** | **Python call graph via Function Type Graph, C3 MRO** |
| **Agat 2000** | **7** | **Security** | **Timing channel analysis, timing-sensitive noninterference** |
| **Guarnieri et al. 2020 (SPECTECTOR)** | **8** | **Security** | **Speculative Non-Interference (SNI)** |
| **Almeida et al. 2016 (CT-Verif)** | **8** | **Security** | **Constant-time verification via product programs** |
| **Cadar et al. 2008 (KLEE)** | **8** | **Testing** | **KLEE symbolic execution engine, constraint optimization** |
| **Clarke, Emerson, Sistla 1986** | **7** | **Verification** | **CTL model checking, branching-time temporal logic** |
| **Naeem et al. 2010** | **7** | **Algorithms** | **Practical IFDS extensions and optimizations** |
| **Russo & Sabelfeld 2006** | **7** | **Security** | **Dynamic monitors for concurrent IFC** |
| **Sabelfeld & Myers 2003** | **8** | **Security** | **Comprehensive language-based IFC survey** |
| **Sabelfeld & Sands 2000** | **7** | **Security** | **Probabilistic noninterference** |
| **Godefroid et al. 2005 (DART)** | **8** | **Testing** | **Directed Automated Random Testing, concolic execution origins** |
| **Sen et al. 2005 (CUTE)** | **8** | **Testing** | **Concolic testing foundations, pointer constraint separation** |
| **Yun et al. 2018 (QSYM)** | **9** | **Testing** | **Optimistic concolic execution, native instrumentation** |
| **Pradel & Sen 2018 (DeepBugs)** | **8** | **ML/Bug Detection** | **Name-based bug detection via word2vec embeddings** |
| **Smith & Volpano 1998** | **7** | **Security** | **Type-based secure information flow** |
| **Spath et al. 2019 (SPDS)** | **9** | **Algorithm** | **Synchronized pushdown systems, WPDS encoding** |
| **Conrado et al. 2025 (MCFL)** | **9** | **Algorithm** | **Multiple context-free language reachability, d-MCFL hierarchy** |

### Collection 2 Priority Justifications

Bierhoff 2007 (9/10)  
Critical for modular analysis with aliasing. The 5-permission system (unique, full, share, immutable, pure) directly fills Gap D.1 for library modeling and enables precise resource tracking where current `ownership_state` is too simple. Frame-based inheritance handling addresses OOP analysis needs.

Siek 2006 (8/10)  
Essential for Python/JavaScript analysis. Type consistency (non-transitive) provides soundness for boundary checking in Section 9.1.2. Cast insertion algorithm enables systematic boundary term generation. Pay-as-you-go semantics allows efficient code generation for known types.

Garcia 2016 AGT (9/10)  
Principled foundation connecting gradual typing to abstract interpretation. Shows gradual types form a Galois connection: $`\gamma(?) = \text{all types}`$, consistency = non-empty intersection. **Critical:** Derives non-transitivity mathematically (not stipulated). Evidence semantics enable precise blame tracking at multi-language boundaries.

Honda 1998 (9/10)  
Foundation for binary session types. Establishes type discipline for structured communication with duality (send/recv correspondence), linearity (channels used exactly once per direction), and progress (no deadlock in single session). Essential for Go channel analysis and Rust mpsc.

Honda 2008 (9/10)  
Extends binary session types to multiparty asynchronous sessions. Global types describe complete protocol scenarios; projection extracts local types for each participant. Critical for microservice protocol analysis and distributed system verification.

Sui & Xue 2016 (9/10)  
Alternative to IFDS for source-sink reachability problems. Sparse value-flow representation is more efficient when memory regions $`R \ll`$ dataflow domain $`D`$. Memory SSA ($`\mu/\chi`$ annotations) precisely tracks address-taken variables.

Li et al. 2020 ZIPPER (9/10)  
Selective context sensitivity for pointer analysis. Identifies precision-critical methods via three value-flow patterns (direct, wrapped, unwrapped). Achieves 98.8% precision of full 2obj analysis with 3.4$`\times`$ speedup.

Lattner, Lenharth, Adve 2007 DSA (9/10)  
Scalable pointer analysis for large C/C++ codebases (100K–355K LOC). Combines Steensgaard-style unification $`O(n \cdot \alpha(n))`$ with context-sensitivity and heap cloning to achieve precision comparable to Andersen. Linux kernel analyzed in 3.1 seconds, $`<46`$MB memory.

Huang et al. 2023 JARVIS (9/10)  
Critical for Python call graph construction. Function Type Graph (FTG) provides flow-sensitive type inference with strong updates. C3 linearization for MRO handles Python’s multiple inheritance. 84% higher precision than flow-insensitive approaches.

Spath et al. 2019 SPDS (9/10)  
Solves combined context+field sensitivity via synchronized pushdown systems. Key insight: encode field access patterns via pushdown automata, avoiding exponential access-path enumeration. Achieves 64–83$`\times`$ speedups on DaCapo benchmarks.

Conrado et al. 2025 MCFL (9/10)  
First decidable analysis for interleaved Dyck reachability via Multiple Context-Free Languages. The d-MCFL hierarchy provides principled underapproximations with $`O(n^{2d})`$ complexity and SETH lower bounds proving tightness.

Muller et al. 2016 Viper (9/10)  
Verification infrastructure for permission-based reasoning. Provides permission-native intermediate language (IVL) with first-class support for `acc(e.f)`, fractional/symbolic permissions, magic wands ($`A \mathbin{-\!\!*}B`$), quantified permissions.

Mulder et al. 2022 Diaframe (9/10)  
Automated Iris proofs with foundational soundness. Key innovation: goal-directed proof search inspired by linear logic programming. Bi-abduction with <span class="smallcaps">Postponed</span> existentials (critical for invariants). 10$`\times`$ less manual proof than raw Iris.

Furr & Foster 2008 (8/10)  
Practical FFI type inference for OCaml-C and Java-JNI boundaries. Introduces *representational types* that model C’s low-level view of high-level data. GC safety analysis via effect annotations. Validated: found 24 errors in OCaml benchmarks, 156 errors in JNI benchmarks.

Patterson & Ahmed 2022 (9/10)  
Foundational framework for semantic soundness of language interoperability via compilation. Key insight: interoperability works by compiling both languages to a shared target, with *realizability models* $`\mathcal{V}\llbracket\tau\rrbracket`$ defining what target terms “behave as” source type $`\tau`$.

Leroy 2009 CompCert (10/10)  
First realistic verified compiler proving semantics preservation for C to assembly. **Critical** for analysis correctness: if source analysis proves safety AND compilation is verified, then executable is safe. The 42K lines Coq proof introduces simulation relations as the standard proof technique.

**Note:** Yamaguchi 2014 (CPG), Steensgaard 1996, Reynolds 2002, and Leijen 2014 from Collection 2 were already integrated into the main synthesis and appear above with their original priorities.

## Language Configuration Table

This appendix provides a reference table of safety guarantees provided by each supported language’s type system and runtime.

<div class="adjustbox">

max width=

<div id="tab:language-safety">

| **Language** | **MemSafe** | **NullSafe** | **TypeSafe** | **RaceFree** | **LeakFree** | **Memory** | **Types** |
|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| Rust | $`\checkmark`$ | $`\checkmark`$ | $`\checkmark`$ | $`\checkmark`$ | $`\checkmark`$ | Owned | Static |
| Haskell | $`\checkmark`$ | $`\checkmark`$ | $`\checkmark`$ | $`\checkmark`$ | $`\checkmark`$ | GC | Static |
| Java | $`\checkmark`$ | $`\times`$ | $`\checkmark`$ | $`\times`$ | $`\checkmark`$ | GC | Static |
| Go | $`\checkmark`$ | $`\times`$ | $`\checkmark`$ | $`\times`$ | $`\checkmark`$ | GC | Static |
| Python | $`\checkmark`$ | $`\times`$ | $`\times`$ | $`\checkmark`$$`^*`$ | $`\checkmark`$ | GC | Dynamic |
| JavaScript | $`\checkmark`$ | $`\times`$ | $`\times`$ | $`\checkmark`$$`^*`$ | $`\checkmark`$ | GC | Dynamic |
| C | $`\times`$ | $`\times`$ | $`\times`$ | $`\times`$ | $`\times`$ | Manual | Static |
| C++ | $`\times`$ | $`\times`$ | $`\times`$ | $`\times`$ | $`\times`$ | Manual | Static |
| Swift | $`\checkmark`$ | $`\checkmark`$ | $`\checkmark`$ | $`\times`$ | $`\checkmark`$ | RC | Static |

Language Safety Properties

</div>

</div>

$`^*`$Single-threaded/GIL

###### Legend.

- **MemSafe**: Memory safety (no buffer overflows, use-after-free)

- **NullSafe**: Null safety (no null pointer dereferences by construction)

- **TypeSafe**: Type safety (no type confusion at runtime)

- **RaceFree**: Data race freedom (no concurrent access to shared mutable state)

- **LeakFree**: Resource leak prevention (automatic cleanup)

- **Memory**: Memory management model (Owned/GC/Manual/RC)

- **Types**: Type system (Static/Dynamic)

## Glossary

This appendix provides definitions for key terms used throughout the synthesis document.

### Core Concepts

Abstract Interpretation  
Computing sound approximations of program behavior.

Abstracting Gradual Typing (AGT)  
Garcia 2016. Framework showing gradual types are abstract interpretations of static type sets. $`\gamma(?) = \text{all types}`$. Consistency = non-empty set intersection. Provides Galois connection foundation for gradual typing. See Section 9.1.2.

Affine Typing  
Substructural type system requiring values be used *at most once* (can drop without use). Rust uses affine, not strictly linear. See RustBelt/Iris.

Backsubstitution  
DeepPoly technique (Singh 2019). Recursively substitute polyhedral constraints backward through network layers until only input variables remain. Enables relational reasoning without exponential constraint explosion. Key to DeepPoly’s precision. See Section 2.1.7c.

Bi-Abduction  
Calcagno 2009. Given $`p`$ and $`q`$, find anti-frame $`M`$ and frame $`F`$ such that $`p * M \vdash q * F`$. Enables compositional shape analysis.

Binary Session Type  
Honda 1998. Type discipline for structured two-party communication. Types describe sequences of send/receive actions, selections/branches, and delegation. Duality principle ensures compatible communication. See Section 14.0.

Bounded Unrolling  
Under-approximate loop analysis. Unroll $`k`$ times (typically $`k=3`$), then cut off. Sound for bug finding.

Camera  
Step-indexed partial commutative monoid (from Iris).

CFL-Reachability  
Graph reachability where valid paths form a context-free language.

Causality Analysis  
Honda 2008. Analysis of dependency relations (II, IO, OO) between communication prefixes. Determines ordering constraints for safe multiparty interaction.

Channel Effect  
Effect type for communication operations (<span class="smallcaps">ESend</span>, <span class="smallcaps">ERecv</span>, <span class="smallcaps">EChanCreate</span>, <span class="smallcaps">EChanClose</span>). Linear effects that must be accounted for exactly once.

Chopped Symbolic Execution (CSE)  
Trabish 2018. Symbolic execution that skips user-specified code regions and recovers their effects lazily when observable. Reduces path explosion while preserving soundness. See Section 4.4.6.

Code Property Graph (CPG)  
Unified representation combining AST, CFG, and PDG.

Coherence  
Honda 2008. Property that a global type is linear and projectable to all participants. Ensures protocol is implementable.

CompCert  
Leroy 2009. First realistic verified compiler for C (Clight subset) to PowerPC/ARM/x86 assembly. Proves semantic preservation. 42K lines Coq. See Section 12.26.C.

Completeness Flag  
DSA flag indicating whether all pointers to a node have been discovered. See Section 5.2.5.2, 12.33.2.

Concolic Execution  
Godefroid 2005 (DART), Sen 2005 (CUTE). Hybrid execution that runs concrete and symbolic execution *simultaneously*. Key insight: when symbolic reasoning becomes intractable, substitute concrete values and continue. Term “concolic” = concrete + symbolic. See Section 4.4.5, 4.4.7.

Consistent Subtyping  
Garcia 2016. $`G_1 \lesssim G_2`$ iff $`\exists\, T_1 \in \gamma(G_1), T_2 \in \gamma(G_2)`$ with $`T_1 <: T_2`$. Combines gradual consistency with subtyping for record types. Essential for JS objects, TS interfaces at boundaries. See Section 9.1.2.

Convertibility (Language Interop)  
Patterson & Ahmed 2022. Relation $`\tau_A \sim \tau_B`$ between types from different languages meaning they can safely interoperate. Established by providing target-level *glue code*. See Section 11.2.

Code2Inv  
Si et al. 2018 (NeurIPS). RL-based loop invariant synthesis using GNN program embeddings and TreeLSTM decoder. Discovers complex invariants that widening cannot find. See Section 2.1.5c, 3.1.8.

### Data Structures and Algorithms

CUTE (Concolic Unit Testing Engine)  
Sen, Marinov, Agha 2005. Foundational concolic testing tool introducing: (1) *logical input map* decoupling memory structure from physical layout, (2) *constraint separation* solving pointer constraints separately from arithmetic, (3) *fast unsat check* eliminating 60–95% of SMT calls, (4) *incremental solving*. See Section 4.4.5.

DART (Directed Automated Random Testing)  
Godefroid, Klarlund, Sen 2005 (PLDI). First tool combining concrete and symbolic execution. Three key techniques: automatic interface extraction, random test driver generation, dynamic test generation. See Section 4.4.5.

Data Structure Analysis (DSA)  
Lattner/Adve unification-based heap analysis with heap cloning for context sensitivity. See Section 5.2.5, 12.33.

DeepBugs  
Pradel & Sen 2018. ML-based bug detection using semantic identifier embeddings. Key insight: names encode semantic intent that reveals bugs. See Section 3.1.8.1.

DeepPoly  
Singh et al. 2019 (POPL). Abstract domain for neural network verification. Combines restricted polyhedral constraints with backsubstitution for tight bounds. 100–1000$`\times`$ faster than MILP. See Section 2.1.7c.

Dependent Load  
Trabish 2018. A load instruction where the loaded address may have been modified by a skipped function. Triggers recovery in CSE.

Diaframe  
Mulder 2022. Automated Iris proof framework using bi-abduction with postponed existentials. 10$`\times`$ less manual proof than raw Iris. See Section 7.4.10.

Divergence Analysis  
Detection of non-termination via repeating abstract states (loops) or same-state recursion.

DS Graph  
Lattner 2007. Data Structure Graph—representation used by DSA. Nodes represent disjoint sets of memory objects; edges are field-sensitive pointers. See Section 5.2.5.1.

Duality  
Honda 1998. Relationship between send and receive types: $`\overline{\overline{T}} = T`$. If two endpoints have dual types, their communication is safe.

Eval Algorithm  
Calcagno 2009. Forward symbolic execution with bi-abduction for compositional memory analysis.

Evidence (AGT)  
Garcia 2016. Metadata tracking *how* type consistency was established during type checking. Enables precise blame tracking at boundaries. See Section 9.1.2.

Execution Tree  
Dynamic tree structure from symbolic execution. Computed on-demand from CPG.

### Effects and Types

Effect Row  
Row-polymorphic type for tracking computational effects.

Expectation Transformer  
Cousot 2012. Abstract transformer for probabilistic programs that operates on probability distributions over abstract states. See Section 2.1.6c.

FFI Type Safety  
Furr & Foster 2008. Static analysis ensuring type-safe foreign function calls. Uses representational types and GC registration analysis. See Section 9.4.6.

Flow-Sensitive Type (FFI)  
Furr & Foster 2008. Type of form $`\mathit{ct}\{B, I, T\}`$ where $`B`$=boxedness, $`I`$=offset, $`T`$=tag value. See Section 9.4.6.

Four-Point Lattice  
Chong & Myers 2004. Security label lattice with both confidentiality ($`C_{\text{Low}}/C_{\text{High}}`$) and integrity ($`I_{\text{Low}}/I_{\text{High}}`$). Required for robust declassification analysis. See Section 8.1.1, 12.34.1.

Galois Connection  
Abstraction-concretization pair with soundness guarantee.

Global Type  
Honda 2008. Bird’s-eye view description of multiparty protocol specifying all interactions. Notation: $`p \to q : k(U).G`$ (p sends $`U`$ to q via channel $`k`$).

Gradual Guarantee  
Garcia 2016. Theorem: making types less precise (more $`?`$) preserves program semantics but may add runtime checks. See Section 9.1.2.

Graph Neural Network (GNN)  
Neural network operating on graph structures via message passing. Used by Code2Inv for invariant synthesis. See Section 3.1.8.

Guiding Constraints  
Trabish 2018. Path constraints accumulated between snapshot creation and dependent state in CSE. $`\mathit{gc} = \mathit{dependent.pc} - \mathit{snapshot.pc}`$.

### Memory and Ownership

Handle (MS-Wasm)  
Disselkoen 2019. Typed pointer in MS-Wasm consisting of 4-tuple $`(\mathit{base}, \mathit{offset}, \mathit{bound}, \mathit{isCorrupted})`$. See Section 7.7.2.

Heap Cloning  
DSA technique creating separate heap graphs per calling context for precision. See Section 5.2.5.4, 12.33.4.

Higher-Order Ghost State  
Jung 2018 (Iris). Ghost state where the content is itself an Iris proposition (<span class="sans-serif">iProp</span>). Critical for encoding recursive protocols and impredicative invariants. See Section 7.1.1.

Hybrid Fuzzing  
QSYM 2018. Cooperative combination of coverage-guided fuzzing (fast, shallow) with concolic execution (slow, deep). See Section 4.4.7.

Linear Memory (WebAssembly)  
A contiguous, mutable array of raw bytes that forms WebAssembly’s memory model. See Section 7.7.1.

Linear Typing  
Substructural type system requiring values be used *exactly once*. Stricter than affine. See Girard 1987.

Macroscopic Unification  
Lattner 2007. DSA’s approach combining Steensgaard-style unification with context-sensitivity. “Macroscopic” because it operates at data-structure level. See Section 5.2.5.

Magic Wand  
Reynolds 2002, Muller 2016. Separating implication ($`A \mathbin{-\!\!*}B`$): “providing $`A`$ yields $`B`$”. PSPACE-complete in general. See Section 7.4.8.

Marshalling (FFI)  
Converting data between source language representation and target language representation at FFI boundaries. See Section 9.4.4, 9.4.6.

MS-Wasm (Memory-Safe WebAssembly)  
Disselkoen et al. 2019. Backwards-compatible extension to WebAssembly that captures memory safety semantics. Introduces segment memory and handles. See Section 7.7.

Multiplicity  
Aliasing status of capability—$`\ensuremath{\mathsf{unique}}`$ (can free) vs $`\ensuremath{\mathsf{dup}}`$ (aliases may exist).

Owner-as-Dominator  
Boyapati 2003. Ownership type discipline where all heap paths from root to object $`x`$ must pass through $`x`$’s owner. Precursor to Rust’s ownership model. See Part VII.

Ownership Types  
Clarke 1998, Boyapati 2003. Type system associating each object with an owner. Foundation for Rust’s ownership semantics. See Section 7.1, Part VII.

### Program Analysis

IFDS  
Interprocedural Finite Distributive Subset—$`O(ED^3)`$ dataflow algorithm. Requires *distributive* transfer functions.

IMM (Intermediate Memory Model)  
Podkopaev 2019. Declarative memory model serving as intermediate layer for compilation proofs. Reduces $`O(n \cdot m)`$ proofs to $`O(n+m)`$. See Section 6.5.9.

Incorrectness Logic (IL)  
O’Hearn 2020. Under-approximate logic for proving bugs exist. **Deprecated** in favor of OL—IL is incompatible with abstract interpretation.

Interleaved Dyck  
Language of strings where projections to two Dyck alphabets are both valid. Used for combined context+field sensitivity. **Undecidable**—requires MCFL or SPDS approximation. See Section 4.2.3.

ISL Triple  
Incorrectness Separation Logic triple $`[p]\ C\ [q; \mathit{exit}]`$. Under-approximate semantics for bug finding.

KLEE  
Cadar, Dunbar, Engler 2008 (OSDI). Symbolic execution engine for LLVM bitcode achieving high coverage on real programs. Tested all 89 GNU COREUTILS achieving 90%+ line coverage, found 56 bugs. See Section 4.4.4.

Local Completeness  
Bruni et al. 2023. Property $`C^A_c(f)`$ meaning abstract domain $`A`$ is complete for transfer function $`f`$ at specific input $`c`$. See Section 2.1.8b.

MCFL (Multiple Context-Free Language)  
Conrado 2025. Language class strictly between CFL and interleaved Dyck. $`d`$-MCFL has $`O(n^{2d})`$ reachability complexity. See Section 4.2.4.

May-Mod Analysis  
Flow-insensitive pointer analysis computing the set of memory locations a function may modify. Used in CSE to detect dependent loads. See Section 4.4.6.5.

Narrowing  
Cousot 1992. Operator that recovers precision after widening. If $`y \sqsubseteq x`$ then $`y \sqsubseteq (x \bigtriangledown y) \sqsubseteq x`$.

Native Instrumentation  
QSYM 2018. Binary analysis technique using Intel Pin to instrument only instructions touching symbolic data. See Section 4.4.7.

Neural Invariant Synthesis  
Si et al. 2018. RL-based generation of loop invariants using neural networks. See Section 2.1.5c, Code2Inv.

Occurrence Typing  
Tobin-Hochstadt 2008. Flow-sensitive type refinement via type predicates. Complements gradual typing. See Section 2.1.7b, 9.5, 12.3.5.

Optimistic Concolic Execution  
QSYM 2018. Concolic variant that trades solver-level soundness for speed. 10–100$`\times`$ faster than KLEE. See Section 4.4.7.

Outcome Logic (OL)  
Zilberstein 2023. Unified correctness/incorrectness framework. **Preferred** over IL—compatible with abstract interpretation, supports manifest error detection.

Over-Approximation  
$`\alpha(\mathit{concrete}) \subseteq \mathit{abstract}`$. “If safe, truly safe.” May have false positives.

Path Condition  
King 1976. Conjunction of constraints accumulated along symbolic execution path. Determines path feasibility.

PDG  
Program Dependence Graph—data and control dependencies.

Points-to Analysis  
Computing what each pointer may point to.

Precision Ordering (Gradual Types)  
Garcia 2016. $`G_1 \leq G_2`$ ($`G_1`$ more precise than $`G_2`$) iff $`\gamma(G_1) \subseteq \gamma(G_2)`$. See Section 9.1.2.

Probabilistic Abstract Domain  
Cousot 2012. Abstract domain for probabilistic programs. Operates on probability distributions over standard abstract domain $`A`$. See Section 2.1.6c.

SDG  
System Dependence Graph—PDG extended with interprocedural edges. Two-phase slicing required.

Separation Logic  
Logic with separating conjunction for heap reasoning.

SPDS (Synchronized Pushdown System)  
Spath 2019. Two synchronized pushdown automata encoding field and call stacks. Enables polynomial combined context+field sensitivity. See Section 4.2.5.

Symbolic Execution  
King 1976. Path-sensitive analysis tracking symbolic values and path conditions. Commutativity: $`\mathit{instantiate}(\mathit{exec\_symbolic}(P)) = \mathit{exec\_concrete}(P)`$.

Taint Analysis  
Tracking untrusted data from sources to sinks.

Thin Slicing  
Backward slice following only relevant dependencies.

Two-Phase Slicing  
Horwitz 1990. Phase 1 ascends (no param-out), Phase 2 descends (no call/param-in). Prevents spurious paths.

Under-Approximation  
$`\mathit{abstract} \subseteq \mathit{concrete}`$. “If bug, truly a bug.” May have false negatives.

Widening  
Operator that accelerates fixpoint convergence for infinite domains.

### Memory Models

Capped Memory  
Lee 2020. PS 2.0 construction bounding realistic interference during certification. Enables value-range analysis and register promotion. See Section 6.5.7.

Promise (Memory Model)  
Kang 2017. Commitment by a thread to perform a future write. Key mechanism for preventing thin-air values in relaxed memory.

Promising Semantics  
Kang 2017/Lee 2020 memory model using thread-local certification to prevent thin-air values. PS 2.0 uses capped memory for certification. See Section 6.5.7.

RMW Strength  
Podkopaev 2019. Classification of hardware RMW operations. POWER/ARMv7 have strong RMWs; ARMv8/RISC-V have weak RMWs. See Section 6.5.9.

Simulation Relation  
Leroy 2009. Binary relation between execution states used to prove compilation pass correctness. Variants: lock-step, plus, star. See Section 12.26.C.

Thin-Air Problem  
Fundamental soundness bug in C11 axiomatic memory model. Permits values that appear without any legitimate source. Fixed by Promising Semantics. See Section 6.5.7, Theorem 12.26.3.

Thread-Local Certification  
Kang 2017/Lee 2020. Mechanism in Promising Semantics requiring that a thread can fulfill all its promises. See Section 6.5.7.

View (Memory Model)  
Kang 2017. Timemap tracking which messages a thread has observed per location. See Section 6.5.7.

View Shift  
Jung 2018 (Iris). The update modality $`\Rrightarrow P`$ asserting ownership of resources that can be updated to satisfy $`P`$. See Section 7.1.1.

### Security

Declassification  
Controlled release of secret data to lower security levels. See Section 8.1.4.3, 12.34.

Delimited Release  
Chong & Myers 2004. Declassification policy specifying *what* information may be released via escape hatches. See Section 8.1.4.3, 12.34.2.

Endorsement  
Chong & Myers 2004. The integrity dual of declassification: controlled acceptance of low-integrity data as high-integrity. See Section 8.1.4.3, 12.34.7.

Escape Hatch  
Declassification mechanism with policy, condition, and escaper components. See Section 12.34.2.

Integrity Label  
Chong & Myers 2004. Security label component tracking whether adversary can *influence* a value. $`I_{\text{Low}}`$ = untrusted, $`I_{\text{High}}`$ = trusted. See Section 8.1.1, 12.34.1.

Robust Declassification  
Zdancewic/Myers 2001 criterion ensuring attackers cannot influence what gets declassified. See Section 12.34.5–6.

Sanitizer  
Security operation that transforms a *value* to make it safe while preserving the security *label*. Contrast with declassification which changes the *label*. See Section 8.1.2, 12.34.9.

Semantic Security (under Declassification)  
Chong & Myers 2004. Formal security property: for all low-equivalent memories $`M_1`$ and $`M_2`$, $`\mathit{visible}(P(M_1))`$ and $`\mathit{visible}(P(M_2))`$ are equivalent. See Section 8.1.4.3, 12.34.4.

### Session Types

Input-Input Dependency (II)  
Honda 2008. Ordering between two inputs at same participant in same session.

Input-Output Dependency (IO)  
Honda 2008. Ordering where output depends on data from prior input at same participant.

Output-Output Dependency (OO)  
Honda 2008. Ordering between two outputs from same sender to same channel.

Projection (Session Types)  
Honda 2008. Operation $`G \restriction p`$ extracting participant $`p`$’s local type from global type $`G`$.

Session Delegation  
Honda 1998. Transferring channel capability through a channel. Enables dynamic protocol reconfiguration.

Session Fidelity  
Honda 2008. Property that process communication follows declared session type.

Session Type  
Honda 1998/2008 protocol type specifying channel communication structure. See Section 12.28, Part XIV.

### Testing and Verification

Concrete Fallback  
QSYM 2018. Strategy of executing complex operations concretely rather than symbolically. See Section 4.4.7.

Latent Error  
Bug that requires specific calling context to trigger. Contrast with Manifest Error.

Latent Predicate  
Tobin-Hochstadt 2008. Type proposition attached to a function result. Specifies what type test the function “remembers”. See Section 2.1.7b.

Manifest Error  
Le 2022. Bug that triggers regardless of calling context. True Positives Property guarantees 0% false positive rate.

Name Embedding  
Pradel 2018. Dense vector representation of identifier names capturing semantic meaning. See Section 3.1.8.1.

Recovery Caching  
Trabish 2018. Optimization storing recovery results to avoid redundant recovery executions in CSE.

Recovery State  
Trabish 2018. State created to lazily execute a skipped function when its side effects become relevant. See Section 4.4.6.3.

Semantics Preservation  
Leroy 2009. Property that compiled code has same observable behaviors as source program. See Section 12.26.C.

Skip Region  
Trabish 2018. User-specified function to exclude from symbolic exploration in CSE. See Section 4.4.6.2.

Snapshot State  
Trabish 2018. Symbolic state cloned immediately before entering skip region. See Section 4.4.6.3.

Synthetic Bug Generation  
Pradel 2018. Technique for generating ML training data by simple mutations of correct code. See Section 3.1.8.1.

Trilean  
Three-valued logic result from SMT queries: <span class="smallcaps">Definitely</span>, <span class="smallcaps">DefinitelyNot</span>, or <span class="smallcaps">Unknown</span>.

True Positives Property  
Le 2022, Theorem 3.4. Manifest error implies dead code OR real bug exists. Guarantees 0% false positive rate.

Type Consistency  
Siek 2006, Garcia 2016. Relation $`G_1 \sim G_2`$ for gradual types. Reflexive and symmetric but *not* transitive. AGT derivation: $`G_1 \sim G_2`$ iff $`\gamma(G_1) \cap \gamma(G_2) \neq \emptyset`$. See Section 9.1.2.

Type Narrowing  
Tobin-Hochstadt 2008. Reducing a union type to a subset of its members based on type tests. See Section 2.1.7b, 9.5.3.

Unified Approximation  
Bruni et al. 2023 (LCL$`_A`$). Framework combining over-approximation and under-approximation in a single parameterized proof system. See Section 2.1.8b.

Validation-Based Soundness  
QSYM 2018. Soundness model where a validator filters false positives by concrete execution. See Section 4.4.7.

Verified Compilation  
Leroy 2009. Compiler accompanied by machine-checked proof of semantics preservation. See Section 12.26.C.

Viper  
Muller 2016. Verification infrastructure for permission-based reasoning with fractional permissions, magic wands, and quantified permissions. See Section 7.1.2, 7.4.8–7.4.10.

Visible Predicate  
Tobin-Hochstadt 2008. Type proposition known to hold at current program point. See Section 2.1.7b.

### WebAssembly

Segment Memory (MS-Wasm)  
Disselkoen 2019. Memory region in MS-Wasm separate from Wasm’s linear memory. Segments are bounded, typed, and accessible only via handles. See Section 7.7.2.

WebAssembly (Wasm)  
W3C Standard. Platform-independent bytecode language designed to run C/C++ and similar languages at near-native speed. Provides isolation from host environment but *not* memory safety within the sandbox. See Section 7.7.

WebAssembly Type Safety  
The property that WebAssembly’s typed stack and typed function signatures are preserved at runtime. Wasm is type-safe for its value types but *not* for memory operations. See Section 7.7.1.

### Pointer Analysis (TVLA and SVF)

Canonical Abstraction  
TVLA. Merge nodes with same canonical name (tuple of abstraction predicate values). Ensures bounded domain.

Chi Annotation ($`\chi`$)  
Memory SSA. Annotates a store statement with potential memory locations that may be modified. For `*p = x`, $`o = \chi(o)`$ indicates location $`o`$ may be updated.

Coerce Operation  
TVLA. Apply compatibility constraints ($`\varphi_1 \Rightarrow \varphi_2`$), sharpen indefinite predicates, detect inconsistencies.

Conversion Strategy  
M&F 2007. Decouples type from conversion behavior. Examples: type-directed, zero-for-error, null-for-none.

Embedding Theorem  
TVLA Theorem 3.7. If $`S \sqsubseteq^f S'`$ then $`\llbracket\varphi\rrbracket^3_S(Z) \sqsubseteq \llbracket\varphi\rrbracket^3_{S'}(f \circ Z)`$. Links concrete to abstract soundly.

Focus Operation  
TVLA. Split summary nodes until formula evaluates to definite (0 or 1). Enables materialization for loops.

Fold/Unfold  
Viper 2016. Explicit operations for recursive predicates. Fold exchanges body permissions for predicate instance; unfold exchanges predicate for body permissions.

Footprint  
Reynolds 2002. Memory locations accessed by a command (reads, writes, allocates, frees). Essential for frame rule.

Frame Rule  
Reynolds 2002. $`\{P\}\ C\ \{Q\} \implies \{P * R\}\ C\ \{Q * R\}`$. Enables compositional analysis by preserving unrelated resources.

Guard Polarity  
M&F 2007. Positive = value entering stricter language (must check). Negative = from stricter (can skip). *Flips* at function arguments.

Hole Tagging  
M&F 2007. Tag evaluation holes with expected language to prevent “language bleeding” during reduction.

Inductive System  
Aiken 1999. Finite representation of all set constraint solutions via cascade $`\alpha_i = L_i \cup (\beta_i \cap U_i)`$.

Inhale/Exhale  
Muller 2016 (Viper). Asymmetric assertion $`[\mathit{on\_exhale}, \mathit{on\_inhale}]`$: different behavior when checked (exhale) vs assumed (inhale). Used for leak checks. See Section 7.4.10.

Instrumentation Predicates  
TVLA. Derived predicates (reachability, cycle membership, sharing) that dramatically improve precision.

Local Consistency  
Cousot 1977. $`\alpha(f_c(\gamma(x))) \sqsubseteq f_a(x)`$. Abstract transfer over-approximates concrete transfer after abstraction.

Memory SSA  
Chow 1996, Sui 2016. Extension of classical SSA to handle indirect memory accesses via $`\mu/\chi`$ annotations. Enables def-use chains for address-taken variables.

Mu Annotation ($`\mu`$)  
Memory SSA. Annotates a load statement with potential memory locations that may provide the value. For `x = *p`, $`\mu(o)`$ indicates $`x`$ may read from location $`o`$.

On-the-Fly CG  
Rupta 2024. Interleave call graph and points-to analysis. Resolves chicken-and-egg problem.

Projection Path  
Rupta 2024. Field representation $`(\mathit{base}, [f_1, f_2, f_3])`$ instead of index. More precise for nested structs.

Quantified Permission  
Muller 2016. Pointwise permission specification for arrays: $`\forall i :: \mathit{range}(i) \Rightarrow \mathit{acc}(\mathit{arr}[i])`$. See Section 7.4.9.

Realizability Model  
Patterson 2022. Semantic interpretation $`\mathcal{V}\llbracket\tau\rrbracket`$ = target values behaving as source type $`\tau`$.

Representational Type (FFI)  
Furr & Foster 2008. Type that models C’s low-level view of high-level language data. See Section 9.4.6.

Resource Algebra  
Algebraic structure for modeling ownership.

Semantic IR  
Patterson & Ahmed 2022. IR design philosophy where the IR serves as a semantically-typed target language for multi-language interoperability. See Section 11.2.

Set Constraints  
Aiken 1999. Unified framework: $`X \subseteq Y`$ constraints with conditionals and constructors. $`O(n^3)`$ resolution.

Shape Analysis  
Track structural properties (list, tree, DAG, cycle) of heap data using three-valued logic.

Source-Sink Analysis  
Analysis pattern tracking data flow from allocation sites (sources) to deallocation sites (sinks). SVF is optimized for this pattern.

Stack Filtering  
Rupta 2024. *Novel.* Eliminate points-to targets for stack objects whose frames aren’t on call stack. 2–5$`\times`$ faster + more precise.

Strong Update  
Sui 2016. A store that *kills* the old value at a memory location. Occurs when pointer uniquely points to a concrete (non-summary) location.

Summary Node  
TVLA. Abstract node representing multiple concrete nodes. $`\mathit{sm}(v) = \frac{1}{2}`$ indicates summary.

SVFG (Sparse Value-Flow Graph)  
Sui 2016. Graph where nodes are variable definitions and edges capture value-flow relationships.

Three-Valued Logic  
TVLA. Values: 1 (true), 0 (false), $`\frac{1}{2}`$ (unknown). Information ordering: $`\frac{1}{2} \sqsubseteq 0`$ and $`\frac{1}{2} \sqsubseteq 1`$.

Weak Update  
Sui 2016. A store that must *preserve* old values. Occurs when pointer may point to multiple locations or target is a summary node.

### ZIPPER-Related Terms

ZIPPER  
Li 2020 selective context sensitivity identifying precision-critical methods via flow patterns. See Section 5.3.2, 12.30.

Precision-Critical Method (PCM)  
ZIPPER. A method that participates in direct, wrapped, or unwrapped flow patterns. Only PCMs benefit from context sensitivity.

In Method  
ZIPPER. A method of class $`C`$ with one or more parameters. Objects can flow *into* the class through In method parameters.

Out Method  
ZIPPER. A method of class $`C`$ with non-void return type. Objects can flow *out* of the class through Out method returns.

Direct Flow  
ZIPPER. Flow pattern where object enters via In method parameter, flows through assignments/field ops, exits via Out method return of same class.

Wrapped Flow  
ZIPPER. Flow pattern where object enters via In method, gets stored in wrapper object, wrapper flows out via Out method.

Unwrapped Flow  
ZIPPER. Flow pattern where carrier object enters via In method, contents are loaded from carrier, loaded contents flow out via Out method.

Precision Flow Graph (PFG)  
ZIPPER. Graph extending OFG with wrap/unwrap edges. Built per-class, restricted to nodes reachable from In method parameters.

Zipper$`_e`$ (Express)  
ZIPPER. Variant with efficiency threshold. Excludes classes where $`\mathit{pts}_c > PV \times \mathit{total\_pts}`$. Default $`PV=5\%`$ achieves 94.7% precision with 25.5$`\times`$ speedup.

### JARVIS-Related Terms

Function Type Graph (FTG)  
JARVIS 2023. Per-function graph tracking type relations for Python call graph construction. Enables flow-sensitive type inference with strong updates. See Section 5.3.3.

Application-Centered Analysis  
JARVIS 2023. Analysis mode that starts from application entry points and only analyzes reachable code. Critical for scalability on Python codebases with 200+ dependencies.

Method Resolution Order (MRO)  
Python’s algorithm for determining method lookup order in class hierarchies. Uses C3 linearization to handle multiple inheritance consistently. See Section 5.3.3.3.

C3 Linearization  
Python’s MRO algorithm. Merges parent class MROs while preserving local precedence and monotonicity. Ensures consistent method lookup in diamond inheritance patterns.

Duck Typing  
Dynamic typing paradigm where object capabilities determine validity, not declared type. Requires type inference (FTG) rather than class-based dispatch for precise call graph construction.

Magic Method  
Python special methods (`__getattr__`, `__call__`, `__get__`, etc.) that intercept attribute access and invocation. Must be handled by call graph construction. See Section 5.3.3.4.

Decorator Resolution  
JARVIS 2023. Tracking function wrapping via decorators. Call graph must trace through wrapper to original function.

Import Summary  
JARVIS 2023. Pre-computed mapping of import statements to resolved types.

Class Summary  
JARVIS 2023. Pre-computed class hierarchy and method containment. Includes inheritance relations and cached MRO computations.

PyCG  
Prior state-of-the-art Python call graph tool. Uses flow-insensitive worklist algorithm. JARVIS achieves 84% higher precision.

## Gap Analysis, Theoretical Tensions, and Engineering Considerations

This appendix provides a systematic analysis of identified gaps between the theoretical framework established in Parts I–XII and the engineering requirements for production deployment.

### Executive Summary

This appendix derives from:

1.  **External Review**: Feedback from static analysis practitioners

2.  **Literature Cross-Reference**: Comparison with extended paper collection

3.  **Implementation Experience**: Lessons from prototype development

#### Gap Classification

Identified gaps are classified into three severity levels:

<div id="tab:gap-severity">

| **Severity**    | **Definition**            | **Resolution Approach**       |
|:----------------|:--------------------------|:------------------------------|
| **Critical**    | Blocks core functionality | Must resolve before Phase 3   |
| **Significant** | Reduces analysis quality  | Should resolve before Phase 5 |
| **Minor**       | Engineering convenience   | Address in Phase 6 or defer   |

Gap Severity Classification

</div>

#### Gap Summary Matrix

<div class="adjustbox">

max width=

<div id="tab:gap-summary">

| **Gap** | **Severity** | **Status** | **Resolution** |
|:---|:---|:---|:---|
| Library Modeling | Significant | Addressed | D.2 (bi-abduction) |
| Call Graph Construction | Critical | $`\checkmark`$ Resolved | D.3 (Qilin, 5.3) |
| Path Sensitivity | Significant | $`\checkmark`$ Resolved | D.4 (4.3, 4.4) |
| Memory Layout / ABI | Significant | Partial | D.5 (IR ext.) |
| Dynamic Code (eval) | Minor | Partial | D.6 (conservative) |
| Build System Integration | Minor | Engineering | D.7 |
| False Positive Mgmt | Critical | $`\checkmark`$ Resolved | D.8 (OL class.) |
| Time Budgets | Significant | Addressed | D.9 |

Gap Analysis Summary

</div>

</div>

#### Theoretical Tensions

Section <a href="#sec:d10" data-reference-type="ref" data-reference="sec:d10">12.10</a> documents tensions between foundational papers that require explicit resolution in the implementation:

- IFDS distributivity requirements vs general dataflow (Aiken 1999)

- Separation logic assumptions vs garbage-collected languages (Reynolds 2002)

- Field indexing vs projection paths (Rupta 2024)

- Language-agnostic vs language-specific pointer analysis

- Two-valued vs three-valued logic (TVLA 2002)

- Type-directed vs arbitrary conversions (Matthews-Findler 2007)

### Gap 1: Library and Environment Modeling

**Severity**: Significant **Status**: Addressed

#### Problem Characterization

The theoretical framework assumes complete source code availability. Production deployments must handle dependencies on:

- **Standard Libraries**: libc, JDK runtime, Python stdlib, Go runtime

- **Frameworks**: React, Django, Spring Boot, Express, Rails

- **Third-Party Packages**: npm, pip, cargo crates, Maven artifacts

Without semantic models for external code, analysis chains degrade:

- Taint propagation terminates at library boundaries

- Pointer analysis returns $`\top`$ for library allocations

- Call graphs omit edges through unmodeled code

#### Reassessment: Problem Scope is Narrower Than Initially Estimated

**Critical Observation**: Modern package ecosystems distribute source code by default, substantially reducing the scope of the “library modeling problem.”

<div id="tab:source-availability">

| **Ecosystem**    | **Source Availability**                               |
|:-----------------|:------------------------------------------------------|
| Python / PyPI    | Source distributed by default; `pip –download-source` |
| JavaScript / npm | Full source in `node_modules/`                        |
| Rust / Cargo     | Source fetched to `~/.cargo/registry/src/`            |
| Go / Modules     | `go mod download` retrieves full source               |
| Java / Maven     | `-sources.jar` artifacts available for most packages  |
| C/C++            | **Exception**: `.so`/`.dll` without headers common    |

Ecosystem Source Code Availability Analysis

</div>

**Revised Problem Statement**: The library modeling challenge reduces to:

1.  **Scalability**: Whole-program analysis of transitive dependency closure—addressed via summary-based compositional analysis (bi-abduction)

2.  **Closed Binaries**: Rare except in C/C++ ecosystem—addressed via conservative approximation + user-provided contracts

3.  **External Boundaries**: Network APIs, databases, system calls—addressed via contract specification language

#### Resolution: Formal Specification Inference Techniques

The following established techniques address library modeling without requiring machine learning or manual annotation:

<div class="pillarbox">

**Reference**: Calcagno 2009 “Compositional Shape Analysis by Means of Bi-Abduction”

Given partial specification $`\{P\}\ \mathit{code}\ \{Q\}`$, bi-abduction infers:

- **Anti-frame**: Additional preconditions $`P'`$ required for safety

- **Frame**: Additional postconditions $`Q'`$ established by code

**Application**: Automatically infer specifications for library functions by analyzing their implementations.

**Integration**: Phase 4, Milestone 4.3 (Bi-abduction for specs)

</div>

<div class="pillarbox">

**Reference**: Bierhoff 2007 “Modular Typestate Checking of Aliased Objects”

Modules verified independently against declared contracts:

- Libraries publish pre/post conditions

- Callers verify against published contracts

- Compositional verification without whole-program analysis

**Application**: Standard library contracts enable modular analysis

</div>

<div class="pillarbox">

**References**: Rondon 2008 “Liquid Types”, Vazou 2014 “Refinement Types for Haskell”

Type signatures encode behavioral contracts directly:
``` math
\mathit{read\_file} : \{f:\mathsf{String} \mid \mathit{valid\_path}(f)\} \to \{s:\mathsf{String} \mid \mathit{length}(s) \geq 0\}
```

**Application**: Extract contracts from existing type annotations (TypeScript types, Python type hints, Rust traits)

</div>

<div class="pillarbox">

**Reference**: Xi 1999 “Dependent Types in Practical Programming”

Types indexed by values encode rich specifications:
``` math
\begin{aligned}
\mathit{vector} &: (n:\mathsf{Nat}) \to \mathsf{Type} \\
\mathit{append} &: \mathit{vector}\ n \to \mathit{vector}\ m \to \mathit{vector}\ (n+m)
\end{aligned}
```

**Application**: Leverage F\* dependent types for verified specification

</div>

#### Strategy for Genuinely Opaque Code

For unavailable source code (closed binaries, external network APIs, proprietary libraries), the following tiered strategy applies:

<div class="pillarbox">

**Tier 1: Conservative Approximation (Default)**

- Return value: $`\top`$ (top of lattice)

- Effect set: $`\{\mathsf{Read}, \mathsf{Write}, \mathsf{IO}, \mathsf{Alloc}, \mathsf{Free}, \mathsf{Throw}\}`$

- Taint propagation: $`\mathit{tainted}(\mathit{input}) \Rightarrow \mathit{tainted}(\mathit{output})`$

- Pointer analysis: fresh allocation site per call

**Tier 2: User-Provided Contracts**

- ACSL-style annotations for C/C++

- JML-style specifications for Java

- Brrr-contract language (to be specified)

- Inline annotations: `@taint_source`, `@taint_sink`, `@pure`, etc.

**Tier 3: API Schema Extraction**

- OpenAPI/Swagger $`\to`$ REST endpoint contracts

- Protocol Buffers/gRPC $`\to`$ RPC type contracts

- GraphQL schemas $`\to`$ query/mutation contracts

- Database schemas $`\to`$ SQL query contracts

**Tier 4: Uncertainty Tracking**

- Classify findings crossing opaque boundaries as <span class="smallcaps">Incomplete</span>

- Propagate uncertainty through dependent analyses

- Report boundary crossings in SARIF output

</div>

#### Remaining Implementation Items

<div id="tab:library-impl">

| **Item** | **Priority** | **Target Phase** |
|:---|:---|:--:|
| Contract specification language design | High | Phase 4 |
| Automatic contract extraction from type annotations | Medium | Phase 4 |
| Contract composition rules for multi-language | Medium | Phase 5 |
| Uncertainty propagation through analysis pipeline | High | Phase 3 |
| Standard library contract database (Python, JS, Go) | Low | Phase 6 |

Library Modeling Implementation Items

</div>

### Gap 2: Call Graph Construction — Addressed

<div class="pillarbox">

**Solution Implemented:**

- Section 5.3: On-the-fly call graph construction (Qilin algorithm)

- Section 12.22: Stack filtering theorems (Rupta/Li 2024)

- Interleaved CG + pointer analysis resolves chicken-and-egg

**Key Insights from Rupta 2024:**

- 1-callsite sensitivity better than Andersen for Rust

- Stack filtering: 2–5$`\times`$ faster *and* more precise

- On-the-fly: resolve virtual calls as points-to sets refine

</div>

#### Remaining Open Questions

1.  **Reflection handling**—`Class.forName()`, eval-like constructs

2.  **Dynamic dispatch prediction** heuristics for unsolvable cases

3.  **Incremental CG update** when code changes

4.  **Language-specific resolution rules** (Python MRO, JS prototype chain)

### Gap 3: Path Sensitivity — Addressed

<div class="pillarbox">

**Solution Implemented:**

- Section 4.3: Eval algorithm with path-sensitive bi-abduction

- Section 4.4: Full symbolic execution with path conditions

- Section 4.3.3: Hybrid IFDS + Eval/Symbolic architecture

- Section 12.15: Symbolic execution theorems (King 1976)

**Key Theorems:**

- Commutativity: $`\mathit{instantiate}(\mathit{exec\_symbolic}(P)) = \mathit{exec\_concrete}(P)`$

- Forking semantics for conditionals

- SMT integration with trilean (<span class="smallcaps">Definitely</span>/<span class="smallcaps">DefinitelyNot</span>/<span class="smallcaps">Unknown</span>)

</div>

#### What’s Still Missing

1.  **SMT solver integration**—Z3, CVC5 for constraint solving

2.  **Path merging heuristics**—When to merge vs. keep separate

3.  **Widening for symbolic paths**—Bound path explosion

4.  **Integration with IFDS**—Seamless hybrid approach

### Gap 4: Memory Layout and ABI

#### The Problem Statement

The synthesis IR defines $`\mathsf{Int}`$ and $`\mathsf{Struct}`$ but ignores:

- Struct padding and alignment

- Integer width differences (`long`: 32-bit Windows, 64-bit Linux)

- Endianness

- Calling conventions

``` objectivec
// C code (Windows x64)
struct Data { int x; long y; };  // sizeof = 8 (long is 4 bytes)

// Rust code (Windows x64)
#[repr(C)]
struct Data { x: i32, y: i64 }   // sizeof = 16 (i64 is 8 bytes)

// FFI call passes wrong struct layout -> buffer overflow
```

#### Proposed IR Extension

To detect ABI mismatches at FFI boundaries, the IR must be extended with explicit layout and platform information. The key insight is that the same high-level type (e.g., `long`) may have different physical representations on different platforms (4 bytes on Windows x64, 8 bytes on Linux x64). The following type definitions capture the necessary metadata:

<div class="fstarcode">

(\* Endianness affects multi-byte value interpretation \*) type endian = \| BigEndian \| LittleEndian

(\* Physical memory layout for a type \*) type layout_info = size : nat; (\* Size in bytes \*) alignment : nat; (\* Alignment requirement \*) endianness : endian; (\* Big or little \*)

(\* Platform-specific ABI configuration \*) type target_abi = pointer_size : nat; (\* 4 or 8 bytes \*) int_sizes : map int_width nat; (\* Width-specific sizes \*) struct_packing : packing_mode; calling_convention : calling_conv;

type packing_mode = \| PackDefault (\* Platform default padding \*) \| PackPacked (\* No padding, \#pragma pack(1) \*) \| PackExplicit (\* Explicit padding fields \*)

(\* Extended struct type with layout metadata \*) type ir_struct = name : string; fields : list (string \* ir_type \* offset); (\* Include byte offset \*) layout : layout_info; abi : target_abi;

(\* Function to detect ABI incompatibilities between source and target \*) val check_abi_compatibility : source_struct:ir_struct -\> target_struct:ir_struct -\> list abi_mismatch

(\* Specific types of ABI mismatches that can cause memory corruption \*) type abi_mismatch = \| SizeMismatch : expected:nat -\> actual:nat -\> abi_mismatch \| AlignmentMismatch : field:string -\> expected:nat -\> actual:nat -\> abi_mismatch \| OffsetMismatch : field:string -\> expected:nat -\> actual:nat -\> abi_mismatch \| TypeWidthMismatch : field:string -\> expected:nat -\> actual:nat -\> abi_mismatch

</div>

The `check_abi_compatibility` function compares two struct representations across a boundary. Each `abi_mismatch` variant identifies a specific problem: `SizeMismatch` means the total struct size differs, `OffsetMismatch` means a field starts at different positions (likely due to padding differences), and `TypeWidthMismatch` means the same field name has different sizes (e.g., `long` on different platforms).

#### What’s Still Missing

1.  **ABI database** for common platforms (Windows, Linux, macOS $`\times`$ x86, x64, ARM)

2.  **Struct layout calculation** algorithm per ABI

3.  **FFI marshaling verification** at boundaries

4.  **Endianness conversion tracking** for network protocols

### Gap 5: Dynamic Code Generation (Eval)

#### The Problem Statement

The synthesis recognizes `eval()` as a taint sink, but not as a program modifier:

``` python
# This isn't just a sink -- it CHANGES the program
user_method = input()
eval(f"obj.{user_method}()")  # Creates new call edge at runtime!
```

#### Conservative Strategy

<div class="pillarbox">

**1. Detection:**

- Flag all `eval`/`exec`/`Function()` calls

- Report as “dynamic code generation” finding

**2. Taint Tracking:**

- If eval argument is tainted $`\to`$ HIGH severity (RCE)

- If eval argument is constant $`\to`$ analyze the constant

**3. Constant Propagation:**

- If `eval("x.foo()")` where string is known $`\to`$ add call edge

- Use existing constant propagation analysis

**4. Over-Approximation:**

- If string unknown: assume eval can call ANY method

- Add edges to all methods (conservative)

- Mark these edges as “speculative”

**5. User Annotation:**

- Allow `@eval_targets("foo", "bar")` annotation

- Restrict analysis to specified targets

</div>

#### What’s Still Missing

1.  **String constraint solving** for eval argument analysis

2.  **Dynamic call graph edges** marked as uncertain

3.  **Reflection API modeling** (`Class.forName`, `getattr`, etc.)

4.  **Code generation patterns** (template engines, ORMs)

### Gap 6: Build System Integration

#### The Problem Statement

Cannot correctly parse code without build configuration:

- **C/C++**: `-DDEBUG`, `-I/include/path` affect preprocessing

- **Java**: `CLASSPATH` determines which classes are visible

- **TypeScript**: `tsconfig.json` controls module resolution

- **Python**: `PYTHONPATH`, virtualenv affect imports

#### This is Engineering, Not Research

No papers directly address this because it’s an engineering concern.

**Existing Solutions:**

- `compile_commands.json` for C/C++ (CMake, Bear, intercept-build)

- Language Server Protocol (LSP) provides project model

- Build system plugins (Bazel aspects, Gradle plugins)

#### Practical Implementation

<div class="pillarbox">

C/C++  
Require `compile_commands.json`. Parse with clang’s libtooling. Each TU gets its own preprocessor state.

Java  
Read `pom.xml` / `build.gradle`. Extract classpath. Use javac API or ECJ.

TypeScript  
Parse `tsconfig.json`. Use TypeScript compiler API. Respect module resolution.

Python  
Detect virtualenv / conda. Parse `pyproject.toml` / `setup.py`. Use `ast` module with correct `sys.path`.

Rust  
Parse `Cargo.toml`. Use rustc or rust-analyzer. Respect feature flags.

Go  
Use `go/packages` API. Respects `go.mod` automatically.

</div>

#### What’s Still Missing

1.  **Unified project model** abstraction in brrr-machine

2.  **Build system detection** heuristics

3.  **Incremental build integration**—trigger re-analysis on build

4.  **Cross-project analysis** for monorepos

### Gap 7: False Positive Management

False positive management uses the provable **Manifest/Latent classification** from Le 2022.

- **Manifest bugs** have *zero* false positive rate by Theorem 3.4

- **Latent bugs** require specific calling context (context provided)

- **Incomplete** classifications include explicit reason for incompleteness

**See Section 12.3** for the complete formal foundation (ISL triples, `classify_bug` algorithm, theorems).

**See Section 13.4 Layer 6** for system integration.

#### Implementation Status

<div id="tab:fp-impl">

| **Component**                 | **Status**                    | **Section** |
|:------------------------------|:------------------------------|:------------|
| ISL Triple type               | $`\checkmark`$ Defined        | 12.3        |
| Manifest proof structure      | $`\checkmark`$ Defined        | 12.3        |
| True Positives Property       | $`\checkmark`$ Theorem stated | 12.3        |
| `classify_bug` algorithm      | $`\checkmark`$ Implemented    | 12.3        |
| Witness generation (concolic) | $`\square`$ TODO              | 12.15       |

False Positive Management Implementation Status

</div>

### Gap 8: Extended Confidence Sources (Implementation Details)

<div class="pillarbox">

Core types are in Section 12.3; this section provides algorithms & detail.

**D.9.1 Test Discrepancy**: Compare test assumptions vs code behavior. Match $`\to`$ confidence BOOST $`|`$ Contradict $`\to`$ confidence REDUCE $`|`$ None $`\to`$ NEUTRAL

**D.9.2 Uncertainty Estimation**: Each analysis block outputs confidence. Propagation rules and adaptive precision algorithms.

**D.9.3 Runtime Debugger**: Ground truth from actual execution. Confirms $`\to`$ 0.95+ $`|`$ Contradicts $`\to`$ 0.0 (false positive!) $`|`$ Never $`\to`$ unchanged

</div>

#### Test vs Code Discrepancy

##### The Insight

Tests encode programmer *assumptions* about code behavior. Code encodes *actual* behavior. These often diverge. Detecting discrepancies *statically* (without running tests) reveals:

1.  **Test bugs**—Tests that would never catch the bugs they’re meant to catch

2.  **Code bugs**—Code that behaves differently than any test expects

3.  **Specification gaps**—Behaviors neither tested nor clearly intended

##### Detection Categories

The following F\* type definitions model different categories of test/code discrepancies. Each constructor captures a specific way that test assumptions can diverge from actual code behavior, enabling automated detection of test quality issues.

<div class="fstarcode">

(\* Note: ’side_effect’ is used instead of ’effect’ which is reserved in F\* \*) type side_effect = \| SideRead \| SideWrite \| SideIO

type io_category = \| FileIO \| NetworkIO \| DatabaseIO \| FFIBinding \| SystemCall

type test_code_discrepancy = (\* Test assumes input type that code doesn’t actually accept \*) \| InputTypeMismatch : test_input:ir_type -\> code_accepts:ir_type -\> test_code_discrepancy (\* Test assertion can never fail given actual code behavior \*) \| UnreachableAssertion : assertion:node_id -\> reason:string -\> test_code_discrepancy (\* Test assertion can never PASS given actual code behavior \*) \| ImpossibleAssertion : assertion:node_id -\> reason:string -\> test_code_discrepancy (\* Code has error path that no test exercises \*) \| UntestedErrorPath : error_node:node_id -\> condition:ir_expr -\> test_code_discrepancy (\* Mock returns values code can never actually produce \*) \| MockBehaviorMismatch : mock:node_id -\> impossible_value:ir_expr -\> test_code_discrepancy (\* Test expects side effect that code doesn’t perform (or vice versa) \*) \| SideEffectMismatch : expected:side_effect -\> actual:option side_effect -\> test_code_discrepancy (\* File/network/binding behavior differs \*) \| IOBehaviorMismatch : io_type:io_category -\> test_assumes:behavior -\> code_does:behavior -\> test_code_discrepancy

</div>

The key insight is that each discrepancy type provides actionable information: `InputTypeMismatch` indicates the test is using inputs the function will reject, while `UnreachableAssertion` means the test is checking something that can never fail (a useless test).

##### Language-Specific Test Framework Integration

- **Rust**: `#[test]`, `#[should_panic]`, `assert!`, `assert_eq!`  
  $`\to`$ Parse test functions, extract assertions, compare with code paths

- **Go**: `func Test*`, `t.Error`, `t.Fatal`, testify assertions  
  $`\to`$ Extract test table patterns, compare with actual branches

- **Python**: pytest, unittest, assert statements  
  $`\to`$ Parse fixtures, mocks, parametrize decorators

- **JS/TS**: jest, mocha, `expect()`, describe/it blocks  
  $`\to`$ Extract mock implementations, compare with real modules

#### Classification Composition (Provable, Not Heuristic)

##### The Principle

Every analysis block must output not just a *result* but also:

1.  **Classification**—Manifest, Latent, or Incomplete (not a float!)

2.  **Complexity**—How expensive was this? (provable bound)

3.  **Incompleteness Reasons**—Explicit list if analysis is incomplete

##### Classification Composition Rules

<div class="pillarbox">

**Old (Heuristic):** $`\mathit{conf\_out} = \mathit{conf}_1 \times \mathit{conf}_2`$ or $`\mathit{conf\_out} = \min(\mathit{conf}_1, \mathit{conf}_2)`$

Problem: What does $`0.7 \times 0.8 = 0.56`$ mean semantically?

**New (Formal):** Classification is preserved through composition

**Rule 1: Incomplete propagates**  
If ANY dependency is Incomplete $`\to`$ result is Incomplete  
Reason: Can’t prove manifest if inputs uncertain

**Rule 2: Manifest preserved through complete analysis**  
Manifest finding + Complete analysis $`\to`$ Manifest  
Reason: True Positives Property still applies

**Rule 3: Latent context accumulates**  
$`\textsc{Latent}(\mathit{ctx}_1) + \textsc{Latent}(\mathit{ctx}_2) \to \textsc{Latent}(\mathit{ctx}_1 \land \mathit{ctx}_2)`$  
Reason: Both contexts required to trigger

**Rule 4: Manifest + Latent = Latent**  
$`\textsc{Manifest} \circ \textsc{Latent}(\mathit{ctx}) \to \textsc{Latent}(\mathit{ctx})`$  
Reason: The latent constraint dominates

</div>

The following F\* implementation formalizes the classification composition rules. The key insight is that `completeness` is tracked separately from `bug_classification`, and incomplete results always propagate conservatively. This differs from heuristic confidence scores (e.g., $`0.7 \times 0.8 = 0.56`$) which have no clear semantic interpretation.

<div class="fstarcode">

(\* Completeness tracks whether an analysis is definitive or approximated \*) type completeness = \| Complete (\* No approximations used \*) \| Incomplete of incompleteness_reason (\* Explicit reason for approximation \*)

(\* Type signature: compose two bug classifications with their completeness \*) val compose_classifications : upstream:bug_classification -\> upstream_complete:completeness -\> downstream:bug_classification -\> downstream_complete:completeness -\> bug_classification

let compose_classifications up up_c down down_c = (\* Rule 1: Incomplete propagates \*) match up_c, down_c with \| Incomplete r1, \_ -\> (\* Upstream incomplete - can’t trust downstream \*) RelaxedManifest \[IncompleteDependency r1\] \| \_, Incomplete r2 -\> (\* Downstream incomplete \*) RelaxedManifest \[IncompleteDependency r2\] \| Complete, Complete -\> (\* Both complete - apply composition rules \*) match up, down with (\* Rule 2: Manifest preserved \*) \| Manifest p1, Manifest p2 -\> Manifest (combine_proofs p1 p2) (\* Rule 3: Latent contexts accumulate \*) \| Latent ctx1, Latent ctx2 -\> Latent (ctx1 ‘conj‘ ctx2) (\* Rule 4: Manifest + Latent = Latent \*) \| Manifest \_, Latent ctx -\> Latent ctx \| Latent ctx, Manifest \_ -\> Latent ctx (\* RelaxedManifest composes conservatively \*) \| RelaxedManifest vs1, RelaxedManifest vs2 -\> RelaxedManifest (vs1 @ vs2) \| RelaxedManifest vs, \_ -\> RelaxedManifest vs \| \_, RelaxedManifest vs -\> RelaxedManifest vs

</div>

The function preserves the True Positives Property (Le 2022, Theorem 3.4): if a `Manifest` bug is found through complete analyses, it is guaranteed to be a real bug.

##### Uncertainty Sources

Every analysis operation that introduces approximation or encounters missing information must explicitly record the uncertainty source. This enables: (1) explaining to users why a finding has reduced confidence, (2) adaptive precision—retrying with more precise analysis when uncertainty is high, and (3) audit trails for verification.

<div class="fstarcode">

type uncertainty_source = (\* Analysis approximations \*) \| WideningApplied : iterations:nat -\> uncertainty_source \| ContextDepthLimited : max_k:nat -\> uncertainty_source \| HeapAbstractionMerged : merged_count:nat -\> uncertainty_source (\* Missing information \*) \| ExternalCodeUnmodeled : func:func_id -\> uncertainty_source \| DynamicDispatchUnresolved : call_site:node_id -\> uncertainty_source \| ReflectionOrEval : location:node_id -\> uncertainty_source (\* Language weirdness \*) \| UndefinedBehaviorPossible : ub_type:string -\> uncertainty_source \| LanguageSpecContradiction : spec_section:string -\> uncertainty_source \| PlatformDependent : varies_on:string -\> uncertainty_source

(\* Every analysis result bundles value with metadata about how it was computed \*) type analysis_result (a : Type) = value : a; confidence : float; (\* LEGACY: For backwards compatibility only. \*) (\* NEW CODE should use bug_classification from Section 12.3 \*) complexity : complexity_class; uncertainties : list uncertainty_source; approximations_used : list approximation;

(\* Complexity classification for time budget management \*) type complexity_class = \| Linear : complexity_class \| NLogN : complexity_class \| Quadratic : complexity_class \| Cubic : complexity_class \| CubicInDomain : domain_size:nat -\> complexity_class (\* IFDS: O(ED^3) \*) \| Exponential : base:nat -\> complexity_class

</div>

The `analysis_result` record separates the computed `value` from its provenance metadata. Note that the `confidence : float` field is marked as legacy; new code should use the structured `bug_classification` type from Section 12.3 which provides provable guarantees rather than heuristic scores.

##### Adaptive Precision Based on Uncertainty

```
IF uncertainty > threshold THEN
  TRY more precise analysis:
    - Increase context depth
    - Disable widening (if bounded iterations possible)
    - Use relational domain instead of non-relational
    - Run symbolic execution on uncertain paths

IF complexity > budget THEN
  DEGRADE precision:
    - Reduce context depth
    - Apply widening earlier
    - Use cheaper domain (Steensgaard vs Andersen)
    - Skip shape analysis
    - Mark as "partial analysis"
```

#### Runtime Debugger Integration

##### The Vision

Build debugger-like plugins for each target language that:

1.  **Intercept** execution at configurable points

2.  **Capture** concrete values, types, and execution paths

3.  **Feed** this ground truth back to static analysis

4.  **Validate** static predictions against runtime reality

##### Architecture

<div id="tab:runtime-hooks">

| **Language** | **Hook Mechanism** |
|:-------------|:-------------------|
| Python       | `sys.settrace`     |
| Node         | V8 inspector       |
| Rust         | LLDB/rr            |
| Go           | delve DAP          |
| C/C++        | GDB/LLDB           |

Language Runtime Hooks

</div>

**Trace Collector** captures:

- Function entry/exit with arguments

- Variable values at key points

- Branch decisions taken

- Memory allocations/deallocations

- I/O operations performed

**Trace Analyzer** classifies traces:

- <span class="smallcaps">AlwaysHappens</span>: seen in 100% of runs

- <span class="smallcaps">Common</span>: seen in $`>`$<!-- -->50% of runs

- <span class="smallcaps">Rare</span>: seen in $`<`$<!-- -->10% of runs

- <span class="smallcaps">NeverSeen</span>: predicted by static, never observed

**Static $`\leftrightarrow`$ Dynamic Reconciliation**:

- Static predicted $`X`$, runtime showed $`Y`$:

  - If $`Y \subset X`$: static is sound (expected)

  - If $`Y \not\subset X`$: static MISSED something (bug in us!)

  - If $`X \gg Y`$: static too imprecise (optimize)

##### Trace Classification

The trace classification system bridges static and dynamic analysis by recording how often execution paths are observed at runtime. This enables: (1) identifying false positives when static predictions are never observed, (2) finding soundness bugs when runtime discovers paths static analysis missed, and (3) precision tuning when static analysis is too conservative.

<div class="fstarcode">

(\* Frequency categories based on observed execution runs \*) type trace_frequency = \| AlwaysHappens (\* 100 \| VeryCommon (\* \>90 \| Common (\* \>50 \| Occasional (\* 10-50 \| Rare (\* \<10 \| NeverObserved (\* 0

(\* Complete classification of a trace with static/dynamic comparison \*) type trace_classification = path : list node_id; (\* Sequence of CPG nodes in the trace \*) frequency : trace_frequency; observed_count : nat; total_runs : nat; static_says : reachability; (\* What static analysis predicted \*) discrepancy : option discrepancy_type; (\* Any mismatch detected \*)

(\* Types of disagreement between static and dynamic analysis \*) type discrepancy_type = \| StaticMissed : trace_classification -\> discrepancy_type (\* Runtime found path static missed \*) \| StaticOverApprox : ratio:float -\> discrepancy_type (\* Static predicted 100, only 5 taken \*) \| StaticWrong : expected:value -\> actual:value -\> discrepancy_type (\* Value mismatch \*)

</div>

The `discrepancy` field is critical for analysis quality: `StaticMissed` indicates a soundness bug in our analysis (we missed a real path), while `StaticOverApprox` indicates an opportunity for precision improvement.

##### Integration with Confidence Model

Runtime traces BOOST or REDUCE confidence:

- Finding $`X`$ with static confidence 0.7:

  - If runtime confirms: confidence $`\to`$ 0.95+

  - If runtime contradicts: confidence $`\to`$ 0.0 (false positive!)

  - If runtime never exercises: confidence unchanged

- Path $`Y`$ predicted reachable:

  - If runtime reaches: <span class="smallcaps">Confirmed</span>

  - If runtime never reaches after $`N`$ runs: likely <span class="smallcaps">Unreachable</span> (but could be rare edge case)

### Theoretical Tensions and Resolutions

This section documents tensions between foundational papers that require explicit reconciliation in the implementation. Each tension is characterized by its source, implications, and adopted resolution.

#### IFDS Distributivity vs General Dataflow (Aiken 1999)

<div class="pillarbox">

**Synthesis Position (Part IV):**  
IFDS serves as the primary interprocedural dataflow framework

**Contradicting Reference:**  
Aiken 1999 “Introduction to Set Constraint-Based Program Analysis”  
IFDS is a *restricted fragment* of the more general set constraint formalism

**Theoretical Implications:**

- IFDS requires *distributive* transfer functions: $`f(a \sqcup b) = f(a) \sqcup f(b)`$

- Pointer analysis (Part V) is *not* distributive—cannot use IFDS directly

- Set constraints handle non-distributive cases via projection operations

**Adopted Resolution:**

- **IFDS**: Efficient $`O(ED^3)`$ implementation for distributive analyses (taint, nullability, reaching definitions, live variables)

- **Set Constraints**: General framework for non-distributive analyses (pointer analysis, type inference)

- Section 12.18: Provides set constraint theorems for non-distributive cases

**Implementation Impact:**

- Phase 2: Pointer analysis uses dedicated solver (not IFDS)

- Phase 3: IFDS for taint/nullability only

</div>

#### Separation Logic vs Garbage-Collected Languages (Reynolds 2002)

<div class="pillarbox">

**Synthesis Position (Part VII):**  
Use separation logic uniformly for memory reasoning across languages

**Contradicting Reference:**  
Reynolds 2002 “Separation Logic: A Logic for Shared Mutable Data Structures”  
Explicitly notes: “GC interaction is problematic”

**Theoretical Implications:**

- Separation logic frame rule assumes explicit deallocation timing

- GC may collect memory at unpredictable points

- “Disconnected garbage” has no separation logic representation

- Ownership transfer semantics differ fundamentally

**Adopted Resolution:**

**For GC Languages** (Python, Go, Java, JavaScript, Ruby):

- Separation logic applies to *resources* only: file handles, network connections, database cursors, locks, semaphores, condition variables

- Memory safety granted axiomatically from runtime

- Track resource ownership, not heap cell ownership

**For Manual Memory Languages** (C, C++, unsafe Rust):

- Full separation logic reasoning applies

- Frame rule enables compositional analysis

- Track allocation/deallocation correspondence

**Implementation Impact:**  
Language configuration specifies `memory_model`: `GC` $`|`$ `Manual` $`|`$ `Hybrid`

</div>

#### Field Index vs Projection Path (Rupta 2024)

**Tension:**

- **Synthesis Position (Part V)**: Use field *index* for field sensitivity:  
  `FieldLoad(dst, base, field_index)`

- **Rupta 2024**: Use *projection path* for nested structs:  
  $`(\mathit{base}, [field_1, field_2, field_3])`$

**Implications:**

- Field index: `x.a` and `y.a` conflated if same index

- Projection path: distinguished by *full* path

- Nested structs lose precision with index-based approach

**Resolution:** Adopt projection-based representation for Rust analysis. For simpler languages, index-based may suffice. Update Part V Section 5.1 to note both approaches.

#### Steensgaard Default vs Language-Specific (Rupta 2024)

**Tension:**

- **Synthesis Position (Part V)**: “Steensgaard for speed, Andersen for precision”

- **Rupta 2024**: “1-callsite-sensitive is FASTER and MORE PRECISE for Rust”

**Implications:**

- Generic recommendation ignores language characteristics

- Rust ownership model changes aliasing patterns

- Stack filtering (novel) enables context-sensitivity at low cost

**Resolution:** Add language-specific recommendations:

- **Rust**: 1-callsite + stack filtering (Rupta)

- **C/C++**: Andersen or demand-driven (Sridharan)

- **Python**: Type-based + dynamic traces

- **Java**: Object-sensitive for OOP patterns

#### Two-Valued vs Three-Valued Logic (TVLA 2002)

**Tension:**

- **Synthesis Position (Part II)**: Boolean lattices with explicit “Maybe” variants:  
  $`\mathsf{TaintLevel} = \mathsf{Tainted} \mid \mathsf{Untainted} \mid \mathsf{Unknown}`$

- **TVLA 2002**: Three-valued logic with Kleene semantics:  
  $`\mathsf{three\_value} = \mathsf{TV0} \mid \mathsf{TV1} \mid \mathsf{TVHalf}`$

**Implications:**

- Synthesis approach is ad-hoc per-domain

- TVLA approach is principled with information ordering

- Kleene semantics provide correct conjunction/disjunction

**Resolution:** Section 12.23 provides three-valued foundation. Section 5.4 uses three-valued for shape analysis. Consider refactoring existing domains to use three-valued base.

#### Type-Directed vs Arbitrary Conversion (M&F 2007)

**Tension:**

- **Synthesis Position (Part IX)**: Assume type-directed conversion:  
  $`\mathit{type\_map} : \mathsf{Type}_1 \to \mathsf{Type}_2`$

- **M&F 2007**: Conversion strategies ($`\kappa`$) decouple type from behavior:  
  Zero-for-error, sentinel values, custom conversions

**Implications:**

- Real FFIs have arbitrary conventions (C’s $`-1`$ for error, etc.)

- Type-directed assumption breaks for C/Python interop

- Need explicit conversion strategy abstraction

**Resolution:** Section 12.20 adds `conversion_strategy` type. Section 9.3 integrates with guard generation. Part IX should note non-type-directed cases.

#### GC vs Manual Memory (VeriFFI 2025)

**Tension:** The framework supports both GC languages (Python, Java, Go) and manual memory languages (C, C++, Rust). Their ownership models conflict:

<div id="tab:gc-manual">

| **Aspect**         | **GC Languages**            | **Manual Memory**       |
|:-------------------|:----------------------------|:------------------------|
| Deallocation       | Automatic, timing uncertain | Explicit, deterministic |
| Pointer validity   | Until collected             | Until freed             |
| Object movement    | GC may relocate             | Fixed address           |
| Ownership transfer | Reference semantics         | Move/copy semantics     |

GC vs Manual Memory Comparison

</div>

**Implications:**

- GC languages don’t track deallocation—GC handles it

- Manual languages must track use-after-free, double-free

- FFI boundaries are especially dangerous:

  - GC may collect object still referenced by C code

  - C may free memory still visible to GC language

- Ownership states don’t translate directly

**Resolution Strategy:**

1.  **Language-specific ownership states** (Section 7.6.2):

    - GC languages: <span class="smallcaps">GCRooted</span> / <span class="smallcaps">GCReachable</span> / <span class="smallcaps">GCFinalized</span>

    - Manual: <span class="smallcaps">Acquired</span> / <span class="smallcaps">InUse</span> / <span class="smallcaps">Released</span> (Section 7.1)

2.  **FFI boundary requirements** (Section 7.6.3):

    - Pin objects during cross-boundary calls

    - Copy small data to avoid cross-heap references

    - Register pointers as roots when necessary

3.  **GC-isomorphism preservation** (Section 7.6.1):

    - Representation predicates survive GC cycles

    - Analysis results valid before AND after GC

    - Addresses may change but structure preserved

4.  **Boundary guards**:

    - Source GC, target non-GC: Pin or Copy

    - Source non-GC, target GC: Register root if storing reference

    - Both GC but different runtimes: Cross-heap reference handling

**Reference:** VeriFFI (Wang et al. 2025)—“Verified FFI for GC Languages”

#### Summary Table

<div class="adjustbox">

max width=

<div id="tab:tensions-summary">

| **Tension** | **Synthesis** | **Paper** | **Resolution** |
|:---|:---|:---|:---|
| IFDS scope | Primary framework | Restricted fragment | Both: IFDS for distributive, constraints for general |
| Sep logic + GC | Use everywhere | GC problematic | Resources only for GC languages |
| Field handling | Index-based | Projection path | Adopt projection for precision |
| Pointer default | Steensgaard | Language-specific | Per-language recommendations |
| Value logic | Two-valued + Maybe | Three-valued | Use three-valued foundation |
| Conversions | Type-directed | Arbitrary strategies | Add conversion strategy type |
| GC vs Manual | Unified ownership | Incompatible models | Language-specific states + FFI requirements |

Theoretical Tensions Summary

</div>

</div>

### Collection 2 Paper Integration Summary

This section documents the integration of Collection 2 papers into the synthesis.

#### Papers Already Integrated in Main Synthesis

| **Paper** | **Integration Location** | **Key Contribution** |
|:---|:---|:---|
| **Paper** | **Integration Location** | **Key Contribution** |
| *Continued on next page* |  |  |
| **Leijen 2014** | Section 12.27 (Effect Absence Theorems) | Row-polymorphic effects, semantic soundness theorems. Effect absence proves bug impossibility. |
| **Yamaguchi 2014** | Part II (CPG), Section 5.3, throughout | CPG (AST+CFG+PDG), traversal algebra. Found 18 Linux kernel CVEs. Foundation of analysis infrastructure. |
| **Reynolds 2002** | Section 7.1 (Ownership), Part VII | Separation logic, frame rule, compositional heap reasoning. Enables modular memory analysis. |
| **Steensgaard 1996** | Section 5.1, Section 12.22 | $`O(n \cdot \alpha)`$ unification-based points-to, cjoin optimization. Fast pointer analysis baseline. |
| **Sivaramakrishnan 2021** | Section 6.1.2, 6.4, 6.4.1 | Production effect handlers in OCaml. Fiber-based implementation, 1% overhead, C FFI limitations. |
| **Petricek 2014** | Section 6.6 (Coeffects), Part VII intro | Coeffect systems dual to effects. Flat (capabilities) and structural (liveness, usage). Connects to linear types. |

#### Papers Added to Appendix A (Priority Matrix)

| **Paper** | **Priority** | **Category** | **Key Contribution** | **Gap Addressed** |
|:---|:--:|:--:|:---|:---|
| **Paper** | **Priority** | **Category** | **Key Contribution** | **Gap Addressed** |
| *Continued on next page* |  |  |  |  |
| **Bierhoff 2007** | 9 | Ownership | 5 access permissions, modular typestate with aliasing | D.1, D.7 |
| **Siek 2006** | 8 | Types | Gradual typing, non-transitive consistency, cast insertion | D.5, Section 9.1.2 |
| **Kang 2017** | 10 | Memory Model | Promising Semantics—prevents thin-air, validates compiler opts | Theorem 12.26.3 soundness |
| **Lee 2020** | 10 | Memory Model | Promising 2.0—capped memory, value-range analysis, ARMv8 fix | Section 6.5.7 |
| **Podkopaev 2019** | 10 | Memory Model | IMM—$`O(n+m)`$ compilation proofs, 33K Coq | Section 6.5.9 |
| **Godefroid 2005** | 8 | Testing | DART—concolic execution origins, interface extraction | Section 4.4.5 |

#### Integration Details

###### Bierhoff 2007 (Modular Typestate Checking).

- **Enhances**: Part VII (Ownership and Resources), Section 7.1.3

- **Addresses**: Gap D.1 by providing formal aliasing handling for library specifications

- **Key insight**: Permission taxonomy with fraction *functions* (not just fractions) enables precise tracking of shared state

- **Temporary state**: Weak permissions carry state that is forgotten after effects, enabling practical use of share/pure

###### Siek 2006 (Gradual Typing).

- **Enhances**: Part IX (Multi-Language Analysis), Section 9.1.2 (boundary risk analysis)

- **Critical fix**: Replaces informal “types_compatible” with rigorous *type consistency*

- **Non-transitivity**: $`t_1 \sim ?`$ and $`? \sim t_2`$ does *not* imply $`t_1 \sim t_2`$. This prevents unsound coercion chains.

- **Cast insertion**: Systematic algorithm for generating boundary casts from Section 9.1

###### Garcia 2016 (Abstracting Gradual Typing).

- **Enhances**: Part IX (Section 9.1.2), connects to Part II (Abstract Interpretation)

- **Critical insight**: Gradual types *are* abstract interpretations of static type sets

- **Galois connection**: $`\gamma(?) = \text{all types}`$, $`\gamma(\mathsf{Int}) = \{\mathsf{Int}\}`$. Consistency is: $`\gamma(G_1) \cap \gamma(G_2) \neq \emptyset`$

- **Derived non-transitivity**: Non-transitivity *emerges* from set intersection (not stipulated)

- **Consistent subtyping**: $`G_1 \lesssim G_2`$ when $`\exists\, T_1 \in \gamma(G_1), T_2 \in \gamma(G_2)`$ with $`T_1 <: T_2`$. Essential for record types.

- **Evidence semantics**: Tracks *how* consistency was established for precise blame at boundaries

- **Gradual guarantee**: Less precise types $`\to`$ more runtime checks, same semantics

- **Integration**: Connects Section 9.1.2 to Section 2.1.2, unifying gradual typing with abstract interpretation framework

###### Sivaramakrishnan 2021 (Retrofitting Effect Handlers onto OCaml).

- **Enhances**: Section 6.1.2 (Algebraic Effects), Section 6.4 (Effect Handler Limitations)

- **New section**: Section 6.4.1 (Production Effect Handler Implementation)

- **Key contributions**:

  - Fiber-based stack segmentation for delimited continuations

  - 1% mean overhead for non-effect code (validated on 54 benchmarks)

  - C FFI boundary limitation: effects cannot cross C stack frames

  - Runtime-enforced continuation linearity (one-shot by default)

  - DWARF compatibility for debugging (GDB, perf work correctly)

- **Practical insight**: Effect handlers *are* practical for production systems when carefully implemented

###### Petricek 2014 (Coeffects: A Calculus of Context-Dependent Computation).

- **Enhances**: Part VI (complete effect/coeffect duality), Part VII (ownership via usage coeffects)

- **New section**: Section 6.6 (Coeffect Systems)

- **Key contributions**:

  - Effects and coeffects are *dual*: effects = what computation *produces*, coeffects = what it *requires*

  - Flat coeffects for capabilities (network, filesystem, platform requirements)

  - Structural coeffects for per-variable properties (liveness, usage counting)

  - Semiring algebra enables compositional reasoning

  - Usage coeffects connect to linear types and Rust ownership

- **Critical insight**: Complete analysis requires *both* effects and coeffects

#### Cross-Paper Connections

Both Bierhoff 2007 and Siek 2006 address partial knowledge:

- **Siek**: Types are partially known ($`?`$ represents unknown portions)

- **Garcia (AGT)**: $`?`$ represents *set* of all types; partial knowledge = set abstraction

- **Bierhoff**: State is partially known (weak permissions have temporary assumptions)

Effect/Coeffect duality connects multiple papers:

- **Petricek 2014**: Coeffects are dual to effects (Moggi 1991, Plotkin 2003)

- **Sivaramakrishnan 2021**: Production implementation validates effect handler theory

- **Girard 1987**: Linear types map to usage coeffects (Section 6.6.3 and Section 7.2.2)

AGT connects gradual typing to abstract interpretation (Part II $`\leftrightarrow`$ Part IX):

- **Garcia 2016**: Gradual types form Galois connection with static type sets

- **Consistency derived**: $`G_1 \sim G_2`$ iff $`\gamma(G_1) \cap \gamma(G_2) \neq \emptyset`$

- **Precision ordering**: $`G_1`$ more precise than $`G_2`$ iff $`\gamma(G_1) \subseteq \gamma(G_2)`$

- **Implication**: Part IX boundary analysis is an *instance* of Part II framework

This suggests a unified treatment of uncertainty:

<div class="pillarbox">

- **For types (Siek/Garcia)**: consistency = non-transitive (from set intersection)

- **For states (Bierhoff)**: assumption = temporary (may be invalidated)

- **For permissions**: fractions = summable (ownership accounting)

- **For gradual (AGT)**: precision = set inclusion (Galois connection)

</div>

The integration enables the synthesis to handle:

1.  **Dynamic language analysis** (Python, JavaScript) via gradual typing

2.  **Aliased resource tracking** via access permissions

3.  **Modular library verification** via frame-based inheritance

4.  **Precise boundary checking** via type consistency (not subtyping)

<div class="center">

*End of Appendix D*

</div>
