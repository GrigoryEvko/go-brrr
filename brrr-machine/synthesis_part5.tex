%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  PART V: POINTER ANALYSIS - THE PRECISION FOUNDATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Pointer Analysis --- The Precision Foundation}
\label{part:pointer-analysis}

%--------------------------------------------------
% TENSION RESOLUTION BOXES
%--------------------------------------------------

\begin{contributionbox}[title={Tension Resolution: Steensgaard Default vs Language-Specific}]
\textbf{Source}: \textbf{[Rupta24]}, \textbf{[Lattner07]} DSA. See Appendix D.10.4 for full analysis.

\textbf{OLD RECOMMENDATION}: ``Steensgaard for speed, Andersen for precision''

\textbf{UPDATED}: Language AND codebase size matter!

\textbf{Language-Specific Recommendations}:
\begin{itemize}
    \item \textbf{\LangRust{}}: 1-callsite-sensitive + stack filtering (Section 12.22) --- \emph{FASTER AND more precise than Steensgaard for Rust!}
    \item \textbf{\LangC{}/\LangCpp{} ($<$ 50K LOC)}: Andersen (most precise, subset-based)
    \item \textbf{\LangC{}/\LangCpp{} ($>$ 100K LOC)}: DSA (Section 5.2.5) --- unification + heap cloning. Linux kernel (355K LOC) in 3.1 seconds, $<$ 46MB memory. Precision \emph{comparable} to Andersen via context-sensitivity.
    \item \textbf{\LangC{}/\LangCpp{} source-sink}: SVF (Section 5.6) --- uses DSA/Andersen as input
    \item \textbf{\LangC{}/\LangCpp{} flow-sensitive}: Demand-driven (\textbf{[Sridharan05]})
    \item \textbf{\LangJava{}/OOP}: Qilin + ZIPPER (Sections 5.3, 5.3.2) for selective CS
    \item \textbf{\LangPython{}/\LangJS{}}: Type-based + dynamic traces
\end{itemize}

\textbf{Key Insight}: Rust's ownership model changes aliasing patterns dramatically. Stack filtering (Section 5.3, 12.22) eliminates spurious stack aliasing. DSA's heap cloning distinguishes data structure \emph{instances} at scale.
\end{contributionbox}

\begin{contributionbox}[title={Tension Resolution: Field Index vs Projection Path}]
\textbf{Source}: \textbf{[Rupta24]}. See Appendix D.10.3 for full analysis.

\textbf{This Section} uses field \textsc{index}: \texttt{FieldLoad(dst, base, field\_index)}

\textbf{Rupta 2024} uses \textsc{projection path}: \texttt{(base, [field1, field2, field3])}

\textbf{Trade-off}:
\begin{itemize}
    \item \textbf{Index-based}: Simpler, sufficient for flat structs
    \item \textbf{Projection-based}: More precise for \emph{nested} structs --- \texttt{x.a.b.c} and \texttt{y.a.b.c} distinguished by full path. Required for Rust where nested borrows are common.
\end{itemize}

\textbf{Recommendation}: Use projection-based for Rust/\LangCpp{} with deep nesting. Section 12.22 provides \texttt{typed\_loc} with \texttt{type\_view} for cast handling.
\end{contributionbox}

\begin{contributionbox}[title={Empirical Finding: Field-Dereference Depth Bound}]
\textbf{Source}: \textbf{[TAJ09]}

When tracking taint through field accesses (e.g., \texttt{x.a.b.c}), how deep should analysis go? TAJ found an empirical bound:

\begin{center}
\fbox{$k=2$ is \textsc{sufficient} for 95\%+ of real vulnerabilities}
\end{center}

Meaning: Most taint flows involve at most 2 levels of field dereference:
\begin{itemize}
    \item \textbf{COMMON}: \texttt{x.field} ($k=1$)
    \item \textbf{COMMON}: \texttt{x.outer.inner} ($k=2$)
    \item \textbf{RARE}: \texttt{x.a.b.c.d} ($k=4$) --- diminishing returns
\end{itemize}

\textbf{Recommendation}:
\begin{itemize}
    \item Default to $k=2$ for field-dereference depth
    \item Make configurable for specific analyses requiring deeper tracking
    \item Cost grows exponentially with $k$; keep it small
\end{itemize}

Cross-reference: TAJ taint analysis (Section 8.1.3, 8.1.5)
\end{contributionbox}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Andersen's Analysis}
\label{ch:andersen}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{pillarbox}[title={Foundational Reference: Andersen 1994}]
\textbf{[Andersen94]} L.~O.~Andersen. \emph{Program Analysis and Specialization for the C Programming Language}. PhD thesis, DIKU, University of Copenhagen, 1994.

Andersen's analysis is the gold standard for precise points-to analysis. It models pointer operations as set constraints.
\end{pillarbox}

%--------------------------------------------------
\section{The Constraint Language}
\label{sec:andersen-constraints}
%--------------------------------------------------

Pointer operations translate to set inclusion constraints:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Statement} & \textbf{Constraint} & \textbf{Meaning} \\
\midrule
\texttt{x = \&y} & $\{y\} \subseteq \mathit{pts}(x)$ & $x$ points to $y$'s location \\
\texttt{x = y} & $\mathit{pts}(y) \subseteq \mathit{pts}(x)$ & $x$ points to everything $y$ points to \\
\texttt{x = *y} & $\forall v \in \mathit{pts}(y).\; \mathit{pts}(v) \subseteq \mathit{pts}(x)$ & $x$ points to what $y$'s targets point to \\
\texttt{*x = y} & $\forall v \in \mathit{pts}(x).\; \mathit{pts}(y) \subseteq \mathit{pts}(v)$ & $y$'s targets added to $x$'s targets' pts \\
\bottomrule
\end{tabular}
\end{center}

\begin{remark}[Example Constraint Solving]
Consider:
\begin{verbatim}
  p = &x           {x} <= pts(p)
  q = &y           {y} <= pts(q)
  r = p            pts(p) <= pts(r)
  s = *r           forall v in pts(r). pts(v) <= pts(s)
\end{verbatim}
Solution:
\begin{align*}
\mathit{pts}(p) &= \{x\} \\
\mathit{pts}(q) &= \{y\} \\
\mathit{pts}(r) &= \{x\} \quad \text{(from } \mathit{pts}(p)\text{)} \\
\mathit{pts}(s) &= \mathit{pts}(x) \quad \text{(from dereferencing } r \text{ which points to } x\text{)}
\end{align*}
\end{remark}

%--------------------------------------------------
\section{Constraint Solving}
\label{sec:andersen-solving}
%--------------------------------------------------

The F* formalization below defines the core types for Andersen's analysis. The \texttt{abstract\_loc} type distinguishes between stack, heap, and global memory locations---this stratification is essential for precision, as different storage classes have different aliasing behaviors. The \texttt{constraint\_} type directly encodes the four fundamental pointer operations (address-of, copy, load, store) plus field operations for struct handling.

\begin{fstarcode}[title={Andersen's Points-To Analysis}]
(* ==================================================
   ANDERSEN'S POINTS-TO ANALYSIS
   Source: Andersen 1994
   ================================================== *)
module BrrrMachine.PointerAnalysis.Andersen

type abstract_loc =
  | StackLoc : func:string -> var:string -> abstract_loc
  | HeapLoc : alloc_site:node_id -> abstract_loc
  | GlobalLoc : var:string -> abstract_loc
  | UnknownLoc : abstract_loc

type constraint_ =
  | AddrOf : lhs:string -> rhs:abstract_loc -> constraint_
      (* lhs = &rhs: {rhs} <= pts(lhs) *)
  | Copy : lhs:string -> rhs:string -> constraint_
      (* lhs = rhs: pts(rhs) <= pts(lhs) *)
  | Load : lhs:string -> rhs:string -> constraint_
      (* lhs = *rhs: forall v in pts(rhs). pts(v) <= pts(lhs) *)
  | Store : lhs:string -> rhs:string -> constraint_
      (* *lhs = rhs: forall v in pts(lhs). pts(rhs) <= pts(v) *)
  | FieldLoad : lhs:string -> base:string -> field:string -> constraint_
      (* lhs = base->field *)
  | FieldStore : base:string -> field:string -> rhs:string -> constraint_
      (* base->field = rhs *)

type pts_solution = map string (set abstract_loc)
\end{fstarcode}

The \texttt{pts\_solution} type maps each variable name to its points-to set---a set of abstract locations that the variable may reference at runtime. This is the central data structure computed by the analysis; all client analyses (taint tracking, use-after-free detection, etc.) query this map to determine aliasing relationships.

The following code demonstrates how constraints are extracted from the Code Property Graph (\CPG{}). Each assignment node is classified by its pointer operation type, and the corresponding constraint is generated. This extraction is language-independent; the \CPG{} abstraction (Section 3.1) normalizes language-specific syntax into a uniform representation.

\begin{fstarcode}[title={Constraint Extraction from CPG}]
val extract_constraints : cpg -> list constraint_
let extract_constraints cpg =
  fold_nodes cpg (fun constraints node ->
    match node.kind with
    (* x = &y *)
    | NAssign when is_addr_of cpg node.id ->
        let lhs = get_lhs cpg node.id in
        let rhs = get_addr_of_target cpg node.id in
        AddrOf lhs rhs :: constraints
    (* x = y *)
    | NAssign when is_simple_copy cpg node.id ->
        let lhs = get_lhs cpg node.id in
        let rhs = get_rhs_var cpg node.id in
        Copy lhs rhs :: constraints
    (* x = *y *)
    | NAssign when is_load cpg node.id ->
        let lhs = get_lhs cpg node.id in
        let rhs = get_deref_target cpg node.id in
        Load lhs rhs :: constraints
    (* *x = y *)
    | NAssign when is_store cpg node.id ->
        let lhs = get_store_target cpg node.id in
        let rhs = get_rhs_var cpg node.id in
        Store lhs rhs :: constraints
    (* x = y->f *)
    | NAssign when is_field_load cpg node.id ->
        let (lhs, base, field) = get_field_load_info cpg node.id in
        FieldLoad lhs base field :: constraints
    (* x->f = y *)
    | NAssign when is_field_store cpg node.id ->
        let (base, field, rhs) = get_field_store_info cpg node.id in
        FieldStore base field rhs :: constraints
    | _ -> constraints
  ) []
\end{fstarcode}

The constraint extraction function \texttt{extract\_constraints} performs a single pass over the \CPG{}, producing a list of constraints. The helper functions (\texttt{is\_addr\_of}, \texttt{get\_lhs}, etc.) are \CPG{} query operations that inspect node attributes. This design separates the constraint \emph{language} from the constraint \emph{extraction}, enabling the same solver to work with constraints from different front-ends.

The worklist algorithm below is the heart of Andersen's analysis. It iteratively propagates points-to information until a fixpoint is reached. The key insight is that only variables whose points-to sets have \emph{changed} need to be re-examined---the worklist tracks these ``dirty'' variables. For load/store constraints, the algorithm must handle the indirection: when \texttt{pts(y)} changes, any constraint of the form \texttt{x = *y} must be re-evaluated.

\begin{fstarcode}[title={Iterative Worklist Algorithm}]
val solve_andersen : list constraint_ -> pts_solution
let solve_andersen constraints =
  (* Initialize with base constraints (AddrOf) *)
  let init = List.fold_left (fun sol c ->
    match c with
    | AddrOf lhs rhs ->
        Map.update lhs (fun s ->
          Set.add rhs (Option.value s ~default:Set.empty)) sol
    | _ -> sol
  ) Map.empty constraints in

  (* Build constraint graph for worklist *)
  let copy_edges = List.filter_map (fun c ->
    match c with Copy lhs rhs -> Some (rhs, lhs) | _ -> None
  ) constraints in
  let load_constraints = List.filter (fun c ->
    match c with Load _ _ -> true | _ -> false
  ) constraints in
  let store_constraints = List.filter (fun c ->
    match c with Store _ _ -> true | _ -> false
  ) constraints in

  (* Worklist: variables whose pts changed *)
  let worklist = ref (Map.keys init) in
  let solution = ref init in

  while not (List.is_empty !worklist) do
    let var = List.hd !worklist in
    worklist := List.tl !worklist;
    let pts_var = Map.find_default var Set.empty !solution in

    (* Propagate through copy edges *)
    List.iter (fun (src, dst) ->
      if src = var then
        let old_pts = Map.find_default dst Set.empty !solution in
        let new_pts = Set.union old_pts pts_var in
        if not (Set.equal old_pts new_pts) then begin
          solution := Map.add dst new_pts !solution;
          worklist := dst :: !worklist
        end
    ) copy_edges;

    (* Handle complex constraints *)
    List.iter (fun c ->
      match c with
      | Load lhs rhs when rhs = var ->
          Set.iter (fun loc ->
            let loc_pts = Map.find_default (loc_to_var loc) Set.empty !solution in
            let old_pts = Map.find_default lhs Set.empty !solution in
            let new_pts = Set.union old_pts loc_pts in
            if not (Set.equal old_pts new_pts) then begin
              solution := Map.add lhs new_pts !solution;
              worklist := lhs :: !worklist
            end
          ) pts_var
      | Store lhs rhs when lhs = var ->
          let rhs_pts = Map.find_default rhs Set.empty !solution in
          Set.iter (fun loc ->
            let loc_var = loc_to_var loc in
            let old_pts = Map.find_default loc_var Set.empty !solution in
            let new_pts = Set.union old_pts rhs_pts in
            if not (Set.equal old_pts new_pts) then begin
              solution := Map.add loc_var new_pts !solution;
              worklist := loc_var :: !worklist
            end
          ) pts_var
      | _ -> ()
    ) (load_constraints @ store_constraints)
  done;
  !solution
\end{fstarcode}

\begin{remark}[Complexity Analysis]
Let $n$ = number of variables/locations.

\textbf{Worst Case}: $\BigO{n^3}$
\begin{itemize}
    \item Each variable can point to $\BigO{n}$ locations
    \item Each change propagates to $\BigO{n}$ dependents
    \item $\BigO{n}$ variables can change
\end{itemize}

\textbf{Practical}: Much better on real programs due to sparse constraint graphs.

\textbf{Comparison} (language-dependent --- see Part V tension box):
\begin{itemize}
    \item \textbf{Andersen}: $\BigO{n^3}$, precise (inclusion-based)
    \item \textbf{Steensgaard}: $\BigO{n \cdot \alpha(n)}$, less precise (unification-based)
    \item \textbf{Rust}: 1-callsite + stack filtering (Section 12.22) --- \emph{faster} than Steensgaard!
\end{itemize}

The traditional ``Steensgaard=fast, Andersen=precise'' trade-off does \textbf{not} hold for all languages. Rust's ownership model changes aliasing patterns.
\end{remark}


%--------------------------------------------------
\section{Limitations from Andersen 1994}
\label{sec:andersen-limitations}
%--------------------------------------------------

\begin{contributionbox}[title={Critical: Limitations from Andersen 1994 --- Avoid These Extensions}]
The paper explicitly identifies problems with no good solutions:

\textbf{1. Flow-Sensitive Constraint-Based Analysis}

``Currently, we have no good solution to this problem.'' (Section 4.10)

\emph{Problem}: When \texttt{x = y} updates abstract location $p$ at point 3, but $p$ does not syntactically appear in \texttt{*q = \&y} at point 4, flow-sensitive constraints cannot express this update.

\emph{Recommendation}: Use flow-\textsc{insensitive} analysis. For most programs with many small functions, this is sufficient.

\textbf{2. Must-Alias Analysis}

``We shall only consider may point-to analysis in this chapter.''

Must-alias requires flow-sensitive reasoning to determine when two pointers \emph{definitely} alias. This is significantly more expensive.

\emph{Recommendation}: Start with may-alias. Add must-alias only when needed for specific optimizations.

\textbf{3. Unknown Explosion from LHS Store Dereferences}

``If the analysis reveals that an Unknown pointer may be dereferenced in the left hand side of an assignment, the analysis stops with `worst-case' message.'' (Section 4.3.3)

\emph{Problem}: \texttt{*x = y} where $\mathit{pts}(x)$ contains Unknown is \textsc{catastrophic}. Everything becomes Unknown transitively.

\emph{Recommendation}:
\begin{itemize}
    \item Track which constraints come from LHS of stores
    \item Only abort when LHS dereference involves Unknown
    \item RHS dereferences with Unknown are safe (just propagate Unknown)
\end{itemize}

\textbf{4. Full Array Element Tracking}

``Our analysis treats arrays as aggregates.'' (Section 4.12.1)

Tracking individual array elements requires array dependence analysis from parallelizing compilers.

\textbf{5. Recursive Data Structure Unrolling}

``In our experience elements in a recursive data structure are used `the same way''' --- merge all nodes into single abstract location. For precise tracking, use Shape Analysis (Section 5.4).
\end{contributionbox}


%--------------------------------------------------
\section{Inter-Procedural Context Sensitivity}
\label{sec:andersen-context}
%--------------------------------------------------

\begin{definition}[Context-Sensitive Extension via Constraint Vectors]
\textbf{Source}: \textbf{[Andersen94]}, Section 4.6

Instead of copying functions for each call site (exponential blowup), use \emph{vectors} of type variables indexed by call-site variants:
\[
\langle T^0, T^1, \ldots, T^n \rangle
\]
where:
\begin{itemize}
    \item $T^0$ = summary (context-insensitive)
    \item $T^i$ = specific call context $i$
\end{itemize}

At each call site, the Static Call Graph determines which variant index to use. This achieves context sensitivity \textbf{without} exponential cloning of constraints.
\end{definition}

\begin{verbatim}
IMPLEMENTATION:
  struct ContextSensitiveVar {
    base: VarId,
    variant: u32,  // 0 = summary, 1..n = specific call contexts
  }

  fn get_callee_variant(
    call_site: u32,
    caller_variant: u32,
    scg: &StaticCallGraph
  ) -> u32 {
    scg.lookup(call_site, caller_variant)
  }
\end{verbatim}

\textbf{Trade-off}:
\begin{itemize}
    \item More precise than context-insensitive (fewer spurious flows)
    \item Less precise than full context cloning (variants merge at recursion)
    \item Polynomial complexity preserved
\end{itemize}


%--------------------------------------------------
\section{Cross-References: Pointer Analysis Dependencies}
\label{sec:andersen-xref}
%--------------------------------------------------

\begin{artifactbox}
\textbf{What Depends on Pointer Analysis ($\mathit{pts}()$)}:

\begin{enumerate}
    \item \textbf{Effect Resolution} (Section 6.1): \texttt{ERead(loc)}, \texttt{EWrite(loc)} need $\mathit{pts}()$ to determine which locations are accessed. Without $\mathit{pts}()$, \texttt{Read(*p)} affects ``everything''.

    \item \textbf{Use-After-Free Detection} (Section 6.1.4): \texttt{Free(ptr)} marks $\mathit{pts}(\mathit{ptr})$ as freed. Read/Write checks if any target in $\mathit{pts}()$ is in freed set.

    \item \textbf{Null Dereference Detection} (Section 2.1.7): Track nullable sources through pointer assignments. $\mathit{pts}()$ determines which pointers may contain null.

    \item \textbf{Data Race Detection} (Section 6.1.4): Two accesses conflict if $\mathit{pts}()$ sets overlap AND at least one is a write AND not synchronized.

    \item \textbf{Taint Analysis} (Section 8.1): Source(tainted) $\to^*$ Sink(tainted) through pointer indirection. \texttt{*p = tainted\_value} taints $\mathit{pts}(p)$.

    \item \textbf{Call Graph Construction} (Section 5.3): Virtual/indirect calls: target $= \mathit{pts}(\text{function\_pointer})$.

    \item \textbf{IFDS Precision} (Section 4.1): \IFDS{} is \textbf{not} applicable to pointer analysis (non-distributive), but \IFDS{} analyses (taint, uninit) \emph{use} $\mathit{pts}()$ for precision.

    \item \textbf{Sparse Value-Flow Analysis} (Section 5.6): SVF \textsc{requires} $\mathit{pts}()$ for Memory SSA construction. $\mu$/$\chi$ annotations use $\mathit{pts}()$ to determine affected regions.

    \item \textbf{Alternative for Large \LangC{}/\LangCpp{} Codebases} (Section 5.2.5): DSA provides $\BigO{n \cdot \alpha(n)}$ pointer analysis with heap cloning. For $>$ 100K LOC: DSA faster than Andersen, precision via context-sensitivity.
\end{enumerate}
\end{artifactbox}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Steensgaard's Analysis}
\label{ch:steensgaard}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{pillarbox}[title={Foundational Reference: Steensgaard 1996}]
\textbf{[Steensgaard96]} B.~Steensgaard. \emph{Points-to Analysis in Almost Linear Time}. POPL 1996.

Steensgaard's analysis trades precision for speed using unification. Key insight: Use \textbf{equality} instead of \textbf{subset}. \texttt{x = y} means $\mathit{pts}(x) = \mathit{pts}(y)$, not $\mathit{pts}(y) \subseteq \mathit{pts}(x)$. This enables union-find for $\BigO{n \cdot \alpha(n)}$ complexity.
\end{pillarbox}

\begin{remark}[Language-Dependent Trade-off]
This trade-off is \textbf{language-dependent}. For Rust, context-sensitive analysis with stack filtering (Section 12.22) is often \emph{faster} due to reduced spurious aliasing. See tension resolution at start of Part V.
\end{remark}

The F* formalization below implements Steensgaard's unification-based analysis using the classic union-find data structure. The \texttt{uf\_node} type stores both the union-find parent pointer and the points-to set for each equivalence class representative. Path compression (\texttt{find}) and union-by-rank (\texttt{union}) ensure near-linear time complexity.

The key difference from Andersen is in \texttt{union}: when two variables are assigned (\texttt{x = y}), their equivalence classes are \emph{merged}, causing all members to share the same points-to set. This loses precision but enables the $\BigO{n \cdot \alpha(n)}$ complexity that makes Steensgaard practical for very large codebases.

\begin{fstarcode}[title={Steensgaard's Points-To Analysis}]
(* ==================================================
   STEENSGAARD'S POINTS-TO ANALYSIS
   Source: Steensgaard 1996
   ================================================== *)
module BrrrMachine.PointerAnalysis.Steensgaard

(* Union-find with path compression *)
type uf_node = {
  mutable parent : int;
  mutable rank : int;
  mutable pts : set abstract_loc;
}

type union_find = array uf_node

val find : union_find -> int -> int
let rec find uf i =
  if uf.(i).parent = i then i
  else begin
    uf.(i).parent <- find uf uf.(i).parent;  (* Path compression *)
    uf.(i).parent
  end

val union : union_find -> int -> int -> unit
let union uf i j =
  let ri = find uf i in
  let rj = find uf j in
  if ri <> rj then begin
    (* Union by rank *)
    if uf.(ri).rank < uf.(rj).rank then
      uf.(ri).parent <- rj
    else if uf.(ri).rank > uf.(rj).rank then
      uf.(rj).parent <- ri
    else begin
      uf.(rj).parent <- ri;
      uf.(ri).rank <- uf.(ri).rank + 1
    end;
    (* Merge points-to sets *)
    let root = find uf i in
    uf.(root).pts <- Set.union uf.(ri).pts uf.(rj).pts
  end
\end{fstarcode}

The union-find operations above maintain the invariant that equivalent variables share a single representative. When points-to sets are merged during \texttt{union}, the combined set is stored only at the new root---this avoids duplicating storage across the equivalence class.

The solving algorithm below processes each constraint type differently. \texttt{AddrOf} constraints directly add to points-to sets. \texttt{Copy} constraints trigger unification. \texttt{Load} and \texttt{Store} constraints are more complex: they unify through the indirection, which can cause significant precision loss when many variables flow together. The final solution is extracted by finding the root of each variable's equivalence class.

\begin{fstarcode}[title={Steensgaard Solving Algorithm}]
val solve_steensgaard : list constraint_ -> pts_solution
let solve_steensgaard constraints =
  let vars = collect_all_vars constraints in
  let var_to_id = Map.of_list (List.mapi (fun i v -> (v, i)) vars) in
  let n = List.length vars in

  (* Initialize union-find *)
  let uf = Array.init n (fun i -> { parent = i; rank = 0; pts = Set.empty }) in

  (* Process constraints *)
  List.iter (fun c ->
    match c with
    | AddrOf lhs rhs ->
        let id = Map.find lhs var_to_id in
        uf.(id).pts <- Set.add rhs uf.(id).pts
    | Copy lhs rhs ->
        (* Key difference from Andersen: UNIFY instead of subset *)
        let id_lhs = Map.find lhs var_to_id in
        let id_rhs = Map.find rhs var_to_id in
        union uf id_lhs id_rhs
    | Load lhs rhs ->
        let id_lhs = Map.find lhs var_to_id in
        let id_rhs = find uf (Map.find rhs var_to_id) in
        Set.iter (fun loc ->
          let id_loc = Map.find (loc_to_var loc) var_to_id in
          union uf id_lhs id_loc
        ) uf.(id_rhs).pts
    | Store lhs rhs ->
        let id_lhs = find uf (Map.find lhs var_to_id) in
        let id_rhs = Map.find rhs var_to_id in
        Set.iter (fun loc ->
          let id_loc = Map.find (loc_to_var loc) var_to_id in
          union uf id_loc id_rhs
        ) uf.(id_lhs).pts
    | _ -> ()
  ) constraints;

  (* Extract solution *)
  Map.mapi (fun var id ->
    let root = find uf id in
    uf.(root).pts
  ) var_to_id
\end{fstarcode}

\begin{remark}[Complexity and Precision Loss]
\textbf{Complexity}: $\BigO{n \cdot \alpha(n)} \approx \BigO{n}$ where $\alpha$ is the inverse Ackermann function (effectively constant).

\textbf{Precision Loss Example}:
\begin{verbatim}
    p = &x
    q = &y
    r = p
    s = q
\end{verbatim}

\textbf{Andersen}: $\mathit{pts}(p) = \{x\}$, $\mathit{pts}(q) = \{y\}$, $\mathit{pts}(r) = \{x\}$, $\mathit{pts}(s) = \{y\}$

\textbf{Steensgaard} (with \texttt{r = q} somewhere): All unified: $\mathit{pts}(p) = \mathit{pts}(q) = \mathit{pts}(r) = \mathit{pts}(s) = \{x, y\}$

\textbf{Use Cases}:
\begin{itemize}
    \item Fast pre-analysis to identify ``interesting'' pointers
    \item Scalability-first scenarios (millions of LOC)
    \item When precision loss is acceptable
\end{itemize}
\end{remark}


%--------------------------------------------------
\section{Conditional Join (cjoin)}
\label{sec:steensgaard-cjoin}
%--------------------------------------------------

\begin{definition}[Conditional Join]
\textbf{Source}: \textbf{[Steensgaard96]}, Figures 5 \& 6

Key insight: When joining two ECRs where one has type $\bot$ (unknown), \textbf{defer} the join until the type becomes known. This prevents premature merging that loses precision.

\begin{itemize}
    \item \textbf{Without cjoin}: \texttt{x = y} immediately unifies $\mathit{pts}(x)$ and $\mathit{pts}(y)$
    \item \textbf{With cjoin}: If $\mathit{pts}(y) = \bot$, record ``join with $x$ when $y$ gets a type''
\end{itemize}

This is critical for handling uninitialized pointers and forward references.
\end{definition}

The implementation below extends the basic Steensgaard analysis with conditional joins. The \texttt{steens\_type} distinguishes between \texttt{TBot} (type unknown) and \texttt{TRef} (pointer with known target). The \texttt{pending} map tracks deferred joins: when we try to join with a variable whose type is still $\bot$, we record the pending join and execute it later when \texttt{settype} assigns a concrete type.

This deferred execution pattern is essential for handling forward references in the program---a pointer may be used before its allocation site is analyzed. The \texttt{cjoin} function implements the conditional logic: if the target type is known, join immediately; otherwise, record the pending join.

\begin{fstarcode}[title={Conditional Join Implementation}]
(* Steensgaard type: tau (location) x lambda (function signature) *)
type steens_type =
  | TBot                                           (* Bottom: type unknown *)
  | TRef : tau:ecr_id -> lam:ecr_id -> steens_type (* Pointer to tau, function lam *)

(* Extended state with pending conditional joins *)
type steens_state = {
  uf : union_find;
  types : map ecr_id steens_type;
  pending : map ecr_id (list ecr_id);  (* Pending joins: ecr -> waiting ECRs *)
}

(* Conditional join: defer if target type is bottom *)
val cjoin : steens_state -> ecr_id -> ecr_id -> steens_state
let cjoin st ecr1 ecr2 =
  let ecr2' = find st.uf ecr2 in
  match Map.find ecr2' st.types with
  | None | Some TBot ->
      (* Type unknown: DEFER the join until type is set *)
      let waiting = Map.find_default ecr2' [] st.pending in
      let pending' = Map.add ecr2' (ecr1 :: waiting) st.pending in
      { st with pending = pending' }
  | Some _ ->
      (* Type known: perform immediate join via union *)
      { st with uf = union st.uf ecr1 ecr2 }

(* Set type and trigger all pending conditional joins *)
val settype : steens_state -> ecr_id -> steens_type -> steens_state
let settype st ecr ty =
  let ecr' = find st.uf ecr in
  let st' = { st with types = Map.add ecr' ty st.types } in
  let waiting = Map.find_default ecr' [] st'.pending in
  let st'' = List.fold_left (fun s pending_ecr ->
    { s with uf = union s.uf ecr' pending_ecr }
  ) st' waiting in
  { st'' with pending = Map.remove ecr' st''.pending }
\end{fstarcode}


%--------------------------------------------------
\section{Data Structure Analysis (DSA)}
\label{sec:dsa}
%--------------------------------------------------

\begin{pillarbox}[title={Foundational Reference: Lattner, Lenharth, Adve 2007}]
\textbf{[Lattner07]} C.~Lattner, A.~Lenharth, V.~Adve. \emph{Making Context-sensitive Points-to Analysis with Heap Cloning Practical For The Real World}. PLDI 2007.
\end{pillarbox}

\begin{contributionbox}[title={DSA: Unification + Context-Sensitivity + Heap Cloning}]
\textbf{Key Insight}: The ``fast vs precise'' dichotomy is \textbf{false} at scale.

DSA combines:
\begin{itemize}
    \item \textbf{Unification-based} (like Steensgaard) for $\BigO{n \cdot \alpha(n)}$ base complexity
    \item \textbf{Context-sensitivity} to recover precision lost by unification
    \item \textbf{Heap cloning by acyclic call paths} to distinguish data structure \emph{instances}
\end{itemize}

\textbf{Result}: Precision comparable to Andersen, speed of Steensgaard
\begin{itemize}
    \item Linux kernel (355K LOC): 3.1 seconds, $<$ 46MB memory
    \item Analysis time $<$ 5\% of GCC compile time
\end{itemize}

\textbf{When to Use DSA} (vs Andersen):
\begin{itemize}
    \item Codebase $>$ 100K LOC where Andersen is too slow
    \item Heavy use of wrapper functions around malloc
    \item Generic data structure libraries (\texttt{std::vector}, linked lists)
    \item Need to distinguish instances created through same allocator
\end{itemize}
\end{contributionbox}

\subsection{DS Graph Structure}

DSA represents points-to information using \textbf{Data Structure Graphs (DS Graphs)}:

\begin{verbatim}
CROSS-REFERENCE: The F* type definitions for DSA are in Section 12.33.
See Section 12.33.1 for:
  - ds_flag: Node flags (Heap, Stack, Global, Unknown, Array, Modified,
             Read, Complete, Collapsed)
  - ds_node: Memory object representation with fields and flags
  - ds_graph: Per-function graph with nodes, edges, and variable mapping
  - call_node: Unresolved indirect call representation
  - dsa_state: Full analysis state across all functions

CONCEPTUAL SUMMARY:
  DS Graphs use disjoint-set data structures where:
  - Nodes represent abstract memory locations (heap, stack, global)
  - Fields provide field-sensitive pointer edges
  - Flags track provenance (H/S/G/U) and analysis state (M/R/C/O/A)
  - The Complete (C) flag is CRITICAL - see below
\end{verbatim}

\subsection{The Completeness Flag}

\begin{definition}[Completeness Flag Semantics]
A node is \textbf{Complete} when all operations on it have been analyzed:
\begin{itemize}
    \item All callers/callees have been incorporated
    \item No unknown external code can modify it
    \item Node will \textbf{never} be merged with other nodes in subsequent phases
\end{itemize}

\textbf{Aliasing Rules}:
\begin{itemize}
    \item Two nodes \textbf{without} C flag may represent common objects $\Rightarrow$ must assume they may alias (conservative)
    \item Two nodes \textbf{with} C flag $\Rightarrow$ guaranteed distinct (precise)
\end{itemize}

\textbf{Use Cases}:
\begin{enumerate}
    \item \textbf{Incomplete Programs}: External functions create incomplete nodes
    \item \textbf{Incremental Analysis}: Only reanalyze incomplete portions
    \item \textbf{Speculative Field-Sensitivity}: Assume type-safe until proven wrong
    \item \textbf{Library Analysis}: Mark library boundaries as incomplete
\end{enumerate}
\end{definition}

\subsection{Three-Phase Algorithm}

\begin{verbatim}
DSA ALGORITHM STRUCTURE:

PHASE 1: LOCAL ANALYSIS (per function)
  - Build DS graph using only intraprocedural information
  - Track loads, stores, allocations, direct calls
  - Create call nodes for unresolved indirect calls
  - Mark all nodes as INCOMPLETE (no C flag)

PHASE 2: BOTTOM-UP (BU) ANALYSIS
  - Traverse call graph SCCs in POSTORDER
  - Clone callee graphs into caller at call sites
  - Merge argument/return cells
  - Incrementally resolve indirect calls as function pointers become known
  - Use extended Tarjan's SCC algorithm with revisitation

PHASE 3: TOP-DOWN (TD) ANALYSIS
  - Traverse call graph SCCs in REVERSE POSTORDER
  - Merge caller context into callees
  - Mark nodes COMPLETE when all callers incorporated
  - Final completeness determination

COMPLEXITY:
  Time:  O(n*alpha(n) + K*alpha(K)*e)
  Space: O(f*K)
  where:
    n = number of instructions
    K = maximum DS graph size
    e = number of call graph edges
    f = number of functions
    alpha = inverse Ackermann function (effectively constant)
\end{verbatim}

\subsection{Heap Cloning: Distinguishing Data Structure Instances}

DSA's \textbf{heap cloning by acyclic call paths} distinguishes instances of data structures:

\begin{verbatim}
HEAP CLONING EXAMPLE:

// Two disjoint linked lists created via same wrapper
list *X = makeList(10);   // Call context: main -> makeList
list *Y = makeList(100);  // Call context: main -> makeList (different site)

void addToList(list *L) {
  // Without heap cloning: X and Y conflated (same malloc site)
  // With heap cloning: X and Y DISTINGUISHED by call context
}

DSA proves X and Y are DISJOINT because:
  - They reach malloc through DIFFERENT acyclic call paths
  - Each path creates a separate DS node
  - No aliasing between nodes with different call contexts

COMPARISON WITH ANDERSEN:
  - Andersen uses allocation-site naming: ONE abstract object per malloc
  - DSA uses call-path naming: SEPARATE objects per call context
  - For wrapper-heavy code: DSA more precise AND faster
\end{verbatim}

\subsection{Engineering Optimizations}

\begin{verbatim}
O(N^2) ELIMINATION TECHNIQUES:

1. GLOBALS GRAPH
   Problem: Propagating globals through call tree is O(N^2)
   Solution: Separate graph for global-reachable nodes
     - Functions reference the globals graph, not copy it
     - Speedup: 2-3.5x on large programs

2. GLOBAL EQUIVALENCE CLASSES
   Problem: Many globals point to same DS node
   Solution: Keep only representative global per DS node
     - Reduces EV entries from O(N^2) to O(N)
     - Speedup: up to 21.8x on Linux kernel

3. EFFICIENT GRAPH INLINING
   Problem: Cloning allocates nodes only to discard them
   Solution: Recursive traversal from common pointers only
     - Only visit nodes that will be reflected in target
     - Avoid allocate-then-discard pattern

GENERALIZABLE PATTERNS:
  - Separate representation for global entities
  - Representative-based compression
  - Lazy/demand-driven cloning
\end{verbatim}

\subsection{Precision Comparison with Andersen}

\begin{center}
\begin{tabular}{llll}
\toprule
\textbf{Benchmark} & \textbf{DSA May-Alias \%} & \textbf{Andersen May-Alias \%} & \textbf{Winner} \\
\midrule
181.mcf & 1.8\% & 20.5\% & DSA \\
175.vpr & 3.2\% & 18.1\% & DSA \\
186.crafty & 8.2\% & 23.4\% & DSA \\
300.twolf & 4.1\% & 15.8\% & DSA \\
197.parser & 22.1\% & 18.5\% & Andersen \\
255.vortex & 19.8\% & 16.2\% & Andersen \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key Finding}:
\begin{itemize}
    \item \textbf{DSA better when}: wrapper functions, data structure libraries
    \item \textbf{Andersen better when}: no wrappers, direct malloc usage
    \item Context-sensitivity + heap cloning compensates for unification
\end{itemize}

\textbf{Hybrid Strategy}:
\begin{itemize}
    \item Use DSA for initial fast analysis
    \item Demand-driven refinement for specific queries where DSA is imprecise
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{On-the-Fly Call Graph Construction}
\label{ch:call-graph}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{pillarbox}[title={Foundational Reference: Qilin}]
\textbf{[Qilin22]} D.~He, J.~Lu, J.~Xue. \emph{Qilin: A New Framework For Supporting Fine-Grained Context-Sensitivity in Java Pointer Analysis}. ECOOP 2022.
\end{pillarbox}

\begin{contributionbox}[title={The Chicken-and-Egg Problem}]
For OOP languages with virtual dispatch:
\begin{itemize}
    \item Call graph construction needs pointer analysis (to resolve receivers)
    \item Pointer analysis needs call graph (to know which methods to analyze)
\end{itemize}

\textbf{Solution}: Solve them \textsc{together}, on-the-fly.

\textbf{Result}: Qilin (single-threaded) is 2.4x faster than Doop (8-threaded) while achieving identical precision.
\end{contributionbox}

The Qilin framework parameterizes pointer analysis over three orthogonal dimensions, enabling fine-grained control over the precision/performance trade-off. The \texttt{context\_constructor} determines \emph{how} calling contexts are represented: call-site strings ($k$-CFA), receiver objects (object-sensitivity), or receiver types (type-sensitivity). The \texttt{context\_selector} determines \emph{which} methods receive context-sensitive treatment---ZIPPER (Section~\ref{sec:zipper}) uses this to selectively apply context-sensitivity only where it improves precision. The \texttt{heap\_abstractor} controls allocation-site naming granularity.

\begin{fstarcode}[title={Parameterized Context-Sensitivity}]
(* ==================================================
   ON-THE-FLY CALL GRAPH + POINTER ANALYSIS
   Source: Qilin (He, Lu, Xue 2022)
   Three parameters control precision/performance tradeoff:
   1. Context Constructor (Cons): HOW to build contexts
   2. Context Selector (Sel): WHEN to apply context-sensitivity
   3. Heap Abstractor (HeapAbs): HOW to abstract allocation sites
   ================================================== *)
module BrrrMachine.PointerAnalysis.OnTheFly

type context_constructor =
  | ConsInsens      (* No context - fastest, least precise *)
  | ConsCallsite    (* k-CFA: use call sites as context *)
  | ConsObject      (* Object-sensitivity: use receiver objects *)
  | ConsType        (* Type-sensitivity: use receiver types *)
  | ConsHybrid      (* Selective based on heuristics *)

type context_selector =
  | SelUniform : k:nat -> hk:nat -> context_selector
  | SelSelective : cs_methods:set method_id -> context_selector
  | SelPartial : cs_vars:set var_id -> context_selector

type heap_abstractor =
  | HeapAllocationSite   (* Standard: one abstract loc per new *)
  | HeapTypeConsistent   (* Merge by type *)
  | HeapHeuristic        (* Exclude certain types like String, StringBuilder *)
\end{fstarcode}

These type definitions encode the configuration space for context-sensitive analysis. The \texttt{SelSelective} variant enables ZIPPER-style selective context-sensitivity, where only methods in \texttt{cs\_methods} receive context-sensitive treatment. The \texttt{SelPartial} variant provides even finer granularity at the variable level.

The core algorithm below interleaves call graph construction with points-to propagation. The key insight is maintaining \emph{old} and \emph{new} partitions of both reachable methods and points-to facts. When a virtual call's receiver type becomes more precise, new callees are discovered and added to \texttt{new\_part}. The algorithm continues until both partitions stabilize (fixpoint).

\begin{fstarcode}[title={Core Algorithm: Interleaved CG + PTS Construction}]
val solve_on_the_fly :
  cpg ->
  cons:context_constructor ->
  sel:context_selector ->
  heap:heap_abstractor ->
  (call_graph * pts_solution)
let solve_on_the_fly cpg cons sel heap =
  let state = {
    pts = Map.empty;
    call_graph = { old_part = Set.empty; new_part = Set.empty };
    reachable = { old_part = Set.empty;
                  new_part = Set.singleton (entry_method cpg, empty_ctx) };
    worklist = [];
  } in

  let rec process state =
    (* Step 1: Process newly reachable methods *)
    let state = process_new_reachable state cons sel heap in
    (* Step 2: Propagate points-to through worklist *)
    match pop_worklist state with
    | None -> state  (* Fixpoint reached *)
    | Some ((v, ctx), state') ->
        let pts_new = get_new_pts state' v ctx in
        (* Step 3: Propagate through direct constraints *)
        let state' = propagate_direct state' v ctx pts_new in
        (* Step 4: Propagate through indirect constraints *)
        let state' = propagate_indirect state' v ctx pts_new in
        (* Step 5: Resolve virtual calls based on new receiver types *)
        let state' = resolve_virtual_calls state' v ctx pts_new cons sel in
        (* Step 6: Flush and continue *)
        let state' = flush_pts state' v ctx in
        process state'
  in
  let final = process state in
  (extract_call_graph final, extract_pts final)
\end{fstarcode}

\begin{theorem}[Variable-Level CS Subtlety]
\textbf{Source}: Qilin Theorem 1

Under method-level CS: When method $m$ is analyzed under new context $c$, $\mathit{PTS}(v, \mathit{Sel}(v, c))_{\text{old}} = \emptyset$ always holds.

Under variable-level CS: $\mathit{PTS}(v, \mathit{Sel}(v, c))_{\text{old}} \neq \emptyset$ may hold because $\mathit{Sel}(v, c) = \mathit{Sel}(v, c')$ can be true for $c \neq c'$.

\textbf{Implication}: Must propagate through \textsc{new} edges using \textsc{old} points-to facts.
\end{theorem}

%--------------------------------------------------
\section{When to Use Each Approach}
\label{sec:cg-decision}
%--------------------------------------------------

\begin{verbatim}
DECISION TREE: Phased vs On-the-Fly

Is the language...
  |
  +-- Purely procedural (C, Fortran)?
  |   -> Use PHASED (Section 3.1.7) - simpler, sufficient
  |
  +-- Has virtual dispatch (Java, Python, C++, JS)?
  |   -> Use ON-THE-FLY (Section 5.3) - REQUIRED for soundness
  |
  +-- Mixed (Rust, Go)?
      -> Use ON-THE-FLY for trait objects / interfaces
         Phased is OK for static dispatch portions
\end{verbatim}


%--------------------------------------------------
\section{Selective Context Sensitivity (ZIPPER)}
\label{sec:zipper}
%--------------------------------------------------

\begin{pillarbox}[title={Foundational Reference: ZIPPER}]
\textbf{[ZIPPER20]} Y.~Li, T.~Tan, A.~M{\o}ller, Y.~Smaragdakis. \emph{A Principled Approach to Selective Context Sensitivity for Pointer Analysis}. OOPSLA 2020.
\end{pillarbox}

Context sensitivity dramatically improves pointer analysis precision but is expensive. The fundamental insight from ZIPPER is that \textbf{only $\sim$38\% of methods actually benefit from context sensitivity}. The remaining 62\% receive context-sensitive treatment that consumes analysis time without improving results.

\subsection{The Uniform Context Sensitivity Problem}

\begin{verbatim}
OBSERVATION: Applying context sensitivity uniformly is wasteful.

Traditional approaches:
- 2-object-sensitive: ALL methods get 2-level context
- 2-callsite-sensitive: ALL methods get 2-level call string

Reality:
- ~62% of methods: CS provides NO precision benefit
- ~38% of methods: CS is precision-critical
- Wasted analysis time on the 62% can be 3-25x the useful work
\end{verbatim}

\subsection{The Three Precision-Loss Patterns}

ZIPPER identifies \textbf{why} context-insensitive analysis loses precision through three observable value-flow patterns:

\begin{definition}[Pattern 1: Direct Flow]
Object $O$ enters via \textsc{In} method parameter, flows through assignments and field operations, exits via \textsc{Out} method return of \textsc{same} class.

\begin{verbatim}
class Person {
  String name;
  void setName(String nm) { this.name = nm; }  // In method
  String getName() { return this.name; }        // Out method
}
\end{verbatim}

\textbf{Without CS}: Objects from different \texttt{setName()} calls merge in \texttt{nm} parameter.

\textbf{With CS}: Each call site distinguished, precise points-to.
\end{definition}

\begin{definition}[Pattern 2: Wrapped Flow]
Object $O$ enters via \textsc{In} method, gets stored in wrapper object $W$, wrapper $W$ flows out via \textsc{Out} method return.

\begin{verbatim}
void add(Object el) { this.elem = el; }           // Object wrapped
Iterator iterator() { return new Iterator(elem); } // Wrapper flows out
\end{verbatim}

Imprecision manifests when wrapper is later accessed.
\end{definition}

\begin{definition}[Pattern 3: Unwrapped Flow]
Object $O$ (a carrier) enters via \textsc{In} method, contents are loaded from $O$, loaded contents flow out via \textsc{Out} method return.

\begin{verbatim}
SyncBox(Box box) { this.box = box; }
Object getItem() {
  Box b = this.box;       // Carrier loaded
  return b.getItem();     // Unwrapped content flows out
}
\end{verbatim}
\end{definition}

\subsection{The ZIPPER Algorithm}

\begin{verbatim}
ALGORITHM: Zipper(program, OFG)
INPUT:  program - Source program
        OFG     - Object Flow Graph from CI pre-analysis

1. PCM <- {}  // Precision-Critical Methods

2. FOR EACH class c in program:
3.   PFG_c <- build_pfg(c, OFG)  // Add wrap/unwrap edges
4.
5.   // Backward reachability from Out method returns
6.   FlowNodes <- {}
7.   FOR EACH Out method m of c:
8.     FOR EACH return var r of m:
9.       FlowNodes <- FlowNodes U backward_reach(PFG_c, r)
10.
11.  // Extract methods containing flow nodes
12.  FOR EACH node n in FlowNodes:
13.    PCM <- PCM U {method_of(n)}

14. RETURN PCM

COMPLEXITY: O(|classes| * |OFG|)
            In practice: ~11 seconds for large Java programs
\end{verbatim}

\subsection{Empirical Results}

\begin{center}
\begin{tabular}{llll}
\toprule
\textbf{Configuration} & \textbf{Precision Preserved} & \textbf{Speedup} & \textbf{PCM Coverage} \\
\midrule
Zipper-2obj & 98.8\% & 3.4x (avg), 9.4x (max) & $\sim$38\% methods \\
Zipper$_e$-2obj (PV=5\%) & 94.7\% & 25.5x (avg), 88x (max) & $\sim$14\% methods \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Surprising Result}: For 5 programs, Zipper$_e$-guided CS is \emph{faster} than context-insensitive analysis while being \emph{more precise}. This occurs because the reduced method set enables better cache locality.


%--------------------------------------------------
\section{Python-Specific Call Graph via Type Inference}
\label{sec:python-cg}
%--------------------------------------------------

\begin{pillarbox}[title={Foundational Reference: JARVIS}]
\textbf{[JARVIS23]} M.~Huang et al. \emph{JARVIS: Scalable and Precise Application-Centered Call Graph Construction for Python}. ICSE 2023.
\end{pillarbox}

\begin{contributionbox}[title={Why Qilin Is Insufficient for Python}]
Qilin (Section 5.3) is designed for \textbf{class-based OOP} with:
\begin{itemize}
    \item Explicit type declarations (\LangJava{}, \LangCpp{})
    \item Receiver-based dispatch (\texttt{obj.method(args)})
    \item Static method signatures
\end{itemize}

\LangPython{} requires \textbf{different handling} for:
\begin{itemize}
    \item Duck typing (no static type at receiver)
    \item Module-level functions (no receiver)
    \item First-class functions and closures
    \item Magic methods (\texttt{\_\_getattr\_\_}, \texttt{\_\_call\_\_}, descriptors)
    \item MRO (Method Resolution Order) via C3 linearization
    \item Decorator wrapping
\end{itemize}

\textbf{Solution}: Function Type Graph (FTG) --- per-function type inference with flow-sensitive strong updates.

\textbf{Results}: 84\% higher precision than flow-insensitive (PyCG), 67\% faster on exhaustive whole-program analysis. Handles 200k+ LOC codebases (PyCG runs OOM).
\end{contributionbox}

The Function Type Graph (FTG) represents Python's type inference results. Unlike Java where types are declared, Python types must be inferred from usage patterns. The \texttt{python\_type\_element} discriminates between modules, classes, functions, variables, and builtins---each has different scoping and attribute lookup rules. The \texttt{python\_namespace} captures the name-to-type bindings visible at each scope level.

The \texttt{ftg\_relation} type encodes three kinds of type facts: \texttt{FTGPointsTo} for pointer-like relationships, \texttt{FTGTypeOf} for expression types, and \texttt{FTGFieldOf} for attribute access. These relations form a graph that can be queried to resolve method calls.

\begin{fstarcode}[title={Function Type Graph Structure}]
module BrrrMachine.Python.FTG

type python_type_element =
  | PTEModule : name:string -> is_external:bool -> python_type_element
  | PTEClass : name:string -> module:string -> python_type_element
  | PTEFunction : name:string -> module:string -> python_type_element
  | PTEVariable : name:string -> scope:string -> python_type_element
  | PTEBuiltin : name:string -> python_type_element

type python_namespace = list (string * python_type_element)

type python_type = {
  element : python_type_element;
  namespace : python_namespace;
  name : string;
}

type ftg_relation =
  | FTGPointsTo : src:python_type -> dst:python_type -> ftg_relation
  | FTGTypeOf : expr:ir_expr -> ty:python_type -> ftg_relation
  | FTGFieldOf : base:python_type -> field:string -> value:python_type -> ftg_relation

type function_type_graph = {
  types : set python_type;
  exprs : set ir_expr;
  relations : set ftg_relation;
}
\end{fstarcode}

\subsection{C3 Linearization for Python MRO}

Python's multiple inheritance uses the C3 linearization algorithm to compute the Method Resolution Order (MRO). This determines the order in which base classes are searched when resolving a method call. The algorithm must satisfy three properties: (1) children come before parents, (2) the order respects the left-to-right ordering in the class definition, and (3) the result is consistent across all classes. If no linearization satisfying these properties exists, Python raises a \texttt{TypeError} at class definition time.

The \texttt{c3\_merge} function implements the merge step: it finds a class that appears at the head of some list but not in the tail of any other list (the ``good head'' property). The recursion terminates when all lists are empty. A \texttt{None} result indicates an inconsistent hierarchy that Python would reject.

\begin{fstarcode}[title={C3 Linearization}]
(* Python uses C3 linearization for method lookup in multiple inheritance.
   Example:
     class A: pass
     class B(A): pass
     class C(A): pass
     class D(B, C): pass  # MRO: D -> B -> C -> A -> object *)

val c3_merge : list (list class_id) -> option (list class_id)
let rec c3_merge seqs =
  let non_empty = List.filter (fun l -> not (List.is_empty l)) seqs in
  if List.is_empty non_empty then Some []
  else
    let find_candidate () =
      List.find_opt (fun seq ->
        let head = List.hd seq in
        List.for_all (fun other_seq ->
          not (List.mem head (List.tl other_seq))
        ) non_empty
      ) non_empty
    in
    match find_candidate () with
    | None -> None  (* C3 merge failure - inconsistent hierarchy *)
    | Some winner_seq ->
        let head = List.hd winner_seq in
        let remaining = List.map (fun seq ->
          List.filter (fun c -> c <> head) seq
        ) non_empty in
        match c3_merge remaining with
        | None -> None
        | Some rest -> Some (head :: rest)

val compute_mro : class_summary -> class_id -> list class_id
let compute_mro summ cls =
  let parents = get_direct_parents summ cls in
  if List.is_empty parents then [cls; "object"]
  else
    let parent_mros = List.map (compute_mro summ) parents in
    match c3_merge (parent_mros @ [parents]) with
    | Some mro -> cls :: mro
    | None -> cls :: List.concat parent_mros  (* Fallback *)
\end{fstarcode}

\subsection{Call Graph Algorithm Selection}

\begin{verbatim}
                        Language Type?
                             |
         +-------------------+-------------------+
         |                   |                   |
    Static Types      Gradual Types       Dynamic Types
   (Java, C++, Rust)   (TypeScript)      (Python, JS)
         |                   |                   |
         v                   v                   v
    Use Qilin          Use Qilin           Use JARVIS-style
    (Section 5.3)      with type hints     FTG (Section 5.3.3)

  SUMMARY:
    - Java/C++:     Qilin + ZIPPER (object-sensitive, selective CS)
    - Rust:         Qilin + Stack Filtering (Section 12.22)
    - TypeScript:   Qilin with type annotations
    - Python:       JARVIS FTG (flow-sensitive type inference)
    - JavaScript:   Hybrid FTG + prototype chain analysis
    - Mixed:        Use appropriate algorithm per module
\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Shape Analysis via Three-Valued Logic}
\label{ch:shape-tvla}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{pillarbox}[title={Foundational Reference: TVLA}]
\textbf{[SRW02]} M.~Sagiv, T.~Reps, R.~Wilhelm. \emph{Parametric Shape Analysis via 3-Valued Logic}. TOPLAS 2002.
\end{pillarbox}

Shape analysis determines structural properties of heap data: lists, trees, DAGs, cycles. Unlike pointer analysis which tracks aliasing, shape analysis tracks \emph{data structure invariants}.

%--------------------------------------------------
\section{Three-Valued Foundation}
\label{sec:tvla-foundation}
%--------------------------------------------------

\begin{definition}[Three-Valued Logic]
Two-valued logic is \textbf{too imprecise} for heap analysis. Consider: after \texttt{x = y->next} in a list traversal, $x$ might point to a summary node (represents multiple cells). Does \texttt{x->next = y}? Unknown! Neither yes nor no is safe.

\textbf{Three-Valued Logic}:
\begin{itemize}
    \item $1$ = definitely true
    \item $0$ = definitely false
    \item $\frac{1}{2}$ = unknown/indefinite
\end{itemize}

Information ordering: $\frac{1}{2} \sqsubseteq 0$ and $\frac{1}{2} \sqsubseteq 1$ (Unknown is \emph{less} informative than definite).
\end{definition}

%--------------------------------------------------
\section{Shape Structures}
\label{sec:tvla-structures}
%--------------------------------------------------

Shape structures represent abstract heaps using three-valued predicates. The \texttt{universe} contains abstract locations, some of which are \emph{summary nodes} representing multiple concrete heap cells. The \texttt{predicates} map assigns three-valued truth values to predicate applications---for example, \texttt{n(u,v)} might be $1$ (definitely an edge), $0$ (definitely no edge), or $\frac{1}{2}$ (unknown).

The predicates below are standard for linked data structure analysis. Core predicates like \texttt{x} (variable points here) and \texttt{n} (next-pointer edge) describe the heap graph structure. Instrumentation predicates like \texttt{r\_x} (reachable from x), \texttt{c} (on a cycle), and \texttt{is} (shared/aliased) are derived predicates that dramatically improve precision by encoding properties that would otherwise be lost to abstraction.

\begin{fstarcode}[title={Shape Structure Types}]
type shape_structure = {
  universe : set abstract_loc;
  predicates : map pred_name (list loc -> tv);
  is_summary : loc -> tv;
}

(* Core predicates *)
val x : loc -> tv           (* Variable x points here *)
val n : loc -> loc -> tv    (* n(u,v) = u.next points to v *)
val sm : loc -> tv          (* Summary: represents multiple concrete *)

(* Derived instrumentation predicates - CRITICAL for precision *)
val r_x : loc -> tv    (* Reachable from x: r_x(v) = TC(x,v) via n *)
val c : loc -> tv      (* On cycle: c(v) = exists u. n*(v,u) /\ n(u,v) *)
val is : loc -> tv     (* Shared: is(v) = exists u,u'. u!=u' /\ n(u,v) /\ n(u',v) *)
\end{fstarcode}

%--------------------------------------------------
\section{Embedding Theorem}
\label{sec:tvla-embedding}
%--------------------------------------------------

\begin{theorem}[Embedding Theorem]
\textbf{Source}: \textbf{[SRW02]}, Theorem 3.7

If $S \sqsubseteq^f S'$ ($S$ embeds in $S'$ via $f$), then for every formula $\varphi$ and assignment $Z$:
\[
\sem{\varphi}^3_S(Z) \sqsubseteq \sem{\varphi}^3_{S'}(f \circ Z)
\]

\textbf{Meaning}: If concrete heap satisfies $\varphi$ (value 1), abstract shape doesn't refute it (value $\neq 0$).
\end{theorem}

%--------------------------------------------------
\section{Focus-Transform-Coerce Algorithm}
\label{sec:tvla-ftc}
%--------------------------------------------------

\begin{verbatim}
ABSTRACT INTERPRETATION STEP:

1. FOCUS: Make relevant predicates definite
   - If n(x,_) = 1/2, split structure into cases
   - "Materialize" concrete nodes from summaries
   - Essential for loop analysis precision

2. TRANSFORM: Apply statement semantics
   - Update predicates according to operation
   - Standard abstract interpretation

3. COERCE: Apply compatibility constraints
   - Propagate implications: phi_1 => phi_2
   - Detect inconsistencies (return bottom)
   - Sharpen indefinite predicates

LOOP TERMINATION:
  - Bounded structures have finite domain
  - Canonical abstraction: merge by predicate values
  - No explicit widening needed!
\end{verbatim}

%--------------------------------------------------
\section{Integration with Brrr-Machine}
\label{sec:tvla-integration}
%--------------------------------------------------

\begin{artifactbox}
\textbf{Shape Analysis Track}:
\begin{enumerate}
    \item Build initial shape from \CPG{}:
    \begin{itemize}
        \item Stack/global vars $\to$ individual nodes
        \item Allocations $\to$ summary nodes ($\mathit{sm} = \frac{1}{2}$)
    \end{itemize}

    \item Run focus-transform-coerce at each \CPG{} node

    \item Extract properties:
    \begin{itemize}
        \item $\mathit{list}(x, \mathit{null})$ = valid list from $x$
        \item $\mathit{tree}(x)$ = valid tree rooted at $x$
        \item $\mathit{shared}(v) = 0 \Longrightarrow$ unique ownership
        \item $\mathit{cycle}(v) = 0 \Longrightarrow$ acyclic
    \end{itemize}
\end{enumerate}

\textbf{Detected Bugs}:
\begin{itemize}
    \item Use-after-free: $r_x(v) = 1$ but $\mathit{freed}(v) = 1$
    \item Dangling pointer: $n(u,v) = 1$ but $\mathit{freed}(v) = 1$
    \item Memory leak: $\neg(\exists x.\, r_x(v) = 1)$ but $\mathit{freed}(v) = 0$
    \item List corruption: was list, now has cycle
\end{itemize}
\end{artifactbox}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Symbolic Heap Shape Analysis}
\label{ch:shape-symbolic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{pillarbox}[title={Foundational Reference: Distefano, O'Hearn, Yang 2006}]
\textbf{[DOY06]} D.~Distefano, P.~O'Hearn, H.~Yang. \emph{A Local Shape Analysis Based on Separation Logic}. TACAS 2006.
\end{pillarbox}

\begin{contributionbox}[title={Symbolic Heaps vs TVLA}]
An \textbf{alternative} to TVLA (Section 5.4) for shape analysis based on separation logic.

\textbf{Key Advantages}:
\begin{itemize}
    \item Natural integration with frame rule (Section 7.4)
    \item Direct memory leak detection via ``junk'' predicate
    \item Simpler abstraction via canonicalization rules
\end{itemize}

\textbf{Trade-off vs TVLA}:
\begin{itemize}
    \item \textbf{TVLA}: better for materialization/summarization
    \item \textbf{Symbolic Heaps}: better for compositional (per-procedure) analysis
\end{itemize}

Both approaches are \textsc{sound}; choice depends on analysis goals.
\end{contributionbox}

%--------------------------------------------------
\section{Symbolic Heap Structure}
\label{sec:symheap-structure}
%--------------------------------------------------

\begin{definition}[Symbolic Heap]
A symbolic heap has form $\Pi \vdash \Sigma$ where:
\begin{itemize}
    \item $\Pi$ (pure part): equalities and disequalities between variables
    \item $\Sigma$ (spatial part): heap predicates in separation logic
\end{itemize}

This forms a \textsc{finite} abstract domain when combined with canonicalization, bounded by $2^{129n^2 + 18n + 2}$ for $n$ program variables (Theorem 14).
\end{definition}

The F* types below encode symbolic heaps in separation logic style. Variables come in three flavors: program variables (\texttt{SHProgVar}), existentially-quantified primed variables (\texttt{SHPrimedVar}) introduced during bi-abduction, and the distinguished null value (\texttt{SHNil}). The pure formula captures equalities and disequalities between variables, while the spatial formula describes heap structure.

The key spatial connectives are: \texttt{SPointsTo} for a single heap cell, \texttt{SListSeg} for an inductively-defined list segment, \texttt{SJunk} for unreachable garbage (memory leaks), and \texttt{SSep} for the separating conjunction. The \texttt{abstract\_state} is a \emph{set} of symbolic heaps, representing disjunctive information from different execution paths.

\begin{fstarcode}[title={Symbolic Heap Types}]
module BrrrMachine.ShapeAnalysis.SymbolicHeap

type sh_var =
  | SHProgVar : v:var_id -> sh_var
  | SHPrimedVar : v:nat -> sh_var        (* Existentially quantified x' *)
  | SHNil : sh_var

type pure_formula =
  | PEq : sh_var -> sh_var -> pure_formula
  | PNeq : sh_var -> sh_var -> pure_formula
  | PAnd : pure_formula -> pure_formula -> pure_formula
  | PTrue

type spatial_formula =
  | SEmpty                                          (* emp *)
  | SPointsTo : src:sh_var -> field:string -> dst:sh_var -> spatial_formula
  | SListSeg : src:sh_var -> dst:sh_var -> spatial_formula  (* ls(x, y) *)
  | SJunk                                           (* junk - unreachable garbage *)
  | SSep : spatial_formula -> spatial_formula -> spatial_formula  (* * *)

type symbolic_heap = {
  pure : pure_formula;
  spatial : spatial_formula;
}

type abstract_state = set symbolic_heap
\end{fstarcode}

%--------------------------------------------------
\section{Heap Predicates}
\label{sec:symheap-predicates}
%--------------------------------------------------

\begin{definition}[Points-To Predicate]
The cell at address $(x + \mathit{offset}(f))$ contains value $y$. This is the \textbf{atomic} spatial predicate.

\textbf{Critical}: Points-to is \textsc{exclusive} --- it \emph{owns} the cell. Two points-to on same address cannot be separated.
\end{definition}

\begin{definition}[List Segment Predicate]
$\mathit{ls}(x, y)$: There is a well-formed singly-linked list from $x$ to $y$.

Defined \textbf{inductively}:
\begin{itemize}
    \item $\mathit{ls}(x, x) = \mathit{emp}$ (empty list --- base case)
    \item $\mathit{ls}(x, y) = \exists z.\, x.\mathit{next} \mapsto z * \mathit{ls}(z, y)$ (cons cell)
\end{itemize}

\textbf{Critical}: The list segment \emph{owns} all intermediate nodes.
\end{definition}

\begin{definition}[Junk Predicate]
Represents heap cells that are \textsc{unreachable} from program variables. This is garbage --- a \textbf{memory leak indicator}.

\textbf{Critical Property}: $\mathit{junk} * \mathit{junk} = \mathit{junk}$ (idempotent under separation)

This allows \textsc{unbounded} accumulation of leaks without state explosion.
\end{definition}

%--------------------------------------------------
\section{Canonicalization as Widening}
\label{sec:symheap-canon}
%--------------------------------------------------

\begin{theorem}[Termination Guarantee]
\textbf{Source}: \textbf{[DOY06]}, Theorem 14

The number of \textsc{canonical} symbolic heaps is bounded by:
\[
2^{129n^2 + 18n + 2}
\]
where $n$ = number of program variables.

This is \textsc{finite}, ensuring:
\begin{enumerate}
    \item Canonicalization always terminates
    \item The abstract domain is finite
    \item Fixpoint iteration terminates without explicit widening
\end{enumerate}
\end{theorem}

\begin{verbatim}
CANONICALIZATION RULES:

SubstEq:   x' = E  =>  substitute E for x' everywhere
Garbage1:  P(x', E) where x' unreachable => junk
Garbage2:  P1(x', y') * P2(y', x') cycle => junk
Abs1:      P1(E, x') * P2(x', F) => ls(E, nil) when Pi |- F = nil
Abs2:      P1(E, x') * P2(x', F) * P3(G, H) => ls(E, F) * P3(G, H)
           (when F != G)
\end{verbatim}

%--------------------------------------------------
\section{Memory Leak Detection via Junk}
\label{sec:symheap-leak}
%--------------------------------------------------

The \texttt{SJunk} predicate makes leak detection trivial: after canonicalization, any heap cells that become unreachable from program variables are folded into \texttt{junk}. The idempotent property ($\mathit{junk} * \mathit{junk} = \mathit{junk}$) means the abstract state remains bounded even with unbounded leaks.

The leak classification below distinguishes three cases: \texttt{DefiniteLeak} when all abstract states contain junk (the leak is unavoidable), \texttt{PossibleLeak} when some but not all states contain junk (path-dependent), and \texttt{NoLeak} when no states contain junk. This maps directly to the manifest/latent bug classification in incorrectness logic (Section 12.3).

\begin{fstarcode}[title={Leak Detection}]
type leak_result =
  | NoLeak
  | PossibleLeak
  | DefiniteLeak of leaked_cells : nat

val detect_leaks : abstract_state -> leak_result
let detect_leaks final_states =
  let has_junk sh = contains_junk sh.spatial in
  let all_have_junk = Set.for_all has_junk final_states in
  let some_have_junk = Set.exists has_junk final_states in
  if all_have_junk then
    DefiniteLeak (min_junk_size final_states)
  else if some_have_junk then
    PossibleLeak
  else
    NoLeak
\end{fstarcode}

\begin{remark}[Integration with Manifest Bug Classification]
Cross-reference: Section 12.3 (Manifest/Latent bug classification)

\textbf{DefiniteLeak} is a \textsc{manifest} bug because:
\begin{enumerate}
    \item Junk appears on \textsc{all} execution paths
    \item The leak is \textsc{provable} --- not a false positive
    \item It satisfies the ``reachability condition''
\end{enumerate}

\textbf{PossibleLeak} is a \textsc{latent} bug because:
\begin{enumerate}
    \item Junk appears on \textsc{some} but not all paths
    \item The leak depends on which path is taken
    \item Caller context determines if leak manifests
\end{enumerate}
\end{remark}

%--------------------------------------------------
\section{Frame Rule for Compositional Analysis}
\label{sec:symheap-frame}
%--------------------------------------------------

\begin{theorem}[Abstract Frame Rule]
\textbf{Source}: \textbf{[DOY06]}, Theorem 11

If $\{P\}\, C\, \{Q\}$ and $C$ doesn't modify variables in $R$, then $\{P * R\}\, C\, \{Q * R\}$.

For all $X, Y \in \mathcal{P}(\mathit{CSH})$, if $\mathit{Vars}(Y) \cap \mathit{Mod}(C) = \emptyset$ then:
\[
\gamma(\mathcal{A}\sem{c}(X * Y)) \subseteq \gamma((\mathcal{A}\sem{c}X) * Y)
\]

This enables \textsc{compositional} interprocedural analysis:
\begin{itemize}
    \item Analyze each procedure in isolation on its \textsc{footprint}
    \item Combine via frame rule
    \item No need to re-analyze callee for each calling context
\end{itemize}
\end{theorem}

\begin{definition}[Procedure Footprint]
A function's \textsc{footprint} is the \textsc{minimal} heap needed to execute safely. By Theorem 12 of \textbf{[DOY06]}, this footprint soundly approximates the full semantics:
\[
\mathit{foot}(c)\sigma = \begin{cases} \mathcal{A}\sem{c}\{\sigma\} & \text{if } \mathit{onlyaccesses}(c, \sigma) \\ \text{undefined} & \text{otherwise} \end{cases}
\]
\end{definition}

The footprint representation below captures a procedure's memory behavior independently of its calling context. The \texttt{input\_footprint} is the minimal heap the procedure needs to execute (discovered via bi-abduction), and \texttt{output\_footprints} are the possible resulting heaps. The \texttt{modified\_vars} track which variables may be changed.

The \texttt{apply\_footprint} function shows how footprints enable compositional analysis: at a call site, we match the caller's state against the callee's footprint, extracting the \emph{frame}---the part of the heap untouched by the call. After the call, we combine the callee's output with the frame. This is the abstract frame rule in action.

\begin{fstarcode}[title={Footprint Computation and Application}]
type procedure_footprint = {
  input_footprint : symbolic_heap;
  output_footprints : set symbolic_heap;
  modified_vars : set var_id;
}

val compute_footprint : cpg -> func_id -> procedure_footprint
let compute_footprint cpg func =
  let init = { pure = PTrue; spatial = SEmpty } in
  let results = symbolic_exec_function cpg func init in
  let minimal_input = collect_anti_frames results in
  let outputs = Set.map (fun r -> r.post) results in
  { input_footprint = canonicalize minimal_input;
    output_footprints = Set.map canonicalize outputs;
    modified_vars = compute_mod cpg func }

val apply_footprint :
  caller_state : symbolic_heap ->
  callee_footprint : procedure_footprint ->
  call_site : node_id ->
  set symbolic_heap
let apply_footprint caller_state footprint call_site =
  match find_matching_substate caller_state footprint.input_footprint with
  | Some (matching, frame) ->
      Set.map (fun output ->
        canonicalize { output with spatial = SSep output.spatial frame }
      ) footprint.output_footprints
  | None ->
      Set.singleton { pure = PTrue;
                     spatial = SSep SJunk (error_marker "footprint_mismatch") }
\end{fstarcode}

%--------------------------------------------------
\section{Symbolic Execution for Symbolic Heaps}
\label{sec:symheap-exec}
%--------------------------------------------------

Symbolic execution on symbolic heaps discovers preconditions ``on demand'' via bi-abduction. When a statement requires a heap cell that is not currently available, the analysis does not fail---instead, it \emph{abduces} (infers) the missing cell as part of the required precondition. This is captured in the \texttt{anti\_frame} field of \texttt{exec\_result}.

The code below handles key memory operations. For loads (\texttt{SLoad}), if the required cell is available, we extract its value; if not, we create a fresh existential and record the needed cell in \texttt{anti\_frame}. For stores (\texttt{SStore}), we update the heap or abduce the target cell. For allocation (\texttt{SAlloc}), we extend the heap with a fresh cell. For free (\texttt{SFree}), we remove the cell. The \texttt{exit} field tracks whether the operation succeeded (\texttt{Ok}) or errored (\texttt{Er}).

\begin{fstarcode}[title={Symbolic Execution with Bi-Abduction}]
(* Execute commands on symbolic heaps, discovering preconditions via
   bi-abduction when heap access requires unavailable cells. *)

type exec_result = {
  post : symbolic_heap;
  anti_frame : spatial_formula;  (* Discovered precondition *)
  exit : exit_condition;
}

val symbolic_exec_stmt : symbolic_heap -> ir_stmt -> set exec_result
let symbolic_exec_stmt sh stmt =
  match stmt with
  (* LOAD: x := ptr->field *)
  | SLoad x ptr field ->
      (match expose_points_to sh ptr field with
       | Found (sh', v) ->
           Set.singleton {
             post = assign_var sh' x v;
             anti_frame = SEmpty;
             exit = Ok }
       | NeedCell ->
           let fresh_v = fresh_primed_var () in
           Set.singleton {
             post = assign_var sh x fresh_v;
             anti_frame = SPointsTo (var_to_sh ptr) field fresh_v;
             exit = Ok }
       | Contradiction ->
           Set.singleton { post = sh; anti_frame = SEmpty; exit = Er })

  (* STORE: ptr->field := v *)
  | SStore ptr field v ->
      (match expose_points_to sh ptr field with
       | Found (sh', _) ->
           Set.singleton {
             post = update_points_to sh' ptr field v;
             anti_frame = SEmpty;
             exit = Ok }
       | NeedCell ->
           let fresh_v = fresh_primed_var () in
           Set.singleton {
             post = update_points_to sh ptr field v;
             anti_frame = SPointsTo (var_to_sh ptr) field fresh_v;
             exit = Ok }
       | Contradiction ->
           Set.singleton { post = sh; anti_frame = SEmpty; exit = Er })

  (* ALLOC: x := malloc(size) *)
  | SAlloc x size ->
      let fresh_val = fresh_primed_var () in
      Set.singleton {
        post = { sh with
          spatial = SSep sh.spatial (SPointsTo (SHProgVar x) "data" fresh_val) };
        anti_frame = SEmpty;
        exit = Ok }

  (* FREE: free(ptr) *)
  | SFree ptr ->
      (match remove_points_to sh ptr with
       | Some sh' ->
           Set.singleton { post = sh'; anti_frame = SEmpty; exit = Ok }
       | None ->
           Set.singleton { post = sh; anti_frame = SEmpty; exit = Er })

  (* ASSIGN: x := y -- may cause LEAK *)
  | SAssign x y ->
      let sh' = update_var sh x y in
      Set.singleton {
        post = canonicalize sh';  (* Detect leaks via Garbage rules *)
        anti_frame = SEmpty;
        exit = Ok }
\end{fstarcode}

%--------------------------------------------------
\section{Comparison: TVLA vs Symbolic Heaps}
\label{sec:shape-comparison}
%--------------------------------------------------

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{TVLA (Section 5.4)} & \textbf{Symbolic Heaps (Section 5.5)} \\
\midrule
Foundation & Three-valued logic & Separation logic \\
Abstraction & Instrumentation predicates & Canonicalization rules \\
Unknown & $\frac{1}{2}$ with Kleene semantics & Disjunction of heaps \\
Composition & Focus-transform-coerce & Frame rule \\
Leak Detection & Separate predicate needed & Built-in (junk) \\
Termination & Canonical abstraction & Theorem 14 bound \\
Best For & Materialization, complex shapes & Local reasoning, per-procedure \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Recommendation}:
\begin{itemize}
    \item Use TVLA when analyzing complex data structures where materialization of summary nodes is critical
    \item Use Symbolic Heaps when doing compositional analysis where frame rule enables procedure summaries
    \item Hybrid approach: Use symbolic heaps for interprocedural, TVLA for intraprocedural shape reasoning
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Sparse Value-Flow Analysis (SVF)}
\label{ch:svf}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{pillarbox}[title={Foundational References}]
\textbf{[SYX12]} Y.~Sui, D.~Ye, J.~Xue. \emph{Static Memory Leak Detection Using Full-Sparse Value-Flow Analysis}. ISSTA 2012.

\textbf{[SX16]} Y.~Sui, J.~Xue. \emph{SVF: Interprocedural Static Value-Flow Analysis in LLVM}. CC 2016.

\textbf{[CWZ96]} F.~Chow, S.~Chan, R.~Kennedy, S.-M.~Liu, R.~Lo, P.~Tu. \emph{A New Algorithm for Partial Redundancy Elimination Based on SSA Form}. CC 1996.
\end{pillarbox}

\begin{contributionbox}[title={SVF: Alternative to IFDS for Source-Sink Problems}]
SVF is a \textbf{complementary} technique to \IFDS{} (Section 4.1), not an optimization. Both share \CFL{}-reachability for context-sensitivity, but differ in:
\begin{itemize}
    \item \textbf{Representation}: SVFG vs Exploded Supergraph
    \item \textbf{Prerequisites}: Requires pointer analysis vs self-contained
    \item \textbf{Applicability}: Source-sink problems vs general dataflow
\end{itemize}

\textbf{Key Insight}: \textsc{Sparsity} --- most statements affect few memory locations. By pre-computing def-use chains via Memory SSA, analysis propagates only along actual value flows, not all control-flow edges.

\textbf{Pointer Analysis Input}: SVF \textsc{requires} pointer analysis ($\mathit{pts}$) for Memory SSA construction. For large \LangC{}/\LangCpp{} codebases ($>$ 100K LOC):
\begin{itemize}
    \item Use DSA (Section 5.2.5) for fast $\BigO{n \cdot \alpha(n)}$ pointer analysis
    \item DSA's heap cloning provides \textbf{better} precision than allocation-site only
\end{itemize}
For smaller codebases: Use Andersen (Section 5.1) for maximum precision.
\end{contributionbox}

%--------------------------------------------------
\section{Motivation: Dense vs Sparse Dataflow}
\label{sec:svf-motivation}
%--------------------------------------------------

\begin{verbatim}
THE DENSITY PROBLEM:
Traditional IFDS (Section 4.1) propagates dataflow facts along EVERY control-
flow edge. For N statements and D dataflow facts, this is O(E * D^3).

For source-sink problems (memory leaks, use-after-free, taint-to-sink):
  - We only care about reachability from SOURCE to SINK
  - Most statements don't affect the tracked values
  - Dense iteration wastes work on irrelevant paths

SVF INSIGHT:
Pre-compute def-use chains, then analyze ONLY along those chains.
This leverages the SPARSITY of actual value flows.

COMPARISON:
  Dense (IFDS):   Propagate facts at EVERY CFG edge
  Sparse (SVF):   Propagate facts only along DEF-USE edges

When to use each:
  - IFDS: When you need ALL facts at each program point (reaching defs)
  - SVF:  When you need source-sink REACHABILITY (leak detection)
\end{verbatim}

%--------------------------------------------------
\section{Memory SSA}
\label{sec:svf-mssa}
%--------------------------------------------------

Memory SSA extends classical SSA to handle indirect memory accesses using $\mu$/$\chi$ annotations from \textbf{[CWZ96]}.

\begin{definition}[$\mu$ Annotation]
Represents a potential \textsc{use} of an address-taken variable. For load statement \texttt{x = *y}, annotate with $\mu(o)$ for each $o \in \mathit{pts}(y)$.
\end{definition}

\begin{definition}[$\chi$ Annotation]
Represents a potential \textsc{def} (and use) of an address-taken variable. For store statement \texttt{*x = y}, annotate with $o = \chi(o)$ for each $o \in \mathit{pts}(x)$.
\end{definition}

\begin{definition}[Strong vs Weak Update]
\textbf{Strong Update}: If \texttt{ptr} uniquely points to $o$ (singleton points-to set and $o$ is a concrete location), then new value \textsc{kills} old contents.

\textbf{Weak Update}: If \texttt{ptr} may point to multiple locations or $o$ is abstract (summary node), new value must \textsc{join} with old value.

This distinction is \textbf{critical} for precision: strong update enables \textsc{kill}, improving precision.
\end{definition}

\begin{verbatim}
ANNOTATION EXAMPLES:

1. LOAD: x = *y (where pts(y) = {o1, o2})

   Before:  x = *y
   After:   x = *y [mu(o1_3), mu(o2_5)]

   Meaning: x's value MAY come from o1 version 3 or o2 version 5

2. STORE: *x = y (where pts(x) = {o}, singleton)

   Before:  *x = y
   After:   *x = y [o_4 = chi(o_3)]  (STRONG update)

   Meaning: o gets new version 4, old value killed

3. STORE: *x = y (where pts(x) = {o1, o2}, multiple)

   Before:  *x = y
   After:   *x = y [o1_4 = chi(o1_3), o2_6 = chi(o2_5)]  (WEAK update)

   Meaning: Both regions updated but old values preserved in join
\end{verbatim}

%--------------------------------------------------
\section{SVFG Construction}
\label{sec:svf-graph}
%--------------------------------------------------

The Sparse Value-Flow Graph (SVFG) is constructed from Memory SSA by connecting def-use pairs. The F* types below encode SVFG nodes and edges. Nodes come in several flavors: \texttt{CopyNode} and \texttt{PhiNode} handle direct value-flow; \texttt{LoadNode} and \texttt{StoreNode} handle indirect value-flow via memory with $\mu$/$\chi$ annotations; \texttt{DParamNode}/\texttt{IParamNode} and \texttt{DRetNode}/\texttt{IRetNode} handle interprocedural value-flow for direct and indirect parameters/returns; \texttt{SourceNode} and \texttt{SinkNode} mark allocation and deallocation sites for leak detection.

Edge labels distinguish between direct flow (register assignments), indirect flow (through memory), and interprocedural edges (calls and returns with callsite/callee information for context-sensitivity).

\begin{fstarcode}[title={SVFG Types}]
module BrrrMachine.SVFG

type svfg_node =
  (* Direct value-flow nodes *)
  | CopyNode : def_site:node_id -> dst:var_id -> src:var_id -> svfg_node
  | PhiNode : def_site:node_id -> dst:var_id -> srcs:list var_id -> svfg_node

  (* Indirect value-flow nodes *)
  | LoadNode : def_site:node_id -> dst:var_id -> ptr:var_id ->
               mu:mu_annotation -> svfg_node
  | StoreNode : def_site:node_id -> ptr:var_id -> val_:var_id ->
                chi:chi_annotation -> svfg_node

  (* Interprocedural nodes *)
  | DParamNode : func:func_id -> param:var_id -> svfg_node
  | IParamNode : func:func_id -> region:versioned_region -> svfg_node
  | DRetNode : callsite:node_id -> result:var_id -> svfg_node
  | IRetNode : callsite:node_id -> region:versioned_region -> svfg_node

  (* Source/sink markers *)
  | SourceNode : alloc_site:node_id -> svfg_node
  | SinkNode : free_site:node_id -> svfg_node

type svfg_edge_label =
  | DirectFlow
  | IndirectFlow
  | CallEdge : callsite:node_id -> callee:func_id -> svfg_edge_label
  | ReturnEdge : callsite:node_id -> callee:func_id -> svfg_edge_label

type svfg = {
  nodes : map svfg_node_id svfg_node;
  edges : list svfg_edge;
  sources : set svfg_node_id;
  sinks : set svfg_node_id;
}
\end{fstarcode}

%--------------------------------------------------
\section{Context-Sensitivity via CFL-Reachability}
\label{sec:svf-cfl}
%--------------------------------------------------

Context-sensitivity uses the \textbf{same} approach as \IFDS{} (Section 4.2):

\begin{verbatim}
CFL FOR MATCHED CALLS AND RETURNS:

Edge labels form a Dyck language (matched parentheses):
  - Call edge:   (c_g  (open paren for callsite c calling g)
  - Return edge: )c_g  (close paren)

Valid interprocedural path: balanced parentheses

Grammar:
  S -> S S | (i S )i | epsilon    for each callsite i

This is IDENTICAL to IFDS context-sensitivity.
The difference is WHAT is tracked (value-flows vs dataflow facts).
\end{verbatim}

%--------------------------------------------------
\section{IFDS vs SVF Comparison}
\label{sec:svf-comparison}
%--------------------------------------------------

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{IFDS (\textbf{[Reps95]})} & \textbf{SVF (\textbf{[SX16]})} \\
\midrule
Problem Class & Distributive dataflow & Source-sink value-flow \\
Domain & Finite set $D$ of facts & Def-use chains for memory \\
Graph Structure & Exploded supergraph ($N \times D$) & Sparse VFG ($N$ nodes) \\
Context Sensitivity & \CFL{}-reachability & \CFL{}-reachability \\
Distributivity & \textsc{Required} & Not required \\
Pointer Handling & Requires separate $\mathit{pts}$ & Integrated via Memory SSA \\
Pre-analysis & None required & Pointer analysis \textsc{required} \\
Time Complexity & $\BigO{E \cdot D^3}$ & $\BigO{\text{Andersen}} + \BigO{N \cdot R^2}$ \\
Best Use Cases & Taint, reaching defs, uninit & Leak detection, UAF, DFree \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key Insight}: Context-sensitivity mechanism is \textsc{identical} (\CFL{}-reachability). The difference is \textsc{representation} and \textsc{problem class}.

%--------------------------------------------------
\section{When to Use Each Approach}
\label{sec:svf-decision}
%--------------------------------------------------

\begin{verbatim}
USE IFDS WHEN:
  - Transfer functions are naturally distributive
  - Domain D is small and finite (taint labels, null states)
  - No pointer analysis available or needed
  - Need proven O(E * D^3) complexity bound
  - Need complete coverage of ALL reachable facts
  - Problem is NOT source-sink specific

USE SVF WHEN:
  - Tracking value-flows through memory (leak detection)
  - Pointer analysis is already available (from Section 5.1-5.3)
  - Need to handle address-taken variables precisely
  - Analysis is source-sink reachability problem
  - Program has many loads/stores
  - Memory regions are relatively sparse (R << D)

CRITICAL: SVF REQUIRES pointer analysis as pre-requisite.
         IFDS is self-contained (no pre-analysis needed).
\end{verbatim}

%--------------------------------------------------
\section{Leak Detection Algorithm}
\label{sec:svf-leak}
%--------------------------------------------------

The SVFG enables precise leak detection through a two-phase algorithm. The first phase (\texttt{some\_path\_analysis}) performs a forward slice from each allocation site to find which free sites are reachable---if no free is reachable, the allocation definitely leaks. The second phase (\texttt{all\_path\_analysis}) computes path guards: under what conditions does the allocation reach a free? If the guard is a tautology (always true), there is no leak; if it is satisfiable but not a tautology, the leak is conditional.

The \texttt{leak\_result} type distinguishes four cases: \texttt{DefinitelyLeaks} (no path to free), \texttt{ConditionalLeak} (some paths leak, some don't), \texttt{NoLeak} (all paths reach free), and \texttt{ReachesGlobal} (the pointer escapes to global state, so leak detection is inconclusive).

\begin{fstarcode}[title={Leak Detection on SVFG}]
module BrrrMachine.LeakDetection

type leak_result =
  | DefinitelyLeaks : source:svfg_node_id -> leak_result
  | ConditionalLeak : source:svfg_node_id -> condition:guard -> leak_result
  | NoLeak : source:svfg_node_id -> leak_result
  | ReachesGlobal : source:svfg_node_id -> leak_result

(* Phase 1: SOME-PATH ANALYSIS (Forward Slice) *)
val some_path_analysis : svfg -> svfg_node_id -> forward_slice_result
let some_path_analysis svfg source =
  let rec explore visited call_stack worklist =
    match worklist with
    | [] -> visited
    | (node, stack) :: rest ->
        if Set.mem (node, stack) visited then
          explore visited call_stack rest
        else
          let visited' = Set.add (node, stack) visited in
          let successors = get_cfl_successors svfg node stack in
          explore visited' call_stack (successors @ rest)
  in
  let slice = explore Set.empty [] [(source, [])] in
  let sinks = Set.filter (fun (n, _) -> is_sink n svfg) slice in
  let reaches_global = Set.exists (fun (n, _) -> is_global_store n) slice in
  { forward_slice = Set.map fst slice;
    reachable_sinks = Set.map fst sinks;
    reaches_global }

(* Phase 2: ALL-PATH ANALYSIS (Guard Computation) *)
val all_path_analysis : svfg -> svfg_node_id -> set svfg_node_id -> guard
let all_path_analysis svfg source sinks =
  let path_guards = Set.fold (fun sink acc ->
    let paths = enumerate_vf_paths svfg source sink in
    let guards = List.map (compute_path_guard svfg) paths in
    guards @ acc
  ) sinks [] in
  List.fold_left (fun acc g -> GOr acc g) GFalse path_guards

(* Main Leak Detection *)
val detect_leak : svfg -> svfg_node_id -> leak_result
let detect_leak svfg source =
  let fsr = some_path_analysis svfg source in
  if Set.is_empty fsr.reachable_sinks then
    DefinitelyLeaks source
  else if fsr.reaches_global then
    ReachesGlobal source
  else
    let freed = all_path_analysis svfg source fsr.reachable_sinks in
    if guard_is_tautology freed then
      NoLeak source
    else
      ConditionalLeak source (GNot freed)
\end{fstarcode}

\begin{remark}[Practical Performance]
\textbf{Source}: \textbf{[SYX12]}

On average, only 5.75\% of functions and 4.31\% of SVFG nodes are traversed during leak detection. The sparse representation enables demand-driven analysis that skips irrelevant program regions.

\textbf{Speedup over dense approaches}: 10--50x faster than iterative dense dataflow.
\end{remark}

%--------------------------------------------------
\section{Cross-References}
\label{sec:svf-xref}
%--------------------------------------------------

\begin{artifactbox}
\textbf{SVF Integration Points}:

\textbf{Prerequisites}:
\begin{itemize}
    \item Pointer Analysis (Section 5.1--5.3): \textsc{required} for Memory SSA
    \item \CPG{} (Section 3.1): SVFG nodes correspond to \CPG{} statement nodes
\end{itemize}

\textbf{Related Techniques}:
\begin{itemize}
    \item \IFDS{} (Section 4.1): Alternative for distributive problems
    \item \CFL{}-Reachability (Section 4.2): Shared context-sensitivity approach
    \item Interleaved Dyck (Section 4.2.3): Combined context+field problem
    \item Shape Analysis (Section 5.4--5.5): Can use SVFG for heap queries
\end{itemize}

\textbf{Client Analyses}:
\begin{itemize}
    \item Memory leak detection (primary use case)
    \item Use-after-free detection
    \item Double-free detection
    \item Null dereference (if null is tracked as taint)
\end{itemize}
\end{artifactbox}
