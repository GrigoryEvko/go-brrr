%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PART IX: MULTI-LANGUAGE ANALYSIS
%% Converted from synthesis.md
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Multi-Language Analysis}
\label{part:multi-language}

\begin{pillarbox}[title={Tension Resolution: Type-Directed vs Arbitrary Conversion}]
\textbf{Matthews \& Findler 2007}: See Appendix D.10.6 for full analysis.

\textbf{THIS SECTION} assumes type-directed conversion: $\text{type\_map} : \text{Type}_1 \to \text{Type}_2$

M\&F 2007 shows: Real FFIs have \textbf{ARBITRARY} conventions.

\textbf{Real-World FFI Conventions}:
\begin{itemize}
    \item \LangC{} returns $-1$ for error, $0$ for success (zero-for-error)
    \item \LangPython{} \texttt{None} $\leftrightarrow$ \LangC{} \texttt{NULL} (null-for-none)
    \item Sentinel values for special cases
    \item Custom serialization for complex types
\end{itemize}

\textbf{Resolution}:
\begin{itemize}
    \item Section 12.20 adds \texttt{conversion\_strategy} type
    \item Section 9.3 integrates guards with conversion strategies
    \item \texttt{CSTypeDirected}, \texttt{CSZeroForError}, \texttt{CSNullForNone}, \texttt{CSCustom} variants
\end{itemize}

\textbf{Guard Generation (Section 9.3)} handles polarity FLIP at function arguments!
\end{pillarbox}

\textbf{Theoretical Foundation}: The multi-language analysis is grounded in
\textbf{Institution Theory} \textbf{[Goguen92]}. See \textbf{Section 12.25} for the
formal foundation including:
\begin{itemize}
    \item The satisfaction condition (truth invariance under translation)
    \item Theory colimits (combining language theories)
    \item Institution morphisms (cross-system theorem proving)
    \item Constraint institutions (freeness/initiality requirements)
    \item Duplex institutions (combining decidable and expressive logics)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Crossing Language Boundaries}
\label{ch:crossing-boundaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Papers}: \textbf{[Matthews07]}, \textbf{[Goguen92]}

When code crosses from one language to another, safety invariants may be violated.
The brrr-machine detects these risks.

%--------------------------------------------------
\section{Matthews' Boundary Semantics}
\label{sec:matthews-boundary}
%--------------------------------------------------

\begin{pillarbox}[title={Matthews' Key Insight (2007)}]
Multi-language programs have \textbf{EXPLICIT BOUNDARIES}.
At each boundary, values must be converted (marshaled).

\textbf{The Boundary Term}:
\[
\hat{}^{L_2}_{L_1}(e) \quad \text{means: ``embed } L_1 \text{ expression } e \text{ into } L_2\text{''}
\]

\textbf{Evaluation}:
\begin{itemize}
    \item If $e$ evaluates to $v$ in $L_1$
    \item Then $\hat{}^{L_2}_{L_1}(e)$ evaluates to $\text{convert}(v, L_1, L_2)$
\end{itemize}
\end{pillarbox}

\textbf{Conversion Strategies}:

\begin{enumerate}
    \item \textbf{Natural Embedding}: Values with direct equivalents are converted naturally.
    \begin{itemize}
        \item \texttt{int} (\LangPython{}) $\to$ \texttt{i64} (\LangRust{}): natural
        \item \texttt{str} (\LangPython{}) $\to$ \texttt{String} (\LangRust{}): natural
    \end{itemize}

    \item \textbf{Lump Embedding}: Values without equivalents are ``lumped'' as opaque.
    \begin{itemize}
        \item \texttt{object} (\LangPython{}) $\to$ \texttt{PyObject*} (\LangC{}): lump
    \end{itemize}

    \item \textbf{Guarded Embedding}: Runtime checks enforce target language invariants.
    \begin{itemize}
        \item \texttt{Option} (\LangPython{} \texttt{None}) $\to$ Non-null (\LangJava{}): requires check
    \end{itemize}
\end{enumerate}

\textbf{What Can Go Wrong}:

\begin{itemize}
    \item \textbf{\LangPython{} $\to$ \LangC{}}:
    \LangPython{} has GC, \LangC{} doesn't.
    If \LangPython{} frees object while \LangC{} holds pointer $\to$ use-after-free

    \item \textbf{\LangJS{} $\to$ \LangRust{}}:
    \LangJS{} has \texttt{null}, \LangRust{} has \texttt{Option}.
    If \texttt{null} passed where non-null expected $\to$ crash

    \item \textbf{\LangGo{} $\to$ \LangC{}}:
    \LangGo{} has goroutines, \LangC{} has raw threads.
    If \LangGo{} object escapes to \LangC{} $\to$ race condition
\end{itemize}

%--------------------------------------------------
\section{Boundary Risk Analysis}
\label{sec:boundary-risk}
%--------------------------------------------------

The following F* code defines the core data structures for cross-language boundary analysis.
The \texttt{axiom} type enumerates the safety guarantees that different languages provide,
while \texttt{language\_config} captures the complete characterization of a language's
safety properties. When code crosses from one language to another, the difference in
axiom sets determines what properties might be violated at the boundary.

\begin{fstarcode}[title={Boundary Analysis --- Source: Matthews 2007}]
module BrrrMachine.Boundary

(* Language Axioms --- What each language guarantees *)
type axiom =
  | AxMemSafe    (* No use-after-free, double-free, buffer overflow *)
  | AxNullSafe   (* No null pointer dereference *)
  | AxTypeSafe   (* No type errors at runtime *)
  | AxRaceFree   (* No data races *)
  | AxLeakFree   (* No resource leaks *)
  | AxDetDrop    (* Deterministic resource destruction *)
  | AxInitSafe   (* No uninitialized reads *)

type language_config = {
  name : string;
  axioms : set axiom;
  memory_mode : memory_mode;
  type_mode : type_mode;
  null_mode : null_mode;
  eval_mode : eval_mode;  (* Evaluation strategy - affects VC soundness *)
}

type memory_mode = ModeGC | ModeRC | ModeOwned | ModeManual
type type_mode = TStatic | TDynamic | TGradual
type null_mode = NNullable | NOptional | NNonNull
\end{fstarcode}

\begin{fstarcode}[title={Evaluation Mode --- Critical for Verification Condition Soundness (Vazou 2014)}]
(* CRITICAL INSIGHT: VCs that are SOUND under eager evaluation may be
   UNSOUND under lazy evaluation! Under laziness, a binding like:
     let n = diverge 1 in ...
   means the VC can include assumptions about n that are NEVER realized
   because n may never be evaluated.

   The "false" refinement in diverge's output type contaminates the VC:
     false /\ y = 0 => v = 0 => v > 0
   This is VALID (contradiction in antecedent) but UNSOUND under laziness!

   See Section 2.1.5b for stratified types that restore soundness. *)

type eval_mode =
  | EvalStrict    (* Call-by-value: all bindings are values, VCs are sound *)
  | EvalLazy      (* Call-by-need: bindings may be thunks, need stratification *)
  | EvalHybrid    (* Mixed: strict by default with lazy constructs *)
\end{fstarcode}

\begin{fstarcode}[title={Python Configuration with JARVIS-Style Analysis (Huang 2023)}]
(* Python requires specialized call graph construction via Function Type
   Graph (FTG) rather than Qilin-style object-sensitive analysis. *)

type python_mro_algorithm =
  | C3Linearization    (* Standard Python MRO - REQUIRED *)
  | SimpleDFS          (* Fallback for pathological hierarchies *)

type python_magic_handling =
  | MagicFull          (* Handle __getattr__, __call__, descriptors *)
  | MagicPartial       (* Only __init__ and __call__ *)
  | MagicNone          (* Ignore magic methods - fast but imprecise *)

type python_decorator_mode =
  | DecoratorExpand    (* Expand decorator chains for precise call graph *)
  | DecoratorIdentity  (* Treat decorators as identity - fast but imprecise *)

type python_import_resolution =
  | ImportFullPackage  (* Full package hierarchy with __init__.py *)
  | ImportFlat         (* Flat module resolution *)

type python_type_graph_mode =
  | FTGFlowSensitive   (* JARVIS-style with strong updates - RECOMMENDED *)
  | FTGFlowInsensitive (* PyCG-style - faster but less precise *)

type python_analysis_mode =
  | AnalysisApplicationCentered  (* Only reachable from entry points *)
  | AnalysisWholeProgramme       (* Analyze everything - expensive *)
  | AnalysisDemandDriven         (* Only what's queried *)

type python_options = {
  mro_algorithm : python_mro_algorithm;
  magic_handling : python_magic_handling;
  decorator_mode : python_decorator_mode;
  import_resolution : python_import_resolution;
  type_graph_mode : python_type_graph_mode;
  analysis_mode : python_analysis_mode;
}

let default_python_options : python_options = {
  mro_algorithm = C3Linearization;
  magic_handling = MagicFull;
  decorator_mode = DecoratorExpand;
  import_resolution = ImportFullPackage;
  type_graph_mode = FTGFlowSensitive;
  analysis_mode = AnalysisApplicationCentered;
}
\end{fstarcode}

\begin{fstarcode}[title={Concrete Language Configurations}]
let python_config : language_config = {
  name = "Python";
  axioms = set_of [AxMemSafe; AxLeakFree];
  memory_mode = ModeGC;
  type_mode = TDynamic;
  null_mode = NNullable;
  eval_mode = EvalHybrid;  (* Strict by default, generators are lazy *)
}

let rust_config : language_config = {
  name = "Rust";
  axioms = set_of [AxMemSafe; AxNullSafe; AxTypeSafe; AxRaceFree;
                   AxLeakFree; AxDetDrop; AxInitSafe];
  memory_mode = ModeOwned;
  type_mode = TStatic;
  null_mode = NOptional;
  eval_mode = EvalStrict;
}

let c_config : language_config = {
  name = "C";
  axioms = set_of [];  (* C guarantees nothing! *)
  memory_mode = ModeManual;
  type_mode = TStatic;
  null_mode = NNullable;
  eval_mode = EvalStrict;
}

let go_config : language_config = {
  name = "Go";
  axioms = set_of [AxMemSafe; AxTypeSafe; AxLeakFree];
  memory_mode = ModeGC;
  type_mode = TStatic;
  null_mode = NNullable;
  eval_mode = EvalStrict;
}

let javascript_config : language_config = {
  name = "JavaScript";
  axioms = set_of [AxMemSafe; AxLeakFree; AxRaceFree];  (* Single-threaded *)
  memory_mode = ModeGC;
  type_mode = TDynamic;
  null_mode = NNullable;
  eval_mode = EvalHybrid;
}

let haskell_config : language_config = {
  name = "Haskell";
  axioms = set_of [AxMemSafe; AxTypeSafe; AxLeakFree];
  memory_mode = ModeGC;
  type_mode = TStatic;
  null_mode = NOptional;
  eval_mode = EvalLazy;  (* CRITICAL: Lazy evaluation *)
  (* NOTE: EvalLazy requires stratified type analysis (Div/Wnf/Fin).
     Standard VC translation is UNSOUND. *)
}
\end{fstarcode}

The following F* code defines the \textbf{boundary mechanism} taxonomy, which captures the
different ways code can cross language boundaries. Each mechanism type has distinct
security implications: FFI calls may expose memory safety issues, IPC/RPC boundaries
introduce serialization concerns, and language-specific mechanisms like \texttt{ctypes}
or JNI have their own quirks. The \texttt{boundary} record type pairs the source and
target language configurations with the specific mechanism and call site location.

\begin{fstarcode}[title={Boundary Definition}]
type boundary_mechanism =
  | BoundaryFFI              (* Foreign function interface *)
  | BoundaryIPC              (* Inter-process communication *)
  | BoundaryRPC              (* Remote procedure call *)
  | BoundaryFile             (* File-based communication *)
  | BoundarySocket           (* Network socket *)
  | BoundarySerialization    (* JSON, protobuf, etc. *)
  (* Python-C specific FFI mechanisms (PolyCruise, Li 2022) *)
  | BoundaryFFI_Ctypes       (* Python ctypes.CDLL *)
  | BoundaryFFI_Extension    (* Python C extension module *)
  | BoundaryFFI_Callback     (* C calling back to Python *)
  | BoundaryFFI_CFFI         (* Python CFFI *)
  (* Node.js specific *)
  | BoundaryFFI_NAPI         (* Node.js N-API for native addons *)
  | BoundaryFFI_WASM         (* WebAssembly boundary *)
  (* JNI/JNA *)
  | BoundaryFFI_JNI          (* Java Native Interface *)
  | BoundaryFFI_JNA          (* Java Native Access *)

type boundary = {
  source_lang : language_config;
  target_lang : language_config;
  mechanism : boundary_mechanism;
  call_site : node_id;
}
\end{fstarcode}

\begin{fstarcode}[title={Risk Calculation}]
(* Axioms at risk = axioms source has but target lacks *)
val boundary_risks : boundary -> set axiom
let boundary_risks b =
  Set.diff b.source_lang.axioms b.target_lang.axioms

(* Risk severity *)
type risk_level = RiskCritical | RiskHigh | RiskMedium | RiskLow | RiskNone

let axiom_risk_level (ax : axiom) : risk_level =
  match ax with
  | AxMemSafe -> RiskCritical   (* Memory corruption = RCE *)
  | AxTypeSafe -> RiskHigh      (* Type confusion = potential RCE *)
  | AxNullSafe -> RiskMedium    (* Null deref = crash *)
  | AxRaceFree -> RiskHigh      (* Races = unpredictable behavior *)
  | AxLeakFree -> RiskLow       (* Leaks = DoS over time *)
  | AxDetDrop -> RiskLow        (* Non-determinism = subtle bugs *)
  | AxInitSafe -> RiskMedium    (* Uninit = info leak or crash *)

let max_risk_level (risks : set axiom) : risk_level =
  Set.fold (fun ax level ->
    max level (axiom_risk_level ax)
  ) risks RiskNone
\end{fstarcode}

The following F* code implements \textbf{type consistency} for gradual typing, a key concept
for handling type checking at language boundaries. Unlike traditional subtyping, type consistency
is reflexive and symmetric but \textbf{NOT transitive}---this non-transitivity is essential for
soundness. The \texttt{gradual\_type} algebraic data type represents types that may contain the
dynamic type \texttt{GTDynamic} (written as \texttt{?} in the literature), which is consistent
with any other type.

\begin{fstarcode}[title={Type Consistency for Gradual Typing (Siek \& Taha 2006, Garcia 2016)}]
(* CRITICAL: Type consistency is NOT TRANSITIVE!
   This is essential for soundness at language boundaries.

   Example of why transitivity is unsound:
     int ~ ?       (consistent)
     ? ~ string    (consistent)
     int ~ string  (INCONSISTENT!)
   If we assumed transitivity, we'd allow int -> Any -> string coercions. *)

type gradual_type =
  | GTGround : ground_type -> gradual_type
  | GTDynamic : gradual_type  (* The ? type - consistent with anything *)
  | GTFunc : list gradual_type -> gradual_type -> gradual_type
  | GTRef : gradual_type -> gradual_type

(* Type consistency relation - reflexive, symmetric, but NOT transitive *)
val type_consistent : gradual_type -> gradual_type -> bool
let rec type_consistent t1 t2 =
  match t1, t2 with
  | GTDynamic, _ -> true           (* ? ~ t for any t *)
  | _, GTDynamic -> true           (* t ~ ? for any t *)
  | GTGround g1, GTGround g2 -> g1 = g2
  | GTFunc p1 r1, GTFunc p2 r2 ->
      List.length p1 = List.length p2 &&
      List.for_all2 type_consistent p2 p1 &&  (* Contravariant! *)
      type_consistent r1 r2
  | GTRef t1', GTRef t2' -> type_consistent t1' t2'
  | _, _ -> false

(* Cast insertion when types are consistent but not equal *)
type cast_result = CastOK of ir_expr | CastError of string

val insert_boundary_cast : gradual_type -> gradual_type -> ir_expr -> cast_result
let insert_boundary_cast src tgt e =
  if src = tgt then CastOK e
  else if type_consistent src tgt then CastOK (ECast e tgt)
  else CastError ("Inconsistent types: cannot cast " ^ show src ^ " to " ^ show tgt)
\end{fstarcode}

\begin{fstarcode}[title={Abstracting Gradual Typing (AGT) --- Garcia, Clark \& Tanter 2016}]
(* CRITICAL INSIGHT: Gradual types ARE abstract interpretations of static
   type sets. This connects Part IX to Part II (Abstract Interpretation).

   The unknown type ? represents the SET OF ALL STATIC TYPES:
     gamma(?) = {all static types}           -- ? abstracts everything
     gamma(Int) = {Int}                      -- ground types abstract themselves
     gamma(? -> Int) = {T -> Int | any T}    -- partial knowledge

   GALOIS CONNECTION (connects to Section 2.1.2):
     Concrete domain: P(StaticType) ordered by subset inclusion
     Abstract domain: GradualType ordered by precision
     alpha: Set of static types -> minimal gradual type covering set
     gamma: Gradual type -> set of static types it represents

   TYPE CONSISTENCY DERIVED (not stipulated):
     G1 ~ G2  iff  gamma(G1) intersect gamma(G2) != empty

   This derivation EXPLAINS why consistency is non-transitive:
     gamma(Int) intersect gamma(?) = {Int} != empty       -- Int ~ ?
     gamma(?) intersect gamma(String) = {String} != empty -- ? ~ String
     gamma(Int) intersect gamma(String) = empty           -- Int !~ String *)

(* Concretization: gradual type -> set of static types it represents *)
val gamma_gradual : gradual_type -> set static_type
let rec gamma_gradual gt = match gt with
  | GTDynamic -> all_static_types         (* ? represents ALL static types *)
  | GTGround g -> singleton (to_static g)
  | GTFunc params ret ->
      cartesian_func (List.map gamma_gradual params) (gamma_gradual ret)
  | GTRef t -> set_map STRef (gamma_gradual t)

(* AGT-derived consistency: equivalent to type_consistent above *)
val agt_consistent : gradual_type -> gradual_type -> bool
let agt_consistent g1 g2 =
  not (Set.is_empty (Set.intersect (gamma_gradual g1) (gamma_gradual g2)))

(* THEOREM: AGT consistency equals operational consistency (Siek 2006) *)
val agt_equals_siek :
  g1:gradual_type -> g2:gradual_type ->
  Lemma (agt_consistent g1 g2 <==> type_consistent g1 g2)

(* PRECISION ORDERING: G1 is more precise than G2 if gamma(G1) subset gamma(G2) *)
val precision_leq : gradual_type -> gradual_type -> bool
let precision_leq g1 g2 =
  Set.subset (gamma_gradual g1) (gamma_gradual g2)

(* GTDynamic is LEAST precise (bottom in precision order) *)
val dynamic_is_bottom :
  g:gradual_type ->
  Lemma (precision_leq GTDynamic g \/ g = GTDynamic)
\end{fstarcode}

\begin{pillarbox}[title={Gradual Guarantee (Garcia 2016, Theorem 3)}]
Making types \textbf{LESS precise} (more \texttt{?}) preserves semantics but may add runtime checks. Making types \textbf{MORE precise} removes runtime checks.

If $\vdash e : G$ and $G'$ is less precise than $G$, then $\vdash e : G'$ and:
\begin{itemize}
    \item If $e$ evaluates to $v$ under $G$, it evaluates to $v$ under $G'$
    \item Under $G'$, more runtime checks may occur (and may fail)
\end{itemize}

This connects to the \textbf{pay-as-you-go principle} \textbf{[Siek06]}:
\begin{itemize}
    \item Fully annotated code: no runtime checks, static guarantees
    \item Partially annotated: checks at \texttt{?} boundaries
    \item Unannotated (\texttt{?}): maximum runtime checking
\end{itemize}
\end{pillarbox}

\begin{fstarcode}[title={Consistent Subtyping (AGT for Systems with Subtyping)}]
(* For languages with record subtyping (JS objects, Python dicts, TS interfaces),
   we need CONSISTENT SUBTYPING, not just consistency:

     G1 <~ G2  iff  exists T1 in gamma(G1), T2 in gamma(G2). T1 <: T2

   Equivalently (Garcia 2016, Theorem 3):
     G1 <~ G2  iff  exists G. (G1 ~ G) and (G <: G2)
              iff  exists G. (G1 <: G) and (G ~ G2) *)

val consistent_subtype : gradual_type -> gradual_type -> bool
let consistent_subtype g1 g2 =
  Set.exists (gamma_gradual g1) (fun t1 ->
    Set.exists (gamma_gradual g2) (fun t2 ->
      static_subtype t1 t2))

(* Use consistent_subtype for record types at boundaries *)
val boundary_check_record : gradual_type -> gradual_type -> bool
let boundary_check_record src tgt =
  match src, tgt with
  | GTGround (TRecord _), GTGround (TRecord _) -> consistent_subtype src tgt
  | _, _ -> type_consistent src tgt
\end{fstarcode}

In gradual typing systems, when a runtime type check fails, it is essential to provide
\textbf{precise blame information} that identifies exactly which boundary caused the error.
The \texttt{evidence} type below tracks \emph{how} type consistency was established during
compilation. When two types are consistent through the dynamic type (\texttt{?}), evidence
records this fact. At runtime, if a cast fails, the evidence structure can be traversed to
produce actionable error messages like ``Type mismatch at boundary Python$\to$Rust: expected
\texttt{Int}, got \texttt{String}.'' Evidence composition (\texttt{compose\_evidence}) handles
chained casts across multiple boundaries.

\begin{fstarcode}[title={Evidence for Precise Blame Tracking (Garcia 2016, Section 5)}]
(* Evidence tracks HOW consistency was established during type checking.
   When a runtime check fails, evidence pinpoints the BOUNDARY that caused it.

   This improves upon simple cast insertion by providing actionable error
   messages: "Type mismatch at boundary Python->Rust: expected Int, got String" *)

type evidence =
  | EvRefl : gradual_type -> evidence                      (* G ~ G *)
  | EvDynL : gradual_type -> evidence                      (* ? ~ G *)
  | EvDynR : gradual_type -> evidence                      (* G ~ ? *)
  | EvFunc : list evidence -> evidence -> evidence         (* Function evidence *)
  | EvRecord : list (string * evidence) -> evidence        (* Record evidence *)

type evidence_result =
  | EvOK : evidence -> evidence_result
  | EvFail : blame:string -> evidence_result

(* Evidence composition - may fail at runtime *)
val compose_evidence : evidence -> evidence -> evidence_result
let rec compose_evidence ev1 ev2 = match ev1, ev2 with
  | EvRefl _, ev -> EvOK ev
  | ev, EvRefl _ -> EvOK ev
  | EvDynL g1, EvDynR g2 ->
      if type_consistent g1 g2 then EvOK (EvRefl (meet_gradual g1 g2))
      else EvFail ("Incompatible at boundary: " ^ show g1 ^ " vs " ^ show g2)
  | EvDynR _, EvDynL _ -> EvOK (EvDynL GTDynamic)
  | EvFunc ps1 r1, EvFunc ps2 r2 ->
      (* Contravariant for params, covariant for return *)
      compose_func_evidence ps1 r1 ps2 r2
  | _, _ -> EvFail "Evidence structure mismatch"

(* Enhanced cast with evidence for blame tracking *)
type cast_with_blame = CastBlameOK of ir_expr * evidence | CastBlameFail of string

val insert_boundary_cast_with_blame :
  source_lang:string -> target_lang:string ->
  gradual_type -> gradual_type -> ir_expr -> cast_with_blame
\end{fstarcode}

\begin{fstarcode}[title={Occurrence Typing Complements Gradual Typing (Tobin-Hochstadt 2008 + Siek 2006)}]
(* CRITICAL INSIGHT: These two systems solve DIFFERENT problems:
     - Gradual typing: Handle ? types at MODULE BOUNDARIES
     - Occurrence typing: Refine types WITHIN modules via type tests

   TOGETHER they provide a complete solution for dynamic language analysis:
     - At boundary: Use type consistency (Siek 2006, derived via AGT)
     - Within module: Use occurrence typing refinement (Tobin-Hochstadt 2008)

   AGT INTERPRETATION: Occurrence typing NARROWS the concretization set.
   Given gradual type G with gamma(G) = {T1, T2, T3, ...}, a type test
   "if isinstance(x, T1)" narrows gamma to {T1} in the true branch. *)

type refined_gradual_type = {
  base_type : gradual_type;           (* The declared/inferred type *)
  refinements : list type_prop;       (* Active refinement propositions *)
  narrowed_type : option gradual_type;  (* Refined type if different *)
}

(* Refine a gradual type based on occurrence typing propositions *)
val refine_gradual_type : gradual_type -> type_prop -> refined_gradual_type

(* At module boundary: forget refinements, use only base type *)
val boundary_type : refined_gradual_type -> gradual_type
let boundary_type rgt = rgt.base_type

(* Within module: use refined type if available *)
val effective_type : refined_gradual_type -> gradual_type
let effective_type rgt =
  match rgt.narrowed_type with
  | Some t -> t
  | None -> rgt.base_type
\end{fstarcode}

The \texttt{boundary\_issue} algebraic data type enumerates the categories of problems
that can arise when values cross language boundaries. Each constructor captures a specific
risk class identified during boundary analysis: null values entering null-safe languages,
unclear ownership semantics, type mismatches, unsanitized tainted data propagation,
lifetime violations, thread-safety concerns, and serialization incompatibilities. These
issues are reported by the boundary analyzer and can trigger guard generation (Section 9.3)
or compilation errors.

\begin{fstarcode}[title={Boundary Issues --- What Can Go Wrong}]
type boundary_issue =
  | IssueNullCrossing : loc:node_id -> boundary_issue
      (* Nullable value enters null-free language *)
  | IssueOwnershipUnclear : loc:node_id -> boundary_issue
      (* Ownership not specified at boundary *)
  | IssueTypeMismatch : expected:string -> actual:string -> loc:node_id
                        -> boundary_issue
      (* Type doesn't match across boundary *)
  | IssueTaintedCrossing : source:taint_source -> loc:node_id -> boundary_issue
      (* Tainted data crosses without sanitization *)
  | IssueLifetimeMismatch : loc:node_id -> boundary_issue
      (* Reference may outlive its target *)
  | IssueThreadSafety : loc:node_id -> boundary_issue
      (* Non-thread-safe value crosses to concurrent context *)
  | IssueSerializationUnsafe : type_:string -> loc:node_id -> boundary_issue
      (* Type may not round-trip through serialization *)
\end{fstarcode}

The \texttt{analyze\_boundary} function takes a CPG, a boundary specification, and the
arguments being passed across the boundary. It iterates through each argument, checking
for violations of the axioms that the source language guarantees but the target language
does not. The \texttt{detect\_boundaries} function scans the CPG for FFI and RPC call
sites, constructing boundary records for each cross-language call discovered.

\begin{fstarcode}[title={Boundary Analysis and Detection}]
val analyze_boundary :
  cpg ->
  boundary ->
  args:list (node_id * abstract_value) ->
  list boundary_issue

let analyze_boundary cpg boundary args =
  let risks = boundary_risks boundary in
  let issues = ref [] in
  List.iter (fun (arg_node, arg_val) ->
    (* Null safety risk *)
    if Set.mem AxNullSafe risks then
      if may_be_null cpg arg_node then
        issues := IssueNullCrossing arg_node :: !issues;
    (* Memory safety risk *)
    if Set.mem AxMemSafe risks then
      if is_reference arg_val then
        issues := IssueOwnershipUnclear arg_node :: !issues;
    (* Type safety risk *)
    if Set.mem AxTypeSafe risks then
      let expected = get_expected_type boundary arg_node in
      let actual = get_actual_type cpg arg_node in
      if not (types_compatible expected actual) then
        issues := IssueTypeMismatch expected actual arg_node :: !issues;
    (* Race freedom risk *)
    if Set.mem AxRaceFree risks then
      if is_shared_mutable arg_val then
        issues := IssueThreadSafety arg_node :: !issues;
  ) args;
  !issues

val detect_boundaries : cpg -> list boundary
let detect_boundaries cpg =
  fold_nodes cpg (fun boundaries node ->
    match node.kind with
    | NCall when is_ffi_call cpg node.id ->
        let source = get_source_language cpg node.id in
        let target = get_target_language cpg node.id in
        { source_lang = source;
          target_lang = target;
          mechanism = BoundaryFFI;
          call_site = node.id } :: boundaries
    | NCall when is_rpc_call cpg node.id ->
        { source_lang = get_source_language cpg node.id;
          target_lang = infer_target_language cpg node.id;
          mechanism = BoundaryRPC;
          call_site = node.id } :: boundaries
    | _ -> boundaries
  ) []
\end{fstarcode}

%--------------------------------------------------
\section{Realizability Models for Semantic Soundness}
\label{sec:realizability}
%--------------------------------------------------

\textbf{Paper}: \textbf{[Patterson22]} (Semantic Soundness for Language Interoperability)

\begin{pillarbox}[title={Limitation of Syntactic Approach}]
Matthews' boundary terms (Section 9.1.1) provide operational semantics
but don't answer: ``When is a type conversion \textbf{SEMANTICALLY SOUND}?''

\textbf{Patterson's critique}: ``Matthews-Findler-style boundaries give an elegant,
abstract model but they don't reflect reality... understanding of what
datatypes should be convertible depends on how sources are \textbf{COMPILED} and
how data is \textbf{REPRESENTED} in the target.''

For \textbf{REAL} soundness proofs, we need \textbf{SEMANTIC} models.
\end{pillarbox}

\textbf{Realizability Model} \textbf{[Patterson22]}:
\[
V[\tau] = \text{set of TARGET values that ``behave as'' source type } \tau
\]

Instead of syntactic conversion rules, we:
\begin{enumerate}
    \item Interpret types from BOTH languages as sets of target terms
    \item Define convertibility as GLUE CODE that preserves type membership
    \item Prove soundness by showing glue code maps $V[\tau_1]$ into $V[\tau_2]$
\end{enumerate}

\textbf{Convertibility Judgment}:

$\tau_A \sim \tau_B$ means: types are interconvertible with sound glue code.

Requires:
\begin{itemize}
    \item $\text{glue}_{A \to B} : \text{target} \to \text{target}$
    \item $\text{glue}_{B \to A} : \text{target} \to \text{target}$
    \item $\forall v.\; v \in V[\tau_A] \Rightarrow \text{glue}_{A \to B}(v) \in V[\tau_B]$
    \item $\forall v.\; v \in V[\tau_B] \Rightarrow \text{glue}_{B \to A}(v) \in V[\tau_A]$
\end{itemize}

\textbf{Example}: \LangPython{} int $\leftrightarrow$ \LangC{} long
\begin{itemize}
    \item $V[\text{Python int}]$ = arbitrary-precision integers
    \item $V[\text{C long}]$ = 64-bit signed integers
    \item $\text{glue}_{Py \to C}(n) = \textbf{if } |n| > 2^{63} \textbf{ then raise OverflowError else } n$
    \item NOT identity! Glue code enforces target constraints.
\end{itemize}

\begin{theorem}[Shared Memory Identity Constraint (Patterson 2022, Section 4.2)]
To share \texttt{ref} $\tau_1$ with \texttt{ref} $\tau_2$ via IDENTITY (no copy):
\[
\text{glue}_{1 \to 2} = \text{id} \;\land\; \text{glue}_{2 \to 1} = \text{id}
\quad \Rightarrow \quad
V[\tau_1] = V[\tau_2]
\]
Type interpretations must be \textbf{IDENTICAL}.
\end{theorem}

\textbf{Implications}:
\begin{itemize}
    \item \checkmark \LangRust{} \texttt{\&mut i32} $\leftrightarrow$ \LangC{} \texttt{int*}: Both have same target representation. Identity glue is safe.
    \item $\times$ \LangPython{} \texttt{list} $\leftrightarrow$ \LangC{} array: Different representations. MUST copy or use opaque handle.
    \item $\times$ \LangJava{} \texttt{String} $\leftrightarrow$ \LangRust{} \texttt{String}: UTF-16 vs UTF-8. MUST convert encoding + copy.
\end{itemize}

\begin{fstarcode}[title={Realizability Analysis Extension}]
(* Target representation equivalence *)
type target_repr =
  | TRWord of nat            (* n-bit word *)
  | TRPointer of target_repr (* pointer to *)
  | TRStruct of list target_repr
  | TROpaque                 (* Cannot compare *)

let get_target_repr (lang : language_config) (ty : ir_type) : target_repr =
  match lang.name, ty with
  | "Rust", TInt 32 -> TRWord 32
  | "C", TInt 32 -> TRWord 32
  | "Python", TInt _ -> TROpaque  (* Arbitrary precision *)
  | _, TPointer inner -> TRPointer (get_target_repr lang inner)
  | _ -> TROpaque

(* Check if shared mutable reference is safe *)
let check_shared_ref_safe (b : boundary) (ty1 ty2 : ir_type) : option boundary_issue =
  let repr1 = get_target_repr b.source_lang ty1 in
  let repr2 = get_target_repr b.target_lang ty2 in
  if repr1 <> repr2 then
    Some (IssueSharedRefUnsafe ty1 ty2 b.call_site)
  else
    None

type boundary_issue =
  (* ... existing issues ... *)
  | IssueSharedRefUnsafe : ty1:ir_type -> ty2:ir_type -> site:node_id
                           -> boundary_issue  (* NEW *)
\end{fstarcode}

\textbf{GC-Linear Interoperability Asymmetry}:

\textbf{Direction matters}:
\begin{itemize}
    \item \textbf{Linear $\to$ GC}: SAFE to convert directly.
    Linear guarantee = no aliases exist. \texttt{gcmov} instruction transfers ownership to GC. No copy needed.

    \item \textbf{GC $\to$ Linear}: REQUIRES COPY.
    GC reference may have unknown aliases. Cannot guarantee uniqueness. Must copy data to establish linear ownership.
\end{itemize}

This asymmetry affects:
\begin{itemize}
    \item \LangRust{} $\to$ \LangPython{}: can pass \texttt{Box<T>} directly (linear $\to$ GC)
    \item \LangPython{} $\to$ \LangRust{}: must copy \texttt{PyObject} to owned \texttt{T} (GC $\to$ linear)
\end{itemize}

See Section 12.7 for full realizability theorems.

%--------------------------------------------------
\section{Cross-Language Dynamic Information Flow Analysis (PolyCruise)}
\label{sec:polycruise}
%--------------------------------------------------

\textbf{Paper}: \textbf{[Li22]} --- ``PolyCruise: A Cross-Language Dynamic Information Flow Analysis''

\begin{pillarbox}[title={Critical Insight}]
Single-language analyzers miss vulnerabilities at language boundaries. Even if individual language units are secure, the combined system may not be.

\textbf{Example (\LangPython{}-\LangC{})}: Buffer overflow in NumPy
\begin{itemize}
    \item \LangPython{}: \texttt{data = input()}  \# Unconstrained user input
    \item \LangC{} (via FFI): \texttt{memcpy(buf, data)}  \# No bounds check $\to$ buffer overflow
\end{itemize}

Neither \LangPython{} nor \LangC{} analysis alone sees the full flow!

PolyCruise found \textbf{14 unknown cross-language vulnerabilities} (8 CVEs).
\end{pillarbox}

\textbf{The Challenge}:
\begin{itemize}
    \item Static semantic unification across languages is EXTREMELY HARD
    \item Different memory models, type systems, evaluation strategies
    \item Pure static cross-language analysis doesn't scale
\end{itemize}

\textbf{PolyCruise Solution}: Hybrid Static + Dynamic
\begin{itemize}
    \item \textbf{LIGHT STATIC}: Determine WHAT to instrument (cheap)
    \item \textbf{HEAVY DYNAMIC}: Track ACTUAL flows at runtime (precise)
\end{itemize}

Key insight: While SEMANTIC analysis across languages is hard, SYNTACTIC analysis (def/use) can be unified.

\subsection{Language-Independent Symbolic Representation (LISR)}
\label{subsec:lisr}

PolyCruise's key innovation is the \textbf{Language-Independent Symbolic Representation (LISR)},
which provides a unified intermediate form for def/use analysis across different programming
languages. While full semantic unification across languages is extremely difficult due to
differences in memory models and type systems, \textbf{syntactic} def/use analysis can be
unified. The following F* code defines LISR's core types: \texttt{symbolic\_name} abstracts
language-specific identifiers into a canonical form, and \texttt{lisr\_stmt} represents
statements in a language-agnostic way. The \texttt{LisrBoundary} constructor explicitly
marks cross-language calls.

\begin{fstarcode}[title={Language-Independent Symbolic Representation (LISR) --- Li 2022}]
module BrrrMachine.CrossLang.LISR

(* Symbolic names abstract away language-specific identifiers *)
type symbolic_name =
  | SymLocal : func_id:string -> name:string -> symbolic_name
  | SymGlobal : module_id:string -> name:string -> symbolic_name
  | SymParam : func_id:string -> index:nat -> symbolic_name
  | SymReturn : func_id:string -> symbolic_name
  | SymField : base:symbolic_name -> field:string -> symbolic_name

(* LISR statements - language-agnostic representation *)
type lisr_stmt =
  | LisrAssign : lhs:symbolic_name -> rhs:lisr_expr -> lisr_stmt
  | LisrCall : ret:option symbolic_name -> callee:symbolic_name ->
               args:list lisr_expr -> lisr_stmt
  | LisrOutput : sink_kind:taint_sink -> arg:lisr_expr -> lisr_stmt
  | LisrInput : source_kind:taint_source -> target:symbolic_name -> lisr_stmt
  | LisrBranch : cond:lisr_expr -> lisr_stmt
  | LisrBoundary : direction:boundary_dir -> mechanism:boundary_mechanism ->
                   inner:lisr_stmt -> lisr_stmt  (* Cross-language call marker *)

type lisr_expr =
  | LExprSym : symbolic_name -> lisr_expr
  | LExprConst : value -> lisr_expr
  | LExprBinop : op:binop -> lhs:lisr_expr -> rhs:lisr_expr -> lisr_expr
  | LExprField : base:lisr_expr -> field:string -> lisr_expr
  | LExprIndex : base:lisr_expr -> index:lisr_expr -> lisr_expr

type boundary_dir = BoundaryOut | BoundaryIn  (* Calling out / returning in *)
\end{fstarcode}

\begin{fstarcode}[title={Def/Use Extraction from LISR}]
(* This is the key to cross-language analysis: language-agnostic def/use. *)

val def_set : lisr_stmt -> set symbolic_name
let def_set stmt = match stmt with
  | LisrAssign lhs _ -> Set.singleton lhs
  | LisrCall (Some ret) _ _ -> Set.singleton ret
  | LisrInput _ target -> Set.singleton target
  | LisrBoundary _ _ inner -> def_set inner
  | _ -> Set.empty

let rec use_expr_set expr = match expr with
  | LExprSym sym -> Set.singleton sym
  | LExprConst _ -> Set.empty
  | LExprBinop _ l r -> Set.union (use_expr_set l) (use_expr_set r)
  | LExprField base _ -> use_expr_set base
  | LExprIndex base idx -> Set.union (use_expr_set base) (use_expr_set idx)

val use_set : lisr_stmt -> set symbolic_name
let use_set stmt = match stmt with
  | LisrAssign _ rhs -> use_expr_set rhs
  | LisrCall _ _ args -> Set.unions (List.map use_expr_set args)
  | LisrOutput _ arg -> use_expr_set arg
  | LisrBranch cond -> use_expr_set cond
  | LisrBoundary _ _ inner -> use_set inner
  | _ -> Set.empty

(* Convert language-specific IR to LISR *)
val to_lisr : language_config -> ir_func -> list lisr_stmt
\end{fstarcode}

\subsection{Symbolic Dependence Analysis (SDA)}
\label{subsec:sda}

\textbf{Symbolic Dependence Analysis (SDA)} computes which program statements are potentially
dependent on security-relevant criteria (taint sources and sinks). This determines the
\textbf{instrumentation scope} for dynamic analysis---only statements in the dependence set
need runtime instrumentation. The key insight is to include \textbf{both} true dependencies
(def flows to use) \textbf{AND} anti-dependencies (use followed by def of same symbol), which
ensures soundness without requiring expensive pointer analysis. The following F* code defines
the \texttt{symbolically\_dependent} relation and the \texttt{compute\_sym\_dep\_set} function
that computes the transitive closure of symbolic dependencies.

\begin{fstarcode}[title={Symbolic Dependence Analysis (SDA) --- Li 2022}]
(* SDA computes which statements are symbolically dependent on criteria
   (taint sources/sinks). This determines the INSTRUMENTATION SCOPE for
   dynamic analysis - we only instrument statements in the dependence set.

   KEY INSIGHT: Include BOTH true dependencies AND anti-dependencies.
   This ensures soundness without expensive pointer analysis. *)

module BrrrMachine.CrossLang.SDA

(* Symbolic dependence relation *)
val symbolically_dependent : lisr_stmt -> lisr_stmt -> bool
let symbolically_dependent si sj =
  let di = def_set si in
  let ui = use_set si in
  let dj = def_set sj in
  let uj = use_set sj in
  (* True/flow dependence: D(Si) intersect U(Sj) != empty *)
  not (Set.is_empty (Set.inter di uj)) ||
  (* Anti-dependence: U(Si) intersect D(Sj) != empty - ensures soundness without aliasing *)
  not (Set.is_empty (Set.inter ui dj))

(* Compute transitive symbolic dependence set from criteria *)
val compute_sym_dep_set :
  program:list lisr_stmt ->
  criteria:set symbolic_name ->  (* Source/sink symbols *)
  set nat  (* Statement indices in symbolic dependence set *)

(* Symbolic dependence summary for interprocedural analysis *)
type sds_summary = {
  func_id : string;
  return_depends_on_params : list bool;  (* Per-parameter: does return depend on it? *)
  params_depend_on_criteria : list bool; (* Per-parameter: reachable from criteria? *)
}
\end{fstarcode}

\subsection{Dynamic Information Flow Graph (DIFG)}
\label{subsec:difg}

The \textbf{Dynamic Information Flow Graph (DIFG)} is constructed at runtime from instrumented
execution events. Unlike static data flow graphs, the DIFG captures \textbf{actual} flows
that occur during program execution, including those that cross language boundaries via FFI.
The following F* code defines the runtime event types (\texttt{execution\_event}), the DIFG
node types (\texttt{difg\_node}), and the edge types (\texttt{difg\_edge}). The
\texttt{is\_cross\_language} field on edges explicitly tracks whether a flow crossed a
language boundary, enabling detection of cross-language vulnerabilities.

\begin{fstarcode}[title={Dynamic Information Flow Graph (DIFG) --- Li 2022}]
(* DIFG is built at RUNTIME from instrumented execution events.
   It captures ACTUAL flows, including those crossing language boundaries. *)

module BrrrMachine.CrossLang.DIFG

(* Runtime execution events (from instrumentation) *)
type execution_event =
  | EventDef : stmt_id:nat -> sym:symbolic_name -> value:runtime_value ->
               lang:language_config -> execution_event
  | EventUse : stmt_id:nat -> sym:symbolic_name -> value:runtime_value ->
               lang:language_config -> execution_event
  | EventBoundaryCross : direction:boundary_dir -> mechanism:boundary_mechanism ->
                         args:list (symbolic_name * runtime_value) -> execution_event
  | EventCall : caller_lang:language_config -> callee_lang:language_config ->
                callee:symbolic_name -> execution_event
  | EventReturn : func:symbolic_name -> ret_val:option runtime_value -> execution_event

(* DIFG node types *)
type difg_node =
  | DifgSource : source:taint_source -> stmt_id:nat -> lang:language_config -> difg_node
  | DifgSink : sink:taint_sink -> stmt_id:nat -> lang:language_config -> difg_node
  | DifgIntermediate : stmt_id:nat -> sym:symbolic_name -> lang:language_config -> difg_node
  | DifgBoundary : boundary:boundary -> difg_node

(* DIFG edge types *)
type difg_edge = {
  from_node : difg_node;
  to_node : difg_node;
  flow_type : difg_flow_type;
  is_cross_language : bool;
}

type difg_flow_type =
  | FlowDirect       (* Direct assignment *)
  | FlowParameter    (* Function parameter passing *)
  | FlowReturn       (* Function return value *)
  | FlowFFI          (* Cross-language FFI boundary *)
  | FlowCallback     (* C calling back to Python *)

(* Extract vulnerability witnesses from completed DIFG *)
type cross_lang_vulnerability = {
  source : taint_source;
  source_lang : language_config;
  sink : taint_sink;
  sink_lang : language_config;
  path : list difg_node;
  boundary_crossings : list boundary;
}

val find_cross_lang_flows : difg_state -> list cross_lang_vulnerability
\end{fstarcode}

\subsection{Hybrid Cross-Language Taint Analysis}
\label{subsec:hybrid-taint}

\begin{fstarcode}[title={Hybrid Static-Dynamic Cross-Language Analysis --- Li 2022}]
(* KEY INSIGHT: Use static analysis to determine WHAT to instrument,
               use dynamic analysis to track ACTUAL flows.

   This avoids:
   - Scalability issues of pure dynamic (ORBS)
   - Semantic disparity issues of pure static cross-language analysis

   PERFORMANCE (from paper):
   - Static phase: <3 seconds for 220 KSLOC, <3 minutes for 6,419 KSLOC
   - Runtime overhead: 2.71x - 11.96x slowdown
   - Precision: 93.5%, Recall: 100% *)

module BrrrMachine.CrossLang.HybridAnalysis

(* Multi-language program representation *)
type language_unit = {
  language : language_config;
  source_file : string;
  lisr : list lisr_stmt;
  entry_points : list symbolic_name;
}

type multilang_program = {
  units : list language_unit;
  boundaries : list boundary;
  main_entry : symbolic_name;
}

(* Instrumentation point - what to instrument in each language *)
type instrumentation_point = {
  stmt_id : nat;
  kind : instrumentation_kind;
  language : language_config;
}

type instrumentation_kind =
  | InstrDef : symbolic_name -> instrumentation_kind
  | InstrUse : symbolic_name -> instrumentation_kind
  | InstrCall : callee:symbolic_name -> instrumentation_kind
  | InstrBoundary : boundary:boundary -> instrumentation_kind

(* PHASE 1: Static - compute instrumentation scope via SDA *)
val compute_instrumentation_points :
  program:multilang_program ->
  sources:set taint_source ->
  sinks:set taint_sink ->
  list instrumentation_point

(* PHASE 2: Dynamic - execute with instrumentation, build DIFG *)
val execute_with_instrumentation :
  program:multilang_program ->
  instr_points:list instrumentation_point ->
  inputs:list runtime_value ->
  difg_state

(* Combined analysis *)
val cross_language_taint_analysis :
  program:multilang_program ->
  sources:set taint_source ->
  sinks:set taint_sink ->
  inputs:list runtime_value ->
  list cross_lang_vulnerability
\end{fstarcode}

\begin{pillarbox}[title={Integration with Synthesis}]
PolyCruise complements the primarily \textbf{STATIC} boundary analysis in 9.1.1--9.1.3:

\textbf{Static (Sections 9.1.1--9.1.3)}:
\begin{itemize}
    \item Boundary detection and risk classification \textbf{[Matthews07]}
    \item Type consistency checking \textbf{[Siek06]}
    \item Realizability models \textbf{[Patterson22]}
    \item FFI contract verification (VeriFFI, Section 9.4)
\end{itemize}

\textbf{Dynamic (PolyCruise, Section 9.1.4)}:
\begin{itemize}
    \item LISR for unified cross-language def/use
    \item SDA for instrumentation guidance
    \item DIFG for precise runtime flow tracking
    \item Actual vulnerability witness generation
\end{itemize}

\textbf{Recommended Workflow}:
\begin{enumerate}
    \item Static boundary analysis (9.1.1--9.1.3) for risk assessment
    \item PolyCruise SDA (9.1.4.2) to scope instrumentation
    \item Dynamic DIFG (9.1.4.3) for specific test inputs
    \item Feed results into taint classification (Section 12.3)
\end{enumerate}
\end{pillarbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Hole Tagging for Evaluation Contexts}
\label{ch:hole-tagging}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Source}: \textbf{[Matthews07]} --- ``Operational Semantics for Multi-Language Programs''

%--------------------------------------------------
\section{The Language Bleeding Problem}
\label{sec:bleeding-problem}
%--------------------------------------------------

\textbf{Problem}: Without hole tagging, reduction rules can ``bleed'' across languages.

\textbf{Example (ML expression with embedded Scheme)}:
\[
\text{ML}[ 1 + \text{MS}(\text{SM}[\text{Scheme-expr}]) ]
\]

\textbf{Without Tagging}:
If we reduce ``$1 + \_$'' first, the Scheme-expr is evaluated by ML rules!
This violates language semantics.

\textbf{Solution}: Tag holes with their expected language.

%--------------------------------------------------
\section{Tagged Evaluation Contexts}
\label{sec:tagged-eval-ctx}
%--------------------------------------------------

\begin{fstarcode}[title={Evaluation Hole with Language Tag}]
type eval_hole =
  | HoleML : eval_hole
  | HoleScheme : eval_hole
  | HolePython : eval_hole
  | HoleRust : eval_hole
  | HoleBoundary : source:lang_id -> target:lang_id -> eval_hole

(* Evaluation context with language annotations *)
type eval_context =
  | ECHole : hole:eval_hole -> eval_context
  | ECBinOpL : op:binop -> ctx:eval_context -> rhs:expr -> eval_context
  | ECBinOpR : op:binop -> lhs:value -> ctx:eval_context -> eval_context
  | ECApp : ctx:eval_context -> arg:expr -> eval_context
  | ECBoundary : direction:boundary_dir -> ty:ir_type -> ctx:eval_context
                 -> eval_context

(* Fill hole with expression *)
val fill : eval_context -> expr -> expr
let rec fill ctx e = match ctx with
  | ECHole _ -> e
  | ECBinOpL op inner rhs -> EBinOp op (fill inner e) rhs
  | ECBinOpR op lhs inner -> EBinOp op lhs (fill inner e)
  | ECApp inner arg -> EApp (fill inner e) arg
  | ECBoundary dir ty inner -> EBoundary dir ty (fill inner e)
\end{fstarcode}

%--------------------------------------------------
\section{Language-Respecting Reduction}
\label{sec:lang-respecting-reduction}
%--------------------------------------------------

\textbf{Reduction Rule Constraint}:
A reduction rule for language $L$ can only fire when the redex is at a hole tagged with $L$.

\textbf{Formally}:
If $e \to e'$ by $L$-rule, then in context $C[e]$:
\begin{itemize}
    \item $C$ has hole tagged $L$
    \item All intervening boundaries are properly crossed
\end{itemize}

\textbf{Implementation}:
\begin{verbatim}
step(lang, ctx, expr) =
  let hole_lang = tag_of_hole(ctx) in
  if hole_lang = lang then
    apply_rules(lang, expr)
  else
    (* Cannot reduce here - wrong language context *)
    Stuck
\end{verbatim}

%--------------------------------------------------
\section{Boundary Crossing Updates Tags}
\label{sec:boundary-tag-update}
%--------------------------------------------------

\begin{fstarcode}[title={Boundary Crossing Tag Updates}]
(* When crossing a boundary, update the hole tag *)
val cross_boundary : eval_context -> boundary_dir -> lang_id -> eval_context
let cross_boundary ctx dir target_lang =
  (* The inner context now expects target_lang *)
  update_inner_hole ctx (HoleBoundary ... target_lang)

(* Example: ML calling Python *)
let ml_python_example =
  ECApp (ECHole HoleML)                    (* ML context *)
        (EBoundary ToPython int_ty         (* Boundary *)
          (ECHole HolePython))             (* Now Python context *)
\end{fstarcode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Guard Generation Algorithm}
\label{ch:guard-generation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Source}: \textbf{[Matthews07]} Section 3.3

%--------------------------------------------------
\section{Guard Polarity}
\label{sec:guard-polarity}
%--------------------------------------------------

\textbf{Key Insight}: Guards have \textbf{POLARITY} that determines checking direction.

\textbf{Positive polarity}: Value entering STRICTER type system
\begin{itemize}
    \item Dynamic $\to$ Static (e.g., \LangPython{} $\to$ \LangRust{})
    \item Must CHECK at runtime (source doesn't guarantee)
\end{itemize}

\textbf{Negative polarity}: Value from STRICTER type system
\begin{itemize}
    \item Static $\to$ Dynamic (e.g., \LangRust{} $\to$ \LangPython{})
    \item Can SKIP check (source guarantees type)
\end{itemize}

\textbf{Critical}: Polarity \textbf{FLIPS} at function argument position! (Contravariance in domain)

%--------------------------------------------------
\section{Polarity Determination}
\label{sec:polarity-determination}
%--------------------------------------------------

\begin{fstarcode}[title={Polarity Determination}]
(* Determine initial polarity based on language pair *)
val initial_polarity : lang_id -> lang_id -> guard_polarity
let initial_polarity source target =
  let source_strict = type_strictness source in
  let target_strict = type_strictness target in
  if target_strict > source_strict then Positive
  else Negative

(* Flip polarity (used at function arguments) *)
val flip : guard_polarity -> guard_polarity
let flip Positive = Negative
let flip Negative = Positive
\end{fstarcode}

%--------------------------------------------------
\section{Guard Generation}
\label{sec:guard-gen}
%--------------------------------------------------

\begin{fstarcode}[title={Guard Generation Algorithm}]
(* Generate guard for type at given polarity *)
val generate_guard : ir_type -> guard_polarity -> guard
let rec generate_guard ty pol = match ty, pol with
  (* Base types *)
  | TInt, Positive -> GCheckNumber
  | TInt, Negative -> GNoCheck
  | TBool, Positive -> GCheckBool
  | TBool, Negative -> GNoCheck
  | TString, Positive -> GCheckString
  | TString, Negative -> GNoCheck

  (* Nullable / Option types *)
  | TOption inner, Positive ->
      GCompose GCheckNotNull (generate_guard inner Positive)
  | TOption inner, Negative -> GNoCheck

  (* Function types - FLIP polarity for argument! *)
  | TArrow arg_ty ret_ty, pol ->
      GFunctionWrap
        (generate_guard arg_ty (flip pol))  (* CONTRAVARIANT *)
        (generate_guard ret_ty pol)         (* COVARIANT *)

  (* Product types *)
  | TProd t1 t2, pol ->
      GCompose (generate_guard t1 pol) (generate_guard t2 pol)

  (* Reference types need type check *)
  | TRef inner, Positive -> GCheckType (TRef inner)
  | TRef inner, Negative -> GNoCheck

  (* Polymorphic types *)
  | TForall _ _, Positive -> GDynamic  (* Must check dynamically *)
  | TForall _ _, Negative -> GNoCheck

  (* Any/Dynamic type *)
  | TAny, _ -> GDynamic
\end{fstarcode}

%--------------------------------------------------
\section{Function Wrapping Example}
\label{sec:func-wrap-example}
%--------------------------------------------------

\textbf{Example}: \LangPython{} function $\to$ \LangRust{} callback

\begin{itemize}
    \item \LangPython{} function type: \texttt{(Any) -> Any}
    \item \LangRust{} callback type: \texttt{(i32) -> String}
\end{itemize}

At boundary (\LangPython{} $\to$ \LangRust{}), polarity = Positive

Generate guard for \texttt{(i32) -> String} at Positive:
\begin{itemize}
    \item Argument \texttt{i32}: \texttt{flip(Positive)} = Negative $\to$ \texttt{GNoCheck}
    (\LangRust{} provides \texttt{i32}, \LangPython{} receives)
    \item Return \texttt{String}: Positive $\to$ \texttt{GCheckString}
    (\LangPython{} returns, must verify is string)
\end{itemize}

\textbf{Wrapped Function}:
\begin{verbatim}
fn wrapped(arg: i32) -> String {
  let py_result = python_fn(arg);  // No check on arg
  GCheckString(py_result)           // Check return
}
\end{verbatim}

%--------------------------------------------------
\section{Integration with Boundary Analysis}
\label{sec:guard-boundary-integration}
%--------------------------------------------------

\begin{fstarcode}[title={Boundary Crossing with Guards}]
(* Complete boundary crossing with guards *)
val cross_with_guards :
  source:lang_id -> target:lang_id ->
  ty:ir_type -> value:value ->
  brrr_result value

let cross_with_guards source target ty v =
  let pol = initial_polarity source target in
  let guard = generate_guard ty pol in
  match apply_guard guard v with
  | GPass v' -> Ok v'
  | GFail expected actual ->
      Error (BoundaryTypeError {
        at_boundary = (source, target);
        expected_type = expected;
        actual_value = actual;
      })

(* Static analysis: verify guards sufficient *)
val verify_boundary_safety :
  cpg -> boundary_node -> list boundary_issue

let verify_boundary_safety cpg bnd =
  let guard = generate_guard bnd.ty (initial_polarity bnd.source bnd.target) in
  let incoming_types = analyze_incoming_types cpg bnd in
  List.filter_map (fun ty ->
    if guard_covers ty guard then None
    else Some (IssueGuardInsufficient bnd ty guard)
  ) incoming_types
\end{fstarcode}

%--------------------------------------------------
\section{Guards with Type Refinement Propositions}
\label{sec:guards-refinement}
%--------------------------------------------------

\textbf{Source}: \textbf{[TobinHochstadt08]} + \textbf{[Matthews07]}

\begin{pillarbox}[title={Guards Produce Type Propositions}]
\textbf{Key Insight}: Guards are not just runtime checks --- they also establish \textbf{TYPE KNOWLEDGE} for subsequent code.

\textbf{When a guard CHECK PASSES}:
\begin{itemize}
    \item We know the value HAS the checked type
    \item This is a VISIBLE PREDICATE \textbf{[TobinHochstadt08]}
    \item Subsequent code can use this type information
\end{itemize}

\textbf{When a guard CHECK FAILS}:
\begin{itemize}
    \item Exception/error raised --- unreachable code
    \item But in analysis, we can refine with NEGATIVE proposition
\end{itemize}

\textbf{Polarity Determines Which Branch Gets Which Refinement}:
\begin{itemize}
    \item \textbf{Positive guard} (checking incoming value):
    Pass branch: value HAS type $\tau$; Fail branch: unreachable
    \item \textbf{Negative guard} (checking outgoing value):
    Pass branch: value HAS type $\tau$ (trivially); Fail branch: BUG in source
\end{itemize}
\end{pillarbox}

\begin{fstarcode}[title={Guards with Refinement Information}]
(* Guard result includes type proposition for refinement *)
type guard_with_refinement = {
  guard : guard;
  positive_prop : type_prop;   (* Proposition when guard passes *)
  negative_prop : type_prop;   (* Proposition when guard fails *)
  target_var : option string;  (* Variable being checked, if known *)
}

(* Generate guard with associated type propositions *)
val generate_guard_with_prop :
  ir_type -> guard_polarity -> option string -> guard_with_refinement

let generate_guard_with_prop ty pol var_opt =
  let guard = generate_guard ty pol in
  let make_prop = match var_opt with
    | Some v -> (fun t -> PropHasType v t)
    | None -> (fun _ -> PropTrue)
  in
  match pol with
  | Positive ->
      (* Checking incoming value - pass means has type *)
      { guard;
        positive_prop = make_prop ty;
        negative_prop = PropFalse;  (* Unreachable - error raised *)
        target_var = var_opt }
  | Negative ->
      (* Checking outgoing value - should always pass *)
      { guard;
        positive_prop = PropTrue;   (* Source guarantees type *)
        negative_prop = PropFalse;  (* Bug if reached *)
        target_var = var_opt }

(* Apply guard and get refined type environment *)
type guard_application_result =
  | GuardPassed of {
      value : ir_value;
      env_refinement : prop_type_env -> prop_type_env;
    }
  | GuardFailed of {
      expected : ir_type;
      actual : ir_value;
    }
\end{fstarcode}

%--------------------------------------------------
\section{Guard Composition Optimization}
\label{sec:guard-composition}
%--------------------------------------------------

\textbf{Optimization}: Cancel redundant guards at boundary chains.

If value crosses: $A \to B \to C$, then Guard($A \to B$) followed by Guard($B \to C$) can often simplify to: Guard($A \to C$).

\textbf{Rules}:
\begin{itemize}
    \item Positive $\circ$ Positive = Positive (still need check)
    \item Negative $\circ$ Negative = Positive (double negative = positive)
    \item Positive $\circ$ Negative = Negative
    \item Negative $\circ$ Positive = Negative
\end{itemize}

\textbf{Optimization}:
\begin{itemize}
    \item \texttt{GNoCheck} $\circ$ G = G
    \item G $\circ$ \texttt{GNoCheck} = G
    \item \texttt{GCheckT} $\circ$ \texttt{GCheckT} = \texttt{GCheckT} (idempotent)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{FFI Contracts}
\label{ch:ffi-contracts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Source}: VeriFFI \textbf{[Wang25]} --- ``A Verified Foreign Function Interface between Coq and C''

\begin{pillarbox}[title={FFI Contract Framework}]
FFI boundaries require \textbf{EXPLICIT contracts} specifying:
\begin{itemize}
    \item Input type representation requirements
    \item Output type representation guarantees
    \item Memory ownership transfer
    \item Side effect bounds
\end{itemize}

\textbf{Without Contracts}: FFI calls are opaque black boxes.

\textbf{With Contracts}: FFI calls can be verified for type safety.

\textbf{Key Insight}: Contracts bridge the semantic gap between languages by specifying both \textbf{WHAT} the function does and \textbf{HOW} values are represented.

\textbf{Cross-References}:
\begin{itemize}
    \item Section 7.5: Representation predicates used in contracts
    \item Section 9.1.2: Contracts formalize boundary risk mitigation
    \item Section 9.3: Guards generated from contract type requirements
    \item Section 12.7: Realizability theorems justify contract soundness
\end{itemize}
\end{pillarbox}

%--------------------------------------------------
\section{Contract Structure}
\label{sec:contract-structure}
%--------------------------------------------------

FFI contracts formalize the requirements for safely calling across language boundaries.
The following F* code defines the \texttt{ffi\_contract} type which captures:
\begin{itemize}
    \item \textbf{Type mappings} between source and foreign types
    \item \textbf{Preconditions} that the caller must establish (representation predicates, ownership)
    \item \textbf{Postconditions} that the callee guarantees (memory effects, error handling)
    \item \textbf{Ownership transfer} semantics for resource management across boundaries
\end{itemize}
The \texttt{type\_mapping} type describes how values are transformed when crossing
the boundary: directly (\texttt{TMDirect}), via conversion (\texttt{TMConverted}),
or as opaque references (\texttt{TMOpaque}).

\begin{fstarcode}[title={FFI Contract Structure --- VeriFFI (Wang 2025)}]
module BrrrMachine.FFIContract

(* Type Mappings Across Boundaries *)
type type_mapping =
  | TMDirect
      (* Same representation in both languages.
         Example: i32 (Rust) <-> int (C) *)
  | TMBoxed
      (* Heap-allocated wrapper around the value.
         Example: Python int <-> boxed arbitrary-precision integer *)
  | TMConverted of {
      convert_fn : ir_value -> foreign_value;
      unconvert_fn : foreign_value -> option ir_value;
    }
      (* Explicit conversion required.
         Example: Rust String (UTF-8) <-> Java String (UTF-16) *)
  | TMOpaque
      (* No access across boundary - treat as opaque handle.
         Example: Python object <-> void* in C *)

(* FFI Contract Type *)
type ffi_contract = {
  foreign_function : string;
  source_language : language_id;
  target_language : language_id;
  param_types : list (ir_type * foreign_type * type_mapping);
  return_type : (ir_type * foreign_type * type_mapping);
  precondition : ffi_precondition;
  postcondition : ffi_postcondition;
  ownership_transfer : ownership_spec;
  effect_bounds : effect_row;
}
\end{fstarcode}

\begin{fstarcode}[title={Precondition Structure}]
type rep_requirement =
  | RepRequired : t:Type -> rep_predicate t -> rep_requirement
  | RepNonNull : rep_requirement
  | RepInitialized : rep_requirement
  | RepAligned : alignment:nat -> rep_requirement
  | RepSized : min_size:nat -> rep_requirement

type ffi_precondition = {
  rep_requirements : list (var_id * rep_requirement);
  ownership_requirements : list (var_id * access_permission);
  value_constraints : list (var_id * ir_expr);
  thread_requirements : option thread_safety_requirement;
}

type thread_safety_requirement =
  | TSRSingleThreaded     (* Must be called from single thread *)
  | TSRMainThread         (* Must be called from main thread *)
  | TSRWithLock of lock_id  (* Must hold specified lock *)
  | TSRAnyThread          (* Safe to call from any thread *)
\end{fstarcode}

\begin{fstarcode}[title={Postcondition Structure}]
type memory_effect_spec =
  | MEUnchanged           (* Memory not modified *)
  | MEModifiedOnly of set address  (* Only specified addresses modified *)
  | MEMayAllocate         (* May allocate new memory *)
  | MEMayFree of set address  (* May free specified addresses *)
  | MEArbitrary           (* No guarantees (unsafe) *)

type ownership_transfer =
  | OTRetained            (* Ownership unchanged *)
  | OTTransferred         (* Ownership transferred to callee *)
  | OTBorrowed of lifetime_bound  (* Temporarily borrowed *)
  | OTShared              (* Shared access granted *)

type ffi_postcondition = {
  return_rep : rep_requirement;
  memory_effects : memory_effect_spec;
  ownership_effects : list (var_id * ownership_transfer);
  error_conditions : list (ir_expr * error_behavior);
  may_trigger_gc : bool;
}

type error_behavior =
  | EBException of ir_type     (* Throws exception of type *)
  | EBReturnCode of ir_value   (* Returns error code *)
  | EBSetErrno                 (* Sets errno *)
  | EBAbort                    (* Aborts process *)
  | EBUndefined                (* Undefined behavior *)
\end{fstarcode}

%--------------------------------------------------
\section{Contract Generation}
\label{sec:contract-generation}
%--------------------------------------------------

\begin{fstarcode}[title={Automatic Contract Generation --- VeriFFI Section 5}]
(* For common patterns, we can generate contracts automatically
   from type signatures and calling conventions. *)

val generate_ffi_contract :
  source_type : ir_type ->
  target_type : foreign_type ->
  calling_convention : calling_conv ->
  ffi_contract

val infer_type_mapping : ir_type -> foreign_type -> type_mapping
let infer_type_mapping src tgt =
  match src, tgt with
  | TInt I32 true, ForeignInt32 -> TMDirect
  | TInt I64 true, ForeignInt64 -> TMDirect
  | TFloat F32, ForeignFloat -> TMDirect
  | TFloat F64, ForeignDouble -> TMDirect
  | TBool, ForeignBool -> TMDirect
  | TPtr inner_src, ForeignPtr inner_tgt ->
      let inner_mapping = infer_type_mapping inner_src inner_tgt in
      if inner_mapping = TMDirect then TMDirect
      else TMOpaque
  | TString, ForeignCString ->
      TMConverted {
        convert_fn = string_to_cstring;
        unconvert_fn = cstring_to_string;
      }
  | TStruct _ _, _ -> TMOpaque
  | TVariant _ _, _ -> TMOpaque
  | _, _ -> TMOpaque

type calling_conv =
  | CCDefault          (* Default C calling convention *)
  | CCStdcall          (* Windows stdcall *)
  | CCFastcall         (* Fast register-based calling *)
  | CCRust             (* Rust ABI *)
  | CCOwned            (* Ownership transferred *)
  | CCBorrowed         (* Immutable borrow *)
  | CCMutBorrowed      (* Mutable borrow *)
\end{fstarcode}

\begin{fstarcode}[title={Annotation-Based Contract Refinement}]
type ffi_annotation =
  | AnnNonnull of var_id          (* Parameter cannot be null *)
  | AnnNullable of var_id         (* Parameter may be null *)
  | AnnOwned of var_id            (* Ownership transferred *)
  | AnnBorrowed of var_id         (* Borrowed reference *)
  | AnnOut of var_id              (* Output parameter *)
  | AnnInOut of var_id            (* Input/output parameter *)
  | AnnArraySize of var_id * nat  (* Array with fixed size *)
  | AnnBufferSize of var_id * var_id  (* Buffer sized by another param *)
  | AnnMayAllocate                (* May allocate memory *)
  | AnnMayFree                    (* May free memory *)
  | AnnPure                       (* No side effects *)
  | AnnThreadSafe                 (* Safe for concurrent calls *)

val refine_contract_with_annotations :
  ffi_contract -> list ffi_annotation -> ffi_contract
\end{fstarcode}

%--------------------------------------------------
\section{Contract Verification}
\label{sec:contract-verification}
%--------------------------------------------------

\begin{fstarcode}[title={FFI Violation Types}]
type ffi_violation =
  | RepViolation :
      param:var_id ->
      expected:rep_requirement ->
      actual:abs_value ->
      ffi_violation
  | OwnershipViolation :
      param:var_id ->
      required:access_permission ->
      actual:access_permission ->
      ffi_violation
  | TypeMismatch :
      expected:foreign_type ->
      actual:ir_type ->
      ffi_violation
  | NullViolation :
      param:var_id ->
      ffi_violation
  | UninitializedViolation :
      param:var_id ->
      ffi_violation
  | ThreadViolation :
      required:thread_safety_requirement ->
      actual:thread_context ->
      ffi_violation
  | MissingContract :
      function_name:string ->
      ffi_violation
  | ConversionFailure :
      param:var_id ->
      from_type:ir_type ->
      to_type:foreign_type ->
      ffi_violation
\end{fstarcode}

\begin{fstarcode}[title={Verification Algorithm --- VeriFFI Section 6}]
val verify_ffi_call :
  call_site : node_id ->
  contract : ffi_contract ->
  abstract_state : abs_state ->
  result (abs_state, list ffi_violation)

val check_rep_requirement :
  abs_state -> abs_value -> rep_requirement -> option string

let check_rep_requirement state val req =
  match req with
  | RepNonNull ->
      if may_be_null state val then Some "may be null"
      else None
  | RepInitialized ->
      if may_be_uninitialized state val then Some "may be uninitialized"
      else None
  | RepAligned align ->
      if not (is_aligned state val align) then Some "misaligned"
      else None
  | RepSized min_size ->
      if not (has_min_size state val min_size) then Some "too small"
      else None
  | RepRequired t rp ->
      if not (may_satisfy_rep graph addr rp) then Some "rep predicate fails"
      else None

val permission_satisfies : access_permission -> access_permission -> bool
\end{fstarcode}

%--------------------------------------------------
\section{Reified Type Descriptors}
\label{sec:reified-types}
%--------------------------------------------------

\begin{fstarcode}[title={Reified Type Descriptors --- VeriFFI Section 4}]
(* Type descriptors that exist at runtime, enabling:
   - Dynamic type checking at FFI boundaries
   - Automatic marshalling code generation
   - Reflection-based contract verification *)

module BrrrMachine.ReifiedTypes

type type_descriptor =
  | TDPrimitive :
      prim:primitive_type ->
      size:nat ->
      align:nat ->
      type_descriptor
  | TDPointer :
      pointee:type_descriptor ->
      type_descriptor
  | TDStruct :
      name:string ->
      fields:list field_descriptor ->
      type_descriptor
  | TDArray :
      element:type_descriptor ->
      length:nat ->
      type_descriptor
  | TDUnion :
      variants:list (string * type_descriptor) ->
      type_descriptor
  | TDFunction :
      params:list type_descriptor ->
      ret:type_descriptor ->
      type_descriptor
  | TDOpaque :
      name:string ->
      type_descriptor

type primitive_type =
  | PTVoid
  | PTBool
  | PTInt8 | PTInt16 | PTInt32 | PTInt64
  | PTUInt8 | PTUInt16 | PTUInt32 | PTUInt64
  | PTFloat32 | PTFloat64
  | PTChar | PTWChar

type field_descriptor = {
  field_name : string;
  field_type : type_descriptor;
  field_offset : nat;
  field_padding : nat;
}

val reify_type : ir_type -> type_descriptor
val size_of : type_descriptor -> nat
val alignment_of : type_descriptor -> nat
val validate_against_descriptor : raw_bytes -> type_descriptor -> bool
\end{fstarcode}

%--------------------------------------------------
\section{Integration with Boundary Analysis}
\label{sec:ffi-boundary-integration}
%--------------------------------------------------

\begin{pillarbox}[title={Integration: FFI Contracts with Boundary Analysis}]
FFI contracts connect to the broader boundary analysis framework:

\textbf{Section 9.1.2 (Boundary Risk Analysis)}:
When \texttt{BoundaryFFI} edges are detected, verify against FFI contracts.
Contracts specify: representation predicate requirements, ownership transfer semantics, memory effect bounds.

\textbf{Section 9.3 (Guard Generation)}:
Guards are generated from contract type requirements.
Contract specifies WHAT to check; guards specify HOW.

\textbf{Part XI (IR Specification)}:
IR types can be reified into runtime type descriptors (Section 9.4.4)
for FFI boundary verification.

\textbf{Verification Flow}:
\begin{enumerate}
    \item Detect FFI call site
    \item Look up or generate contract
    \item Verify preconditions via abstract state
    \item Generate guards for runtime checks
    \item Apply postcondition to compute post-state
    \item Report violations
\end{enumerate}
\end{pillarbox}

\begin{fstarcode}[title={Verify All FFI Calls in CPG}]
val verify_all_ffi_calls :
  cpg ->
  contracts:map string ffi_contract ->
  list (node_id * ffi_violation)

let verify_all_ffi_calls cpg contracts =
  let ffi_calls = find_ffi_call_sites cpg in
  List.concat_map (fun call_site ->
    let func_name = get_called_function cpg call_site in
    match Map.find func_name contracts with
    | Some contract ->
        let state = get_abstract_state_at cpg call_site in
        (match verify_ffi_call call_site contract state with
         | Ok _ -> []
         | Error violations ->
             List.map (fun v -> (call_site, v)) violations)
    | None ->
        [(call_site, MissingContract func_name)]
  ) ffi_calls
\end{fstarcode}

%--------------------------------------------------
\section{Multilingual Type Inference for FFI}
\label{sec:multilingual-type-inference}
%--------------------------------------------------

\textbf{Source}: \textbf{[Furr08]} --- ``Checking Type Safety of Foreign Function Calls''

\begin{pillarbox}[title={Multilingual Type Inference for FFI (O-Saffire/J-Saffire)}]
\textbf{Key Insight}: \LangC{} glue code has a RICHER view of types than source language.
\LangC{} can observe physical representations (boxed vs unboxed, tags, offsets).

\textbf{Approach}:
\begin{enumerate}
    \item Extract type info from high-level language (OCaml, Java)
    \item Infer REPRESENTATIONAL TYPES for \LangC{} glue code
    \item Compare inferred types against declared types
    \item Report inconsistencies as potential FFI bugs
\end{enumerate}

\textbf{Representational Types}: Model \LangC{}'s low-level view of high-level data.
\begin{itemize}
    \item OCaml: $(n, \pi)$ where $n$ = nullary constructor count, $\pi$ = sum of products
    \item Java: \texttt{jt jobject} where \texttt{jt} embeds Java type info into \LangC{}'s \texttt{jobject}
\end{itemize}

\textbf{Flow-Sensitivity (O-Saffire for OCaml)}: Types of form $ct\{B, I, T\}$ where:
\begin{itemize}
    \item $B$ = boxedness (boxed $|$ unboxed $|$ unknown)
    \item $I$ = offset into structured block
    \item $T$ = tag value or integer value
\end{itemize}

\textbf{Polymorphism (J-Saffire for JNI)}: Unification-based analysis with polymorphic
signatures. Essential for analyzing wrapper functions and the 200+ JNI API functions.
JNI uses string singletons to track class names and field descriptors across \LangC{} code.

\textbf{GC Safety}: Track which functions may invoke garbage collector via effects.
Ensure pointers to GC'd heap are registered before GC-triggering calls.

\textbf{Bugs Found} (experimentally validated):
\begin{itemize}
    \item O-Saffire: 24 errors + 22 suspicious patterns in 11 OCaml benchmarks
    \item J-Saffire: 156 errors + 124 suspicious patterns in 12 JNI benchmarks
\end{itemize}
\end{pillarbox}

The following F* code defines \textbf{representational types} that model how C glue code
views high-level language data. The \texttt{rep\_type} algebraic data type captures
sum types as (nullary\_count, products) pairs, where nullary\_count is the number of
nullary constructors and products describes the fields of non-nullary constructors.
The \texttt{flow\_type} extends this with flow-sensitive information: \texttt{boxedness}
(is it a pointer or an immediate value?), \texttt{offset} (position within a structured block),
and \texttt{tag} (known discriminator value). The \texttt{gc\_effect} type tracks whether
a function may trigger garbage collection, which is critical for ensuring that pointers
to the managed heap are properly registered before GC-triggering calls.

\begin{fstarcode}[title={Representational Types for FFI Analysis --- Furr \& Foster 2008}]
module BrrrMachine.FFITypeSafety

type rep_type =
  | RepInt                             (* Unbounded integer - value unknown *)
  | RepConstructor of nat              (* Known nullary constructor tag *)
  | RepSum of {
      nullary_count : nat;             (* Number of nullary constructors *)
      products : list rep_product;     (* Nonnullary constructor representations *)
    }
  | RepFunc of rep_type -> gc_effect -> rep_type
  | RepVar of string                   (* Type variable for inference *)

and rep_product = list rep_type        (* Fields of a structured block *)

and gc_effect =
  | GCNone                             (* Cannot trigger GC *)
  | GCMay                              (* May trigger GC *)
  | GCVar of string                    (* Effect variable for inference *)

(* Flow-sensitive type extensions *)
type boxedness =
  | BBoxed                             (* Known to be pointer to heap block *)
  | BUnboxed                           (* Known to be unboxed (integer/tag) *)
  | BUnknown                           (* Boxedness unknown *)
  | BBottom                            (* Unreachable code *)

type flow_type = {
  base_type : rep_type;                (* The representational type *)
  boxedness : boxedness;               (* Is it boxed or unboxed? *)
  offset : option nat;                 (* Offset into structured block *)
  tag : option nat;                    (* Known tag value *)
}
\end{fstarcode}

\begin{fstarcode}[title={FFI Type Violation Detection}]
type ffi_type_violation =
  | ViolationTypeMismatch of {
      location : node_id;
      expected : rep_type;
      inferred : rep_type;
    }
  | ViolationUnregisteredGCPointer of {
      location : node_id;
      pointer_var : string;
      gc_triggering_call : string;
    }
  | ViolationInvalidOffset of {
      location : node_id;
      offset : nat;
      block_size : nat;
    }
  | ViolationWrongTag of {
      location : node_id;
      tested_tag : nat;
      max_valid_tag : nat;
    }

(* Translate source type to representational type *)
val translate_source_type : ir_type -> rep_type

(* Analyze C glue code *)
val analyze_ffi_code :
  ir_func ->
  source_types : map string ir_type ->
  ffi_inference_result
\end{fstarcode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Occurrence Typing Analysis}
\label{ch:occurrence-typing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Source}: \textbf{[TobinHochstadt08]} (Typed Scheme)

\begin{pillarbox}[title={Occurrence Typing: Complete Analysis Algorithm}]
This section provides the full occurrence typing analysis, integrating:
\begin{itemize}
    \item Type predicate detection (Section 9.5.1)
    \item Conditional refinement propagation (Section 9.5.2)
    \item Union type narrowing (Section 9.5.3)
    \item Language-specific handling (Section 9.5.4)
\end{itemize}

\textbf{Integration with CPG}:
\begin{itemize}
    \item Type propositions flow along CFG edges
    \item DDG edges track type dependencies
    \item Guards (Section 9.3) produce type propositions
\end{itemize}

\textbf{Complements}:
\begin{itemize}
    \item Gradual typing (Section 9.1.2) at module boundaries
    \item Taint analysis (Section 4.2) for security
    \item Nullability analysis (Section 2.1.7) for null safety
\end{itemize}
\end{pillarbox}

%--------------------------------------------------
\section{Type Predicate Detection}
\label{sec:type-predicate-detection}
%--------------------------------------------------

Occurrence typing refines variable types based on runtime type tests. Different languages
have different idioms for type testing: Python uses \texttt{isinstance()} and \texttt{is None},
TypeScript uses \texttt{typeof} and \texttt{instanceof}, Go uses type assertions and type switches.
The following F* code defines a unified representation \texttt{detected\_type\_test} that captures
the essential information from any type test: the tested expression, the type being tested for,
and both positive and negative type propositions. The \texttt{positive\_prop} is established
when the test succeeds (true branch), and \texttt{negative\_prop} when it fails (false branch).

\begin{fstarcode}[title={Type Predicate Detection --- Tobin-Hochstadt 2008, Section 4}]
module BrrrMachine.OccurrenceTyping

(* Unified type test representation *)
type detected_type_test = {
  tested_expr : ir_expr;         (* The expression being tested *)
  tested_var : option string;    (* Variable if direct var test *)
  test_type : ir_type;           (* Type being tested for *)
  positive_prop : type_prop;     (* Proposition if test is true *)
  negative_prop : type_prop;     (* Proposition if test is false *)
  language : language_id;        (* Source language *)
  node_id : node_id;             (* Location in CPG *)
}

(* Python: isinstance(x, T), type(x) is T *)
val detect_python_type_test : ir_expr -> node_id -> option detected_type_test
let detect_python_type_test expr node =
  match expr with
  (* isinstance(x, T) *)
  | ECall (EVar "isinstance") [EVar var; EType ty] ->
      Some {
        tested_expr = EVar var;
        tested_var = Some var;
        test_type = ty;
        positive_prop = PropHasType var ty;
        negative_prop = PropNotType var ty;
        language = Python;
        node_id = node;
      }
  (* x is None *)
  | EBinOp OpIs (EVar var) ENone ->
      Some {
        tested_expr = EVar var;
        tested_var = Some var;
        test_type = TNone;
        positive_prop = PropHasType var TNone;
        negative_prop = PropNotType var TNone;
        language = Python;
        node_id = node;
      }
  | _ -> None
\end{fstarcode}

\begin{fstarcode}[title={TypeScript/JavaScript Type Test Detection}]
val detect_typescript_type_test : ir_expr -> node_id -> option detected_type_test
let detect_typescript_type_test expr node =
  match expr with
  (* typeof x === "string" etc. *)
  | EBinOp (OpEq | OpStrictEq) (ECall (EVar "typeof") [EVar var]) (EString ty_name) ->
      let ty = typescript_typeof_to_type ty_name in
      Some {
        tested_expr = EVar var;
        tested_var = Some var;
        test_type = ty;
        positive_prop = PropHasType var ty;
        negative_prop = PropNotType var ty;
        language = TypeScript;
        node_id = node;
      }
  (* x instanceof T *)
  | EBinOp OpInstanceof (EVar var) (EType ty) ->
      Some {
        tested_expr = EVar var;
        tested_var = Some var;
        test_type = ty;
        positive_prop = PropHasType var ty;
        negative_prop = PropNotType var ty;
        language = TypeScript;
        node_id = node;
      }
  (* "prop" in obj - discriminated union check *)
  | EBinOp OpIn (EString prop) (EVar var) ->
      Some {
        tested_expr = EVar var;
        tested_var = Some var;
        test_type = TWithProperty prop;
        positive_prop = PropHasType var (TWithProperty prop);
        negative_prop = PropNotType var (TWithProperty prop);
        language = TypeScript;
        node_id = node;
      }
  | _ -> None

(* TypeScript typeof string to type mapping *)
let typescript_typeof_to_type (s : string) : ir_type =
  match s with
  | "string" -> TString
  | "number" -> TNumber
  | "boolean" -> TBool
  | "undefined" -> TUndefined
  | "object" -> TObject
  | "function" -> TFunction
  | "symbol" -> TSymbol
  | "bigint" -> TBigInt
  | _ -> TAny
\end{fstarcode}

%--------------------------------------------------
\section{Conditional Refinement Propagation}
\label{sec:conditional-refinement}
%--------------------------------------------------

\begin{fstarcode}[title={Conditional Refinement Propagation --- Tobin-Hochstadt 2008, Section 3.3}]
type branch_refinement = {
  condition_node : node_id;
  true_branch : node_id;
  false_branch : node_id;
  true_refinement : prop_type_env -> prop_type_env;
  false_refinement : prop_type_env -> prop_type_env;
}

(* Analyze a conditional and extract branch refinements *)
val analyze_conditional :
  cpg -> node_id -> language_id -> option branch_refinement

(* Truthiness test: if x: ... (Python), if (x) { ... } (JS/TS) *)
val analyze_truthiness_test :
  cpg -> node_id -> language_id -> option branch_refinement

(* Falsy types by language *)
let get_falsy_types (lang : language_id) : list ir_type =
  match lang with
  | Python -> [TNone; TBool (* False *); TInt (* 0 *); TString (* "" *)]
  | JavaScript | TypeScript ->
      [TNull; TUndefined; TBool; TNumber (* 0, NaN *); TString (* "" *)]
  | _ -> []
\end{fstarcode}

\begin{fstarcode}[title={Forward Propagation Algorithm}]
type occurrence_analysis_state = {
  env_at_node : map node_id prop_type_env;
  worklist : set node_id;
}

val run_occurrence_analysis : cpg -> language_id -> occurrence_analysis_state

let run_occurrence_analysis cpg lang =
  let entry = get_entry_node cpg in
  let initial_env = create_initial_env cpg in
  let state = ref {
    env_at_node = Map.singleton entry initial_env;
    worklist = Set.singleton entry;
  } in
  (* Worklist algorithm *)
  while not (Set.is_empty !state.worklist) do
    let node = Set.choose !state.worklist in
    state := { !state with worklist = Set.remove node !state.worklist };
    let current_env = Map.find_default initial_env node !state.env_at_node in
    if is_conditional_node cpg node then begin
      match analyze_conditional cpg node lang with
      | Some br ->
          let true_env = br.true_refinement current_env in
          let false_env = br.false_refinement current_env in
          propagate_env state br.true_branch true_env;
          propagate_env state br.false_branch false_env
      | None ->
          propagate_to_successors cpg state node current_env
    end
    else begin
      let new_env = occurrence_transfer current_env (get_stmt cpg node) in
      propagate_to_successors cpg state node new_env
    end
  done;
  !state
\end{fstarcode}

%--------------------------------------------------
\section{Union Type Narrowing}
\label{sec:union-narrowing}
%--------------------------------------------------

When occurrence typing refines a union type, it can either \textbf{narrow} the union
(keeping only types that overlap with the test type) or \textbf{remove} specific types
from the union. The following F* code implements these operations. The \texttt{narrow\_union}
function filters a union to types that overlap with the tested type---if the result is
\texttt{TBottom}, the branch is unreachable (useful for exhaustiveness checking). The
\texttt{remove\_from\_union} function removes types that are subtypes of the tested type,
used in the false branch of type tests. TypeScript's discriminated unions receive special
handling via the \texttt{discriminated\_union} type.

\begin{fstarcode}[title={Union Type Narrowing --- Tobin-Hochstadt 2008, Section 3.2}]
(* Narrow a union type by restricting to a subtype *)
val narrow_union : ir_type -> ir_type -> ir_type
let rec narrow_union original test_type =
  match original with
  | TUnion types ->
      let narrowed = List.filter (fun t ->
        types_overlap t test_type
      ) types in
      (match narrowed with
       | [] -> TBottom  (* Contradiction - unreachable *)
       | [t] -> t       (* Single type remaining *)
       | ts -> TUnion ts)
  | _ ->
      if types_overlap original test_type then original
      else TBottom

(* Narrow a union type by removing a subtype *)
val remove_from_union : ir_type -> ir_type -> ir_type
let rec remove_from_union original remove_type =
  match original with
  | TUnion types ->
      let remaining = List.filter (fun t ->
        not (subtype t remove_type)
      ) types in
      (match remaining with
       | [] -> TBottom
       | [t] -> t
       | ts -> TUnion ts)
  | _ ->
      if subtype original remove_type then TBottom
      else original

(* Check if two types have any overlap *)
val types_overlap : ir_type -> ir_type -> bool
let rec types_overlap t1 t2 =
  match t1, t2 with
  | TBottom, _ | _, TBottom -> false
  | TAny, _ | _, TAny -> true
  | TUnion ts1, _ -> List.exists (fun t -> types_overlap t t2) ts1
  | _, TUnion ts2 -> List.exists (fun t -> types_overlap t1 t) ts2
  | _, _ -> subtype t1 t2 || subtype t2 t1
\end{fstarcode}

\begin{fstarcode}[title={Discriminated Union Handling (TypeScript)}]
(* TypeScript discriminated union: union of objects with common literal property *)
type discriminated_union = {
  discriminant : string;          (* Property name used to discriminate *)
  variants : list (ir_value * ir_type);  (* (literal value, variant type) *)
}

(* Detect discriminated union pattern *)
val detect_discriminated_union : ir_type -> option discriminated_union

(* Narrow discriminated union based on discriminant check *)
val narrow_discriminated_union :
  discriminated_union -> string -> ir_value -> ir_type

(* Example:
   type Shape =
     | { kind: "circle", radius: number }
     | { kind: "square", side: number }

   After: if (shape.kind === "circle")
   Narrowed to: { kind: "circle", radius: number } *)
\end{fstarcode}

%--------------------------------------------------
\section{Language-Specific Type Refinement}
\label{sec:lang-specific-refinement}
%--------------------------------------------------

\begin{fstarcode}[title={Python-Specific Refinement}]
(* Python: hasattr() check *)
val refine_hasattr : string -> string -> prop_type_env -> prop_type_env
let refine_hasattr var attr env =
  refine_positive env (PropHasType var (TWithAttribute attr))

(* Python: callable() check *)
val refine_callable : string -> prop_type_env -> prop_type_env
let refine_callable var env =
  refine_positive env (PropHasType var TCallable)

(* Python: Literal type narrowing from match statement *)
val refine_match_case :
  string -> ir_pattern -> prop_type_env -> prop_type_env
\end{fstarcode}

\begin{fstarcode}[title={TypeScript-Specific Refinement}]
(* TypeScript: Array.isArray() *)
val refine_is_array : string -> prop_type_env -> prop_type_env
let refine_is_array var env =
  refine_positive env (PropHasType var (TArray TAny))

(* TypeScript: type predicate functions *)
type type_predicate_fn = {
  fn_name : string;
  param_index : nat;
  asserted_type : ir_type;
}

(* Detect user-defined type predicates: function isString(x): x is string *)
val detect_type_predicate : ir_func -> option type_predicate_fn

(* Apply user-defined type predicate *)
val refine_type_predicate :
  type_predicate_fn -> list ir_expr -> prop_type_env -> prop_type_env
\end{fstarcode}

\begin{fstarcode}[title={Go-Specific Refinement}]
(* Go: type switch *)
type go_type_switch = {
  switched_var : string;
  cases : list (ir_type * node_id);  (* type, case body *)
  default_case : option node_id;
}

val refine_type_switch_case :
  go_type_switch -> ir_type -> prop_type_env -> prop_type_env

(* Go: nil check for interfaces *)
val refine_nil_check : string -> bool -> prop_type_env -> prop_type_env
let refine_nil_check var is_nil env =
  if is_nil then
    refine_positive env (PropHasType var TNil)
  else
    refine_positive env (PropNotType var TNil)
\end{fstarcode}

%--------------------------------------------------
\section{Integration with CPG Analysis}
\label{sec:occurrence-cpg-integration}
%--------------------------------------------------

\begin{fstarcode}[title={CPG Integration for Occurrence Typing}]
(* CPG node annotation for occurrence typing *)
type occurrence_annotation = {
  refined_types : map string ir_type;  (* var -> refined type at this point *)
  active_props : list type_prop;       (* Active propositions *)
}

(* Annotate CPG with occurrence typing results *)
val annotate_cpg_with_occurrences :
  cpg -> occurrence_analysis_state -> unit

(* Query refined type at a specific node *)
val get_refined_type : cpg -> node_id -> string -> option ir_type
let get_refined_type cpg node var =
  match get_node_annotation cpg node "occurrence" with
  | Some ann -> Map.find_opt var ann.refined_types
  | None -> None

(* Use refined types in taint analysis *)
val refine_taint_with_occurrences :
  cpg -> taint_state -> node_id -> taint_state

(* Use refined types for null analysis *)
val refine_null_with_occurrences :
  cpg -> null_state -> node_id -> null_state
\end{fstarcode}

\textbf{Cross-Reference Summary}:
\begin{itemize}
    \item Section 2.1.7b: Occurrence type domain definition
    \item Section 9.1.2: Integration with gradual typing
    \item Section 9.3.5b: Guards produce type propositions
    \item Section 12.3.5: F* soundness theorem
    \item Section 4.2: Integration with taint analysis
    \item Section 2.1.7: Integration with nullability domain
\end{itemize}
