% ==================================================
% PART IV: ANALYSIS ALGORITHMS (FIRST HALF)
% Sections: 4.1 IFDS, 4.2 CFL-Reachability, 4.3 Under-Approximate Analysis
% ==================================================

\part{Analysis Algorithms}
\label{part:analysis-algorithms}

\begin{pillarbox}[title={Critical Tension: IFDS vs Bi-Abduction/Eval}]
\textbf{IFDS (Section~\ref{sec:ifds}):}
\begin{itemize}
  \item Requires DISTRIBUTIVE transfer functions: $f(a \sqcup b) = f(a) \sqcup f(b)$
  \item Tracks DATAFLOW FACTS
  \item $\BigO{ED^3}$ guaranteed complexity
  \item Path-INSENSITIVE
\end{itemize}

\textbf{Bi-Abduction/Eval (Section~\ref{sec:under-approx}):}
\begin{itemize}
  \item NOT distributive
  \item Multiple valid $(M, F)$ pairs
  \item Tracks SEPARATION LOGIC assertions
  \item May be exponential
  \item Path-SENSITIVE
\end{itemize}

\textbf{These are different algorithms for different problems:}
\begin{itemize}
  \item Taint analysis $\rightarrow$ IFDS (fast, whole-program)
  \item Shape/memory analysis $\rightarrow$ Eval (precise, local)
  \item Hybrid: IFDS finds candidates, Eval verifies (Section~\ref{sec:ifds-eval-hybrid})
\end{itemize}

\textbf{DO NOT} try to force bi-abduction into IFDS framework! The mathematical properties are incompatible.
\end{pillarbox}

\begin{pillarbox}[title={Tension Resolution: IFDS vs Set Constraints}]
\textbf{Source}: Aiken 1999. See Appendix~D.10.1 for full analysis.

\textbf{Insight}: IFDS is a RESTRICTED FRAGMENT of set constraints.
\begin{itemize}
  \item IFDS requires distributive transfer functions
  \item Set constraints (Section~12.18) handle non-distributive cases
  \item Pointer analysis is NOT distributive---cannot use IFDS directly
\end{itemize}

\textbf{Resolution}:
\begin{itemize}
  \item Use IFDS for taint, reaching definitions, live variables (distributive)
  \item Use set constraints for type inference, pointer analysis (general)
  \item Section~12.18 provides unified constraint framework
\end{itemize}
\end{pillarbox}

\begin{pillarbox}[title={Tension Resolution: Datalog Interpretation vs Compilation}]
\textbf{Sources}: Jordan 2016 (Souffle), Madsen 2016 (Flix)

\textbf{Old View}: ``Don't build custom Datalog engine, use existing or skip''

\textbf{New View}: COMPILE Datalog rules to specialized code, don't interpret

Jordan 2016 demonstrates: Souffle achieves 50x+ speedup over bddbddb by eliminating interpretation overhead via staged specialization:
\begin{enumerate}
  \item Stage 1: Datalog $\rightarrow$ RAM (Relational Algebra Machine)
  \item Stage 2: RAM $\rightarrow$ Optimized RAM (Dilworth indices, join ordering)
  \item Stage 3: RAM $\rightarrow$ C++/Rust (specialized, parallel)
\end{enumerate}

\textbf{Resolution for brrr-machine}:
\begin{itemize}
  \item Development: Use interpreted Datalog (Crepe, Souffle -interpreter)
  \item Production: Compile analysis rules to specialized Rust
  \item See Section~\ref{sec:datalog-compilation} for compilation strategy details
\end{itemize}
\end{pillarbox}

\begin{pillarbox}[title={Key Insight: Flix Unifies IFDS/IDE/Value Analysis}]
\textbf{Source}: Madsen 2016

\textbf{Standard Datalog}: Relations (finite sets of tuples)

\textbf{Flix Extension}: Lattice predicates (map from keys to lattice elements)

This unifies:
\begin{itemize}
  \item IFDS = Flix where lattice is powerset of dataflow facts
  \item IDE = Flix where lattice is micro-function space
  \item Constant propagation = Flix where lattice is constant domain
\end{itemize}

All require MONOTONE transfer functions for soundness. See Section~\ref{sec:flix-lattice} for lattice-extended Datalog details.
\end{pillarbox}


%==================================================
\chapter{IFDS: Interprocedural Finite Distributive Subset}
\label{sec:ifds}
%==================================================

\textbf{Paper}: Reps, Horwitz, Sagiv 1995

\IFDS{} is the workhorse algorithm for precise interprocedural dataflow analysis \textbf{when transfer functions are DISTRIBUTIVE} ($f(a \sqcup b) = f(a) \sqcup f(b)$). Its genius is reducing dataflow problems to graph reachability.

\textbf{Important}: \IFDS{} does NOT apply to pointer analysis (non-distributive). See Section~12.18 for set constraints handling non-distributive cases.

\begin{pillarbox}[title={Flix Perspective (Madsen 2016)}]
\IFDS{} is lattice-extended Datalog where:
\begin{itemize}
  \item Lattice = powerset of dataflow facts (finite)
  \item Transfer = gen/kill functions (distributive)
  \item Distributivity = transfer distributes over join
\end{itemize}

\IDE{} extends \IFDS{} where:
\begin{itemize}
  \item Lattice = micro-function space (environment transformers)
  \item Transfer = function composition
  \item Enables: constant propagation, linear constant propagation
\end{itemize}

\textbf{Souffle Perspective (Jordan 2016)}:
\IFDS{} expressed as Datalog can be COMPILED, not just interpreted:
\begin{enumerate}
  \item Stage 1: \IFDS{} $\rightarrow$ Datalog rules (declarative specification)
  \item Stage 2: Datalog $\rightarrow$ RAM (semi-naive evaluation, index selection)
  \item Stage 3: RAM $\rightarrow$ Rust/C++ (specialized code via templates)
\end{enumerate}
This achieves 50x+ speedup over interpreted Datalog (bddbddb, muZ). See Section~\ref{sec:datalog-compilation} for compilation strategy details.
\end{pillarbox}


%--------------------------------------------------
\section{The Key Insight}
\label{sec:ifds-insight}
%--------------------------------------------------

\textbf{The Problem}: Interprocedural analysis is hard because:
\begin{itemize}
  \item Must track calling context (don't mix up callers)
  \item Must handle recursion
  \item Naive approach: exponential in call depth
\end{itemize}

\textbf{Example (imprecise without context)}:
\begin{verbatim}
def f(x):
  return x + 1
a = f(1)   # Should be 2
b = f(10)  # Should be 11
\end{verbatim}

Without context sensitivity: \texttt{f} receives $\{1, 10\}$, returns $\{2, 11\}$, so both \texttt{a} and \texttt{b} get $\{2, 11\}$ --- IMPRECISE!

With context sensitivity: Call 1 has \texttt{f} receive 1, return 2; Call 2 has \texttt{f} receive 10, return 11. Thus $a = 2$, $b = 11$ --- PRECISE!

\textbf{The Reps Insight}: Frame the problem as GRAPH REACHABILITY with context encoded as MATCHED PARENTHESES:
\begin{itemize}
  \item $(_{1} \ldots )_{1}$ means: call site 1, return to site 1
  \item $(_{1} (_{2} )_{2} )_{1}$ means: nested call, properly matched
\end{itemize}

INVALID paths like $(_{1} )_{2}$ are automatically rejected. This is \CFL{}-reachability with the Dyck language.


%--------------------------------------------------
\section{The Exploded Supergraph}
\label{sec:exploded-supergraph}
%--------------------------------------------------

\textbf{Construction}: Given:
\begin{itemize}
  \item Program with procedures $p_1, \ldots, p_n$
  \item \CFG{} for each procedure
  \item Finite set $D$ of dataflow facts ($|D| = d$)
\end{itemize}

Build the \textbf{Exploded Supergraph} $G^\#$:
\begin{align*}
\text{NODES} &: \{ \langle n, d \rangle \mid n \text{ is a program point}, d \in D \cup \{0\} \} \\
\text{EDGES} &: \text{For each CFG edge } n_1 \rightarrow n_2 \text{ and its transfer function } f: \\
&\quad \text{Add edge } \langle n_1, d_1 \rangle \rightarrow \langle n_2, d_2 \rangle \text{ iff } d_2 \in f(\{d_1\})
\end{align*}

The $0$ fact represents ``nothing'' --- it's the identity for the dataflow function.

\begin{theorem}[Reps 1995]
Fact $d$ holds at program point $n$ if and only if there exists a \textbf{realizable path} from $\langle s_{\text{main}}, 0 \rangle$ to $\langle n, d \rangle$.

Realizable = matched call/return parentheses.
\end{theorem}


%--------------------------------------------------
\section{The Tabulation Algorithm}
\label{sec:ifds-tabulation}
%--------------------------------------------------

\begin{fstarcode}[title={\IFDS{} Tabulation Algorithm (RHS95, Figure 5)}]
module BrrrMachine.IFDS

(* --------------------------------------------------
   PROBLEM DEFINITION
   -------------------------------------------------- *)

(* An IFDS problem instance *)
type ifds_problem (d : Type) = {
  (* The supergraph *)
  supergraph : cpg;
  (* The dataflow domain --- MUST BE FINITE *)
  domain : finite_set d;
  (* The zero fact *)
  zero : d;
  (* Flow functions for each edge type *)
  flow_function : cpg_edge -> d -> set d;
  (* Call flow: how facts map at call site *)
  call_flow : node_id -> d -> set d;
  (* Return flow: how facts map at return *)
  return_flow : node_id -> node_id -> d -> d -> set d;
  (* Call-to-return flow: for facts not passed to callee *)
  call_to_return_flow : node_id -> d -> set d;
}

(* --------------------------------------------------
   DATA STRUCTURES
   -------------------------------------------------- *)

(* Path edge: same-level realizable path *)
type path_edge (d : Type) = {
  (* Procedure containing this path *)
  proc_entry : node_id;
  (* Starting fact at procedure entry *)
  d1 : d;
  (* Current node reached *)
  n : node_id;
  (* Current fact *)
  d2 : d;
}

(* Summary edge: captures procedure's effect *)
type summary_edge (d : Type) = {
  (* Call site *)
  call_site : node_id;
  (* Fact at call *)
  d1 : d;
  (* Return site *)
  return_site : node_id;
  (* Fact at return *)
  d2 : d;
}

(* Solver state *)
type ifds_state (d : Type) = {
  path_edges : set (path_edge d);
  summary_edges : set (summary_edge d);
  worklist : list (path_edge d);
}

(* --------------------------------------------------
   THE ALGORITHM
   -------------------------------------------------- *)

val solve : #d:Type -> ifds_problem d -> set (node_id * d)
let solve #d prob =
  (* Initialize with entry points *)
  let entries = get_entry_points prob.supergraph in
  let init_edges = Set.of_list (List.map (fun entry ->
    { proc_entry = entry; d1 = prob.zero; n = entry; d2 = prob.zero }
  ) entries) in
  let init_state = {
    path_edges = init_edges;
    summary_edges = Set.empty;
    worklist = Set.to_list init_edges;
  } in

  (* Main loop *)
  let rec process state =
    match state.worklist with
    | [] -> state
    | edge :: rest ->
      let state' = { state with worklist = rest } in
      let state'' = process_edge prob edge state' in
      process state''
  in
  let final = process init_state in
  (* Extract results *)
  Set.map (fun pe -> (pe.n, pe.d2)) final.path_edges

val process_edge : #d:Type -> ifds_problem d -> path_edge d ->
                   ifds_state d -> ifds_state d
let process_edge #d prob edge state =
  let n = edge.n in
  let d2 = edge.d2 in
  match get_node_type prob.supergraph n with

  (* ---------------------------------------------------------------------
     CASE 1: Call node
     --------------------------------------------------------------------- *)
  | NCall ->
    let callee = get_callee prob.supergraph n in
    let callee_entry = get_entry_node prob.supergraph callee in
    let return_site = get_return_site prob.supergraph n in

    (* Propagate to callee entry *)
    let callee_facts = prob.call_flow n d2 in
    let callee_edges = Set.map (fun d3 ->
      { proc_entry = callee_entry; d1 = d3; n = callee_entry; d2 = d3 }
    ) callee_facts in

    (* Apply existing summaries *)
    let matching_summaries = Set.filter (fun se ->
      se.call_site = n && Set.mem se.d1 callee_facts
    ) state.summary_edges in
    let summary_edges = Set.map (fun se ->
      { proc_entry = edge.proc_entry; d1 = edge.d1;
        n = return_site; d2 = se.d2 }
    ) matching_summaries in

    (* Call-to-return flow (for facts not passed to callee) *)
    let ctr_facts = prob.call_to_return_flow n d2 in
    let ctr_edges = Set.map (fun d3 ->
      { proc_entry = edge.proc_entry; d1 = edge.d1;
        n = return_site; d2 = d3 }
    ) ctr_facts in
    propagate_all (Set.unions [callee_edges; summary_edges; ctr_edges]) state

  (* ---------------------------------------------------------------------
     CASE 2: Exit node
     --------------------------------------------------------------------- *)
  | NExit proc ->
    let callers = get_call_sites prob.supergraph proc in
    (* For each caller with matching entry fact *)
    let new_edges = Set.concat_map (fun call_site ->
      let return_site = get_return_site prob.supergraph call_site in
      (* Find path edges reaching this call with matching callee entry fact *)
      let matching_paths = Set.filter (fun pe ->
        pe.n = call_site &&
        Set.mem edge.d1 (prob.call_flow call_site pe.d2)
      ) state.path_edges in
      Set.concat_map (fun caller_edge ->
        let d4 = caller_edge.d2 in
        let d5 = prob.return_flow call_site return_site d4 d2 in
        (* Create summary edge *)
        let new_summary = {
          call_site = call_site; d1 = edge.d1;
          return_site = return_site; d2 = d2
        } in
        (* Propagate to return site *)
        let return_edges = Set.map (fun d5' ->
          { proc_entry = caller_edge.proc_entry; d1 = caller_edge.d1;
            n = return_site; d2 = d5' }
        ) d5 in
        (new_summary, return_edges)
      ) matching_paths
    ) callers in
    let (new_summaries, new_path_edges) = Set.partition_pair new_edges in
    let state' = { state with
      summary_edges = Set.union state.summary_edges new_summaries
    } in
    propagate_all (Set.flatten new_path_edges) state'

  (* ---------------------------------------------------------------------
     CASE 3: Ordinary node
     --------------------------------------------------------------------- *)
  | _ ->
    let successors = get_cfg_successors prob.supergraph n in
    let new_edges = Set.concat_map (fun succ ->
      let d3_set = prob.flow_function
        { source = n; target = succ; label = CfgNext } d2 in
      Set.map (fun d3 ->
        { proc_entry = edge.proc_entry; d1 = edge.d1; n = succ; d2 = d3 }
      ) d3_set
    ) successors in
    propagate_all new_edges state

val propagate_all : #d:Type -> set (path_edge d) -> ifds_state d -> ifds_state d
let propagate_all #d edges state =
  let new_edges = Set.diff edges state.path_edges in
  { state with
    path_edges = Set.union state.path_edges new_edges;
    worklist = Set.to_list new_edges @ state.worklist }
\end{fstarcode}

\textbf{Complexity Analysis}: Let $N$ = number of nodes in supergraph, $E$ = number of edges, $D$ = size of dataflow domain.

\textbf{Space}: Path edges: $\BigO{N \times D^2}$ --- bounded by (proc\_entry, $d_1$, $n$, $d_2$). Summary edges: $\BigO{\text{Call\_sites} \times D^2}$. Total: $\BigO{N \times D^2}$.

\textbf{Time}: Each path edge processed once: $\BigO{N \times D^2}$ iterations. Each iteration: $\BigO{D}$ work for flow functions. Total: $\BigO{N \times D^3} = \BigO{E \times D^3}$ since $E \geq N$.

\textbf{Key Insight}: Polynomial in $D$, not exponential! This is because we track (entry\_fact, current\_fact) pairs, not full paths through the exploded graph.

\begin{pillarbox}[title={Bidirectional IFDS Extension (FlowDroid)}]
\textbf{Source}: Arzt 2014 (FlowDroid). Cross-reference: Section~8.1.6.

For HEAP-SENSITIVE taint analysis, standard forward-only \IFDS{} is insufficient. FlowDroid extends \IFDS{} with bidirectional analysis:
\begin{itemize}
  \item \textbf{Forward}: Propagate taint from sources, spawn backward on heap writes
  \item \textbf{Backward}: Find heap aliases, spawn forward with INACTIVE taint
\end{itemize}

The two directions SPAWN each other:
\begin{itemize}
  \item Forward $\rightarrow$ finds \texttt{p.f = tainted} $\rightarrow$ spawns Backward to find aliases of \texttt{p.f}
  \item Backward $\rightarrow$ finds \texttt{q} aliased to \texttt{p} $\rightarrow$ spawns Forward with (\texttt{q.f}, INACTIVE)
\end{itemize}

\textbf{Important}: This is NOT pure \IFDS{}!
\begin{itemize}
  \item Forward taint propagation: \IFDS{} (distributive)
  \item Backward alias finding: NOT \IFDS{} (may-alias is non-distributive)
  \item Integration: Ad-hoc spawning mechanism
\end{itemize}

A cleaner formalization would use \IDE{} (Section~\ref{sec:flix-lattice}) for the backward phase. See Section~8.1.6.1 for full activation statement semantics.
\end{pillarbox}


%--------------------------------------------------
\section{Common IFDS Problems}
\label{sec:ifds-problems}
%--------------------------------------------------

\begin{fstarcode}[title={Instantiations of IFDS for Common Analyses}]
(* --------------------------------------------------
   REACHING DEFINITIONS
   -------------------------------------------------- *)

type reaching_def = { var : string; def_site : node_id }

let reaching_definitions_problem (cpg : cpg) : ifds_problem reaching_def = {
  supergraph = cpg;
  domain = all_definitions cpg;
  zero = { var = ""; def_site = 0 };  (* Dummy zero *)

  flow_function = fun edge d ->
    let n = edge.source in
    match get_defined_var cpg n with
    | Some v when d.var = v ->
        (* Kill: this definition kills previous defs of same var *)
        Set.empty
    | Some v ->
        (* Gen: add this definition, keep others *)
        Set.add { var = v; def_site = n } (Set.singleton d)
    | None ->
        Set.singleton d;  (* Pass through *)

  call_flow = fun call_site d ->
    (* Pass definitions through call *)
    Set.singleton d;

  return_flow = fun call_site return_site d_call d_exit ->
    (* Definitions from callee flow back *)
    Set.singleton d_exit;

  call_to_return_flow = fun call_site d ->
    (* Local definitions not affected by call *)
    if is_local_def cpg d.var then Set.singleton d else Set.empty;
}

(* --------------------------------------------------
   TAINT ANALYSIS (via IFDS)
   Source: Livshits 2005
   -------------------------------------------------- *)

type taint_fact =
  | TaintedVar : var:string -> taint_fact
  | TaintedField : base:string -> field:string -> taint_fact
  | TaintedReturn : taint_fact  (* Return value is tainted *)

let taint_analysis_problem
  (cpg : cpg)
  (sources : set node_id)
  (sanitizers : set node_id)
: ifds_problem taint_fact = {
  supergraph = cpg;
  domain = all_taint_facts cpg;
  zero = TaintedVar "";  (* Dummy *)

  flow_function = fun edge d ->
    let n = edge.source in
    (* Source: introduce taint *)
    if Set.mem n sources then
      match get_assigned_var cpg n with
      | Some v -> Set.add (TaintedVar v) (Set.singleton d)
      | None -> Set.singleton d
    (* Sanitizer: remove taint *)
    else if Set.mem n sanitizers then
      match get_assigned_var cpg n with
      | Some v when d = TaintedVar v -> Set.empty
      | _ -> Set.singleton d
    (* Assignment: propagate taint *)
    else match get_assignment cpg n with
    | Some (lhs, rhs_vars) ->
        let rhs_tainted = List.exists (fun v -> d = TaintedVar v) rhs_vars in
        if rhs_tainted then
          Set.add (TaintedVar lhs) (Set.singleton d)
        else if d = TaintedVar lhs then
          Set.empty  (* Overwritten with clean value *)
        else
          Set.singleton d
    | None -> Set.singleton d;

  call_flow = fun call_site d ->
    (* Map tainted actuals to tainted formals *)
    match d with
    | TaintedVar v ->
        let param_index = get_arg_index cpg call_site v in
        begin match param_index with
        | Some i -> Set.singleton (TaintedVar (get_formal cpg call_site i))
        | None -> Set.empty
        end
    | _ -> Set.empty;

  return_flow = fun call_site return_site d_call d_exit ->
    (* Map tainted return to tainted result variable *)
    match d_exit with
    | TaintedReturn ->
        begin match get_result_var cpg return_site with
        | Some v -> Set.singleton (TaintedVar v)
        | None -> Set.empty
        end
    | _ -> Set.empty;

  call_to_return_flow = fun call_site d ->
    (* Taint of locals not affected by call *)
    match d with
    | TaintedVar v when not (is_arg cpg call_site v) -> Set.singleton d
    | _ -> Set.empty;
}

(* LIMITATION: This basic IFDS taint is FLOW-INSENSITIVE for heap.
   For PRECISE heap taint with flow-sensitivity, see Section 8.1.6.1
   (FlowDroid activation statements) which extends IFDS with:
     - Access paths (x.f.g) instead of simple variables
     - Bidirectional analysis (forward taint + backward alias)
     - Activation statements for flow-sensitive heap precision

   For CROSS-LANGUAGE taint, see Section 9.1.4 (PolyCruise) which uses
   LISR for language-agnostic def/use analysis. *)

(* --------------------------------------------------
   UNINITIALIZED VARIABLE ANALYSIS
   -------------------------------------------------- *)

type uninit_fact =
  | Uninitialized : var:string -> uninit_fact

let uninitialized_analysis_problem (cpg : cpg) : ifds_problem uninit_fact = {
  supergraph = cpg;
  domain = Set.map (fun v -> Uninitialized v) (all_variables cpg);
  zero = Uninitialized "";

  flow_function = fun edge d ->
    let n = edge.source in
    match d with
    | Uninitialized v ->
        (* Declaration without init: add uninit fact *)
        if is_decl_without_init cpg n v then
          Set.singleton (Uninitialized v)
        (* Assignment: remove uninit fact *)
        else if assigns_to cpg n v then
          Set.empty
        (* Use of uninit var: REPORT BUG *)
        else if uses_var cpg n v then
          let _ = report_uninit_use cpg n v in
          Set.singleton d  (* Keep tracking *)
        else
          Set.singleton d;
  (* ... call/return flows ... *)
}
\end{fstarcode}


%--------------------------------------------------
\section{IFDS Implementation Optimizations}
\label{sec:ifds-optimizations}
%--------------------------------------------------

\textbf{Sources}: Reps, Horwitz, Sagiv 1995; Naeem et al. 2010

\begin{pillarbox}[title={Cross-References}]
\begin{itemize}
  \item \IFDS{} is DISTRIBUTIVE fragment only --- see Section~12.18 (Set Constraints) and Appendix~D.10.1 for non-distributive alternatives
  \item Taint analysis is classic \IFDS{} application --- see Section~8.1
  \item \IFDS{} is OVER-approximate; for under-approximate bug finding use Eval algorithm --- see Section~\ref{sec:under-approx}
  \item For SOURCE-SINK problems (leak detection), consider SVF (Section~5.6) as alternative --- SVF uses sparse value-flow graphs instead of exploded supergraph, which can be more efficient when memory regions $R \ll$ domain $D$
\end{itemize}
\end{pillarbox}


\subsection{Representation Relations}
\label{sec:repr-relations}

\begin{fstarcode}[title={Representation Relations (RHS95 Section 4)}]
(* ==================================================
   Key insight: Distributive functions can be compactly represented.
   ================================================== *)

(* --------------------------------------------------
   THEOREM: A function f : 2^D -> 2^D is distributive iff:
     f(X \cup  Y) = f(X) \cup  f(Y)

   Such functions can be encoded as a RELATION R \subseteq  (D \cup  {0}) \times  D
   where (d_1, d_2) \in  R means "if d_1 holds, then d_2 is generated"

   SPACE COMPLEXITY: O(D^2) instead of O(2^D) for arbitrary functions
   COMPOSITION: R_1 ; R_2 = { (a,c) | \exists b. (a,b) \in  R_1 \land  (b,c) \in  R_2 }
                Relation composition = Function composition!
   -------------------------------------------------- *)

type repr_relation (d : Type) = set (option d * d)
  (* None represents the "zero" fact - unconditional generation *)

(* Convert a transfer function to its representation relation *)
val repr_of_transfer : #d:Type -> all_facts:set d -> (set d -> set d) ->
                       repr_relation d
let repr_of_transfer #d all_facts f =
  (* For each input fact (including 0), compute generated facts *)
  let zero_gen = f Set.empty in  (* Unconditional generation: f({}) *)
  let zero_edges = Set.map (fun d2 -> (None, d2)) zero_gen in
  (* For each fact d1, compute what d1 generates *)
  let fact_edges = Set.concat_map (fun d1 ->
    let out = f (Set.singleton d1) in
    Set.map (fun d2 -> (Some d1, d2)) out
  ) all_facts in
  Set.union zero_edges fact_edges

(* Compose two representation relations *)
val compose_repr : #d:Type -> repr_relation d -> repr_relation d ->
                   repr_relation d
let compose_repr #d r1 r2 =
  (* R_1 ; R_2 = relational composition *)
  Set.concat_map (fun (a, b) ->
    let matching = Set.filter (fun (b', c) ->
      match (b, b') with
      | (Some x, Some y) -> x = y
      | (None, None) -> true
      | _ -> false
    ) r2 in
    Set.map (fun (_, c) -> (a, c)) matching
  ) r1

(* Apply a representation relation to get output facts *)
val apply_repr : #d:Type -> repr_relation d -> set d -> set d
let apply_repr #d r input =
  (* Output = { d2 | (0, d2) \in  R } \cup  { d2 | \exists d1 \in  input. (d1, d2) \in  R } *)
  let from_zero = Set.filter_map (fun (src, tgt) ->
    match src with None -> Some tgt | Some _ -> None
  ) r in
  let from_facts = Set.concat_map (fun d1 ->
    Set.filter_map (fun (src, tgt) ->
      match src with Some d when d = d1 -> Some tgt | _ -> None
    ) r
  ) input in
  Set.union from_zero from_facts

(* --------------------------------------------------
   THEOREM (Correctness of Representation)
   For any distributive f and its representation R:
     apply_repr R input = f input
   -------------------------------------------------- *)

val repr_correct :
  #d:Type -> all_facts:set d -> f:(set d -> set d) ->
  Lemma (requires is_distributive f)
        (ensures forall input. apply_repr (repr_of_transfer all_facts f) input
                               = f input)
\end{fstarcode}


\subsection{H-Sparse Optimization}
\label{sec:h-sparse}

\begin{fstarcode}[title={H-Sparse Optimization (Naeem et al. 2010)}]
(* ==================================================
   Observation: Most transfer functions affect only h << D facts.
   "Sparse" = few facts generated or killed per edge.
   ================================================== *)

(*
  COMPLEXITY IMPROVEMENT:
    Standard IFDS:  O(E \cdot  D^3)
    H-sparse IFDS:  O(Call \cdot  D^3 + h \cdot  E \cdot  D^2)
    where h = max facts affected per edge (the "sparsity parameter")

  PRACTICAL EXAMPLES:
    For taint analysis: h \approx  2-3 (one source, one propagation per stmt)
    For null analysis:  h \approx  1-2 (one variable nullified per stmt)
    For live variables: h \approx  2   (one def, one use per stmt typically)

  INTUITION: Most statements touch few variables, so most transfer
  functions are nearly identity. Exploit this sparsity!
*)

type sparse_repr (d : Type) = {
  gen : set d;              (* Facts generated unconditionally *)
  kill : set d;             (* Facts killed (removed from input) *)
  propagate : set (d * d);  (* Conditional propagation: d1 \to  d2 *)
  (* INVARIANT: |gen| + |kill| + |propagate| \leq  h *)
}

(* Check if a representation relation is h-sparse *)
val is_h_sparse : #d:Type -> repr_relation d -> h:nat -> bool
let is_h_sparse #d r h =
  Set.size r <= h * h  (* At most h^2 edges in representation *)

(* Convert full representation to sparse form (if possible) *)
val to_sparse_repr : #d:Type -> all_facts:set d -> repr_relation d ->
                     option (sparse_repr d)
let to_sparse_repr #d all_facts r =
  (* gen = facts generated from zero (unconditional) *)
  let gen = Set.filter_map (fun (src, tgt) ->
    match src with None -> Some tgt | _ -> None
  ) r in
  (* identity = facts that map to themselves *)
  let identity_facts = Set.filter (fun d ->
    Set.mem (Some d, d) r
  ) all_facts in
  (* kill = facts in domain but NOT in identity *)
  let kill = Set.diff all_facts identity_facts in
  (* propagate = non-identity, non-zero mappings *)
  let propagate = Set.filter (fun (src, tgt) ->
    match src with
    | Some d when d <> tgt -> true
    | _ -> false
  ) r in
  Some { gen; kill;
         propagate = Set.map (fun (Some s, t) -> (s, t)) propagate }

(* Sparse propagation - only process affected facts *)
val propagate_sparse : #d:Type -> sparse_repr d -> set d -> set d
let propagate_sparse #d sr input =
  (* 1. Remove killed facts *)
  let after_kill = Set.diff input sr.kill in
  (* 2. Apply conditional propagations *)
  let propagated = Set.concat_map (fun d ->
    let targets = Set.filter_map (fun (d', d'') ->
      if d' = d then Some d'' else None
    ) sr.propagate in
    if Set.is_empty targets then Set.singleton d else targets
  ) after_kill in
  (* 3. Add generated facts *)
  Set.union sr.gen propagated

(* --------------------------------------------------
   THEOREM (Sparse Complexity)
   For h-sparse problems:
     Time:  O(Call \cdot  D^3 + h \cdot  E \cdot  D^2)
     Space: O(N \cdot  D^2 + Call \cdot  D^2)
   When h << D, this is significantly better than O(E \cdot  D^3)
   -------------------------------------------------- *)
\end{fstarcode}


\subsection{Locally Separable Problems (Gen/Kill)}
\label{sec:locally-separable}

\begin{fstarcode}[title={Locally Separable Problems (RHS95 Corollary)}]
(* ==================================================
   A problem is "locally separable" if ALL transfer functions are gen/kill:
     f(X) = gen \cup  (X \ kill)
   For such problems: O(E \cdot  D) instead of O(E \cdot  D^3) !!!
   ================================================== *)

(*
  EXAMPLES OF LOCALLY SEPARABLE PROBLEMS:
  1. REACHING DEFINITIONS:
     gen  = { def }                     -- this definition reaches
     kill = { prev defs of same var }  -- previous defs killed

  2. LIVE VARIABLES:
     gen  = { used vars }              -- used variables are live
     kill = { defined var }            -- defined var no longer live before

  3. AVAILABLE EXPRESSIONS:
     gen  = { computed expr }          -- expression now available
     kill = { exprs using modified var } -- invalidated expressions

  4. VERY BUSY EXPRESSIONS:
     gen  = { expr used on all paths } -- expression busy
     kill = { exprs with modified vars } -- no longer busy

  WHAT IS NOT LOCALLY SEPARABLE:
  - Pointer analysis (aliasing creates dependencies)
  - Constant propagation (values depend on other values)
  - Shape analysis (heap structure depends on operations)
*)

type gen_kill_function (d : Type) = {
  gen : set d;
  kill : set d;
}

(* Check if a transfer function is gen/kill form *)
val is_gen_kill : #d:Type -> (set d -> set d) -> option (gen_kill_function d)
let is_gen_kill #d f =
  (* A function is gen/kill iff:
     1. f({}) gives us the gen set
     2. For all d, either d \in  f({d}) (preserved) or d \notin  f({d}) (killed) *)
  let gen = f Set.empty in
  (* To find kill: facts d where d \notin  f({d}) *)
  (* This requires knowing the domain, simplified here *)
  Some { gen; kill = Set.empty (* computed from domain *) }

(* Check if an IFDS problem is locally separable *)
val is_locally_separable : #d:Type -> ifds_problem d -> bool
let is_locally_separable #d problem =
  (* All flow functions must be gen/kill *)
  forall_edges problem.supergraph (fun edge ->
    match is_gen_kill (problem.flow_function edge) with
    | Some _ -> true
    | None -> false
  )

(* --------------------------------------------------
   FAST SOLVER FOR LOCALLY SEPARABLE PROBLEMS
   Complexity: O(E \cdot  D) instead of O(E \cdot  D^3)
   KEY INSIGHT: For gen/kill, we don't need the full IFDS machinery.
   Each fact can be tracked independently!
   -------------------------------------------------- *)

val solve_gen_kill : #d:Type ->
  problem:ifds_problem d ->
  Lemma (requires is_locally_separable problem) ->
  (node_id -> set d)
let solve_gen_kill #d problem _ =
  (* For gen/kill, facts are independent - track each separately *)
  (* This is essentially running |D| separate bit-vector dataflows *)
  let solve_for_fact (fact : d) : set node_id =
    (* Standard worklist for single-fact reachability *)
    let rec worklist visited frontier =
      match frontier with
      | [] -> visited
      | n :: rest ->
          if Set.mem n visited then worklist visited rest
          else
            let visited' = Set.add n visited in
            let succs = get_successors problem.supergraph n in
            let live_succs = List.filter (fun succ ->
              let gk = get_gen_kill problem n succ in
              not (Set.mem fact gk.kill)  (* Not killed on this edge *)
            ) succs in
            worklist visited' (live_succs @ rest)
    in
    (* Also include nodes where fact is generated *)
    let gen_nodes = get_gen_nodes problem fact in
    worklist Set.empty (Set.to_list gen_nodes)
  in
  (* Combine results for all facts *)
  fun node ->
    Set.filter (fun fact ->
      Set.mem node (solve_for_fact fact)
    ) problem.domain

(*
  COMPLEXITY ANALYSIS:
    - For each fact: O(E) traversal
    - Total: O(D \cdot  E) = O(E \cdot  D)
    - Compare to IFDS: O(E \cdot  D^3)

  PRACTICAL SPEEDUP:
    For D = 1000 facts: 1000^3 = 10^9 vs 10^3 = factor of 10^6 improvement!
*)
\end{fstarcode}


\subsection{Demand-Driven IFDS}
\label{sec:demand-driven-ifds}

\begin{fstarcode}[title={Demand-Driven IFDS}]
(* ==================================================
   Standard IFDS: Compute ALL reachable facts (exhaustive, whole-program)
   Demand-driven: Only compute facts needed to answer a SPECIFIC QUERY
   ================================================== *)

(*
  USE CASE: "Is variable x tainted at line 42?"

  STANDARD IFDS:
    1. Run full taint analysis on entire program
    2. Look up result for (line 42, tainted(x))
    3. Cost: O(E \cdot  D^3) even if we only care about ONE fact at ONE location

  DEMAND-DRIVEN IFDS:
    1. Start from query point (line 42, tainted(x))
    2. Search BACKWARD: "how could this fact become true?"
    3. Stop when we reach a source or prove unreachable
    4. Cost: O(relevant subgraph) << O(full program)

  RELATION TO CFL-REACHABILITY (Section 4.2):
    Demand-driven IFDS is backward CFL-reachability in the exploded supergraph.
    The Dyck language ensures matched call/return context.

  DIFFERENCES FROM PURE CFL:
    - IFDS has transfer functions that transform facts
    - CFL just tracks reachability with grammar rules
    - Demand-driven IFDS = CFL + inverse transfer functions
*)

(* Query type: does a fact hold at a specific program point? *)
type ifds_query (d : Type) = {
  query_node : node_id;
  query_fact : d;
}

(* Demand-driven solver *)
val demand_driven_ifds :
  #d:Type ->
  problem : ifds_problem d ->
  query : ifds_query d ->
  bool  (* Does fact hold at node? *)

let demand_driven_ifds #d problem query =
  (* ALGORITHM: Backward search in exploded supergraph from query point *)
  let target = (query.query_node, query.query_fact) in
  let source = (start_main problem.supergraph, Zero) in

  (* Compute inverse transfer functions for backward search *)
  let inverse_flow edge d_out =
    (* Find d_in such that d_out \in  flow(edge)(d_in) *)
    Set.filter (fun d_in ->
      Set.mem d_out (problem.flow_function edge (Set.singleton d_in))
    ) (Set.add Zero problem.domain)
  in

  (* Backward reachability with context sensitivity *)
  let rec backward_search visited frontier =
    match frontier with
    | [] -> false  (* Query fact is not reachable from any source *)
    | (node, fact, call_stack) :: rest ->
        (* Check if we reached a source (entry of main with Zero fact) *)
        if node = fst source && fact = Zero then
          true  (* Found a valid path! *)
        else if Set.mem (node, fact, call_stack) visited then
          backward_search visited rest
        else
          let visited' = Set.add (node, fact, call_stack) visited in
          (* Get predecessors and inverse-flow facts *)
          let preds = get_predecessors problem.supergraph node in
          let new_frontier = List.concat_map (fun pred ->
            let edge = { source = pred; target = node } in
            let d_ins = inverse_flow edge fact in
            (* Handle call/return context *)
            match node_type problem.supergraph pred with
            | NCall call_site ->
                (* Push call site onto stack *)
                List.map (fun d -> (pred, d, call_site :: call_stack))
                         (Set.to_list d_ins)
            | NReturn ret_site ->
                (* Must match top of stack *)
                (match call_stack with
                 | call :: rest_stack when matches_return call ret_site ->
                     List.map (fun d -> (pred, d, rest_stack))
                              (Set.to_list d_ins)
                 | _ -> [])  (* Invalid context - prune this path *)
            | _ ->
                List.map (fun d -> (pred, d, call_stack)) (Set.to_list d_ins)
          ) preds in
          backward_search visited' (new_frontier @ rest)
  in
  backward_search Set.empty [(query.query_node, query.query_fact, [])]

(* --------------------------------------------------
   WHEN TO USE DEMAND-DRIVEN VS EXHAUSTIVE

   USE EXHAUSTIVE (standard IFDS) WHEN:
     - Need results for ALL program points
     - Building a whole-program summary
     - Analysis is cheap relative to program size

   USE DEMAND-DRIVEN WHEN:
     - Only need specific query answers
     - Interactive tools (IDE integration)
     - Incremental analysis after edits
     - Query is localized (few relevant paths)

   HYBRID APPROACH:
     1. Run fast over-approximate analysis (exhaustive)
     2. For reported issues, use demand-driven to verify
     3. See Section 4.3.3 for IFDS + Eval combination
   -------------------------------------------------- *)
\end{fstarcode}


\subsection{Summary: IFDS Complexity Variants}
\label{sec:ifds-complexity-summary}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Variant} & \textbf{Time Complexity} & \textbf{When to Use} \\
\hline
Standard IFDS & $\BigO{E \cdot D^3}$ & General distributive problems \\
H-sparse IFDS & $\BigO{\text{Call} \cdot D^3 + h \cdot E \cdot D^2}$ & Sparse facts (taint, null) \\
Locally separable & $\BigO{E \cdot D}$ & Gen/kill problems \\
Demand-driven & $\BigO{\text{relevant paths}}$ & Specific queries, IDE tools \\
\hline
\end{tabular}
\end{center}

Where: $E$ = edges in supergraph, $D$ = size of dataflow domain, $h$ = sparsity parameter (max facts affected per edge), Call = number of call sites.

\textbf{Reminder}: \IFDS{} requires DISTRIBUTIVE transfer functions! For non-distributive (pointer analysis): Use set constraints (Section~12.18). For under-approximate bug finding: Use Eval algorithm (Section~\ref{sec:under-approx}).


%--------------------------------------------------
\section{Datalog Compilation Strategy (Souffle)}
\label{sec:datalog-compilation}
%--------------------------------------------------

\textbf{Paper}: Jordan, Scholz, Subotic 2016 (CAV)

For high-performance deployment, \IFDS{} problems expressed as Datalog can be COMPILED to specialized code rather than interpreted.

\begin{pillarbox}[title={Key Insight: Compile, Don't Interpret}]
Jordan 2016 shows: Datalog-based analysis is elegant but SLOW when interpreted. Compilation achieves 50x+ speedup over bddbddb/muZ.

\textbf{Benchmark} (OpenJDK7: 1.4M variables, 350K objects, 160K methods):
\begin{itemize}
  \item Souffle (compiled): 35 seconds (CI points-to)
  \item bddbddb: 30 minutes
  \item SQLite: 6 hours 20 minutes
  \item muZ: Did not finish
\end{itemize}
\end{pillarbox}


\subsection{Staged Compilation via Futamura Projections}
\label{sec:futamura}

\textbf{Souffle Compilation Pipeline}:

\textbf{Stage 1}: Datalog $\rightarrow$ RAM (Relational Algebra Machine)
\[
P_{\text{RAM}} = \text{Mix}(\text{Int}_{\text{dl}}, I_{\text{db}})
\]
Where: $\text{Int}_{\text{dl}}$ = Semi-naive evaluation interpreter, $I_{\text{db}}$ = Intensional database (analysis rules). Result: Relational Algebra Machine program. Optimizations: Rule reordering, semi-naive transformation.

\textbf{Stage 2}: RAM $\rightarrow$ Optimized RAM. Index selection via DILWORTH'S THEOREM (see Section~\ref{sec:dilworth}). Join reordering for cache efficiency. Stratum computation for stratified negation.

\textbf{Stage 3}: RAM $\rightarrow$ C++/Rust
\[
P_{\text{C++}} = \text{Mix}(\text{Int}_{\text{RAM}}, P_{\text{RAM}})
\]
Via template metaprogramming: inlined comparison functions, specialized B-tree iterators, OpenMP parallel annotations.

\begin{theorem}[Semantic Preservation]
\[
\text{result} = \sem{\text{Int}}(\text{Source}, \text{Input}) = \sem{\text{Source}}_{\text{Int}}(\text{Input}) = \sem{\text{Mix}(\text{Int}, \text{Source})}(\text{Input}) = \sem{\text{Compiled}}(\text{Input})
\]
\end{theorem}


\subsection{Optimal Index Selection (Dilworth's Theorem)}
\label{sec:dilworth}

\textbf{Problem}: Given $N$ index requirements from queries, find MINIMAL set of indices such that each requirement is subsumed by at least one selected index.

\textbf{Insight}: Index subsumption forms a partial order. A lexicographic index $[a, b, c]$ subsumes $[a, b]$ and $[a]$. That is: if I can look up by $(a, b, c)$, I can also look up by $(a)$ or $(a, b)$.

\begin{theorem}[Dilworth's Theorem]
The width of a partial order equals the minimum number of chains needed to cover all elements.
\end{theorem}

\textbf{Algorithm} (polynomial time):
\begin{enumerate}
  \item Build lattice of required indices under subsumption
  \item Compute minimum chain partition (via bipartite matching)
  \item Select maximum element from each chain
\end{enumerate}

\textbf{Result}: For analyses with 100s of index requirements, reduces to minimal set. Each chain's maximum subsumes all requirements in that chain.

\textbf{Example}:
\begin{itemize}
  \item Required indices: $[a], [a,b], [a,c], [a,b,c], [b], [b,c]$
  \item Chain 1: $[a] < [a,b] < [a,b,c]$ $\rightarrow$ select $[a,b,c]$
  \item Chain 2: $[a,c]$ $\rightarrow$ select $[a,c]$
  \item Chain 3: $[b] < [b,c]$ $\rightarrow$ select $[b,c]$
  \item Minimal cover: $\{[a,b,c], [a,c], [b,c]\}$ instead of all 6 indices
\end{itemize}

\begin{fstarcode}[title={Datalog Compilation Types (Jordan et al. 2016)}]
module BrrrMachine.DatalogCompile

(* Relational Algebra Machine operations *)
type ram_expr =
  | RAMScan : relation:string -> ram_expr
  | RAMFilter : expr:ram_expr -> column:nat -> value:string -> ram_expr
  | RAMProject : expr:ram_expr -> columns:list nat -> ram_expr
  | RAMJoin : left:ram_expr -> right:ram_expr ->
              left_col:nat -> right_col:nat -> ram_expr
  | RAMUnion : left:ram_expr -> right:ram_expr -> ram_expr
  | RAMDiff : left:ram_expr -> right:ram_expr -> ram_expr

type ram_stmt =
  | RAMInsert : target:string -> source:ram_expr -> ram_stmt
  | RAMLoop : delta:string -> body:list ram_stmt -> ram_stmt  (* Semi-naive *)
  | RAMSeq : stmts:list ram_stmt -> ram_stmt

(* Index specification *)
type index_spec = {
  relation : string;
  key_columns : list nat;  (* Lexicographic order *)
}

(* Index subsumption: [a,b,c] subsumes [a,b] and [a] *)
let subsumes (super sub : index_spec) : bool =
  super.relation = sub.relation &&
  List.length sub.key_columns <= List.length super.key_columns &&
  List.for_all2 (=) sub.key_columns
    (List.take (List.length sub.key_columns) super.key_columns)

(* Dilworth-based minimal index selection *)
val compute_minimal_indices :
  required:list index_spec ->
  minimal:list index_spec{
    (* Every required index is subsumed by some minimal index *)
    forall r. List.mem r required ==>
      exists m. List.mem m minimal /\ subsumes m r
  }
\end{fstarcode}


\subsection{Implementation Strategy for brrr-machine}
\label{sec:brrr-datalog-strategy}

\begin{pillarbox}[title={Recommended Approach}]
\textbf{Development Mode}: Use interpreted Datalog for rapid iteration
\begin{itemize}
  \item Crepe (Rust embedded Datalog) for prototyping
  \item Souffle interpreter mode for debugging
  \item Modify rules without recompilation
\end{itemize}

\textbf{Production Mode}: Compile analysis rules to specialized Rust
\begin{itemize}
  \item Express \IFDS{}/\IDE{} as Datalog rules
  \item Compile via Souffle $\rightarrow$ C++ or custom Datalog$\rightarrow$Rust pipeline
  \item Link compiled analysis into brrr-machine
\end{itemize}

\textbf{Why not just use Souffle directly?}
\begin{itemize}
  \item Souffle generates C++, we want Rust integration
  \item We need lattice predicates (Section~\ref{sec:flix-lattice}) which Souffle lacks
  \item But Souffle's TECHNIQUES (RAM, Dilworth indices) are essential
\end{itemize}
\end{pillarbox}


%--------------------------------------------------
\section{Lattice-Extended Datalog and IDE (Flix)}
\label{sec:flix-lattice}
%--------------------------------------------------

\textbf{Paper}: Madsen, Yee, Lhotak 2016 (PLDI)

Standard Datalog operates on relations (finite sets). Many analyses require lattices with potentially infinite domains. Flix extends Datalog with lattice predicates, enabling elegant expression of \IFDS{}, \IDE{}, and value analyses.

\begin{pillarbox}[title={The Flix Insight: Relations vs Lattices}]
\textbf{Standard Datalog (rel)}:
\begin{verbatim}
rel Edge(x, y)
Multiple tuples = set
{Edge(1,2), Edge(1,3)} -> keeps both
\end{verbatim}

\textbf{Flix Extension (lat)}:
\begin{verbatim}
lat LocalVar(x: Var, v: Constant)
Same key = JOIN lattice values
{LocalVar(x, Cst(1)), LocalVar(x, Cst(2))} -> {LocalVar(x, Top)}
\end{verbatim}

\textbf{What this enables}:
\begin{itemize}
  \item Constant propagation (lattice = constants with Top)
  \item Interval analysis (lattice = intervals)
  \item \IDE{} algorithm (lattice = micro-function space)
  \item Strong update analysis (hybrid flow-sensitive/insensitive)
\end{itemize}
\end{pillarbox}


\subsection{Lattice Predicates and Transfer Functions}
\label{sec:flix-predicates}

\begin{verbatim}
// Standard Datalog relation - set semantics
rel AddExp(r: Var, x: Var, y: Var)    // r = x + y in source code

// Flix lattice predicate - join semantics
lat LocalVar(x: Var, v: Constant)     // x has abstract value v
//           ^^^^    ^^^^^^^^^^
//           key     lattice element (joined on same key)

// User-defined lattice
enum Constant {
  case Top,       // Unknown (join of all)
  case Cst(Int),  // Known constant
  case Bot        // Unreachable (bottom)
}

// Transfer function: MUST BE MONOTONE
def sum(e1: Constant, e2: Constant): Constant =
  match (e1, e2) with {
    case (Bot, _) | (_, Bot) => Bot      // Unreachable stays unreachable
    case (Cst(n1), Cst(n2))  => Cst(n1 + n2)
    case _                   => Top       // Unknown
  }

// Filter function: guards rule application
def isMaybeZero(e: Constant): Bool =
  match e with {
    case Bot    => false    // Unreachable - no error
    case Cst(n) => n == 0
    case Top    => true     // Could be zero
  }

// Flix rule with transfer function
LocalVar(r, sum(x, y)) :- AddExp(r, v1, v2),
                          LocalVar(v1, x),
                          LocalVar(v2, y).

// Flix rule with filter function
ArithmeticError(r) :- isMaybeZero(y),
                      DivExp(r, n, d),
                      LocalVar(d, y).
\end{verbatim}


\subsection{IDE as Flix with Micro-Function Lattice}
\label{sec:ide-flix}

\begin{pillarbox}[title={\IDE{} Is Not Complex---It's Just Flix with a Specific Lattice}]
\IFDS{} path edge: \texttt{PathEdge(d1, n, d2)} --- fact \texttt{d2} holds at \texttt{n}

\IDE{} path edge: \texttt{PathEdge(d1, n, d2, f)} --- with environment transformer $f$

The micro-function space forms a lattice under pointwise ordering:
\begin{align*}
\bot &= \lambda x.\, \bot \quad \text{(constant bottom function)} \\
\top &= \lambda x.\, \top \quad \text{(constant top function)} \\
f \sqcup g &= \lambda x.\, (f\, x) \sqcup (g\, x) \quad \text{(pointwise join)}
\end{align*}

\IDE{} key insight: transformers COMPOSE along paths:
\begin{verbatim}
PathEdge(d1, m, d3, compose(f, g)) :-
  PathEdge(d1, n, d2, f),
  CFG(n, m),
  EdgeFunction(n, d2, m, d3, g).
\end{verbatim}
\end{pillarbox}

\begin{fstarcode}[title={\IDE{} as Flix Instance (Madsen et al. 2016)}]
module BrrrMachine.IDE

(* Micro-function: environment transformer *)
type micro_fn (v : Type) = v -> v

(* Micro-functions form a lattice under pointwise ordering *)
instance micro_fn_lattice (v : Type) {| complete_lattice v |} :
  complete_lattice (micro_fn v) = {
  bot = (fun _ -> bot);
  top = (fun _ -> top);
  join = (fun f g -> fun x -> join (f x) (g x));
  meet = (fun f g -> fun x -> meet (f x) (g x));
  leq = (fun f g -> forall x. leq (f x) (g x));
}

(* IDE path edge: track micro-function along path *)
type ide_path_edge (d : Type) (v : Type) = {
  entry_fact : d;
  current_node : node_id;
  current_fact : d;
  transformer : micro_fn v;
}

(* Composition is the IDE transfer function *)
let ide_compose (#v:Type) (f g : micro_fn v) : micro_fn v =
  fun x -> f (g x)

(* Composition is monotone in both arguments *)
val ide_compose_monotone : #v:Type -> {| complete_lattice v |} ->
  f1:micro_fn v -> f2:micro_fn v -> g1:micro_fn v -> g2:micro_fn v ->
  Lemma (requires micro_fn_leq f1 f2 /\ micro_fn_leq g1 g2)
        (ensures micro_fn_leq (ide_compose f1 g1) (ide_compose f2 g2))

(* --------------------------------------------------
   WHEN TO USE IDE vs IFDS

   USE IFDS (Section 4.1):
     - Taint analysis (binary: tainted/untainted)
     - Uninitialized variables (set of "maybe uninitialized")
     - Typestate (finite state machine)
     - Any analysis where facts are BINARY or small finite set

   USE IDE:
     - Constant propagation (need to track actual constant values)
     - Linear constant propagation (x = a*y + b transformers)
     - Copy constant analysis (track "which variable copied from")
     - Any analysis where you need VALUE TRANSFORMATIONS along paths
   -------------------------------------------------- *)
\end{fstarcode}


\subsection{Semi-Naive Evaluation for Lattices}
\label{sec:semi-naive-lattice}

\textbf{Standard Semi-Naive (for relations)}:
\[
\delta_R = \text{new TUPLES added to } R
\]

\textbf{Flix Semi-Naive (for lattices)}:
\[
\delta_L = (\text{key}, \text{old\_value}, \text{new\_value}) \text{ where } \text{new\_value} \sqsupset \text{old\_value}
\]

The iteration terminates when no lattice element strictly increases.

\begin{theorem}[Madsen 2016, Theorem 1]
Every Flix program has a unique minimal model, computable via iteration, PROVIDED all transfer functions are monotone.
\end{theorem}

\textbf{Monotonicity Requirement}: For soundness, all transfer functions $f$ must satisfy:
\[
x \sqsubseteq y \implies f(x) \sqsubseteq f(y)
\]
This is the Flix equivalent of \IFDS{} distributivity requirement.

\begin{fstarcode}[title={Semi-Naive Evaluation for Lattice Predicates}]
(* Transfer function monotonicity - critical for soundness *)
type monotone_transfer (#l:Type) {| complete_lattice l |} =
  f:(l -> l){ forall x y. leq x y ==> leq (f x) (f y) }

(* Binary monotone transfer (e.g., addition in constant lattice) *)
type monotone_transfer2 (#l:Type) {| complete_lattice l |} =
  f:(l -> l -> l){
    forall x1 x2 y1 y2.
      leq x1 x2 /\ leq y1 y2 ==> leq (f x1 y1) (f x2 y2)
  }

(* Semi-naive iteration for lattice predicates *)
type lat_delta (k l : Type) = {
  changed_keys : set k;
  old_values : k -> l;
  new_values : k -> l;
  (* Invariant: forall k in changed_keys. old_values k \sqsubset  new_values k *)
}

val flix_semi_naive :
  #k:Type -> #l:Type -> {| complete_lattice l |} ->
  rules:list flix_rule ->
  initial:lat_predicate k l ->
  lat_predicate k l  (* Returns minimal model *)
\end{fstarcode}


%==================================================
\chapter{CFL-Reachability}
\label{sec:cfl-reachability}
%==================================================

\textbf{Paper}: Reps 1997

\CFL{}-reachability generalizes \IFDS{} by allowing arbitrary context-free grammars to describe valid paths, not just matched parentheses.


%--------------------------------------------------
\section{The Framework}
\label{sec:cfl-framework}
%--------------------------------------------------

\begin{definition}[CFL-Reachability]
Given:
\begin{itemize}
  \item Graph $G = (V, E)$ with edge labels from alphabet $\Sigma$
  \item Context-free grammar $\mathcal{G}$ with terminals $\Sigma$
  \item Start symbol $S$
\end{itemize}

\textbf{Question}: For which pairs $(u, v)$ is there a path from $u$ to $v$ whose edge labels form a string in $L(\mathcal{G})$?
\end{definition}

\textbf{The Dyck Language (matched parentheses)}:
\[
S \rightarrow S\, S \mid (_i\, S\, )_i \mid \varepsilon \quad \text{for each call site } i
\]
This is exactly what \IFDS{} uses for context sensitivity.

\textbf{Other Useful Grammars}:

\textit{Field-sensitive alias analysis}:
\begin{align*}
\text{FlowsTo} &\rightarrow \texttt{new} \\
&\mid \text{FlowsTo}\ \texttt{assign} \\
&\mid \text{FlowsTo}\ \texttt{load}\ \text{FlowsTo}\ \texttt{store}
\end{align*}
``x flows to y if there's a path: new, then assigns, then matched load/store on same field''

\textit{Typestate analysis}:
\begin{align*}
\text{Valid} &\rightarrow \texttt{open}\ \text{Valid}\ (\texttt{read} \mid \texttt{write})^*\ \texttt{close} \\
&\mid \text{Valid}\ \text{Valid}
\end{align*}
``Valid use is: open, then reads/writes, then close''


%--------------------------------------------------
\section{Demand-Driven Analysis via CFL}
\label{sec:demand-cfl}
%--------------------------------------------------

\textbf{Paper}: Sridharan 2005

\begin{fstarcode}[title={Demand-Driven Points-To Analysis (Sridharan et al. 2005)}]
(*
  KEY INSIGHT:
  Instead of computing points-to for ALL variables (expensive),
  compute on-demand for specific queries.

  Query: "What does variable x point to at program point p?"

  Algorithm:
    1. Start backward search from (p, x)
    2. Follow assign edges backward: x = y means search for y
    3. Follow load/store with matched fields: x = y.f, z.f = w
    4. Stop at allocation sites: new T

  This is CFL-reachability with grammar:
    FlowsTo -> new
            | assign FlowsTo
            | load_f PointsTo store_f FlowsTo
*)

type pts_edge_label =
  | New : alloc_site:node_id -> pts_edge_label
  | Assign : pts_edge_label
  | Load : field:string -> pts_edge_label
  | Store : field:string -> pts_edge_label

(* Demand-driven points-to query *)
val points_to_query : cpg -> node_id -> string -> set node_id
let points_to_query cpg point var =
  (* Build PEG (Pointer Expression Graph) *)
  let peg = build_peg cpg in
  (* Backward CFL-reachability from (point, var) *)
  let rec search visited (node, var) =
    if Set.mem (node, var) visited then Set.empty
    else
      let visited' = Set.add (node, var) visited in
      (* Check incoming edges *)
      let edges = get_incoming_peg_edges peg (node, var) in
      Set.concat_map (fun edge ->
        match edge.label with
        | New alloc_site ->
            Set.singleton alloc_site  (* Found an allocation! *)
        | Assign ->
            search visited' edge.source
        | Load field ->
            (* Need to find matching store *)
            let base = get_base_var edge in
            let base_pts = search visited' (edge.source_node, base) in
            Set.concat_map (fun alloc ->
              (* Find stores to this field on this allocation *)
              let stores = find_stores peg alloc field in
              Set.concat_map (fun store_edge ->
                search visited' store_edge.source
              ) stores
            ) base_pts
        | Store _ ->
            Set.empty  (* Stores are targets, not sources *)
      ) edges
  in
  search Set.empty (point, var)
\end{fstarcode}


%--------------------------------------------------
\section{Interleaved Dyck and the Combined Context+Field Problem}
\label{sec:interleaved-dyck}
%--------------------------------------------------

\textbf{Sources}: Reps 2000, Conrado et al. 2025

\CFL{}-reachability (Section~\ref{sec:cfl-framework}) uses a \textbf{single Dyck language} for matched call/return. Field-sensitive alias analysis uses another Dyck language for matched load/store. \textbf{Combining both} requires \textbf{interleaved Dyck reachability}, which is \textbf{undecidable}.

\begin{pillarbox}[title={The Interleaved Dyck Problem}]
\textbf{Context sensitivity}: $D_\alpha$ = matched parentheses $(\quad)$

\textbf{Field sensitivity}: $D_\beta$ = matched brackets $[\quad]$

\textbf{Interleaved Language} $I_{\alpha,\beta}$: A string $s$ is in $I_{\alpha,\beta}$ iff:
\begin{itemize}
  \item $s$ restricted to $(\,)$ is in $D_\alpha$ (valid call/return)
  \item $s$ restricted to $[\,]$ is in $D_\beta$ (valid load/store)
\end{itemize}

\textbf{Example valid string}: $(_{1}\, [_f\, (_{2}\, ]_f\, )_{2}\, [_g\, ]_g\, )_{1}$

Projections: $(_{1} (_{2} )_{2} )_{1}$ = valid $D_\alpha$; $[_f ]_f [_g ]_g$ = valid $D_\beta$

\begin{theorem}[Reps 2000 --- Undecidability]
Interleaved Dyck reachability is UNDECIDABLE. Reduction from Post Correspondence Problem.
\end{theorem}

\textbf{Practical Consequence}: Cannot have BOTH context-sensitivity AND field-sensitivity with: (1) Soundness (no false negatives), (2) Completeness (no false positives), (3) Decidability (termination). Must sacrifice ONE property.
\end{pillarbox}

Two principled solutions exist: \textbf{MCFL underapproximation} (Section~\ref{sec:mcfl}) sacrifices completeness, while \textbf{SPDS overapproximation} (Section~\ref{sec:spds}) sacrifices soundness of the precision claim.


%--------------------------------------------------
\section{Multiple Context-Free Language Reachability (MCFL)}
\label{sec:mcfl}
%--------------------------------------------------

\textbf{Paper}: Conrado, Kjelstrom, Pavlogiannis, van de Pol 2025

MCFL provides a \textbf{decidable underapproximation} of interleaved Dyck via a hierarchy of increasingly expressive languages.

\begin{pillarbox}[title={MCFL Hierarchy}]
\begin{itemize}
  \item 1-MCFL = CFL (context-free languages, $\BigO{n^3}$ reachability)
  \item 2-MCFL strictly contains 1-MCFL ($\BigO{n^4}$ reachability)
  \item $d$-MCFL strictly contains $(d-1)$-MCFL ($\BigO{n^{2d}}$ reachability)
\end{itemize}

\textbf{Completeness in Limit}: For any string $s \in I_{\alpha,\beta}$, there exists $d$ such that $s$ is in $d$-MCFL. Union over all $d$ gives exactly $I_{\alpha,\beta}$.

\textbf{Practical Finding (Conrado 2025)}: 2-MCFL matches overapproximation in 8/11 benchmarks. 2-MCFL confirms 94.3\% of paths on remaining benchmarks. $d=2$ suffices for most real taint analysis problems.
\end{pillarbox}

\begin{fstarcode}[title={Multiple Context-Free Grammar (MCFG) Definition}]
module BrrrMachine.MCFG

(* A nonterminal with arity k derives k-tuples of strings *)
type mcfg_nonterminal = {
  name: string;
  arity: pos  (* Number of string components it derives, >= 1 *)
}

(* Right-hand side element: terminal or variable reference *)
type mcfg_rhs_elem =
  | MCFGTerminal : char -> mcfg_rhs_elem
  | MCFGVariable : nt_idx:nat -> component:nat -> mcfg_rhs_elem

(* Production rule in d-MCFG(r) *)
type mcfg_rule (d r: pos) = {
  lhs: mcfg_nonterminal;
  lhs_patterns: list (list mcfg_rhs_elem);  (* arity-many patterns *)
  rhs_nts: list mcfg_nonterminal;           (* up to r nonterminals *)

  (* Well-formedness invariants *)
  wf_arity: List.length lhs_patterns = lhs.arity;
  wf_rank: List.length rhs_nts <= r;
  wf_dimension: lhs.arity <= d
}

(* d-MCFG(r): dimension d, rank r *)
type mcfg (d r: pos) = {
  nonterminals: list mcfg_nonterminal;
  terminals: set char;
  rules: list (mcfg_rule d r);
  start: mcfg_nonterminal;
  start_arity_1: start.arity = 1
}

(* The G_d^+ grammar for approximating interleaved Dyck *)
val construct_interleaved_grammar : d:pos -> k:pos -> mcfg d 2

(* Soundness: derived strings are in interleaved Dyck *)
val grammar_sound : d:pos -> k:pos -> s:string ->
  Lemma (requires s `in_language` (construct_interleaved_grammar d k))
        (ensures is_interleaved_dyck s)

(* Completeness in limit: every interleaved Dyck string is captured *)
val grammar_complete_limit : k:pos -> s:string ->
  Lemma (requires is_interleaved_dyck s)
        (ensures exists d. s `in_language` (construct_interleaved_grammar d k))
\end{fstarcode}

\textbf{Complexity Bounds (Conrado 2025)}:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Language Class} & \textbf{Reachability Complexity} & \textbf{Lower Bound} \\
\hline
CFL / 1-MCFL & $\BigO{n^3}$ & BMM ($n^3$) \\
$d$-MCFL(1) & $\BigO{n^{2d}}$ & SETH \\
$d$-MCFL($r$) & $\BigO{n^{d(r+1)}}$ & SETH \\
Interleaved & Undecidable & PCP \\
\hline
\end{tabular}
\end{center}

The SETH lower bound shows $\BigO{n^{2d}}$ is essentially optimal for $d$-MCFL(1).


%--------------------------------------------------
\section{Synchronized Pushdown Systems (SPDS)}
\label{sec:spds}
%--------------------------------------------------

\textbf{Paper}: Spath, Ali, Bodden 2019

SPDS provides an \textbf{automaton-based approach} to combined context+field sensitivity via synchronized pushdown systems, achieving polynomial complexity under a \textbf{precision hypothesis}.

\begin{pillarbox}[title={Synchronized Pushdown Systems (SPDS)}]
\textbf{Idea}: Use TWO pushdown automata that operate synchronously:
\begin{itemize}
  \item $P_F$: Field PDS (stack encodes field access sequence)
  \item $P_S$: Stack PDS (stack encodes calling context)
  \item Sync: Synchronization predicate on transitions
\end{itemize}

\textbf{Encoding}:
\begin{itemize}
  \item Field store \texttt{x.f = y}: push rule $(l, \varepsilon) \longrightarrow (l', f)$
  \item Field load \texttt{y = x.f}: pop rule $(l, f) \longrightarrow (l', \varepsilon)$
  \item Method call: push rule $(l, \varepsilon) \longrightarrow (l', c)$
  \item Method return: pop rule $(l, c) \longrightarrow (l', \varepsilon)$
\end{itemize}

\textbf{Key Innovation}: Field automaton $A_F$ represents set of valid access paths IMPLICITLY. Avoids exponential enumeration of access paths. $\BigO{N}$ automaton states vs $\BigO{2^N}$ explicit access paths.

\textbf{Precision Hypothesis (Spath 2019)}: An improperly matched call site does not induce a properly matched field access, and vice versa. Under this hypothesis, SPDS = infinite access path precision.
\end{pillarbox}

\begin{fstarcode}[title={Synchronized Pushdown Systems (Spath et al. 2019)}]
module BrrrMachine.SPDS

(* Stack symbols *)
type field_symbol = | FieldSym : string -> field_symbol | FEps
type call_symbol = | CallSiteSym : nat -> call_symbol | CEps

(* Pushdown configuration *)
type field_config = { f_state: nat; f_stack: list field_symbol }
type call_config = { c_state: nat; c_stack: list call_symbol }
type spds_config = field_config * call_config

(* Pushdown rules *)
type field_rule =
  | FPush : nat -> field_symbol -> nat -> field_symbol -> field_rule
  | FPop : nat -> field_symbol -> nat -> field_rule

type call_rule =
  | CPush : nat -> call_symbol -> nat -> call_symbol -> call_rule
  | CPop : nat -> call_symbol -> nat -> call_rule

(* Synchronization predicate *)
type sync_spec = field_rule -> call_rule -> bool

(* The full SPDS *)
type spds = {
  field_rules: set field_rule;
  call_rules: set call_rule;
  sync: sync_spec;
  initial: spds_config
}

(* Field automaton: implicitly represents set of valid access paths *)
type field_automaton = {
  fa_states: set nat;
  fa_initial: nat;
  fa_final: set nat;
  fa_transitions: map (nat * field_symbol) (set nat)
}

(* Post* computation via saturation *)
val post_star : set field_rule -> field_config -> field_automaton

(* SPDS synchronized reachability *)
val spds_post_star : spds -> set spds_config
let spds_post_star sys =
  let fa = post_star sys.field_rules (fst sys.initial) in
  let ca = post_star sys.call_rules (snd sys.initial) in
  (* Intersect and filter by synchronization *)
  filter_by_sync sys.sync (cartesian fa ca)

(* Complexity: O(|P_F|^3 * |P_S|^3 * |Q|^3) *)
val spds_complexity :
  sys:spds ->
  Lemma (time_complexity (spds_post_star sys) <=
         pow3 (size sys.field_rules) * pow3 (size sys.call_rules))
\end{fstarcode}

\textbf{Performance Results (Spath 2019, DaCapo Benchmark)}:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Analysis} & \textbf{SPDS Speedup} & \textbf{Timeouts (Before/After)} \\
\hline
IO Property & 64x & 160 $\rightarrow$ 28 \\
Iterator Property & 83x & 137 $\rightarrow$ 3 \\
Vector Property & 1.8x & 57 $\rightarrow$ 25 \\
\hline
\end{tabular}
\end{center}

SPDS matches access-graph precision on all benchmarks with no false positives.

\begin{pillarbox}[title={Tension Resolution: CFL vs MCFL vs SPDS}]
\textbf{MCFL (Conrado 2025)}:
\begin{itemize}
  \item Grammar-based formalism
  \item Underapproximation (sound)
  \item First decidable interleaved Dyck
  \item Tight complexity bounds (SETH)
  \item Optimal for verification
\end{itemize}

\textbf{SPDS (Spath 2019)}:
\begin{itemize}
  \item Automaton-based formalism
  \item Overapproximation under hypothesis
  \item Practical WPDS encoding
  \item Proven speedups (64--83x)
  \item Optimal for bug finding
\end{itemize}

\textbf{Combined Use}:
\begin{itemize}
  \item Run MCFL underapprox: paths found are DEFINITELY reachable
  \item Run SPDS overapprox: paths NOT found are DEFINITELY unreachable
  \item Intersection: sound AND complete when they agree
  \item Conrado 2025 shows agreement in 8/11 benchmarks
\end{itemize}

\textbf{Recommendation}:
\begin{itemize}
  \item For taint verification (need soundness): Use MCFL
  \item For bug finding (need precision): Use SPDS
  \item For maximum precision: Combine both
\end{itemize}
\end{pillarbox}


%--------------------------------------------------
\section{Cross-Reference: Sparse Value-Flow Analysis (SVF)}
\label{sec:cfl-svf-crossref}
%--------------------------------------------------

For \textbf{source-sink reachability} problems (memory leaks, use-after-free, taint-to-sink), an alternative to \IFDS{}-based dataflow is \textbf{Sparse Value-Flow Analysis} (Section~5.6).

\begin{pillarbox}[title={SVF vs IFDS for Source-Sink Problems}]
Both use \CFL{}-reachability for context-sensitivity (Section~\ref{sec:cfl-framework}). The difference is in REPRESENTATION and PREREQUISITES:

\textbf{\IFDS{} (Section~\ref{sec:ifds})}:
\begin{itemize}
  \item Exploded supergraph: (program\_point, dataflow\_fact) pairs
  \item Self-contained: no pre-analysis required
  \item Best for: general dataflow, small finite domains
\end{itemize}

\textbf{SVF (Section~5.6)}:
\begin{itemize}
  \item Sparse VFG: nodes are variable definitions, edges are def-use chains
  \item REQUIRES pointer analysis for Memory SSA construction
  \item Best for: source-sink problems, address-taken variable tracking
\end{itemize}

For leak detection in C/C++, SVF typically achieves 10--50x speedup over dense \IFDS{} approaches (ISSTA 2012) due to sparse traversal.
\end{pillarbox}

\textbf{See Section~5.6} for complete SVF formalization including Memory SSA, SVFG construction rules, leak detection algorithms, and comparison with \IFDS{}.


%==================================================
\chapter{Under-Approximate Analysis (Bug Finding)}
\label{sec:under-approx}
%==================================================

\textbf{Sources}: Le et al. 2022 (ISL), Vanegue 2025 (Pulse-infinity), O'Hearn 2020

\begin{pillarbox}[title={Critical: IFDS is Over-Approximate. For Bug Finding, We Need Under-Approx.}]
\textbf{Over-Approximation (\IFDS{}, Section~\ref{sec:ifds})}:
\begin{itemize}
  \item Sound for ABSENCE: ``No taint found'' means truly safe
  \item May have FALSE POSITIVES: Reports bugs that don't exist
\end{itemize}

\textbf{Under-Approximation (This section)}:
\begin{itemize}
  \item Sound for PRESENCE: ``Bug found'' means truly a bug
  \item May have FALSE NEGATIVES: Misses some bugs
\end{itemize}

\textbf{Combined}: Use \IFDS{} to find candidates, then under-approx to verify.
\end{pillarbox}


%--------------------------------------------------
\section{The Eval Algorithm (Pulse/Infer)}
\label{sec:eval-algorithm}
%--------------------------------------------------

\textbf{Paper}: Calcagno et al. 2009, 2011 (Bi-Abduction, Infer)

The Eval algorithm performs forward symbolic execution with bi-abduction to discover both bugs AND missing preconditions.

\begin{fstarcode}[title={Eval: Forward Symbolic Execution with Bi-Abduction}]
(* --------------------------------------------------
   Unlike IFDS which propagates dataflow facts, Eval propagates
   separation logic assertions through the program.
   -------------------------------------------------- *)

type eval_state = {
  pre : assertion;      (* Discovered precondition (accumulated anti-frame) *)
  post : assertion;     (* Current symbolic state *)
  path : list node_id;  (* Execution path for witness *)
  exit : exit_condition; (* Ok or Er *)
}

val eval_stmt : eval_state -> ir_stmt -> list eval_state
(* May return multiple states for conditionals/loops *)

let eval_stmt state stmt =
  match stmt with
  | Assign (x, e) ->
      (* x := e --- update symbolic state *)
      let new_post = substitute state.post x (eval_expr e state.post) in
      [{ state with post = new_post }]

  | Load (x, ptr, field) ->
      (* x := ptr->field --- need ptr |-> {field: v} *)
      let required = points_to ptr field (fresh_var "v") in
      match biabduct state.post required with
      | None ->
          (* NULL DEREFERENCE: can't satisfy requirement *)
          [{ state with exit = Er;
             post = state.post `star` error "null_deref" }]
      | Some { anti_frame = m; frame = f } ->
          (* Add anti-frame to precondition, update post *)
          let state' = { state with
            pre = state.pre `star` m;
            post = f `star` (x |-> fresh_var "v");
          } in
          [state']

  | Store (ptr, field, v) ->
      (* ptr->field := v *)
      let required = points_to ptr field (fresh_var "_") in
      match biabduct state.post required with
      | None ->
          [{ state with exit = Er;
             post = state.post `star` error "null_deref" }]
      | Some { anti_frame = m; frame = f } ->
          [{ state with
            pre = state.pre `star` m;
            post = f `star` points_to ptr field v;
          }]

  | Free ptr ->
      (* free(ptr) --- need ptr |-> _ with UNIQUE capability *)
      let required = points_to ptr "_" (fresh_var "_") in
      match biabduct state.post required with
      | None ->
          [{ state with exit = Er; post = error "double_free_or_invalid" }]
      | Some { anti_frame = m; frame = f } ->
          (* Remove the freed memory from post *)
          [{ state with pre = state.pre `star` m; post = f }]

  | If (cond, then_branch, else_branch) ->
      (* Fork execution for both branches *)
      let then_state = { state with post = state.post `star` (cond = true) } in
      let else_state = { state with post = state.post `star` (cond = false) } in
      (* Prune infeasible paths *)
      let then_results = if sat then_state.post
                         then eval_block then_state then_branch
                         else [] in
      let else_results = if sat else_state.post
                         then eval_block else_state else_branch
                         else [] in
      then_results @ else_results

  | While (cond, body) ->
      (* UNDER-APPROXIMATE: Bounded unrolling, NOT widening *)
      eval_loop_bounded state cond body 3  (* k=3 unrollings *)

  | Call (ret, func, args) ->
      (* Use function summary if available *)
      match get_summary func with
      | Some summary ->
          apply_summary state summary args ret
      | None ->
          (* Inline or skip with havoc *)
          [{ state with post = havoc ret state.post }]

(* --------------------------------------------------
   BOUNDED LOOP UNROLLING (UNDER-APPROXIMATE)
   For BUG FINDING: unroll k times, then cut off.
   This is SOUND for finding bugs: any bug found is real.
   May MISS bugs that require more iterations.
   -------------------------------------------------- *)

val eval_loop_bounded : eval_state -> ir_expr -> ir_block -> nat ->
                        list eval_state
let eval_loop_bounded init_state cond body max_unroll =
  let rec unroll fuel state =
    if fuel = 0 then
      (* Cut off: return current state as "exited loop" *)
      [{ state with post = state.post `star` (cond = false) }]
    else
      (* Check loop condition *)
      let continue_state = { state with
                             post = state.post `star` (cond = true) } in
      let exit_state = { state with
                         post = state.post `star` (cond = false) } in
      let continue_results =
        if sat continue_state.post then
          let body_results = eval_block continue_state body in
          List.concat_map (fun s -> unroll (fuel - 1) s) body_results
        else []
      in
      let exit_results =
        if sat exit_state.post then [exit_state] else []
      in
      continue_results @ exit_results
  in
  unroll max_unroll init_state
\end{fstarcode}


%--------------------------------------------------
\section{ISL Triple Semantics}
\label{sec:isl-triples}
%--------------------------------------------------

\textbf{Paper}: Le et al. 2022 --- ``Finding Real Bugs in Big Programs with ISL''

\begin{fstarcode}[title={Incorrectness Separation Logic (ISL) Triples}]
(* --------------------------------------------------
   ISL Triple: [p] C [q; exit]
   Meaning: If execution starts in state satisfying p,
            and terminates with exit condition,
            then final state satisfies q.

   Key difference from Hoare logic:
   - Hoare: {P} C {Q} --- ALL executions from P end in Q (over-approx)
   - ISL: [p] C [q] --- SOME execution from p ends in q (under-approx)
   -------------------------------------------------- *)

(* ISL triple computed by Eval *)
val eval_to_isl_triple : cpg -> func_id -> isl_triple
let eval_to_isl_triple cpg func =
  let init_state = {
    pre = emp;  (* Start with empty precondition *)
    post = emp; (* Start with empty postcondition *)
    path = [];
    exit = Ok;
  } in
  let final_states = eval_func cpg func init_state in
  (* Collect error states *)
  let error_states = List.filter (fun s -> s.exit = Er) final_states in
  match error_states with
  | [] ->
      (* No bugs found *)
      { presumption = emp; code = func; result = emp; exit_cond = Ok }
  | errors ->
      (* Combine error states *)
      let combined_pre = List.fold_left star emp
                           (List.map (fun s -> s.pre) errors) in
      let combined_post = List.fold_left disj bot
                            (List.map (fun s -> s.post) errors) in
      { presumption = combined_pre;
        code = func;
        result = combined_post;
        exit_cond = Er }
\end{fstarcode}


%--------------------------------------------------
\section{Latent vs Manifest Errors}
\label{sec:latent-manifest}
%--------------------------------------------------

\textbf{Paper}: Le, Raad, Villard, Berdine, Dreyer, O'Hearn 2022 ``Finding Real Bugs in Big Programs with ISL''

\begin{pillarbox}[title={Key Contribution: Compositional Bug Reporting via Manifest/Latent Split}]
\textbf{Problem}: ISL generates many error specs. Which to report?

\textbf{Example}: \texttt{deref(x) \{ *x = 10; \}}

ISL triple: $[\texttt{x = NULL}]\ \texttt{deref(x)}\ [\texttt{er : x = NULL}]$

Is this a bug? Only if called with NULL. But we don't know callers.

\textbf{Solution}: Distinguish MANIFEST from LATENT errors.
\begin{itemize}
  \item \textbf{Manifest}: Bug reachable from ANY calling context
  \item \textbf{Latent}: Bug only reachable under specific preconditions
\end{itemize}

\textbf{Policy}: Report manifest bugs unconditionally. Use latent specs compositionally to find manifest bugs in callers.
\end{pillarbox}

\begin{definition}[Manifest Error (Le 2022, Section 3.2)]
An error triple $\models [p]\ C\ [\texttt{er} : q]$ denotes a MANIFEST error if:
\begin{enumerate}
  \item The presumption is trivial: $p \equiv \texttt{emp} \land \texttt{true}$
  \item The result is satisfiable: $\texttt{sat}(q)$
  \item All heap locations in $q$ are existentially quantified (fresh)
  \item All pure constraints in $q$ are satisfiable under any valuation
\end{enumerate}

Formally, for $q = \exists X_q.\, \kappa_q \land \pi_q$:
\begin{enumerate}
  \item $p \equiv \texttt{emp} \land \texttt{true}$ --- No precondition requirements
  \item $\texttt{sat}(q)$ holds --- Error state is reachable
  \item $\texttt{locs}(\kappa_q) \subseteq X_q$ --- Heap is fresh (not from caller)
  \item $\forall v.\, \texttt{sat}(\pi_q[v / Y \cup \texttt{locs}(\kappa_q)])$ --- Pure part always satisfiable
\end{enumerate}
\end{definition}

\textbf{Intuition}: Manifest = error reachable regardless of how function is called. The heap portion is freshly allocated (condition 3), so caller can't prevent error by providing different inputs.

\begin{theorem}[True Positives Property (Le 2022, Theorem 3.4)]
If procedure \texttt{f()} in a complete program has a MANIFEST error, then either:
\begin{enumerate}[label=(\alph*)]
  \item \texttt{f()} is dead code (not reachable from \texttt{main()}), OR
  \item There exists a concrete trace from \texttt{main()} to the error.
\end{enumerate}

\textbf{Consequence}: Manifest errors have 0\% false positive rate. If we report it, it's a real bug (unless dead code).
\end{theorem}

\begin{fstarcode}[title={Manifest Error Detection (Algorithmic)}]
type error_classification =
  | Manifest : error_classification      (* Report unconditionally *)
  | Latent : precondition:assertion -> error_classification
      (* Report at call site *)
  | LatentLeak : error_classification
      (* Report anyway - leaks are caller's fault rarely *)

val classify_error : isl_triple -> error_classification
let classify_error triple =
  match triple.exit_cond with
  | Ok -> failwith "Not an error triple"
  | Er ->
      let p = triple.presumption in
      let q = triple.result in
      (* Condition 1: Trivial presumption *)
      let trivial_pre = is_emp_and_true p in
      (* Condition 2: Satisfiable result *)
      let sat_result = sat q in
      (* Condition 3: Fresh heap locations *)
      let fresh_heap = all_locs_existentially_quantified q in
      (* Condition 4: Pure constraints satisfiable *)
      let pure_sat = pure_always_satisfiable q in

      if trivial_pre && sat_result && fresh_heap && pure_sat then
        Manifest
      else if is_memory_leak_error triple then
        LatentLeak  (* Report leaks even when latent *)
      else
        Latent triple.presumption

(* BUG REPORTING POLICY (Le 2022 Section 2.3) *)
val should_report : func_id -> isl_triple -> bool
let should_report func triple =
  match classify_error triple with
  | Manifest -> true
  | LatentLeak -> true  (* Always report leaks *)
  | Latent _ ->
      (* Report latent NPE only in main() *)
      is_main_function func && is_null_deref_error triple
\end{fstarcode}

\textbf{Practical Results (Pulse-X tool)}:

\textit{OpenSSL-1.0.1h} (2015, 2.83M bytes IR, 8,658 procedures):
\begin{itemize}
  \item Pulse-X: 26 bugs reported, 19 fixed (73\% fix rate)
  \item Infer: 80 bugs reported, 39 fixed (49\% fix rate)
  \item Pulse-X has HIGHER fix rate due to manifest filtering
\end{itemize}

\textit{OpenSSL-3.0.0} (2021):
\begin{itemize}
  \item Pulse-X found 15 NEW bugs, all confirmed and fixed by maintainers
  \item Including NPEs and memory leaks in core crypto code
\end{itemize}

\textbf{Comparison to Infer}:

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{INFER (Over-Approximate)} & \textbf{PULSE-X (Under-Approximate)} \\
\hline
Based on separation logic & Based on ISL \\
Proves ABSENCE of bugs & Proves PRESENCE of bugs \\
Uses heuristics for reporting & Uses manifest criterion \\
May have false positives & 0\% FP for manifest bugs \\
Wider coverage (all paths) & Targeted (bounded paths) \\
Needs widening for loops & Simple bounded unrolling \\
\hline
\end{tabular}
\end{center}

\textbf{Algorithmic Simplification}: Le 2022 Remark 1: The ISL-based algorithm is ``strikingly simple'' compared to over-approximate biabduction. Key reason: NO LOOP INVARIANTS NEEDED. Under-approximation uses bounded unrolling---sound for finding bugs that trigger within $k$ iterations.


%--------------------------------------------------
\section{Integration: IFDS + Eval Hybrid}
\label{sec:ifds-eval-hybrid}
%--------------------------------------------------

\begin{center}
\textbf{Hybrid Analysis Architecture}
\end{center}

\textbf{Phase 1: \IFDS{} (Fast, Over-Approximate)}

Run \IFDS{} with complexity $\BigO{ED^3}$ to produce candidate bugs (may include false positives), e.g., ``taint reaches sink at line 42''.

\textbf{Phase 2: Eval (Precise, Under-Approximate)}

Run Eval on backward slice to produce ISL triples with paths, e.g., $[\texttt{emp}]\ \text{slice}\ [\texttt{err; Er}]$.

\textbf{Phase 3: Classification}

For each \IFDS{} finding:
\begin{enumerate}
  \item Compute backward slice from finding
  \item Run Eval on slice
  \item Check manifest conditions (Le 2022 Definition 3.3)
  \item Classify as Manifest/Latent/Refuted
\end{enumerate}

Results:
\begin{itemize}
  \item \textbf{Manifest} $\rightarrow$ TRUE BUG (0\% FP by theorem)
  \item \textbf{Latent} $\rightarrow$ TRUE BUG with required context
  \item \textbf{Refuted} $\rightarrow$ FALSE POSITIVE (don't report)
\end{itemize}

\begin{fstarcode}[title={Hybrid Analysis Implementation}]
type hybrid_result =
  | Confirmed : classification:bug_classification ->
                witness:list node_id -> hybrid_result
  | Refuted : reason:string -> hybrid_result
  | Timeout : partial:option eval_state -> hybrid_result

val verify_ifds_finding_with_eval :
  cpg:cpg -> finding:dataflow_fact -> timeout_ms:nat -> hybrid_result

let verify_ifds_finding_with_eval cpg finding timeout =
  (* Step 1: Compute backward slice *)
  let slice = backward_slice cpg finding.sink_node in
  (* Step 2: Run Eval on slice with timeout *)
  match eval_with_timeout slice timeout with
  | Timeout partial -> Timeout partial
  | Complete final_states ->
      (* Step 3: Check if any state confirms the finding *)
      let confirming = List.filter (confirms_finding finding) final_states in
      match confirming with
      | [] -> Refuted "Eval found no path to error"
      | states ->
          (* Step 4: Build ISL triple and classify *)
          let triple = states_to_isl_triple states in
          let classification = classify_bug triple in
          Confirmed classification (extract_witness states)
\end{fstarcode}


% ==================================================
% END OF PART 4A - CONTINUES IN PART 4B
% Part 4B will contain:
%   - Section 4.4: Symbolic Execution (Path-Sensitive)
%     Including: King 1976, DART/CUTE, KLEE, QSYM, Soundness Spectrum
% ==================================================
