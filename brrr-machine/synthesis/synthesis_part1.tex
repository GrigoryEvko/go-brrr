%==================================================
% PART I: PREAMBLE - Fragment for inclusion in synthesis_combined.tex
%==================================================
\part{Preamble: Theoretical Foundations and Architectural Principles}

\chapter{Introduction and Motivation}

The brrr-machine represents a novel approach to multi-language program analysis,
founded on the principle that programming languages constitute \emph{parameterized
instantiations} of a universal computational model rather than fundamentally
distinct formal systems.

\begin{principle}[Language Parameterization]
Programming languages differ in their configuration parameters rather than their
essential semantics. Each language is characterized by specific choices across
orthogonal semantic dimensions: memory management, type discipline, null handling,
effect tracking, and concurrency model.
\end{principle}

This theoretical perspective---wherein languages differ in their configuration
parameters rather than their essential semantics---yields several significant
capabilities that distinguish the brrr-machine from conventional static analyzers.

\section{Principal Contributions}

\begin{contributionbox}[title={\textbf{Contribution 1: Language-Agnostic Analysis Infrastructure}}]
A unified analysis framework applicable across heterogeneous language ecosystems
(\LangPython, \LangRust, \LangGo, \LangC/\LangCpp, \LangJS, \LangTS, \LangJava).
Analyses are specified once at the IR level and instantiated for each source
language through systematic lowering transformations.
\end{contributionbox}

\begin{contributionbox}[title={\textbf{Contribution 2: Cross-Language Boundary Analysis}}]
Precise identification of safety invariant violations at language boundaries
(FFI, IPC, RPC, serialization interfaces). The framework formally characterizes
which properties are preserved, weakened, or invalidated when control or data
crosses linguistic boundaries.
\end{contributionbox}

\begin{contributionbox}[title={\textbf{Contribution 3: Compositional Semantic Reasoning}}]
Analysis results compose across function, module, and language boundaries via
the \IFDS/\IDE{} algorithmic framework and algebraic effect composition.
Summaries computed for callees are reusable across call sites without
re-analysis, enabling scalability to large codebases.
\end{contributionbox}

\begin{contributionbox}[title={\textbf{Contribution 4: Formal Soundness Guarantees}}]
All analyses derive soundness from established theoretical foundations:
abstract interpretation (Cousot \& Cousot), separation logic (Reynolds),
algebraic effects (Plotkin \& Power), and substructural type theory (Girard).
Proofs are mechanized in F* where feasible.
\end{contributionbox}

\begin{contributionbox}[title={\textbf{Contribution 5: Algorithmic Efficiency}}]
Despite theoretical rigor, the implementation leverages asymptotically efficient
algorithms: $\BigO(ED^3)$ interprocedural dataflow via \IFDS{}, $\BigO(n \cdot \alpha(n))$
pointer analysis via Steensgaard unification, demand-driven evaluation for
interactive workloads, and incremental re-analysis via dirty-marking with
DRedL lattice updates.
\end{contributionbox}

%--------------------------------------------------
\chapter{The Core Thesis: Languages as Parameterized Type Theories}

The central theoretical claim of this work is that programming languages
constitute \emph{parameterized instantiations} of a universal computational
substrate rather than fundamentally distinct formal systems.

\begin{definition}[Universal Computational Substrate]
The brrr-machine IR constitutes a \emph{universal computational substrate} from
which each programming language derives as a specific \emph{parameter configuration}.
Formally, a language $L$ is defined by a configuration tuple:
\[
  L = \langle \mathcal{M}, \mathcal{T}, \mathcal{N}, \mathcal{E}, \mathcal{C} \rangle
\]
where $\mathcal{M}$ is the memory model, $\mathcal{T}$ is the type discipline,
$\mathcal{N}$ is the null handling strategy, $\mathcal{E}$ is the effect tracking
mode, and $\mathcal{C}$ is the concurrency model.
\end{definition}

\section{Language Parameter Configuration Matrix}

Each language is characterized by a specific configuration across several
orthogonal semantic dimensions:

\begin{table}[h]
\centering
\small
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{}llllll@{}}
\toprule
\textbf{Language} & \textbf{Memory} & \textbf{Types} & \textbf{Null} & \textbf{Effects} & \textbf{Concurrency} \\
\midrule
\LangC        & Manual   & Static  & Nullable & Untracked & POSIX Threads \\
\LangCpp      & Manual$^*$ & Static & Nullable & Untracked & POSIX Threads \\
\LangRust     & Affine   & Static  & Option   & Untracked & Threads+Async \\
\LangGo       & GC       & Static  & Nullable & Untracked & CSP Channels \\
\LangJava     & GC       & Static  & Nullable & Checked   & JMM Threads \\
\LangPython   & GC       & Dynamic & Nullable & Unchecked & GIL-Protected \\
\LangJS       & GC       & Dynamic & Nullable & Untracked & Event Loop \\
\LangTS       & GC       & Gradual & Nullable$^\dagger$ & Untracked & Event Loop \\
\LangSwift    & ARC      & Static  & Optional & Checked   & Actors+Async \\
\LangKotlin   & GC       & Static  & Optional & Checked   & Coroutines \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Language parameter configuration matrix. $^*$\LangCpp{} supports RAII.
$^\dagger$\LangTS{} nullable depends on \texttt{strictNullChecks} flag.}
\label{tab:lang-params}
\end{table}

This formulation enables uniform treatment of cross-language interactions through
explicit parameter reconciliation at linguistic boundaries.

\begin{definition}[Boundary Reconciliation]
When control or data crosses from language $L_1$ to language $L_2$, the boundary
reconciliation function $\mathcal{R}$ computes:
\[
  \mathcal{R}(L_1, L_2) = \{ p \in \mathsf{Properties} \mid
    \text{$p$ preserved across } L_1 \to L_2 \}
\]
Properties not in this set require explicit guards or are flagged as potential risks.
\end{definition}

%--------------------------------------------------
\chapter{Foundational Literature and Theoretical Pillars}

This framework synthesizes contributions from 29 foundational papers in
programming language theory, organized into seven coherent theoretical pillars.

%--------------------------------------------------
\section{Pillar 1: Abstract Interpretation --- Semantic Foundation}

\begin{pillarbox}[title={\textbf{Foundational References}}]
\begin{itemize}
  \item \textbf{[Cousot77]} Cousot \& Cousot. ``Abstract Interpretation: A Unified
        Lattice Model for Static Analysis of Programs by Construction or
        Approximation of Fixpoints.'' POPL 1977.
  \item \textbf{[Cousot92]} Cousot \& Cousot. ``Comparing the Galois Connection
        and Widening/Narrowing Approaches to Abstract Interpretation.'' PLILP 1992.
\end{itemize}

\textbf{Criticality:} Essential --- Provides mathematical foundation for all analyses.
\end{pillarbox}

\subsection{Theoretical Contribution}

Static analysis is formalized as computing sound approximations of program semantics.
The abstraction-concretization relationship forms a Galois connection $\galois{\abstr}{\concr}$
between concrete and abstract semantic domains. Soundness derives from monotonicity
of abstract transfer functions with respect to the lattice ordering.

\begin{definition}[Galois Connection]
A \emph{Galois connection} between posets $(\Concrete, \leq_\Concrete)$ and
$(\Abstract, \leq_\Abstract)$ is a pair of monotone functions
$\galois{\abstr}{\concr}$ such that:
\[
  \forall c \in \Concrete, a \in \Abstract: \quad
  \abstr(c) \leq_\Abstract a \iff c \leq_\Concrete \concr(a)
\]
\end{definition}

\begin{theorem}[Soundness via Galois Connection]
If $\galois{\abstr}{\concr}$ forms a Galois connection and the abstract
transfer function $f^\sharp$ satisfies $\abstr \circ f \sqsubseteq f^\sharp \circ \abstr$,
then the abstract analysis is sound: any property verified abstractly holds concretely.
\end{theorem}

\begin{artifactbox}
\textbf{Implementation Artifacts:}
\begin{itemize}
  \item \textbf{Required:}
  \begin{itemize}
    \item Complete lattice typeclass with verified algebraic laws (F*)
    \item Galois connection interface with mechanized soundness proof
    \item Widening/narrowing operators for non-Noetherian domains
    \item Chaotic iteration with Bourdoncle weak topological ordering
  \end{itemize}
  \item \textbf{Deferred:}
  \begin{itemize}
    \item Relational numeric domains (Octagon, Polyhedra) --- Phase 2
    \item Automatic widening point inference --- use loop header heuristic
  \end{itemize}
\end{itemize}
\end{artifactbox}

%--------------------------------------------------
\section{Pillar 2: Program Representation --- Unified Graph Structure}

\begin{pillarbox}[title={\textbf{Foundational References}}]
\begin{itemize}
  \item \textbf{[Yamaguchi14]} Yamaguchi et al. ``Modeling and Discovering
        Vulnerabilities with Code Property Graphs.'' IEEE S\&P 2014.
  \item \textbf{[Ferrante87]} Ferrante, Ottenstein \& Warren. ``The Program
        Dependence Graph and Its Use in Optimization.'' TOPLAS 1987.
  \item \textbf{[Horwitz90]} Horwitz, Reps \& Binkley. ``Interprocedural Slicing
        Using Dependence Graphs.'' TOPLAS 1990.
  \item \textbf{[Weiser84]} Weiser. ``Program Slicing.'' IEEE TSE 1984.
\end{itemize}

\textbf{Criticality:} Essential (Yamaguchi), High (others).
\end{pillarbox}

\subsection{Theoretical Contribution}

The Code Property Graph (\CPG) unifies Abstract Syntax Tree (\AST), Control Flow
Graph (\CFG), and Program Dependence Graph (\PDG) into a single queryable structure.
This representation reduces all program analyses to graph reachability and traversal
problems. The \PDG{} captures both data dependencies (def-use chains) and control
dependencies, enabling precise backward and forward slicing.

\begin{definition}[Code Property Graph]
A \emph{Code Property Graph} $\CPG = (V, E, \lambda, \mu)$ consists of:
\begin{itemize}
  \item $V$: set of nodes representing program elements (statements, expressions, declarations)
  \item $E \subseteq V \times V \times \mathsf{EdgeType}$: labeled edges where
        $\mathsf{EdgeType} = \{\AST, \CFG, \PDG_{\mathsf{data}}, \PDG_{\mathsf{ctrl}}, \mathsf{Call}, \mathsf{Effect}\}$
  \item $\lambda : V \to \mathsf{NodeLabel}$: node labeling function
  \item $\mu : E \to \mathsf{EdgeLabel}$: edge labeling function
\end{itemize}
\end{definition}

\begin{theorem}[CPG Completeness]
The \CPG{} representation is complete for interprocedural analysis: any
path-sensitive dataflow fact computable on the original program is computable
via graph reachability queries on the \CPG.
\end{theorem}

\begin{artifactbox}
\textbf{Implementation Artifacts:}
\begin{itemize}
  \item \textbf{Required:}
  \begin{itemize}
    \item Unified \CPG{} type with \AST, \CFG, \PDG, and effect edge categories
    \item Efficient traversal primitives (successors, predecessors, transitive closure, filtered reachability)
    \item System Dependence Graph (\SDG) extension for interprocedural analysis
    \item Effect edges (novel extension to Yamaguchi formulation)
  \end{itemize}
  \item \textbf{Deferred:}
  \begin{itemize}
    \item External graph database backend --- in-memory representation suffices
    \item Domain-specific query language --- typed Rust/F* traversals preferred
  \end{itemize}
\end{itemize}
\end{artifactbox}

%--------------------------------------------------
\section{Pillar 3: Interprocedural Dataflow --- Algorithmic Framework}

\begin{pillarbox}[title={\textbf{Foundational References}}]
\begin{itemize}
  \item \textbf{[Reps95]} Reps, Horwitz \& Sagiv. ``Precise Interprocedural
        Dataflow Analysis via Graph Reachability.'' POPL 1995.
  \item \textbf{[Reps97]} Reps. ``Program Analysis via Graph Reachability.''
        Information and Software Technology, 1997.
  \item \textbf{[Sridharan05]} Sridharan \& Bod\'{\i}k. ``Demand-Driven Points-to
        Analysis for Java.'' OOPSLA 2005.
  \item \textbf{[Smaragdakis11]} Bravenboer \& Smaragdakis. ``Strictly Declarative
        Specification of Sophisticated Points-to Analyses.'' OOPSLA 2009.
  \item \textbf{[Jordan16]} Jordan et al. ``Souffl\'{e}: On Synthesis of Program
        Analyzers.'' CAV 2016.
  \item \textbf{[Madsen16]} Madsen et al. ``From Datalog to Flix: A Declarative
        Language for Fixed Points on Lattices.'' PLDI 2016.
\end{itemize}

\textbf{Criticality:} High --- Core algorithmic infrastructure.
\end{pillarbox}

\subsection{Theoretical Contribution}

Interprocedural dataflow analysis reduces to graph reachability over an exploded
supergraph. The \IFDS{} algorithm achieves $\BigO(ED^3)$ complexity for distributive
dataflow problems. Context-free language (\CFL) reachability provides context
sensitivity through matched call-return parentheses. Demand-driven formulations
compute only query-relevant facts.

\begin{definition}[\IFDS{} Problem]
An \IFDS{} problem is a tuple $(G^*, D, M)$ where:
\begin{itemize}
  \item $G^* = (N^*, E^*)$ is the supergraph (interprocedural \CFG)
  \item $D$ is a finite set of dataflow facts
  \item $M : E^* \to 2^{D \times D}$ assigns a distributive transfer function
        to each edge, represented as a relation on $D$
\end{itemize}
\end{definition}

\begin{theorem}[\IFDS{} Complexity]
The \IFDS{} tabulation algorithm solves interprocedural distributive dataflow
problems in time $\BigO(E \cdot D^3)$ where $E = |E^*|$ and $D = |D|$.
\end{theorem}

\begin{designnote}
The \IDE{} (Interprocedural Distributive Environment) extension is expressible
as lattice-extended Datalog (Flix formulation) where the lattice element represents
the micro-function space. \IDE{} is appropriate for analyses requiring environment
transformers (constant propagation, linear constant analysis). \IFDS{} suffices
for binary fact problems (taint tracking, nullability analysis).
\end{designnote}

\begin{artifactbox}
\textbf{Implementation Artifacts:}
\begin{itemize}
  \item \textbf{Required:}
  \begin{itemize}
    \item \IFDS{} tabulation algorithm with interprocedural summary edge caching
    \item \CFL-reachability solver for context-sensitive analysis
    \item Demand-driven query interface for interactive workloads
    \item Optional Datalog compilation backend (Souffl\'{e} target)
  \end{itemize}
  \item \textbf{Deferred:}
  \begin{itemize}
    \item Custom Datalog interpreter --- compilation to native code preferred
  \end{itemize}
\end{itemize}
\end{artifactbox}

%--------------------------------------------------
\section{Pillar 4: Pointer and Alias Analysis --- Precision Infrastructure}

\begin{pillarbox}[title={\textbf{Foundational References}}]
\begin{itemize}
  \item \textbf{[Andersen94]} Andersen. ``Program Analysis and Specialization
        for the C Programming Language.'' PhD Thesis, DIKU, 1994.
  \item \textbf{[Steensgaard96]} Steensgaard. ``Points-to Analysis in Almost
        Linear Time.'' POPL 1996.
  \item \textbf{[Calcagno09]} Calcagno et al. ``Compositional Shape Analysis
        by Means of Bi-Abduction.'' POPL 2009.
\end{itemize}

\textbf{Criticality:} High --- Precision foundation for all client analyses.
\end{pillarbox}

\subsection{Theoretical Contribution}

Pointer analysis computes may-alias and points-to relations. Andersen's
inclusion-based formulation achieves cubic complexity $\BigO(n^3)$ with higher
precision. Steensgaard's unification-based approach achieves near-linear
complexity $\BigO(n \cdot \alpha(n))$ with reduced precision. Bi-abduction enables
compositional heap analysis by simultaneously inferring preconditions
(anti-frame) and postcondition frames.

\begin{definition}[Points-to Analysis]
A \emph{points-to analysis} computes a function $\mathsf{pts} : \mathsf{Var} \to 2^{\mathsf{Loc}}$
such that for all program executions, if variable $v$ holds location $\ell$,
then $\ell \in \mathsf{pts}(v)$.
\end{definition}

\begin{theorem}[Pointer Analysis Complexity Trade-off]
\begin{itemize}
  \item \textbf{Andersen (inclusion-based):} $\BigO(n^3)$ complexity, higher precision
  \item \textbf{Steensgaard (unification-based):} $\BigO(n \cdot \alpha(n))$ complexity, lower precision
\end{itemize}
where $\alpha$ is the inverse Ackermann function.
\end{theorem}

\begin{artifactbox}
\textbf{Implementation Artifacts:}
\begin{itemize}
  \item \textbf{Required:}
  \begin{itemize}
    \item Steensgaard unification for initial fast points-to computation
    \item Andersen inclusion solver for precision-critical analysis paths
    \item Bi-abduction engine for compositional memory reasoning
  \end{itemize}
  \item \textbf{Deferred:}
  \begin{itemize}
    \item Full shape analysis --- prioritize points-to infrastructure first
    \item BDD-based set representations --- complexity not yet justified
  \end{itemize}
\end{itemize}
\end{artifactbox}

%--------------------------------------------------
\section{Pillar 5: Effect Systems --- Computational Behavior Tracking}

\begin{pillarbox}[title={\textbf{Foundational References}}]
\begin{itemize}
  \item \textbf{[Moggi91]} Moggi. ``Notions of Computation and Monads.''
        Information and Computation, 1991.
  \item \textbf{[Plotkin03]} Plotkin \& Power. ``Algebraic Operations and
        Generic Effects.'' Applied Categorical Structures, 2003.
  \item \textbf{[Plotkin09]} Plotkin \& Pretnar. ``Handlers of Algebraic Effects.''
        ESOP 2009.
  \item \textbf{[Leijen17]} Leijen. ``Type Directed Compilation of Row-Typed
        Algebraic Effects.'' POPL 2017.
\end{itemize}

\textbf{Criticality:} High --- Semantic framework for behavior characterization.
\end{pillarbox}

\subsection{Theoretical Contribution}

Effect systems distinguish pure values from effectful computations. Moggi's
monadic semantics provides compositional treatment of effects. Plotkin's
algebraic effects formulation enables modular effect handlers. Leijen's
row-polymorphic effect types support effect inference and effect polymorphism
without explicit effect annotations.

\begin{definition}[Effect Row]
An \emph{effect row} is either:
\begin{itemize}
  \item $\langle \rangle$: the empty row (pure computation)
  \item $\langle e \mid \rho \rangle$: row extension with effect $e$ and tail $\rho$
  \item $\rho$: a row variable (for polymorphism)
\end{itemize}
Effect rows form a lattice under the subsumption ordering $\effsub$.
\end{definition}

\begin{theorem}[Effect Composition]
Effects compose via row join: if $f : A \xrightarrow{\varepsilon_1} B$ and
$g : B \xrightarrow{\varepsilon_2} C$, then $g \circ f : A \xrightarrow{\varepsilon_1 \effjoin \varepsilon_2} C$.
\end{theorem}

\begin{artifactbox}
\textbf{Implementation Artifacts:}
\begin{itemize}
  \item \textbf{Required:}
  \begin{itemize}
    \item Effect taxonomy aligned with brrr-machine semantic model
    \item Row-polymorphic effect type representation for function signatures
    \item Effect composition lattice with subsumption ordering
    \item Effect violation detection (e.g., use-after-free as resource effect violation, null dereference as totality effect violation)
  \end{itemize}
  \item \textbf{Deferred:}
  \begin{itemize}
    \item Full algebraic effect handlers --- analysis focus, not execution
    \item Complete effect inference for arbitrary source languages
  \end{itemize}
\end{itemize}
\end{artifactbox}

%--------------------------------------------------
\section{Pillar 6: Substructural Types and Ownership --- Resource Invariants}

\begin{pillarbox}[title={\textbf{Foundational References}}]
\begin{itemize}
  \item \textbf{[Girard87]} Girard. ``Linear Logic.'' Theoretical Computer Science, 1987.
  \item \textbf{[Reynolds02]} Reynolds. ``Separation Logic: A Logic for Shared
        Mutable Data Structures.'' LICS 2002.
  \item \textbf{[Jung18]} Jung et al. ``RustBelt: Securing the Foundations of
        the Rust Programming Language.'' POPL 2018.
  \item \textbf{[Aiken99]} Aiken. ``Introduction to Set Constraint-Based Program
        Analysis.'' Science of Computer Programming, 1999.
\end{itemize}

\textbf{Criticality:} High --- Resource safety and ownership verification.
\end{pillarbox}

\subsection{Theoretical Contribution}

Substructural type systems provide resource-sensitive reasoning:
\begin{itemize}
  \item \textbf{LINEAR:} resources used exactly once (no implicit weakening)
  \item \textbf{AFFINE:} resources used at most once (Rust's ownership model)
\end{itemize}
Separation logic enables local reasoning about mutable heap state through the
frame rule. Iris provides step-indexed partial commutative monoids (cameras) for
ownership verification. Set constraints unify type inference with dataflow analysis.

\begin{remark}[Clarification]
Rust implements \emph{affine} typing (implicit drop permitted), not strictly linear typing.
\end{remark}

\begin{definition}[Substructural Type System]
A type system is \emph{substructural} if it restricts the structural rules:
\begin{itemize}
  \item \textbf{Linear:} No weakening (must use) and no contraction (use exactly once)
  \item \textbf{Affine:} No contraction only (use at most once, implicit drop permitted)
  \item \textbf{Relevant:} No weakening only (must use, can duplicate)
\end{itemize}
\end{definition}

\begin{theorem}[Frame Rule]
In separation logic, if $\{P\}\, c\, \{Q\}$ is valid and $c$ does not modify
variables free in $R$, then $\{P * R\}\, c\, \{Q * R\}$ is valid.
\end{theorem}

\begin{artifactbox}
\textbf{Implementation Artifacts:}
\begin{itemize}
  \item \textbf{Required:}
  \begin{itemize}
    \item Resource algebra (camera) infrastructure for ownership tracking
    \item Separation logic assertion language for heap specifications
    \item Per-location ownership state machine (Acquired $\to$ InUse $\to$ Released)
    \item Set constraint solver for unified type/flow analysis
  \end{itemize}
  \item \textbf{Deferred:}
  \begin{itemize}
    \item Complete Iris mechanization in F* --- adapt essential constructs only
    \item Step-indexed semantics --- not required for static analysis application
  \end{itemize}
\end{itemize}
\end{artifactbox}

%--------------------------------------------------
\section{Pillar 7: Security Analysis --- Information Flow and Vulnerability Detection}

\begin{pillarbox}[title={\textbf{Foundational References}}]
\begin{itemize}
  \item \textbf{[Denning77]} Denning \& Denning. ``Certification of Programs
        for Secure Information Flow.'' Communications of the ACM, 1977.
  \item \textbf{[Livshits05]} Livshits \& Lam. ``Finding Security Vulnerabilities
        in Java Applications with Static Analysis.'' USENIX Security 2005.
  \item \textbf{[Tripp09]} Tripp et al. ``TAJ: Effective Taint Analysis of
        Web Applications.'' PLDI 2009.
\end{itemize}

\textbf{Criticality:} High --- Security property verification.
\end{pillarbox}

\subsection{Theoretical Contribution}

Information flow security is characterized by lattice-theoretic ordering of
security classes; secure programs permit only upward information flow. Taint
analysis tracks propagation of untrusted data from sources (user input, network)
to security-sensitive sinks (SQL queries, system calls). TAJ demonstrates
industrial-scale taint analysis via hybrid thin slicing that focuses on
security-relevant data dependencies.

\begin{definition}[Information Flow Security]
A program satisfies \emph{information flow security} with respect to a security
lattice $(\mathcal{S}, \leq)$ if for all variables $x, y$:
\[
  \text{information flows from $x$ to $y$} \implies \mathsf{level}(x) \leq \mathsf{level}(y)
\]
\end{definition}

\begin{definition}[Taint Analysis]
\emph{Taint analysis} tracks propagation of untrusted data from \emph{sources}
(user input, network) to security-sensitive \emph{sinks} (SQL queries, system calls).
A taint violation occurs when tainted data reaches a sink without passing through
a \emph{sanitizer}.
\end{definition}

\begin{artifactbox}
\textbf{Implementation Artifacts:}
\begin{itemize}
  \item \textbf{Required:}
  \begin{itemize}
    \item Taint lattice with source/sink/sanitizer semantic model
    \item Context-sensitive taint propagation via \IFDS{} tabulation
    \item Thin slicing for security-relevant dependency extraction
    \item SARIF output format for IDE and CI/CD integration
  \end{itemize}
  \item \textbf{Deferred:}
  \begin{itemize}
    \item Implicit flow tracking --- explicit dataflow only in initial version
    \item Full declassification policy language --- simple sanitizer model first
  \end{itemize}
\end{itemize}
\end{artifactbox}

%--------------------------------------------------
\section{Cross-Cutting Concerns: Multi-Language Interoperability and Incrementality}

\begin{pillarbox}[title={\textbf{Foundational References}}]
\begin{itemize}
  \item \textbf{[Matthews07]} Matthews \& Findler. ``Operational Semantics for
        Multi-Language Programs.'' TOPLAS 2009.
  \item \textbf{[Goguen92]} Goguen \& Burstall. ``Institutions: Abstract Model
        Theory for Specification and Programming.'' JACM, 1992.
  \item \textbf{[Hammer14]} Hammer et al. ``Adapton: Composable, Demand-Driven
        Incremental Computation.'' PLDI 2014.
  \item \textbf{[Wagner98]} Wagner \& Graham. ``Efficient and Flexible Incremental
        Parsing.'' TOPLAS, 1998.
  \item \textbf{[Szabo18]} Szab\'{o} et al. ``Incrementalizing Lattice-Based Program
        Analyses in Datalog.'' OOPSLA 2018.
  \item \textbf{[Distefano19]} Distefano et al. ``Scaling Static Analyses at Facebook.''
        Communications of the ACM, 2019.
\end{itemize}

\textbf{Criticality:} Essential (Szab\'{o}, Distefano), High (Matthews, Hammer).
\end{pillarbox}

\subsection{Theoretical Contribution}

Matthews-Findler provides operational semantics for multi-language programs with
explicit boundary terms mediating cross-language calls. Goguen's institutions
provide categorical abstraction over logical systems, enabling formal treatment
of language heterogeneity. Adapton introduces demand-driven incremental computation
with memoization. Szab\'{o}'s DRedL provides lattice-based incremental Datalog
evaluation achieving 65x--243x speedup. Distefano demonstrates industrial deployment
at 100M+ LOC scale with 70\% developer fix rates via diff-time analysis.

\begin{artifactbox}
\textbf{Implementation Artifacts:}
\begin{itemize}
  \item \textbf{Required:}
  \begin{itemize}
    \item Cross-language boundary analysis (Matthews-Findler formulation)
    \item Property preservation verification at linguistic boundaries
    \item Incremental analysis via dirty-marking with DRedL lattice propagation
    \item Tree-sitter integration for incremental syntax tree maintenance
    \item Diff-time deployment pipeline (Infer-style CI/CD integration)
    \item Interprocedural summary caching with dependency-based invalidation
  \end{itemize}
  \item \textbf{Deferred:}
  \begin{itemize}
    \item Full Adapton framework --- simpler dirty-marking with DRedL suffices
    \item Custom incremental parser --- tree-sitter provides adequate solution
  \end{itemize}
\end{itemize}
\end{artifactbox}

%--------------------------------------------------
\chapter{Document Organization and Conventions}

Each subsequent part of this specification adheres to a consistent organizational
structure to facilitate both implementation and verification.

\section{Section Template}

\begin{sectiontemplatebox}[title={\textbf{Section Template}}]
\begin{enumerate}
  \item \textbf{Objective Statement}\\
        Precise characterization of the capability being specified
  \item \textbf{Foundational References}\\
        Authoritative literature providing theoretical grounding
  \item \textbf{Theoretical Framework}\\
        Mathematical formalization and semantic definitions
  \item \textbf{Design Rationale}\\
        Engineering decisions with explicit justification
  \item \textbf{Formal Specification}\\
        F* mechanization with verified properties where applicable
  \item \textbf{Integration Interfaces}\\
        Dependencies and contracts with adjacent components
\end{enumerate}
\end{sectiontemplatebox}

\section{Notational Conventions}

\begin{notation}
Throughout this document:
\begin{itemize}
  \item Mathematical notation follows standard PL theory conventions
  \item F* code blocks contain mechanically verifiable specifications
  \item \textbf{[Author\#\#]} citations reference entries in Appendix A
  \item Complexity bounds use standard asymptotic notation ($\BigO$, $\Omega$, $\Theta$)
  \item $\sqsubseteq$ denotes lattice ordering; $\sqcup$ and $\sqcap$ denote join and meet
  \item $\abstr$ and $\concr$ denote abstraction and concretization functions
  \item $\sem{\cdot}$ denotes semantic brackets (denotation)
  \item Effect rows use angle bracket notation: $\rowext{E}{\rho}$
\end{itemize}
\end{notation}
